1
00:00:15,562 --> 00:00:18,141
- All right welcome to lecture nine.

2
00:00:18,141 --> 00:00:22,506
So today we will be talking about CNN Architectures.

3
00:00:22,506 --> 00:00:23,899
And just a few administrative points

4
00:00:23,899 --> 00:00:28,516
before we get started, assignment two is due Thursday.

5
00:00:28,516 --> 00:00:31,782
The mid term will be in class on Tuesday May ninth,

6
00:00:31,782 --> 00:00:35,091
so next week and it will cover material through Tuesday

7
00:00:35,091 --> 00:00:37,665
through this coming Thursday May fourth.

8
00:00:37,665 --> 00:00:40,077
So everything up to recurrent neural networks

9
00:00:40,077 --> 00:00:42,160
are going to be fair game.

10
00:00:42,160 --> 00:00:44,613
The poster session we've decided on a time,

11
00:00:44,613 --> 00:00:47,601
it's going to be Tuesday June sixth

12
00:00:47,601 --> 00:00:48,704
from twelve to three p.m.

13
00:00:48,704 --> 00:00:49,931
So this is the last week of classes.

14
00:00:49,931 --> 00:00:52,431
So we have our our poster session a little bit early

15
00:00:52,431 --> 00:00:54,638
during the last week so that after that,

16
00:00:54,638 --> 00:00:56,775
once you guys get feedback you still have some time

17
00:00:56,775 --> 00:01:00,942
to work for your final report which will be due finals week.

18
00:01:04,136 --> 00:01:06,622
Okay, so just a quick review of last time.

19
00:01:06,622 --> 00:01:08,091
Last time we talked about different kinds of

20
00:01:08,091 --> 00:01:10,134
deep learning frameworks.

21
00:01:10,134 --> 00:01:12,667
We talked about you know PyTorch, TensorFlow,

22
00:01:12,667 --> 00:01:13,500
Caffe2

23
00:01:15,324 --> 00:01:16,917
and we saw that using these kinds of frameworks

24
00:01:16,917 --> 00:01:19,572
we were able to easily build big computational graphs,

25
00:01:19,572 --> 00:01:23,410
for example very large neural networks and comm nets,

26
00:01:23,410 --> 00:01:25,649
and be able to really easily compute gradients

27
00:01:25,649 --> 00:01:26,594
in these graphs.

28
00:01:26,594 --> 00:01:29,458
So to compute all of the gradients for all the intermediate

29
00:01:29,458 --> 00:01:33,225
variables weights inputs and use that to train our models

30
00:01:33,225 --> 00:01:36,475
and to run all this efficiently on GPUs

31
00:01:38,468 --> 00:01:39,904
And we saw that for a lot of these frameworks

32
00:01:39,904 --> 00:01:43,338
the way this works is by working with these modularized

33
00:01:43,338 --> 00:01:45,788
layers that you guys have been working writing with,

34
00:01:45,788 --> 00:01:48,281
in your home works as well where we have a forward pass,

35
00:01:48,281 --> 00:01:50,738
we have a backward pass,

36
00:01:50,738 --> 00:01:54,382
and then in our final model architecture,

37
00:01:54,382 --> 00:01:56,427
all we need to do then is to just define

38
00:01:56,427 --> 00:01:59,214
all of these sequence of layers together.

39
00:01:59,214 --> 00:02:02,080
So using that we're able to very easily be able to

40
00:02:02,080 --> 00:02:05,747
build up very complex network architectures.

41
00:02:07,436 --> 00:02:09,971
So today we're going to talk about some specific kinds

42
00:02:09,971 --> 00:02:13,282
of CNN Architectures that are used today in cutting edge

43
00:02:13,282 --> 00:02:15,330
applications and research.

44
00:02:15,330 --> 00:02:17,867
And so we'll go into depth in some of the most commonly

45
00:02:17,867 --> 00:02:20,441
used architectures for these that are winners

46
00:02:20,441 --> 00:02:22,935
of ImageNet classification benchmarks.

47
00:02:22,935 --> 00:02:26,407
So in chronological order AlexNet, VGG net,

48
00:02:26,407 --> 00:02:28,895
GoogLeNet, and ResNet.

49
00:02:28,895 --> 00:02:31,616
And so these will go into a lot of depth.

50
00:02:31,616 --> 00:02:33,451
And then I'll also after that, briefly go through

51
00:02:33,451 --> 00:02:36,598
some other architectures that are not

52
00:02:36,598 --> 00:02:40,148
as prominently used these days, but are interesting

53
00:02:40,148 --> 00:02:41,998
either from a historical perspective,

54
00:02:41,998 --> 00:02:44,581
or as recent areas of research.

55
00:02:47,632 --> 00:02:49,272
Okay, so just a quick review.

56
00:02:49,272 --> 00:02:51,649
We talked a long time ago about LeNet,

57
00:02:51,649 --> 00:02:54,202
which was one of the first instantiations of a comNet

58
00:02:54,202 --> 00:02:56,413
that was successfully used in practice.

59
00:02:56,413 --> 00:03:00,094
And so this was the comNet that took an input image,

60
00:03:00,094 --> 00:03:03,646
used com filters five by five filters

61
00:03:03,646 --> 00:03:06,588
applied at stride one and had a couple of conv layers,

62
00:03:06,588 --> 00:03:09,247
a few pooling layers and then some fully connected layers

63
00:03:09,247 --> 00:03:10,145
at the end.

64
00:03:10,145 --> 00:03:13,380
And this fairly simple comNet was very successfully applied

65
00:03:13,380 --> 00:03:15,130
to digit recognition.

66
00:03:17,840 --> 00:03:21,521
So AlexNet from 2012 which you guys have also heard

67
00:03:21,521 --> 00:03:23,685
already before in previous classes,

68
00:03:23,685 --> 00:03:28,219
was the first large scale convolutional neural network

69
00:03:28,219 --> 00:03:31,989
that was able to do well on the ImageNet classification

70
00:03:31,989 --> 00:03:36,889
task so in 2012 AlexNet was entered in the competition,

71
00:03:36,889 --> 00:03:39,380
and was able to outperform all previous non deep

72
00:03:39,380 --> 00:03:41,421
learning based models by a significant margin,

73
00:03:41,421 --> 00:03:44,974
and so this was the comNet that started the spree

74
00:03:44,974 --> 00:03:48,822
of comNet research and usage afterwards.

75
00:03:48,822 --> 00:03:52,051
And so the basic comNet AlexNet architecture

76
00:03:52,051 --> 00:03:54,820
is a conv layer followed by pooling layer,

77
00:03:54,820 --> 00:03:57,237
normalization, com pool norm,

78
00:03:59,231 --> 00:04:01,816
and then a few more conv layers, a pooling layer,

79
00:04:01,816 --> 00:04:04,232
and then several fully connected layers afterwards.

80
00:04:04,232 --> 00:04:06,937
So this actually looks very similar to the LeNet network

81
00:04:06,937 --> 00:04:08,285
that we just saw.

82
00:04:08,285 --> 00:04:10,576
There's just more layers in total.

83
00:04:10,576 --> 00:04:12,421
There is five of these conv layers,

84
00:04:12,421 --> 00:04:15,504
and two fully connected layers before

85
00:04:16,397 --> 00:04:19,258
the final fully connected layer going to the output

86
00:04:19,258 --> 00:04:20,091
classes.

87
00:04:22,699 --> 00:04:24,904
So let's first get a sense of the sizes involved

88
00:04:24,904 --> 00:04:26,740
in the AlexNet.

89
00:04:26,740 --> 00:04:28,417
So if we look at the input to the AlexNet

90
00:04:28,417 --> 00:04:30,465
this was trained on ImageNet, with inputs

91
00:04:30,465 --> 00:04:33,938
at a size 227 by 227 by 3 images.

92
00:04:33,938 --> 00:04:37,825
And if we look at this first layer which is a conv layer

93
00:04:37,825 --> 00:04:41,266
for the AlexNet, it's 11 by 11 filters,

94
00:04:41,266 --> 00:04:44,003
96 of these applied at stride 4.

95
00:04:44,003 --> 00:04:45,966
So let's just think about this for a moment.

96
00:04:45,966 --> 00:04:50,133
What's the output volume size of this first layer?

97
00:04:52,598 --> 00:04:54,181
And there's a hint.

98
00:04:58,579 --> 00:05:01,204
So remember we have our input size,

99
00:05:01,204 --> 00:05:04,351
we have our convolutional filters, ray.

100
00:05:04,351 --> 00:05:07,006
And we have this formula, which is the hint over here

101
00:05:07,006 --> 00:05:09,871
that gives you the size of the output dimensions

102
00:05:09,871 --> 00:05:12,251
after applying com right?

103
00:05:12,251 --> 00:05:15,569
So remember it was the full image, minus the filter size,

104
00:05:15,569 --> 00:05:18,442
divided by the stride, plus one.

105
00:05:18,442 --> 00:05:21,875
So given that that's written up here for you 55,

106
00:05:21,875 --> 00:05:25,274
does anyone have a guess at what's the final output size

107
00:05:25,274 --> 00:05:27,729
after this conv layer?

108
00:05:27,729 --> 00:05:30,633
[student speaks off mic]

109
00:05:30,633 --> 00:05:32,757
- So I had 55 by 55 by 96, yep.

110
00:05:32,757 --> 00:05:33,776
That's correct.

111
00:05:33,776 --> 00:05:35,898
Right so our spatial dimensions at the output

112
00:05:35,898 --> 00:05:38,923
are going to be 55 in each dimension and then we have

113
00:05:38,923 --> 00:05:41,745
96 total filters so the depth after our conv layer

114
00:05:41,745 --> 00:05:43,245
is going to be 96.

115
00:05:44,114 --> 00:05:46,201
So that's the output volume.

116
00:05:46,201 --> 00:05:50,296
And what's the total number of parameters in this layer?

117
00:05:50,296 --> 00:05:53,629
So remember we have 96 11 by 11 filters.

118
00:05:55,661 --> 00:05:58,563
[student speaks off mic]

119
00:05:58,563 --> 00:06:01,563
- [Lecturer] 96 by 11 by 11, almost.

120
00:06:02,755 --> 00:06:05,082
So yes, so I had another by three,

121
00:06:05,082 --> 00:06:06,107
yes that's correct.

122
00:06:06,107 --> 00:06:08,961
So each of the filters is going to

123
00:06:08,961 --> 00:06:12,279
see through a local region of 11 by 11 by three,

124
00:06:12,279 --> 00:06:14,442
right because the input depth was three.

125
00:06:14,442 --> 00:06:18,843
And so, that's each filter size, times we have 96

126
00:06:18,843 --> 00:06:19,793
of these total.

127
00:06:19,793 --> 00:06:23,960
And so there's 35K parameters in this first layer.

128
00:06:26,828 --> 00:06:28,589
Okay, so now if we look at the second layer

129
00:06:28,589 --> 00:06:31,043
this is a pooling layer right and in this case

130
00:06:31,043 --> 00:06:34,814
we have three three by three filters applied at stride two.

131
00:06:34,814 --> 00:06:38,981
So what's the output volume of this layer after pooling?

132
00:06:41,511 --> 00:06:45,678
And again we have a hint, very similar to the last question.

133
00:06:52,061 --> 00:06:53,811
Okay, 27 by 27 by 96.

134
00:06:55,494 --> 00:06:57,077
Yes that's correct.

135
00:06:58,526 --> 00:07:00,409
Right so the pooling layer is basically going to use

136
00:07:00,409 --> 00:07:02,338
this formula that we had here.

137
00:07:02,338 --> 00:07:05,894
Again because these are pooling applied at a stride of two

138
00:07:05,894 --> 00:07:08,506
so we're going to use the same formula to determine

139
00:07:08,506 --> 00:07:11,739
the spatial dimensions and so the spatial dimensions

140
00:07:11,739 --> 00:07:15,871
are going to be 27 by 27, and pooling preserves

141
00:07:15,871 --> 00:07:17,465
the depth.

142
00:07:17,465 --> 00:07:20,117
So we had 96 as depth as input,

143
00:07:20,117 --> 00:07:21,504
and it's still going to be 96 depth

144
00:07:21,504 --> 00:07:22,337
at output.

145
00:07:23,635 --> 00:07:25,104
And next question.

146
00:07:25,104 --> 00:07:28,937
What's the number of parameters in this layer?

147
00:07:32,256 --> 00:07:33,811
I hear some muttering.

148
00:07:33,811 --> 00:07:35,164
[student answers off mic]

149
00:07:35,164 --> 00:07:35,997
- Nothing.

150
00:07:36,882 --> 00:07:37,715
Okay.

151
00:07:37,715 --> 00:07:39,528
Yes, so pooling layer has no parameters, so,

152
00:07:39,528 --> 00:07:41,611
kind of a trick question.

153
00:07:43,549 --> 00:07:46,082
Okay, so we can basically, yes, question?

154
00:07:46,082 --> 00:07:48,002
[student speaks off mic]

155
00:07:48,002 --> 00:07:51,961
- The question is, why are there no parameters in the

156
00:07:51,961 --> 00:07:52,990
pooling layer?

157
00:07:52,990 --> 00:07:54,380
The parameters are the weights right,

158
00:07:54,380 --> 00:07:55,361
that we're trying to learn.

159
00:07:55,361 --> 00:07:57,321
And so convolutional layers have weights that we learn

160
00:07:57,321 --> 00:07:59,702
but pooling all we do is have a rule,

161
00:07:59,702 --> 00:08:01,415
we look at the pooling region,

162
00:08:01,415 --> 00:08:03,046
and we take the max.

163
00:08:03,046 --> 00:08:06,520
So there's no parameters that are learned.

164
00:08:06,520 --> 00:08:08,939
So we can keep on doing this and you can just repeat

165
00:08:08,939 --> 00:08:11,817
the process and it's kind of a good exercise to go through

166
00:08:11,817 --> 00:08:13,810
this and figure out the sizes, the parameters,

167
00:08:13,810 --> 00:08:15,060
at every layer.

168
00:08:17,283 --> 00:08:18,589
And so if you do this all the way,

169
00:08:18,589 --> 00:08:22,517
you can look at this is the final architecture

170
00:08:22,517 --> 00:08:23,498
that you can work with.

171
00:08:23,498 --> 00:08:25,989
There's 11 by 11 filters at the beginning,

172
00:08:25,989 --> 00:08:28,319
then five by five and some three by three filters.

173
00:08:28,319 --> 00:08:32,730
And so these are generally pretty familiar looking sizes

174
00:08:32,730 --> 00:08:34,827
that you've seen before and then at the end

175
00:08:34,827 --> 00:08:36,997
we have a couple of fully connected layers

176
00:08:36,997 --> 00:08:39,933
of size 4096 and finally the last layer,

177
00:08:39,933 --> 00:08:42,350
is FC8 going to the soft max,

178
00:08:43,499 --> 00:08:47,166
which is going to the 1000 ImageNet classes.

179
00:08:48,849 --> 00:08:50,612
And just a couple of details about this,

180
00:08:50,612 --> 00:08:53,515
it was the first use of the ReLu non-linearity

181
00:08:53,515 --> 00:08:55,484
that we've talked about that's the most commonly used

182
00:08:55,484 --> 00:08:57,162
non-linearity.

183
00:08:57,162 --> 00:09:00,602
They used local response normalization layers

184
00:09:00,602 --> 00:09:03,302
basically trying to normalize the response across

185
00:09:03,302 --> 00:09:06,772
neighboring channels but this is something that's not really

186
00:09:06,772 --> 00:09:08,201
used anymore.

187
00:09:08,201 --> 00:09:10,165
It turned out not to, other people showed

188
00:09:10,165 --> 00:09:12,747
that it didn't have so much of an effect.

189
00:09:12,747 --> 00:09:14,709
There's a lot of heavy data augmentation,

190
00:09:14,709 --> 00:09:17,054
and so you can look in the paper for more details,

191
00:09:17,054 --> 00:09:19,673
but things like flipping, jittering, cropping,

192
00:09:19,673 --> 00:09:22,579
color normalization all of these things

193
00:09:22,579 --> 00:09:25,191
which you'll probably find useful for you when

194
00:09:25,191 --> 00:09:27,650
you're working on your projects for example,

195
00:09:27,650 --> 00:09:29,537
so a lot of data augmentation here.

196
00:09:29,537 --> 00:09:33,229
They also use dropout batch size of 128,

197
00:09:33,229 --> 00:09:37,993
and learned with SGD with momentum which we talked about

198
00:09:37,993 --> 00:09:40,608
in an earlier lecture, and basically just started

199
00:09:40,608 --> 00:09:43,105
with a base learning rate of 1e negative 2.

200
00:09:43,105 --> 00:09:46,212
Every time it plateaus, reduce by a factor of 10

201
00:09:46,212 --> 00:09:48,216
and then just keep going.

202
00:09:48,216 --> 00:09:50,955
Until they finish training

203
00:09:50,955 --> 00:09:53,571
and a little bit of weight decay and in the end,

204
00:09:53,571 --> 00:09:57,089
in order to get the best numbers they also did

205
00:09:57,089 --> 00:09:59,822
an ensembling of models and so training multiple of these,

206
00:09:59,822 --> 00:10:02,722
averaging them together and this also gives an improvement

207
00:10:02,722 --> 00:10:03,972
in performance.

208
00:10:05,215 --> 00:10:07,138
And so one other thing I want to point out

209
00:10:07,138 --> 00:10:09,591
is that if you look at this AlexNet diagram up here,

210
00:10:09,591 --> 00:10:13,390
it looks kind of like the normal comNet diagrams

211
00:10:13,390 --> 00:10:16,045
that we've been seeing, except for one difference,

212
00:10:16,045 --> 00:10:18,580
which is that it's, you can see it's kind of split

213
00:10:18,580 --> 00:10:22,747
in these two different rows or columns going across.

214
00:10:23,987 --> 00:10:27,431
And so the reason for this is mostly historical note,

215
00:10:27,431 --> 00:10:31,431
so AlexNet was trained on GTX580 GPUs older GPUs

216
00:10:32,632 --> 00:10:34,916
that only had three gigs of memory.

217
00:10:34,916 --> 00:10:38,065
So it couldn't actually fit this entire network on here,

218
00:10:38,065 --> 00:10:39,757
and so what they ended up doing,

219
00:10:39,757 --> 00:10:42,583
was they spread the network across two GPUs.

220
00:10:42,583 --> 00:10:45,091
So on each GPU you would have half of the neurons,

221
00:10:45,091 --> 00:10:47,265
or half of the feature maps.

222
00:10:47,265 --> 00:10:50,040
And so for example if you look at this first conv layer,

223
00:10:50,040 --> 00:10:52,540
we have 55 by 55 by 96 output,

224
00:10:55,199 --> 00:10:57,568
but if you look at this diagram carefully,

225
00:10:57,568 --> 00:11:00,144
you can zoom in later in the actual paper,

226
00:11:00,144 --> 00:11:02,965
you can see that, it's actually only 48

227
00:11:02,965 --> 00:11:04,965
depth-wise, on each GPU,

228
00:11:05,859 --> 00:11:07,986
and so they just spread it, the feature maps,

229
00:11:07,986 --> 00:11:09,403
directly in half.

230
00:11:11,098 --> 00:11:13,269
And so what happens is that for most of these layers,

231
00:11:13,269 --> 00:11:16,580
for example com one, two, four and five,

232
00:11:16,580 --> 00:11:18,177
the connections are only with feature maps

233
00:11:18,177 --> 00:11:21,816
on the same GPU, so you would take as input,

234
00:11:21,816 --> 00:11:26,072
half of the feature maps that were on the the same GPU

235
00:11:26,072 --> 00:11:29,304
as before and you don't look at the full 96

236
00:11:29,304 --> 00:11:30,493
feature maps for example.

237
00:11:30,493 --> 00:11:34,660
You just take as input the 48 in that first layer.

238
00:11:35,577 --> 00:11:38,527
And then there's a few layers so com three,

239
00:11:38,527 --> 00:11:42,006
as well as FC six, seven and eight,

240
00:11:42,006 --> 00:11:45,195
where here are the GPUs do talk to each other

241
00:11:45,195 --> 00:11:47,673
and so there's connections with all feature maps

242
00:11:47,673 --> 00:11:48,506
in the preceding layer.

243
00:11:48,506 --> 00:11:50,125
so there's communication across the GPUs,

244
00:11:50,125 --> 00:11:52,015
and each of these neurons are then connected

245
00:11:52,015 --> 00:11:55,001
to the full depth of the previous input layer.

246
00:11:55,001 --> 00:11:56,437
Question.

247
00:11:56,437 --> 00:11:59,141
- [Student] It says the full simplified AlexNetwork

248
00:11:59,141 --> 00:12:00,085
architecture.

249
00:12:00,085 --> 00:12:02,252
[mumbles]

250
00:12:06,393 --> 00:12:07,781
- Oh okay, so the question is why does it say

251
00:12:07,781 --> 00:12:10,843
full simplified AlexNet architecture here?

252
00:12:10,843 --> 00:12:13,139
It just says that because I didn't put all the details

253
00:12:13,139 --> 00:12:17,192
on here, so for example this is the full set of layers

254
00:12:17,192 --> 00:12:19,846
in the architecture, and the strides and so on,

255
00:12:19,846 --> 00:12:22,911
but for example the normalization layer, there's other,

256
00:12:22,911 --> 00:12:26,078
these details are not written on here.

257
00:12:31,447 --> 00:12:33,244
And then just one little note,

258
00:12:33,244 --> 00:12:35,576
if you look at the paper and try and write out

259
00:12:35,576 --> 00:12:38,659
the math and architectures and so on,

260
00:12:39,668 --> 00:12:43,178
there's a little bit of an issue on the very first

261
00:12:43,178 --> 00:12:45,474
layer they'll say if you'll look in the figure

262
00:12:45,474 --> 00:12:47,276
they'll say 224 by 224 ,

263
00:12:47,276 --> 00:12:49,439
but there's actually some kind of funny pattern

264
00:12:49,439 --> 00:12:51,364
going on and so the numbers actually work out

265
00:12:51,364 --> 00:12:53,531
if you look at it as 227.

266
00:12:55,792 --> 00:12:59,959
AlexNet was the winner of the ImageNet classification

267
00:13:00,944 --> 00:13:03,396
benchmark in 2012, you can see that

268
00:13:03,396 --> 00:13:06,056
it cut the error rate by quite a large margin.

269
00:13:06,056 --> 00:13:10,223
It was the first CNN base winner, and it was widely used

270
00:13:11,750 --> 00:13:13,430
as a base to our architecture

271
00:13:13,430 --> 00:13:16,530
almost ubiquitously from then until a couple years ago.

272
00:13:16,530 --> 00:13:18,790
It's still used quite a bit.

273
00:13:18,790 --> 00:13:22,066
It's used in transfer learning for lots of different tasks

274
00:13:22,066 --> 00:13:24,881
and so it was used for basically a long time,

275
00:13:24,881 --> 00:13:28,487
and it was very famous and now though there's been

276
00:13:28,487 --> 00:13:31,677
some more recent architectures that have generally

277
00:13:31,677 --> 00:13:34,012
just had better performance and so we'll talk about these

278
00:13:34,012 --> 00:13:36,592
next and these are going to be the more common architectures

279
00:13:36,592 --> 00:13:40,092
that you'll be wanting to use in practice.

280
00:13:41,663 --> 00:13:45,838
So just quickly first in 2013 the ImageNet challenge

281
00:13:45,838 --> 00:13:48,623
was won by something called a ZFNet.

282
00:13:48,623 --> 00:13:49,528
Yes, question.

283
00:13:49,528 --> 00:13:53,539
[student speaks off mic]

284
00:13:53,539 --> 00:13:55,298
- So the question is intuition why AlexNet

285
00:13:55,298 --> 00:13:57,422
was so much better than the ones that came before,

286
00:13:57,422 --> 00:14:01,600
DefLearning comNets [mumbles] this is just

287
00:14:01,600 --> 00:14:05,596
a very different kind of approach in architecture.

288
00:14:05,596 --> 00:14:07,564
So this was the first deep learning based approach

289
00:14:07,564 --> 00:14:09,814
first comNet that was used.

290
00:14:13,255 --> 00:14:16,126
So in 2013 the challenge was won by something called

291
00:14:16,126 --> 00:14:19,108
a ZFNet [Zeller Fergus Net] named after the creators.

292
00:14:19,108 --> 00:14:23,537
And so this mostly was improving hyper parameters

293
00:14:23,537 --> 00:14:24,559
over the AlexNet.

294
00:14:24,559 --> 00:14:26,194
It had the same number of layers,

295
00:14:26,194 --> 00:14:28,566
the same general structure and they made a few

296
00:14:28,566 --> 00:14:31,518
changes things like changing the stride size,

297
00:14:31,518 --> 00:14:34,909
different numbers of filters and after playing around

298
00:14:34,909 --> 00:14:36,545
with these hyper parameters more,

299
00:14:36,545 --> 00:14:40,751
they were able to improve the error rate.

300
00:14:40,751 --> 00:14:42,179
But it's still basically the same idea.

301
00:14:42,179 --> 00:14:44,720
So in 2014 there are a couple of architectures

302
00:14:44,720 --> 00:14:47,291
that were now more significantly different

303
00:14:47,291 --> 00:14:50,653
and made another jump in performance,

304
00:14:50,653 --> 00:14:54,820
and the main difference with these networks first of all

305
00:14:56,699 --> 00:14:58,988
was much deeper networks.

306
00:14:58,988 --> 00:15:01,926
So from the eight layer network that was in 2012

307
00:15:01,926 --> 00:15:06,093
and 2013, now in 2014 we had two very close winners

308
00:15:07,606 --> 00:15:10,999
that were around 19 layers and 22 layers.

309
00:15:10,999 --> 00:15:13,131
So significantly deeper.

310
00:15:13,131 --> 00:15:17,312
And the winner of this was GoogleNet, from Google

311
00:15:17,312 --> 00:15:20,986
but very close behind was something called VGGNet

312
00:15:20,986 --> 00:15:24,750
from Oxford, and on actually the localization challenge

313
00:15:24,750 --> 00:15:28,231
VGG got first place in some of the other tracks.

314
00:15:28,231 --> 00:15:32,768
So these were both very, very strong networks.

315
00:15:32,768 --> 00:15:35,473
So let's first look at VGG in a little bit more detail.

316
00:15:35,473 --> 00:15:39,458
And so the VGG network is the idea of much deeper networks

317
00:15:39,458 --> 00:15:41,628
and with much smaller filters.

318
00:15:41,628 --> 00:15:44,795
So they increased the number of layers

319
00:15:45,632 --> 00:15:48,854
from eight layers in AlexNet right to now they had

320
00:15:48,854 --> 00:15:52,021
models with 16 to 19 layers in VGGNet.

321
00:15:53,100 --> 00:15:55,196
And one key thing that they did was they kept very small

322
00:15:55,196 --> 00:15:58,307
filter so only three by three conv all the way,

323
00:15:58,307 --> 00:16:01,459
which is basically the smallest com filter size

324
00:16:01,459 --> 00:16:04,726
that is looking at a little bit of the neighboring pixels.

325
00:16:04,726 --> 00:16:07,100
And they just kept this very simple structure

326
00:16:07,100 --> 00:16:09,553
of three by three convs with the periodic pooling

327
00:16:09,553 --> 00:16:12,295
all the way through the network.

328
00:16:12,295 --> 00:16:15,486
And it's very simple elegant network architecture,

329
00:16:15,486 --> 00:16:18,591
was able to get 7.3% top five error

330
00:16:18,591 --> 00:16:20,758
on the ImageNet challenge.

331
00:16:23,461 --> 00:16:28,252
So first the question of why use smaller filters.

332
00:16:28,252 --> 00:16:31,931
So when we take these small filters now we have

333
00:16:31,931 --> 00:16:34,181
fewer parameters and we try and stack more of them

334
00:16:34,181 --> 00:16:35,659
instead of having larger filters,

335
00:16:35,659 --> 00:16:37,869
have smaller filters with more depth instead,

336
00:16:37,869 --> 00:16:40,154
have more of these filters instead,

337
00:16:40,154 --> 00:16:43,384
what happens is that you end up having the same effective

338
00:16:43,384 --> 00:16:46,414
receptive field as if you only have one seven by seven

339
00:16:46,414 --> 00:16:48,012
convolutional layer.

340
00:16:48,012 --> 00:16:52,099
So here's a question, what is the effective receptive field

341
00:16:52,099 --> 00:16:55,416
of three of these three by three conv layers

342
00:16:55,416 --> 00:16:56,276
with stride one?

343
00:16:56,276 --> 00:16:58,854
So if you were to stack three three by three conv layers

344
00:16:58,854 --> 00:17:01,999
with Stride one what's the effective receptive field,

345
00:17:01,999 --> 00:17:06,166
the total area of the input, spatial area of the input

346
00:17:07,232 --> 00:17:10,463
that enure at the top layer of the three layers

347
00:17:10,463 --> 00:17:11,630
is looking at.

348
00:17:13,123 --> 00:17:16,797
So I heard fifteen pixels, why fifteen pixels?

349
00:17:16,797 --> 00:17:21,419
- [Student] Okay, so the reason given was because

350
00:17:21,419 --> 00:17:22,276
they overlap--

351
00:17:22,276 --> 00:17:26,662
- Okay, so the reason given was because they overlap.

352
00:17:26,662 --> 00:17:28,179
So it's on the right track.

353
00:17:28,179 --> 00:17:32,350
What actually is happening though is you have to see,

354
00:17:32,350 --> 00:17:34,803
at the first layer, the receptive field is going to be

355
00:17:34,803 --> 00:17:36,478
three by three right?

356
00:17:36,478 --> 00:17:38,558
And then at the second layer,

357
00:17:38,558 --> 00:17:41,379
each of these neurons in the second layer

358
00:17:41,379 --> 00:17:44,003
is going to look at three by three other first layer

359
00:17:44,003 --> 00:17:47,278
filters, but the corners of these three by three

360
00:17:47,278 --> 00:17:49,690
have an additional pixel on each side,

361
00:17:49,690 --> 00:17:52,486
that is looking at in the original input layer.

362
00:17:52,486 --> 00:17:55,476
So the second layer is actually looking at five by five

363
00:17:55,476 --> 00:17:57,233
receptive field and then if you do this again,

364
00:17:57,233 --> 00:18:01,119
the third layer is looking at three by three

365
00:18:01,119 --> 00:18:04,850
in the second layer but this is going to,

366
00:18:04,850 --> 00:18:06,245
if you just draw out this pyramid is looking at

367
00:18:06,245 --> 00:18:07,717
seven by seven in the input layer.

368
00:18:07,717 --> 00:18:09,565
So the effective receptive field here

369
00:18:09,565 --> 00:18:12,470
is going to be seven by seven.

370
00:18:12,470 --> 00:18:16,836
Which is the same as one seven by seven conv layer.

371
00:18:16,836 --> 00:18:18,676
So what happens is that this has the same effective

372
00:18:18,676 --> 00:18:21,128
receptive field as a seven by seven conv layer

373
00:18:21,128 --> 00:18:22,356
but it's deeper.

374
00:18:22,356 --> 00:18:24,801
It's able to have more non-linearities in there,

375
00:18:24,801 --> 00:18:27,011
and it's also fewer parameters.

376
00:18:27,011 --> 00:18:28,599
So if you look at the total number of parameters,

377
00:18:28,599 --> 00:18:33,179
each of these conv filters for the three by threes

378
00:18:33,179 --> 00:18:37,346
is going to have nine parameters in each conv [mumbles]

379
00:18:38,975 --> 00:18:42,338
three times three, and then times the input depth,

380
00:18:42,338 --> 00:18:45,458
so three times three times C, times this total number

381
00:18:45,458 --> 00:18:48,859
of output feature maps, which is again C

382
00:18:48,859 --> 00:18:50,491
is we're going to preserve the total

383
00:18:50,491 --> 00:18:51,844
number of channels.

384
00:18:51,844 --> 00:18:54,300
So you get three times three,

385
00:18:54,300 --> 00:18:57,128
times C times C for each of these layers,

386
00:18:57,128 --> 00:18:59,089
and we have three layers so it's going to be

387
00:18:59,089 --> 00:19:00,975
three times this number,

388
00:19:00,975 --> 00:19:04,167
compared to if you had a single seven by seven layer

389
00:19:04,167 --> 00:19:06,090
then you get, by the same reasoning,

390
00:19:06,090 --> 00:19:08,219
seven squared times C squared.

391
00:19:08,219 --> 00:19:10,675
So you're going to have fewer parameters total,

392
00:19:10,675 --> 00:19:11,842
which is nice.

393
00:19:16,380 --> 00:19:20,511
So now if we look at this full network here

394
00:19:20,511 --> 00:19:22,721
there's a lot of numbers up here that you can go back

395
00:19:22,721 --> 00:19:24,971
and look at more carefully but if we look at all

396
00:19:24,971 --> 00:19:28,572
of the sizes and number of parameters the same way

397
00:19:28,572 --> 00:19:31,526
that we calculated the example for AlexNet,

398
00:19:31,526 --> 00:19:33,327
this is a good exercise to go through,

399
00:19:33,327 --> 00:19:36,563
we can see that you know going the same way

400
00:19:36,563 --> 00:19:40,656
we have a couple of these conv layers and a pooling layer

401
00:19:40,656 --> 00:19:43,973
a couple more conv layers, pooling layer, several more

402
00:19:43,973 --> 00:19:45,120
conv layers and so on.

403
00:19:45,120 --> 00:19:46,644
And so this just keeps going up.

404
00:19:46,644 --> 00:19:49,068
And if you counted the total number of convolutional

405
00:19:49,068 --> 00:19:51,680
and fully connected layers, we're going to have 16

406
00:19:51,680 --> 00:19:53,241
in this case for VGG 16,

407
00:19:53,241 --> 00:19:57,121
and then VGG 19, it's just a very similar

408
00:19:57,121 --> 00:20:01,288
architecture, but with a few more conv layers in there.

409
00:20:03,831 --> 00:20:06,415
And so the total memory usage of this network,

410
00:20:06,415 --> 00:20:10,467
so just making a forward pass through counting up

411
00:20:10,467 --> 00:20:13,538
all of these numbers so in the memory numbers here

412
00:20:13,538 --> 00:20:16,414
written in terms of the total numbers,

413
00:20:16,414 --> 00:20:18,006
like we calculated earlier,

414
00:20:18,006 --> 00:20:20,170
and if you look at four bytes per number,

415
00:20:20,170 --> 00:20:23,935
this is going to be about 100 megs per image,

416
00:20:23,935 --> 00:20:27,657
and so this is the scale of the memory usage

417
00:20:27,657 --> 00:20:29,537
that's happening and this is only for a forward pass right,

418
00:20:29,537 --> 00:20:31,707
when you do a backward pass you're going to have to store

419
00:20:31,707 --> 00:20:36,280
more and so this is pretty heavy memory wise.

420
00:20:36,280 --> 00:20:39,947
100 megs per image, if you have on five gigs

421
00:20:41,200 --> 00:20:44,223
of total memory, then you're only going to be able

422
00:20:44,223 --> 00:20:46,473
to store about 50 of these.

423
00:20:48,110 --> 00:20:50,766
And so also the total number of parameters here we have

424
00:20:50,766 --> 00:20:54,324
is 138 million parameters in this network,

425
00:20:54,324 --> 00:20:56,941
and this compares with 60 million for AlexNet.

426
00:20:56,941 --> 00:20:58,291
Question?

427
00:20:58,291 --> 00:21:01,708
[student speaks off mic]

428
00:21:07,014 --> 00:21:09,097
- So the question is what do we mean by deeper,

429
00:21:09,097 --> 00:21:10,730
is it the number of filters, number of layers?

430
00:21:10,730 --> 00:21:14,897
So deeper in this case is always referring to layers.

431
00:21:16,415 --> 00:21:19,082
So there are two usages of the word depth

432
00:21:19,082 --> 00:21:23,249
which is confusing one is the depth rate per channel,

433
00:21:24,975 --> 00:21:26,892
width by height by depth, you can use

434
00:21:26,892 --> 00:21:27,752
the word depth here,

435
00:21:27,752 --> 00:21:30,078
but in general we talk about the depth of a network,

436
00:21:30,078 --> 00:21:33,065
this is going to be the total number of layers

437
00:21:33,065 --> 00:21:35,108
in the network, and usually in particular

438
00:21:35,108 --> 00:21:38,781
we're counting the total number of weight layers.

439
00:21:38,781 --> 00:21:41,195
So the total number of layers with trainable weight,

440
00:21:41,195 --> 00:21:44,178
so convolutional layers and fully connected layers.

441
00:21:44,178 --> 00:21:47,678
[student mumbles off mic]

442
00:22:01,620 --> 00:22:04,118
- Okay, so the question is, within each layer

443
00:22:04,118 --> 00:22:06,984
what do different filters need?

444
00:22:06,984 --> 00:22:11,238
And so we talked about this back in the comNet lecture,

445
00:22:11,238 --> 00:22:13,853
so you can also go back and refer to that,

446
00:22:13,853 --> 00:22:18,356
but each filter is a set of let's say three by three convs,

447
00:22:18,356 --> 00:22:20,900
so each filter is looking at a,

448
00:22:20,900 --> 00:22:24,253
is a set of weight looking at a three by three value input

449
00:22:24,253 --> 00:22:28,426
input depth, and this produces one feature map,

450
00:22:28,426 --> 00:22:30,759
one activation map of all the responses of the

451
00:22:30,759 --> 00:22:32,764
different spatial locations.

452
00:22:32,764 --> 00:22:35,994
And then we have we can have as many filters as we want

453
00:22:35,994 --> 00:22:38,328
right so for example 96 and each of these

454
00:22:38,328 --> 00:22:40,456
is going to produce a feature map.

455
00:22:40,456 --> 00:22:42,748
And so it's just like each filter corresponds

456
00:22:42,748 --> 00:22:45,249
to a different pattern that we're looking for

457
00:22:45,249 --> 00:22:47,620
in the input that we convolve around and we see

458
00:22:47,620 --> 00:22:49,178
the responses everywhere in the input,

459
00:22:49,178 --> 00:22:52,824
we create a map of these and then another filter

460
00:22:52,824 --> 00:22:56,991
will we convolve over the image and create another map.

461
00:22:59,571 --> 00:23:01,036
Question.

462
00:23:01,036 --> 00:23:04,453
[student speaks off mic]

463
00:23:08,275 --> 00:23:10,808
- So question is, is there intuition behind,

464
00:23:10,808 --> 00:23:13,376
as you go deeper into the network we have more channel depth

465
00:23:13,376 --> 00:23:17,543
so more number of filters right and so you can have

466
00:23:18,486 --> 00:23:22,576
any design that you want so you don't have to do this.

467
00:23:22,576 --> 00:23:25,151
In practice you will see this happen a lot of the times

468
00:23:25,151 --> 00:23:28,547
and one of the reasons is people try and maintain

469
00:23:28,547 --> 00:23:31,408
kind of a relatively constant level of compute,

470
00:23:31,408 --> 00:23:34,718
so as you go higher up or deeper into your network,

471
00:23:34,718 --> 00:23:38,801
you're usually also using basically down sampling

472
00:23:40,416 --> 00:23:43,869
and having smaller total spatial area and then so then

473
00:23:43,869 --> 00:23:46,569
they also increase now you increase by depth a little bit,

474
00:23:46,569 --> 00:23:48,525
it's not as expensive now to increase by depth

475
00:23:48,525 --> 00:23:51,321
because it's spatially smaller and so,

476
00:23:51,321 --> 00:23:54,177
yeah that's just a reason.

477
00:23:54,177 --> 00:23:55,526
Question.

478
00:23:55,526 --> 00:23:58,943
[student speaks off mic]

479
00:24:00,682 --> 00:24:02,478
- So performance-wise is there any reason to use

480
00:24:02,478 --> 00:24:05,463
SBN [mumbles] instead of SouthMax [mumbles],

481
00:24:05,463 --> 00:24:07,671
so no, for a classifier you can use either one,

482
00:24:07,671 --> 00:24:10,571
and you did that earlier in the class as well,

483
00:24:10,571 --> 00:24:13,191
but in general SouthMax losses,

484
00:24:13,191 --> 00:24:16,052
have generally worked well and been standard use

485
00:24:16,052 --> 00:24:18,052
for classification here.

486
00:24:19,319 --> 00:24:20,833
Okay yeah one more question.

487
00:24:20,833 --> 00:24:24,333
[student mumbles off mic]

488
00:24:38,712 --> 00:24:41,091
- Yes, so the question is, we don't have to store

489
00:24:41,091 --> 00:24:43,714
all of the memory like we can throw away the parts

490
00:24:43,714 --> 00:24:46,208
that we don't need and so on?

491
00:24:46,208 --> 00:24:48,230
And yes this is true.

492
00:24:48,230 --> 00:24:50,031
Some of this you don't need to keep,

493
00:24:50,031 --> 00:24:53,095
but you're also going to be doing a backwards pass

494
00:24:53,095 --> 00:24:55,229
through ware for the most part,

495
00:24:55,229 --> 00:24:56,586
when you were doing the chain

496
00:24:56,586 --> 00:24:58,141
rule and so on you needed a lot of these activations

497
00:24:58,141 --> 00:25:01,631
as part of it and so in large part a lot of this

498
00:25:01,631 --> 00:25:03,381
does need to be kept.

499
00:25:04,816 --> 00:25:08,990
So if we look at the distribution of where memory is used

500
00:25:08,990 --> 00:25:11,612
and where parameters are, you can see that a lot

501
00:25:11,612 --> 00:25:15,250
of memories in these early layers right where you still have

502
00:25:15,250 --> 00:25:19,656
spatial dimensions you're going to have more memory usage

503
00:25:19,656 --> 00:25:21,994
and then a lot of the parameters are actually in

504
00:25:21,994 --> 00:25:24,864
the last layers, the fully connected layers

505
00:25:24,864 --> 00:25:26,821
have a huge number of parameters right,

506
00:25:26,821 --> 00:25:29,647
because we have all of these dense connections.

507
00:25:29,647 --> 00:25:33,995
And so that's something just to know and then

508
00:25:33,995 --> 00:25:37,809
keep in mind so later on we'll see some networks actually

509
00:25:37,809 --> 00:25:39,771
get rid of these fully connected layers and be able

510
00:25:39,771 --> 00:25:43,155
to save a lot on the number of parameters.

511
00:25:43,155 --> 00:25:45,029
And then just one last thing to point out,

512
00:25:45,029 --> 00:25:47,483
you'll also see different ways of calling

513
00:25:47,483 --> 00:25:48,869
all of these layers right.

514
00:25:48,869 --> 00:25:51,831
So here I've written out exactly what the layers are.

515
00:25:51,831 --> 00:25:55,151
conv3-64 means three by three convs

516
00:25:55,151 --> 00:25:57,000
with 64 total filters.

517
00:25:57,000 --> 00:26:01,499
But for VGGNet on this diagram on the right here

518
00:26:01,499 --> 00:26:03,868
there's also common ways that people will look

519
00:26:03,868 --> 00:26:06,000
at each group of filters,

520
00:26:06,000 --> 00:26:08,992
so each orange block here, as in conv1

521
00:26:08,992 --> 00:26:11,734
part one, so conv1-1, conv1-2,

522
00:26:11,734 --> 00:26:12,632
and so on.

523
00:26:12,632 --> 00:26:15,465
So just something to keep in mind.

524
00:26:17,404 --> 00:26:20,515
So VGGNet ended up getting second place in the

525
00:26:20,515 --> 00:26:22,930
ImageNet 2014 classification challenge,

526
00:26:22,930 --> 00:26:25,593
first in localization.

527
00:26:25,593 --> 00:26:27,106
They followed a very similar training procedure

528
00:26:27,106 --> 00:26:29,847
as Alex Krizhevsky for the AlexNet.

529
00:26:29,847 --> 00:26:33,895
They didn't use local response normalization,

530
00:26:33,895 --> 00:26:35,333
so as I mentioned earlier,

531
00:26:35,333 --> 00:26:37,367
they found out this didn't really help them,

532
00:26:37,367 --> 00:26:39,574
and so they took it out.

533
00:26:39,574 --> 00:26:44,281
You'll see VGG 16 and VGG 19 are common variants

534
00:26:44,281 --> 00:26:46,657
of the cycle here, and this is just

535
00:26:46,657 --> 00:26:50,425
the number of layers, 19 is slightly deeper than 16.

536
00:26:50,425 --> 00:26:54,348
In practice VGG 19 works very little bit better,

537
00:26:54,348 --> 00:26:57,009
and there's a little bit more memory usage,

538
00:26:57,009 --> 00:27:01,176
so you can use either but 16 is very commonly used.

539
00:27:02,280 --> 00:27:06,247
For best results, like AlexNet, they did ensembling

540
00:27:06,247 --> 00:27:09,198
in order to average several models,

541
00:27:09,198 --> 00:27:10,920
and you get better results.

542
00:27:10,920 --> 00:27:14,432
And they also showed in their work that

543
00:27:14,432 --> 00:27:18,112
the FC7 features of the last fully connected layer before

544
00:27:18,112 --> 00:27:20,968
going to the 1000 ImageNet classes.

545
00:27:20,968 --> 00:27:24,771
The 4096 size layer just before that,

546
00:27:24,771 --> 00:27:27,273
is a good feature representation,

547
00:27:27,273 --> 00:27:29,648
that can even just be used as is,

548
00:27:29,648 --> 00:27:32,714
to extract these features from other data,

549
00:27:32,714 --> 00:27:35,865
and generalized these other tasks as well.

550
00:27:35,865 --> 00:27:38,602
And so FC7 is a good feature representation.

551
00:27:38,602 --> 00:27:39,952
Yeah question.

552
00:27:39,952 --> 00:27:42,742
[student speaks off mic]

553
00:27:42,742 --> 00:27:45,242
- Sorry what was the question?

554
00:27:46,749 --> 00:27:50,846
Okay, so the question is what is localization here?

555
00:27:50,846 --> 00:27:53,921
And so this is a task, and we'll talk about it

556
00:27:53,921 --> 00:27:55,684
a little bit more in a later lecture

557
00:27:55,684 --> 00:27:57,973
on detection and localization so I don't want to

558
00:27:57,973 --> 00:27:59,568
go into detail here but it's basically an image,

559
00:27:59,568 --> 00:28:04,015
not just classifying What's the class of the image,

560
00:28:04,015 --> 00:28:08,518
but also drawing a bounding box around where that

561
00:28:08,518 --> 00:28:10,243
object is in the image.

562
00:28:10,243 --> 00:28:11,677
And the difference with detection,

563
00:28:11,677 --> 00:28:13,641
which is a very related task is that detection

564
00:28:13,641 --> 00:28:16,963
there can be multiple instances of this object in the image

565
00:28:16,963 --> 00:28:18,938
localization we're assuming there's just one,

566
00:28:18,938 --> 00:28:21,481
this classification but we just how this

567
00:28:21,481 --> 00:28:23,481
additional bounding box.

568
00:28:26,153 --> 00:28:29,390
So we looked at VGG which was one of the deep networks

569
00:28:29,390 --> 00:28:33,192
from 2014 and then now we'll talk about GoogleNet

570
00:28:33,192 --> 00:28:34,996
which was the other one that won

571
00:28:34,996 --> 00:28:37,413
the classification challenge.

572
00:28:38,422 --> 00:28:41,288
So GoogleNet again was a much deeper network

573
00:28:41,288 --> 00:28:44,871
with 22 layers but one of the main insights

574
00:28:45,853 --> 00:28:48,586
and special things about GoogleNet is that it really

575
00:28:48,586 --> 00:28:51,653
looked at this problem of computational efficiency

576
00:28:51,653 --> 00:28:54,959
and it tried to design a network architecture that was

577
00:28:54,959 --> 00:28:58,676
very efficient in the amount of compute.

578
00:28:58,676 --> 00:29:01,869
And so they did this using this inception module

579
00:29:01,869 --> 00:29:05,833
which we'll go into more detail and basically stacking

580
00:29:05,833 --> 00:29:09,146
a lot of these inception modules on top of each other.

581
00:29:09,146 --> 00:29:12,292
There's also no fully connected layers in this network,

582
00:29:12,292 --> 00:29:14,515
so they got rid of that were able to save a lot

583
00:29:14,515 --> 00:29:17,133
of parameters and so in total there's only five million

584
00:29:17,133 --> 00:29:20,651
parameters which is twelve times less than AlexNet,

585
00:29:20,651 --> 00:29:25,118
which had 60 million even though it's much deeper now.

586
00:29:25,118 --> 00:29:27,785
It got 6.7% top five error.

587
00:29:32,202 --> 00:29:34,414
So what's the inception module?

588
00:29:34,414 --> 00:29:36,173
So the idea behind the inception module

589
00:29:36,173 --> 00:29:40,833
is that they wanted to design a good local network typology

590
00:29:40,833 --> 00:29:43,203
and it has this idea of this local topology

591
00:29:43,203 --> 00:29:47,177
that's you know you can think of it as a network

592
00:29:47,177 --> 00:29:48,857
within a network and then stack a lot of these

593
00:29:48,857 --> 00:29:53,151
local typologies one on top of each other.

594
00:29:53,151 --> 00:29:55,890
And so in this local network that they're calling

595
00:29:55,890 --> 00:29:59,197
an inception module what they're doing is they're basically

596
00:29:59,197 --> 00:30:02,673
applying several different kinds of filter operations

597
00:30:02,673 --> 00:30:05,823
in parallel on top of the same input coming into

598
00:30:05,823 --> 00:30:07,948
this same layer.

599
00:30:07,948 --> 00:30:09,715
So we have our input coming in from the previous layer

600
00:30:09,715 --> 00:30:12,706
and then we're going to do different kinds of convolutions.

601
00:30:12,706 --> 00:30:16,838
So a one by one conv, right a three by three conv,

602
00:30:16,838 --> 00:30:18,684
five by five conv, and then they also

603
00:30:18,684 --> 00:30:22,208
have a pooling operation in this case three by three

604
00:30:22,208 --> 00:30:24,290
pooling, and so you get all of these different

605
00:30:24,290 --> 00:30:26,457
outputs from these different layers,

606
00:30:26,457 --> 00:30:29,565
and then what they do is they concatenate all these

607
00:30:29,565 --> 00:30:32,309
filter outputs together depth wise, and so

608
00:30:32,309 --> 00:30:36,036
then this creates one tenser output at the end

609
00:30:36,036 --> 00:30:39,703
that is going tom pass on to the next layer.

610
00:30:41,830 --> 00:30:44,368
So if we look at just a naive way of doing this

611
00:30:44,368 --> 00:30:46,616
we just do exactly that we have all of these different

612
00:30:46,616 --> 00:30:50,825
operations we get the outputs we concatenate them together.

613
00:30:50,825 --> 00:30:53,196
So what's the problem with this?

614
00:30:53,196 --> 00:30:56,027
And it turns out that computational complexity

615
00:30:56,027 --> 00:30:58,527
is going to be a problem here.

616
00:30:59,792 --> 00:31:01,514
So if we look more carefully at an example,

617
00:31:01,514 --> 00:31:05,473
so here just for as an example I've put one by one conv,

618
00:31:05,473 --> 00:31:09,476
128 filter so three by three conv 192 filters,

619
00:31:09,476 --> 00:31:11,966
five by five convs and 96 filters.

620
00:31:11,966 --> 00:31:14,582
Assume everything has basically the stride

621
00:31:14,582 --> 00:31:17,041
that's going to maintain the spatial dimensions,

622
00:31:17,041 --> 00:31:20,208
and that we have this input coming in.

623
00:31:22,151 --> 00:31:24,858
So what is the output size of the one by one filter

624
00:31:24,858 --> 00:31:28,708
with 128 , one by one conv with 128 filters?

625
00:31:28,708 --> 00:31:30,041
Who has a guess?

626
00:31:36,720 --> 00:31:40,720
OK so I heard 28 by 28, by 128 which is correct.

627
00:31:41,798 --> 00:31:44,043
So right by one by one conv we're going to maintain

628
00:31:44,043 --> 00:31:46,821
spatial dimensions and then on top of that,

629
00:31:46,821 --> 00:31:50,374
each conv filter is going to look through

630
00:31:50,374 --> 00:31:53,969
the entire 256 depth of the input,

631
00:31:53,969 --> 00:31:55,723
but then the output is going to be,

632
00:31:55,723 --> 00:31:57,684
we have a 28 by 28 feature map

633
00:31:57,684 --> 00:32:00,145
for each of the 128 filters that we have

634
00:32:00,145 --> 00:32:01,004
in this conv layer.

635
00:32:01,004 --> 00:32:03,171
So we get 28 by 28 by 128.

636
00:32:06,279 --> 00:32:08,368
OK and then now if we do the same thing

637
00:32:08,368 --> 00:32:11,771
and we look at the filter sizes of the output sizes sorry

638
00:32:11,771 --> 00:32:15,749
of all of the different filters here, after the

639
00:32:15,749 --> 00:32:17,803
three by three conv we're going to have this volume

640
00:32:17,803 --> 00:32:21,189
of 28 by 28 by 192 right after five by five conv

641
00:32:21,189 --> 00:32:23,323
we have 96 filters here.

642
00:32:23,323 --> 00:32:25,369
So 28 by 28 by 96,

643
00:32:25,369 --> 00:32:28,851
and then out pooling layer is just going

644
00:32:28,851 --> 00:32:32,821
to keep the same spatial dimension here, so pooling layer

645
00:32:32,821 --> 00:32:35,522
will preserve it in depth,

646
00:32:35,522 --> 00:32:36,835
and here because of our stride,

647
00:32:36,835 --> 00:32:41,002
we're also going to preserve our spatial dimensions.

648
00:32:42,035 --> 00:32:44,288
And so now if we look at the output size after filter

649
00:32:44,288 --> 00:32:48,589
concatenation what we're going to get is 28 by 28,

650
00:32:48,589 --> 00:32:52,308
these are all 28 by 28, and we concatenating depth wise.

651
00:32:52,308 --> 00:32:56,681
So we get 28 by 28 times all of these added together,

652
00:32:56,681 --> 00:32:58,807
and the total output size is going to be

653
00:32:58,807 --> 00:33:00,140
28 by 28 by 672.

654
00:33:01,923 --> 00:33:05,798
So the input to our inception module was 28 by 28

655
00:33:05,798 --> 00:33:10,185
by 256, then the output from this module is 28 by 28

656
00:33:10,185 --> 00:33:11,018
by 672.

657
00:33:12,276 --> 00:33:15,148
So we kept the same spatial dimensions,

658
00:33:15,148 --> 00:33:18,064
and we blew up the depth.

659
00:33:18,064 --> 00:33:18,998
Question.

660
00:33:18,998 --> 00:33:22,715
[student speaks off mic]

661
00:33:22,715 --> 00:33:24,393
OK So in this case, yeah, the question is,

662
00:33:24,393 --> 00:33:26,356
how are we getting 28 by 28 for everything?

663
00:33:26,356 --> 00:33:28,436
So here we're doing all the zero padding

664
00:33:28,436 --> 00:33:30,117
in order to maintain the spatial dimensions,

665
00:33:30,117 --> 00:33:32,130
and that way we can do this filter

666
00:33:32,130 --> 00:33:34,213
concatenation depth-wise.

667
00:33:35,205 --> 00:33:37,043
Question in the back.

668
00:33:37,043 --> 00:33:40,460
[student speaks off mic]

669
00:33:45,634 --> 00:33:48,615
- OK The question is what's the 256 deep at the input,

670
00:33:48,615 --> 00:33:51,228
and so this is not the input to the network,

671
00:33:51,228 --> 00:33:53,677
this is the input just to this local module

672
00:33:53,677 --> 00:33:54,624
that I'm looking at.

673
00:33:54,624 --> 00:33:58,460
So in this case 256 is the depth of the previous

674
00:33:58,460 --> 00:34:01,316
inception module that came just before this.

675
00:34:01,316 --> 00:34:05,491
And so now coming out we have 28 by 28 by 672,

676%0