1
00:00:08,555 --> 00:00:12,482
- Okay so welcome to CS 231N Lecture three.

2
00:00:12,482 --> 00:00:15,394
Today we're going to talk about loss functions and optimization

3
00:00:15,394 --> 00:00:17,520
but as usual, before we get to the main content

4
00:00:17,520 --> 00:00:19,762
of the lecture, there's a couple administrative things

5
00:00:19,762 --> 00:00:20,929
to talk about.

6
00:00:21,894 --> 00:00:25,347
So the first thing is that assignment one has been released.

7
00:00:25,347 --> 00:00:27,689
You can find the link up on the website.

8
00:00:27,689 --> 00:00:29,176
And since we were a little bit late

9
00:00:29,176 --> 00:00:30,886
in getting this assignment out to you guys,

10
00:00:30,886 --> 00:00:33,981
we've decided to change the due date to Thursday,

11
00:00:33,981 --> 00:00:36,064
April 20th at 11:59 p.m.,

12
00:00:37,174 --> 00:00:40,081
this will give you a full two weeks from the assignment

13
00:00:40,081 --> 00:00:43,502
release date to go and actually finish and work on it,

14
00:00:43,502 --> 00:00:47,299
so we'll update the syllabus for this new due date

15
00:00:47,299 --> 00:00:49,887
in a little bit later today.

16
00:00:49,887 --> 00:00:51,825
And as a reminder, when you complete the assignment,

17
00:00:51,825 --> 00:00:55,417
you should go turn in the final zip file on Canvas

18
00:00:55,417 --> 00:00:57,579
so we can grade it and get your grades back as quickly

19
00:00:57,579 --> 00:00:58,579
as possible.

20
00:00:59,599 --> 00:01:04,233
So the next thing is always check out Piazza for interesting

21
00:01:04,233 --> 00:01:05,679
administrative stuff.

22
00:01:05,679 --> 00:01:08,588
So this week I wanted to highlight that we have several

23
00:01:08,588 --> 00:01:12,232
example project ideas as a pinned post on Piazza.

24
00:01:12,232 --> 00:01:15,730
So we went out and solicited example of project ideas

25
00:01:15,730 --> 00:01:18,020
from various people in the Stanford community or affiliated

26
00:01:18,020 --> 00:01:20,951
to Stanford, and they came up with some interesting

27
00:01:20,951 --> 00:01:23,713
suggestions for projects that they might want students

28
00:01:23,713 --> 00:01:25,383
in the class to work on.

29
00:01:25,383 --> 00:01:27,786
So check out this pinned post on Piazza and if you want

30
00:01:27,786 --> 00:01:31,384
to work on any of these projects, then feel free to contact

31
00:01:31,384 --> 00:01:35,031
the project mentors directly about these things.

32
00:01:35,031 --> 00:01:37,890
Aditionally we posted office hours on the course website,

33
00:01:37,890 --> 00:01:41,556
this is a Google calendar, so this is something that people

34
00:01:41,556 --> 00:01:45,877
have been asking about and now it's up there.

35
00:01:45,877 --> 00:01:49,107
The final administrative note is about Google Cloud,

36
00:01:49,107 --> 00:01:52,545
as a reminder, because we're supported by Google Cloud

37
00:01:52,545 --> 00:01:55,131
in this class, we're able to give each of you an additional

38
00:01:55,131 --> 00:01:57,833
$100 credit for Google Cloud to work on your assignments

39
00:01:57,833 --> 00:02:01,487
and projects, and the exact details of how to redeem

40
00:02:01,487 --> 00:02:06,046
that credit will go out later today, most likely on Piazza.

41
00:02:06,046 --> 00:02:08,610
So if there's, I guess if there's no questions about

42
00:02:08,610 --> 00:02:12,777
administrative stuff then we'll move on to course content.

43
00:02:14,240 --> 00:02:15,073
Okay cool.

44
00:02:16,359 --> 00:02:18,797
So recall from last time in lecture two,

45
00:02:18,797 --> 00:02:21,212
we were really talking about the challenges of recognition

46
00:02:21,212 --> 00:02:23,002
and trying to hone in on this idea

47
00:02:23,002 --> 00:02:25,276
of a data-driven approach.

48
00:02:25,276 --> 00:02:27,721
We talked about this idea of image classification,

49
00:02:27,721 --> 00:02:29,960
talked about why it's hard, there's this semantic gap

50
00:02:29,960 --> 00:02:34,002
between the giant grid of numbers that the computer sees

51
00:02:34,002 --> 00:02:36,612
and the actual image that you see.

52
00:02:36,612 --> 00:02:38,445
We talked about various challenges regarding this

53
00:02:38,445 --> 00:02:40,757
around illumination, deformation, et cetera,

54
00:02:40,757 --> 00:02:42,924
and why this is actually a really, really hard problem

55
00:02:42,924 --> 00:02:44,986
even though it's super easy for people to do

56
00:02:44,986 --> 00:02:48,712
with their human eyes and human visual system.

57
00:02:48,712 --> 00:02:51,221
Then also recall last time we talked about the k-nearest

58
00:02:51,221 --> 00:02:54,289
neighbor classifier as kind of a simple introduction

59
00:02:54,289 --> 00:02:56,109
to this whole data-driven mindset.

60
00:02:56,109 --> 00:02:58,792
We talked about the CIFAR-10 data set where you can see

61
00:02:58,792 --> 00:03:01,624
an example of these images on the upper left here,

62
00:03:01,624 --> 00:03:04,488
where CIFAR-10 gives you these 10 different categories,

63
00:03:04,488 --> 00:03:06,587
airplane, automobile, whatnot,

64
00:03:06,587 --> 00:03:09,427
and we talked about how the k-nearest neighbor classifier

65
00:03:09,427 --> 00:03:12,002
can be used to learn decision boundaries

66
00:03:12,002 --> 00:03:14,404
to separate these data points into classes

67
00:03:14,404 --> 00:03:16,546
based on the training data.

68
00:03:16,546 --> 00:03:19,399
This also led us to a discussion of the idea of cross

69
00:03:19,399 --> 00:03:21,755
validation and setting hyper parameters by dividing

70
00:03:21,755 --> 00:03:25,990
your data into train, validation and test sets.

71
00:03:25,990 --> 00:03:28,008
Then also recall last time we talked about linear

72
00:03:28,008 --> 00:03:30,857
classification as the first sort of building block

73
00:03:30,857 --> 00:03:33,210
as we move toward neural networks.

74
00:03:33,210 --> 00:03:35,526
Recall that the linear classifier is an example

75
00:03:35,526 --> 00:03:39,338
of a parametric classifier where all of our knowledge

76
00:03:39,338 --> 00:03:41,328
about the training data gets summarized

77
00:03:41,328 --> 00:03:44,146
into this parameter matrix W that is set

78
00:03:44,146 --> 00:03:46,244
during the process of training.

79
00:03:46,244 --> 00:03:49,248
And this linear classifier recall is super simple,

80
00:03:49,248 --> 00:03:51,115
where we're going to take the image and stretch it out

81
00:03:51,115 --> 00:03:52,610
into a long vector.

82
00:03:52,610 --> 00:03:55,774
So here the image is x and then we take that image

83
00:03:55,774 --> 00:03:59,095
which might be 32 by 32 by 3 pixels, stretch it out

84
00:03:59,095 --> 00:04:02,051
into a long column vector of 32 times 32

85
00:04:02,051 --> 00:04:03,718
times 3 entries,

86
00:04:05,144 --> 00:04:07,203
where the 32 and 32 are the height and width,

87
00:04:07,203 --> 00:04:09,023
and the 3 give you the three color channels,

88
00:04:09,023 --> 00:04:10,522
red, green, blue.

89
00:04:10,522 --> 00:04:14,361
Then there exists some parameter matrix, W

90
00:04:14,361 --> 00:04:16,481
which will take this long column vector

91
00:04:16,481 --> 00:04:19,317
representing the image pixels, and convert this

92
00:04:19,317 --> 00:04:21,642
and give you 10 numbers giving scores

93
00:04:21,642 --> 00:04:25,187
for each of the 10 classes in the case of CIFAR-10.

94
00:04:25,187 --> 00:04:26,916
Where we kind of had this interpretation

95
00:04:26,916 --> 00:04:30,417
where larger values of those scores,

96
00:04:30,417 --> 00:04:33,147
so a larger value for the cat class means the classifier

97
00:04:33,147 --> 00:04:35,681
thinks that the cat is more likely for that image,

98
00:04:35,681 --> 00:04:38,350
and lower values for maybe the dog or car class

99
00:04:38,350 --> 00:04:41,353
indicate lower probabilities of those classes being present

100
00:04:41,353 --> 00:04:43,243
in the image.

101
00:04:43,243 --> 00:04:46,564
Also, so I think this point was a little bit unclear

102
00:04:46,564 --> 00:04:50,209
last time that linear classification has this interpretation

103
00:04:50,209 --> 00:04:52,425
as learning templates per class,

104
00:04:52,425 --> 00:04:55,128
where if you look at the diagram on the lower left,

105
00:04:55,128 --> 00:04:58,299
you think that, so for every pixel in the image,

106
00:04:58,299 --> 00:05:00,411
and for every one of our 10 classes,

107
00:05:00,411 --> 00:05:03,244
there exists some entry in this matrix W,

108
00:05:03,244 --> 00:05:07,354
telling us how much does that pixel influence that class.

109
00:05:07,354 --> 00:05:10,416
So that means that each of these rows in the matrix W

110
00:05:10,416 --> 00:05:13,212
ends up corresponding to a template for the class.

111
00:05:13,212 --> 00:05:15,479
And if we take those rows and unravel,

112
00:05:15,479 --> 00:05:17,724
so each of those rows again corresponds

113
00:05:17,724 --> 00:05:20,540
to a weighting between the values of,

114
00:05:20,540 --> 00:05:23,351
between the pixel values of the image and that class,

115
00:05:23,351 --> 00:05:26,246
so if we take that row and unravel it back into an image,

116
00:05:26,246 --> 00:05:28,787
then we can visualize the learned template for each

117
00:05:28,787 --> 00:05:30,700
of these classes.

118
00:05:30,700 --> 00:05:33,324
We also had this interpretation of linear classification

119
00:05:33,324 --> 00:05:36,199
as learning linear decision boundaries between pixels

120
00:05:36,199 --> 00:05:38,588
in some high dimensional space where the dimensions

121
00:05:38,588 --> 00:05:41,611
of the space correspond to the values of the pixel

122
00:05:41,611 --> 00:05:44,574
intensity values of the image.

123
00:05:44,574 --> 00:05:48,371
So this is kind of where we left off last time.

124
00:05:48,371 --> 00:05:51,615
And so where we kind of stopped, where we ended up last

125
00:05:51,615 --> 00:05:54,941
time is we got this idea of a linear classifier,

126
00:05:54,941 --> 00:05:58,354
and we didn't talk about how to actually choose the W.

127
00:05:58,354 --> 00:06:00,189
How to actually use the training data

128
00:06:00,189 --> 00:06:03,428
to determine which value of W should be best.

129
00:06:03,428 --> 00:06:05,256
So kind of where we stopped off at

130
00:06:05,256 --> 00:06:09,092
is that for some setting of W, we can use this W

131
00:06:09,092 --> 00:06:12,868
to come up with 10 with our class scores for any image.

132
00:06:12,868 --> 00:06:16,397
So and some of these class scores might be better or worse.

133
00:06:16,397 --> 00:06:17,964
So here in this simple example,

134
00:06:17,964 --> 00:06:21,633
we've shown maybe just a training data set of three images

135
00:06:21,633 --> 00:06:25,384
along with the 10 class scores predicted for some value of W

136
00:06:25,384 --> 00:06:26,846
for those images.

137
00:06:26,846 --> 00:06:28,647
And you can see that some of these scores are better

138
00:06:28,647 --> 00:06:30,306
or worse than others.

139
00:06:30,306 --> 00:06:33,144
So for example in the image on the left, if you look up,

140
00:06:33,144 --> 00:06:35,042
it's actually a cat because you're a human

141
00:06:35,042 --> 00:06:36,724
and you can tell these things,

142
00:06:36,724 --> 00:06:39,752
but if we look at the assigned probabilities, cat,

143
00:06:39,752 --> 00:06:41,868
well not probabilities but scores,

144
00:06:41,868 --> 00:06:44,236
then the classifier maybe for this setting of W

145
00:06:44,236 --> 00:06:48,882
gave the cat class a score of 2.9 for this image,

146
00:06:48,882 --> 00:06:51,818
whereas the frog class gave 3.78.

147
00:06:51,818 --> 00:06:53,909
So maybe the classifier is not doing not so good

148
00:06:53,909 --> 00:06:56,236
on this image, that's bad, we wanted the true class

149
00:06:56,236 --> 00:06:58,720
to be actually the highest class score,

150
00:06:58,720 --> 00:07:00,909
whereas for some of these other examples, like the car

151
00:07:00,909 --> 00:07:03,529
for example, you see that the automobile class

152
00:07:03,529 --> 00:07:05,193
has a score of six which is much higher

153
00:07:05,193 --> 00:07:07,619
than any of the others, so that's good.

154
00:07:07,619 --> 00:07:11,433
And the frog, the predicted scores are maybe negative four,

155
00:07:11,433 --> 00:07:13,637
which is much lower than all the other ones,

156
00:07:13,637 --> 00:07:15,157
so that's actually bad.

157
00:07:15,157 --> 00:07:17,331
So this is kind of a hand wavy approach,

158
00:07:17,331 --> 00:07:19,140
just kind of looking at the scores and eyeballing

159
00:07:19,140 --> 00:07:21,454
which ones are good and which ones are bad.

160
00:07:21,454 --> 00:07:23,610
But to actually write algorithms about these things

161
00:07:23,610 --> 00:07:26,064
and to actually to determine automatically which W

162
00:07:26,064 --> 00:07:29,660
will be best, we need some way to quantify the badness

163
00:07:29,660 --> 00:07:31,832
of any particular W.

164
00:07:31,832 --> 00:07:35,826
And that's this function that takes in a W,

165
00:07:35,826 --> 00:07:39,283
looks at the scores and then tells us how bad quantitatively

166
00:07:39,283 --> 00:07:42,787
is that W, is something that we'll call a loss function.

167
00:07:42,787 --> 00:07:45,467
And in this lecture we'll see a couple examples

168
00:07:45,467 --> 00:07:48,093
of different loss functions that you can use for this image

169
00:07:48,093 --> 00:07:50,582
classification problem.

170
00:07:50,582 --> 00:07:53,483
So then once we've got this idea of a loss function,

171
00:07:53,483 --> 00:07:57,532
this allows us to quantify for any given value of W,

172
00:07:57,532 --> 00:07:59,298
how good or bad is it?

173
00:07:59,298 --> 00:08:00,834
But then we actually need to find

174
00:08:00,834 --> 00:08:02,730
and come up with an efficient procedure

175
00:08:02,730 --> 00:08:05,570
for searching through the space of all possible Ws

176
00:08:05,570 --> 00:08:08,934
and actually come up with what is the correct value

177
00:08:08,934 --> 00:08:11,488
of W that is the least bad,

178
00:08:11,488 --> 00:08:13,660
and this process will be an optimization procedure

179
00:08:13,660 --> 00:08:17,076
and we'll talk more about that in this lecture.

180
00:08:17,076 --> 00:08:19,091
So I'm going to shrink this example a little bit

181
00:08:19,091 --> 00:08:21,803
because 10 classes is a little bit unwieldy.

182
00:08:21,803 --> 00:08:24,731
So we'll kind of work with this tiny toy data set

183
00:08:24,731 --> 00:08:27,551
of three examples and three classes going forward

184
00:08:27,551 --> 00:08:29,686
in this lecture.

185
00:08:29,686 --> 00:08:33,639
So again, in this example, the cat is maybe not so correctly

186
00:08:33,639 --> 00:08:38,407
classified, the car is correctly classified, and the frog,

187
00:08:38,407 --> 00:08:41,320
this setting of W got this frog image totally wrong,

188
00:08:41,320 --> 00:08:45,225
because the frog score is much lower than others.

189
00:08:45,225 --> 00:08:47,764
So to formalize this a little bit, usually when we talk

190
00:08:47,764 --> 00:08:49,617
about a loss function, we imagine

191
00:08:49,617 --> 00:08:53,670
that we have some training data set of xs and ys,

192
00:08:53,670 --> 00:08:56,996
usually N examples of these where the xs are the inputs

193
00:08:56,996 --> 00:09:00,004
to the algorithm in the image classification case,

194
00:09:00,004 --> 00:09:03,862
the xs would be the actually pixel values of your images,

195
00:09:03,862 --> 00:09:06,207
and the ys will be the things you want your algorithm

196
00:09:06,207 --> 00:09:09,730
to predict, we usually call these the labels or the targets.

197
00:09:09,730 --> 00:09:11,782
So in the case of image classification,

198
00:09:11,782 --> 00:09:14,540
remember we're trying to categorize each image

199
00:09:14,540 --> 00:09:17,597
for CIFAR-10 to one of 10 categories,

200
00:09:17,597 --> 00:09:19,801
so the label y here will be an integer

201
00:09:19,801 --> 00:09:22,948
between one and 10 or maybe between zero and nine

202
00:09:22,948 --> 00:09:25,214
depending on what programming language you're using,

203
00:09:25,214 --> 00:09:27,045
but it'll be an integer telling you

204
00:09:27,045 --> 00:09:31,070
what is the correct category for each one of those images x.

205
00:09:31,070 --> 00:09:35,284
And now our loss function will denote L_i to denote the,

206
00:09:35,284 --> 00:09:37,693
so then we have this prediction function x

207
00:09:37,693 --> 00:09:41,769
which takes in our example x and our weight matrix W

208
00:09:41,769 --> 00:09:43,638
and makes some prediction for y,

209
00:09:43,638 --> 00:09:45,235
in the case of image classification

210
00:09:45,235 --> 00:09:47,246
these will be our 10 numbers.

211
00:09:47,246 --> 00:09:50,738
Then we'll define some loss function L_i

212
00:09:50,738 --> 00:09:53,400
which will take in the predicted scores

213
00:09:53,400 --> 00:09:54,983
coming out of the function f

214
00:09:54,983 --> 00:09:57,604
together with the true target or label Y

215
00:09:57,604 --> 00:10:00,112
and give us some quantitative value for how bad

216
00:10:00,112 --> 00:10:03,310
those predictions are for that training example.

217
00:10:03,310 --> 00:10:06,927
And now the final loss L will be the average of these losses

218
00:10:06,927 --> 00:10:09,776
summed over the entire data set over each of the N examples

219
00:10:09,776 --> 00:10:11,278
in our data set.

220
00:10:11,278 --> 00:10:14,232
So this is actually a very general formulation,

221
00:10:14,232 --> 00:10:17,221
and actually extends even beyond image classification.

222
00:10:17,221 --> 00:10:19,818
Kind of as we move forward and see other tasks,

223
00:10:19,818 --> 00:10:22,123
other examples of tasks and deep learning,

224
00:10:22,123 --> 00:10:25,226
the kind of generic setup is that for any task

225
00:10:25,226 --> 00:10:27,639
you have some xs and ys and you want to write down

226
00:10:27,639 --> 00:10:30,849
some loss function that quantifies exactly how happy

227
00:10:30,849 --> 00:10:34,335
you are with your particular parameter settings W

228
00:10:34,335 --> 00:10:36,615
and then you'll eventually search over the space of W

229
00:10:36,615 --> 00:10:40,782
to find the W that minimizes the loss on your training data.

230
00:10:41,881 --> 00:10:46,330
So as a first example of a concrete loss function

231
00:10:46,330 --> 00:10:50,122
that is a nice thing to work with in image classification,

232
00:10:50,122 --> 00:10:53,555
we'll talk about the multi-class SVM loss.

233
00:10:53,555 --> 00:10:57,069
You may have seen the binary SVM, our support vector

234
00:10:57,069 --> 00:11:00,402
machine in CS 229 and the multiclass SVM

235
00:11:01,462 --> 00:11:06,063
is a generalization of that to handle multiple classes.

236
00:11:06,063 --> 00:11:10,247
In the binary SVM case as you may have seen in 229,

237
00:11:10,247 --> 00:11:12,597
you only had two classes, each example x

238
00:11:12,597 --> 00:11:14,550
was going to be classified as either positive

239
00:11:14,550 --> 00:11:15,959
or negative example,

240
00:11:15,959 --> 00:11:18,415
but now we have 10 categories, so we need to generalize

241
00:11:18,415 --> 00:11:21,562
this notion to handle multiple classes.

242
00:11:21,562 --> 00:11:25,654
So this loss function has kind of a funny functional form,

243
00:11:25,654 --> 00:11:27,006
so we'll walk through it in a bit more,

244
00:11:27,006 --> 00:11:30,601
in quite a bit of detail over the next couple of slides.

245
00:11:30,601 --> 00:11:33,894
But what this is saying is that the loss L_i

246
00:11:33,894 --> 00:11:36,384
for any individual example, the way we'll compute it

247
00:11:36,384 --> 00:11:41,098
is we're going to perform a sum over all of the categories, Y,

248
00:11:41,098 --> 00:11:44,173
except for the true category, Y_i,

249
00:11:44,173 --> 00:11:47,004
so we're going to sum over all the incorrect categories,

250
00:11:47,004 --> 00:11:49,242
and then we're going to compare the score

251
00:11:49,242 --> 00:11:51,046
of the correct category, and the score

252
00:11:51,046 --> 00:11:53,309
of the incorrect category,

253
00:11:53,309 --> 00:11:55,879
and now if the score for the correct category

254
00:11:55,879 --> 00:12:00,389
is greater than the score of the incorrect category,

255
00:12:00,389 --> 00:12:04,614
greater than the incorrect score by some safety margin

256
00:12:04,614 --> 00:12:07,949
that we set to one, if that's the case that means that

257
00:12:07,949 --> 00:12:12,140
the true score is much, or the score for the true category

258
00:12:12,140 --> 00:12:15,461
is if it's much larger than any of the false categories,

259
00:12:15,461 --> 00:12:18,463
then we'll get a loss of zero.

260
00:12:18,463 --> 00:12:22,503
And we'll sum this up over all of the incorrect categories

261
00:12:22,503 --> 00:12:25,254
for our image and this will give us our final loss

262
00:12:25,254 --> 00:12:27,955
for this one example in the data set.

263
00:12:27,955 --> 00:12:30,511
And again we'll take the average of this loss

264
00:12:30,511 --> 00:12:32,794
over the whole training data set.

265
00:12:32,794 --> 00:12:37,737
So this kind of like if then statement, like if the true

266
00:12:37,737 --> 00:12:42,063
class score is much larger than the others,

267
00:12:42,063 --> 00:12:45,960
this kind of if then formulation we often compactify

268
00:12:45,960 --> 00:12:50,127
into this single max of zero S_j minus S_Yi plus one thing,

269
00:12:51,296 --> 00:12:53,849
but I always find that notation a little bit confusing,

270
00:12:53,849 --> 00:12:55,094
and it always helps me

271
00:12:55,094 --> 00:12:57,478
to write it out in this sort of case based notation

272
00:12:57,478 --> 00:12:59,436
to figure out exactly what the two cases are

273
00:12:59,436 --> 00:13:01,720
and what's going on.

274
00:13:01,720 --> 00:13:04,589
And by the way, this style of loss function

275
00:13:04,589 --> 00:13:07,605
where we take max of zero and some other quantity

276
00:13:07,605 --> 00:13:10,927
is often referred to as some type of a hinge loss,

277
00:13:10,927 --> 00:13:14,002
and this name comes from the shape of the graph

278
00:13:14,002 --> 00:13:15,427
when you go and plot it,

279
00:13:15,427 --> 00:13:18,927
so here the x axis corresponds to the S_Yi,

280
00:13:19,903 --> 00:13:23,075
that is the score of the true class for some training

281
00:13:23,075 --> 00:13:26,153
example, and now the y axis is the loss,

282
00:13:26,153 --> 00:13:30,320
and you can see that as the score for the true category

283
00:13:31,323 --> 00:13:33,138
for this example increases, then the loss

284
00:13:33,138 --> 00:13:34,974
will go down linearly

285
00:13:34,974 --> 00:13:38,605
until we get to above this safety margin,

286
00:13:38,605 --> 00:13:41,035
after which the loss will be zero

287
00:13:41,035 --> 00:13:45,202
because we've already correctly classified this example.

288
00:13:46,350 --> 00:13:48,636
So let's, oh, question?

289
00:13:48,636 --> 00:13:51,264
- [Student] Sorry, in terms of notation

290
00:13:51,264 --> 00:13:53,210
what is S underscore Yi?

291
00:13:53,210 --> 00:13:55,970
Is that your right score?

292
00:13:55,970 --> 00:13:57,121
- Yeah, so the question is

293
00:13:57,121 --> 00:14:00,897
in terms of notation, what is S and what is SYI

294
00:14:00,897 --> 00:14:04,706
in particular, so the Ss are the predicted scores

295
00:14:04,706 --> 00:14:08,046
for the classes that are coming out of the classifier.

296
00:14:08,046 --> 00:14:11,734
So if one is the cat class and two is the dog class then S1

297
00:14:11,734 --> 00:14:14,742
and S2 would be the cat and dog scores respectively.

298
00:14:14,742 --> 00:14:18,234
And remember we said that Yi was the category of the ground

299
00:14:18,234 --> 00:14:21,856
truth label for the example which is some integer.

300
00:14:21,856 --> 00:14:26,023
So then S sub Y sub i, sorry for the double subscript,

301
00:14:26,857 --> 00:14:29,766
that corresponds to the score of the true class

302
00:14:29,766 --> 00:14:33,483
for the i-th example in the training set.

303
00:14:33,483 --> 00:14:34,670
Question?

304
00:14:34,670 --> 00:14:36,721
- [Student] So what exactly is this computing?

305
00:14:36,721 --> 00:14:39,477
- Yeah the question is what exactly is this computing here?

306
00:14:39,477 --> 00:14:42,425
It's a little bit funny, I think it will become more clear

307
00:14:42,425 --> 00:14:45,390
when we walk through an explicit example, but in some sense

308
00:14:45,390 --> 00:14:49,475
what this loss is saying is that we are happy if the true

309
00:14:49,475 --> 00:14:52,784
score is much higher than all the other scores.

310
00:14:52,784 --> 00:14:55,393
It needs to be higher than all the other scores

311
00:14:55,393 --> 00:14:59,164
by some safety margin, and if the true score

312
00:14:59,164 --> 00:15:02,692
is not high enough, greater than any of the other scores,

313
00:15:02,692 --> 00:15:07,334
then we will incur some loss and that would be bad.

314
00:15:07,334 --> 00:15:09,457
So this might make a little bit more sense

315
00:15:09,457 --> 00:15:11,400
if we walk through an explicit example

316
00:15:11,400 --> 00:15:14,023
for this tiny three example data set.

317
00:15:14,023 --> 00:15:16,896
So here remember I've sort of removed the case space

318
00:15:16,896 --> 00:15:20,313
notation and just switching back to the zero one notation,

319
00:15:20,313 --> 00:15:21,910
and now if we look at,

320
00:15:21,910 --> 00:15:25,344
if we think about computing this multi-class SVM loss

321
00:15:25,344 --> 00:15:26,986
for just this first training example

322
00:15:26,986 --> 00:15:29,558
on the left, then remember we're going to loop over

323
00:15:29,558 --> 00:15:32,713
all of the incorrect classes, so for this example,

324
00:15:32,713 --> 00:15:35,722
cat is the correct class, so we're going to loop over the car

325
00:15:35,722 --> 00:15:39,889
and frog classes, and now for car, we're going to compare the,

326
00:15:41,888 --> 00:15:45,415
we're going to look at the car score, 5.1, minus the cat score,

327
00:15:45,415 --> 00:15:49,582
3.2 plus one, when we're comparing cat and car we expect

328
00:15:50,769 --> 00:15:53,927
to incur some loss here because the car score is greater

329
00:15:53,927 --> 00:15:55,934
than the cat score which is bad.

330
00:15:55,934 --> 00:15:59,953
So for this one class, for this one example,

331
00:15:59,953 --> 00:16:02,133
we'll incur a loss of 2.9,

332
00:16:02,133 --> 00:16:04,187
and then when we go and compare the cat score

333
00:16:04,187 --> 00:16:07,312
and the frog score we see that cat is 3.2,

334
00:16:07,312 --> 00:16:09,280
frog is minus 1.7,

335
00:16:09,280 --> 00:16:12,361
so cat is more than one greater than frog,

336
00:16:12,361 --> 00:16:15,528
which means that between these two classes

337
00:16:15,528 --> 00:16:17,209
we incur zero loss.

338
00:16:17,209 --> 00:16:21,548
So then the multiclass SVM loss for this training example

339
00:16:21,548 --> 00:16:23,894
will be the sum of the losses across each of these pairs

340
00:16:23,894 --> 00:16:27,912
of classes, which will be 2.9 plus zero which is 2.9.

341
00:16:27,912 --> 00:16:31,280
Which is sort of saying that 2.9 is a quantitative measure

342
00:16:31,280 --> 00:16:32,950
of how much our classifier screwed up

343
00:16:32,950 --> 00:16:35,367
on this one training example.

344
00:16:36,595 --> 00:16:37,897
And then if we repeat this procedure

345
00:16:37,897 --> 00:16:42,374
for this next car image, then again the true class is car,

346
00:16:42,374 --> 00:16:44,851
so we're going to iterate over all the other categories

347
00:16:44,851 --> 00:16:48,005
when we compare the car and the cat score,

348
00:16:48,005 --> 00:16:50,894
we see that car is more than one greater than cat

349
00:16:50,894 --> 00:16:52,872
so we get no loss here.

350
00:16:52,872 --> 00:16:55,019
When we compare car and frog, we again see

351
00:16:55,019 --> 00:16:58,357
that the car score is more than one greater than frog,

352
00:16:58,357 --> 00:17:00,784
so we get again no loss here, and our total loss

353
00:17:00,784 --> 00:17:03,793
for this training example is zero.

354
00:17:03,793 --> 00:17:06,565
And now I think you hopefully get the picture by now, but,

355
00:17:06,565 --> 00:17:09,851
if you go look at frog, now frog, we again compare frog

356
00:17:09,851 --> 00:17:12,566
and cat, incur quite a lot of loss because the frog score

357
00:17:12,566 --> 00:17:15,696
is very low, compare frog and car, incur a lot of loss

358
00:17:15,696 --> 00:17:19,164
because the score is very low, and then our loss for this

359
00:17:19,164 --> 00:17:20,497
example is 12.9.

360
00:17:21,950 --> 00:17:24,658
And then our final loss for the entire data set

361
00:17:24,658 --> 00:17:25,964
is the average of these losses

362
00:17:25,964 --> 00:17:27,379
across the different examples,

363
00:17:27,379 --> 00:17:30,077
so when you sum those out it comes to about 5.3.

364
00:17:30,077 --> 00:17:32,330
So then it's sort of, this is our quantitative measure

365
00:17:32,330 --> 00:17:35,729
that our classifier is 5.3 bad on this data set.

366
00:17:35,729 --> 00:17:37,532
Is there a question?

367
00:17:37,532 --> 00:17:39,952
- [Student] How do you choose the plus one?

368
00:17:39,952 --> 00:17:42,720
- Yeah, the question is how do you choose the plus one?

369
00:17:42,720 --> 00:17:44,374
That's actually a really great question,

370
00:17:44,374 --> 00:17:47,462
it seems like kind of an arbitrary choice here,

371
00:17:47,462 --> 00:17:49,578
it's the only constant that appears in the loss function

372
00:17:49,578 --> 00:17:51,750
and that seems to offend your aesthetic sensibilities

373
00:17:51,750 --> 00:17:53,332
a bit maybe.

374
00:17:53,332 --> 00:17:54,650
But it turns out that this is somewhat

375
00:17:54,650 --> 00:17:58,669
of an arbitrary choice, because we don't actually care

376
00:17:58,669 --> 00:18:01,105
about the absolute values of the scores

377
00:18:01,105 --> 00:18:03,595
in this loss function, we only care

378
00:18:03,595 --> 00:18:06,189
about the relative differences between the scores.

379
00:18:06,189 --> 00:18:07,518
We only care that the correct score

380
00:18:07,518 --> 00:18:09,744
is much greater than the incorrect scores.

381
00:18:09,744 --> 00:18:12,340
So in fact if you imagine scaling up your whole W

382
00:18:12,340 --> 00:18:15,542
up or down, then it kind of rescales all the scores

383
00:18:15,542 --> 00:18:18,339
correspondingly and if you kind of work through the details

384
00:18:18,339 --> 00:18:21,608
and there's a detailed derivation of this in the course notes

385
00:18:21,608 --> 00:18:25,162
online, you find this choice of one actually doesn't matter.

386
00:18:25,162 --> 00:18:28,457
That this free parameter of one kind of washes out

387
00:18:28,457 --> 00:18:30,112
and is canceled with this scale,

388
00:18:30,112 --> 00:18:33,425
like the overall setting of the scale in W.

389
00:18:33,425 --> 00:18:35,456
And again, check the course notes for a bit more detail

390
00:18:35,456 --> 00:18:36,289
on that.

391
00:18:38,753 --> 00:18:41,174
So then I think it's kind of useful to think

392
00:18:41,174 --> 00:18:43,319
about a couple different questions to try to understand

393
00:18:43,319 --> 00:18:46,174
intuitively what this loss is doing.

394
00:18:46,174 --> 00:18:49,515
So the first question is what's going to happen to the loss

395
00:18:49,515 --> 00:18:54,149
if we change the scores of the car image just a little bit?

396
00:18:54,149 --> 00:18:54,982
Any ideas?

397
00:18:57,279 --> 00:19:00,656
Everyone's too scared to ask a question?

398
00:19:00,656 --> 00:19:01,489
Answer?

399
00:19:01,489 --> 00:19:05,072
[student speaking faintly]

400
00:19:07,783 --> 00:19:10,813
- Yeah, so the answer is that if we jiggle the scores

401
00:19:10,813 --> 00:19:14,133
for this car image a little bit, the loss will not change.

402
00:19:14,133 --> 00:19:16,426
So the SVM loss, remember, the only thing it cares

403
00:19:16,426 --> 00:19:19,873
about is getting the correct score to be greater than one

404
00:19:19,873 --> 00:19:22,718
more than the incorrect scores, but in this case,

405
00:19:22,718 --> 00:19:26,306
the car score is already quite a bit larger than the others,

406
00:19:26,306 --> 00:19:28,848
so if the scores for this class changed for this example

407
00:19:28,848 --> 00:19:31,465
changed just a little bit, this margin of one

408
00:19:31,465 --> 00:19:34,070
will still be retained and the loss will not change,

409
00:19:34,070 --> 00:19:36,237
we'll still get zero loss.

410
00:19:37,670 --> 00:19:40,117
The next question, what's the min and max possible loss

411
00:19:40,117 --> 00:19:40,950
for SVM?

412
00:19:44,065 --> 00:19:45,245
[student speaking faintly]

413
00:19:45,245 --> 00:19:46,564
Oh I hear some murmurs.

414
00:19:46,564 --> 00:19:50,174
So the minimum loss is zero, because if you can imagine that

415
00:19:50,174 --> 00:19:52,818
across all the classes, if our correct score was much

416
00:19:52,818 --> 00:19:56,333
larger then we'll incur zero loss across all the classes

417
00:19:56,333 --> 00:19:58,157
and it will be zero,

418
00:19:58,157 --> 00:20:01,452
and if you think back to this hinge loss plot that we had,

419
00:20:01,452 --> 00:20:04,295
then you can see that if the correct score

420
00:20:04,295 --> 00:20:06,992
goes very, very negative, then we could incur

421
00:20:06,992 --> 00:20:08,781
potentially infinite loss.

422
00:20:08,781 --> 00:20:12,338
So the min is zero and the max is infinity.

423
00:20:12,338 --> 00:20:15,227
Another question, sort of when you initialize these things

424
00:20:15,227 --> 00:20:16,920
and start training from scratch,

425
00:20:16,920 --> 00:20:18,815
usually you kind of initialize W

426
00:20:18,815 --> 00:20:22,042
with some small random values, as a result your scores

427
00:20:22,042 --> 00:20:24,742
tend to be sort of small uniform random values

428
00:20:24,742 --> 00:20:26,335
at the beginning of training.

429
00:20:26,335 --> 00:20:28,726
And then the question is that if all of your Ss,

430
00:20:28,726 --> 00:20:30,864
if all of the scores are approximately zero

431
00:20:30,864 --> 00:20:32,249
and approximately equal,

432
00:20:32,249 --> 00:20:33,615
then what kind of loss do you expect

433
00:20:33,615 --> 00:20:36,541
when you're using multiclass SVM?

434
00:20:36,541 --> 00:20:38,302
- [Student] Number of classes minus one.

435
00:20:38,302 --> 00:20:43,248
- Yeah, so the answer is number of classes minus one,

436
00:20:43,248 --> 00:20:46,671
because remember that if we're looping over

437
00:20:46,671 --> 00:20:49,025
all of the incorrect classes, so we're looping over

438
00:20:49,025 --> 00:20:52,289
C minus one classes, within each of those classes

439
00:20:52,289 --> 00:20:54,538
the two Ss will be about the same,

440
00:20:54,538 --> 00:20:55,896
so we'll get a loss of one

441
00:20:55,896 --> 00:20:58,336
because of the margin and we'll get C minus one.

442
00:20:58,336 --> 00:21:01,159
So this is actually kind of useful because when you,

443
00:21:01,159 --> 00:21:02,764
this is a useful debugging strategy

444
00:21:02,764 --> 00:21:04,043
when you're using these things,

445
00:21:04,043 --> 00:21:05,534
that when you start off training,

446
00:21:05,534 --> 00:21:08,882
you should think about what you expect your loss to be,

447
00:21:08,882 --> 00:21:11,731
and if the loss you actually see at the start of training

448
00:21:11,731 --> 00:21:14,584
at that first iteration is not equal to C minus one

449
00:21:14,584 --> 00:21:15,799
in this case,

450
00:21:15,799 --> 00:21:17,153
that means you probably have a bug and you should go check

451
00:21:17,153 --> 00:21:19,455
your code, so this is actually kind of a useful thing

452
00:21:19,455 --> 00:21:21,705
to be checking in practice.

453
00:21:22,686 --> 00:21:26,052
Another question, what happens if, so I said we're summing

454
00:21:26,052 --> 00:21:30,810
an SVM over the incorrect classes, what happens if the sum

455
00:21:30,810 --> 00:21:32,287
is also over the correct class

456
00:21:32,287 --> 00:21:35,123
if we just go over everything?

457
00:21:35,123 --> 00:21:36,834
- [Student] The loss increases by one.

458
00:21:36,834 --> 00:21:40,126
- Yeah, so the answer is that the loss increases by one.

459
00:21:40,126 --> 00:21:42,729
And I think the reason that we do this in practice

460
00:21:42,729 --> 00:21:45,852
is because normally loss of zero is kind of, has this nice

461
00:21:45,852 --> 00:21:48,156
interpretation that you're not losing at all,

462
00:21:48,156 --> 00:21:52,163
so that's nice, so I think your answers

463
00:21:52,163 --> 00:21:53,555
wouldn't really change,

464
00:21:53,555 --> 00:21:55,312
you would end up finding the same classifier

465
00:21:55,312 --> 00:21:57,284
if you actually looped over all the categories,

466
00:21:57,284 --> 00:22:00,779
but if just by conventions we omit the correct class

467
00:22:00,779 --> 00:22:03,529
so that our minimum loss is zero.

468
00:22:05,731 --> 00:22:07,141
So another question, what if we used mean

469
00:22:07,141 --> 00:22:08,808
instead of sum here?

470
00:22:10,743 --> 00:22:12,033
- [Student] Doesn't change.

471
00:22:12,033 --> 00:22:13,876
- Yeah, the answer is that it doesn't change.

472
00:22:13,876 --> 00:22:16,588
So the number of classes is going to be fixed ahead of time

473
00:22:16,588 --> 00:22:18,875
when we select our data set, so that's just rescaling

474
00:22:18,875 --> 00:22:21,300
the whole loss function by a constant,

475
00:22:21,300 --> 00:22:23,134
so it doesn't really matter, it'll sort of wash out

476
00:22:23,134 --> 00:22:24,791
with all the other scale things

477
00:22:24,791 --> 00:22:26,554
because we don't actually care about the true values

478
00:22:26,554 --> 00:22:29,131
of the scores, or the true value of the loss

479
00:22:29,131 --> 00:22:30,464
for that matter.

480
00:22:31,341 --> 00:22:33,510
So now here's another example, what if we change

481
00:22:33,510 --> 00:22:36,735
this loss formulation and we actually added a square term

482
00:22:36,735 --> 00:22:38,734
on top of this max?

483
00:22:38,734 --> 00:22:40,866
Would this end up being the same problem

484
00:22:40,866 --> 00:22:43,768
or would this be a different classification algorithm?

485
00:22:43,768 --> 00:22:44,978
- [Student] Different.

486
00:22:44,978 --> 00:22:46,013
- Yes, this would be different.

487
00:22:46,013 --> 00:22:48,096
So here the idea is that we're kind of changing

488
00:22:48,096 --> 00:22:49,945
the trade-offs between good and badness

489
00:22:49,945 --> 00:22:51,703
in kind of a nonlinear way,

490
00:22:51,703 --> 00:22:53,233
so this would end up actually computing

491
00:22:53,233 --> 00:22:55,064
a different loss function.

492
00:22:55,064 --> 00:22:58,096
This idea of a squared hinge loss actually does get used

493
00:22:58,096 --> 00:23:00,715
sometimes in practice, so that's kind of another trick

494
00:23:00,715 --> 00:23:02,397
to have in your bag when you're making up

495
00:23:02,397 --> 00:23:05,866
your own loss functions for your own problems.

496
00:23:05,866 --> 00:23:08,631
So now you'll end up, oh, was there a question?

497
00:23:08,631 --> 00:23:09,926
- [Student] Why would you use a squared loss

498
00:23:09,926 --> 00:23:12,396
instead of a non-squared loss?

499
00:23:12,396 --> 00:23:14,818
- Yeah, so the question is why would you ever consider

500
00:23:14,818 --> 00:23:17,560
using a squared loss instead of a non-squared loss?

501
00:23:17,560 --> 00:23:19,319
And the whole point of a loss function

502
00:23:19,319 --> 00:23:23,081
is to kind of quantify how bad are different mistakes.

503
00:23:23,081 --> 00:23:25,943
And if the classifier is making different sorts of mistakes,

504
00:23:25,943 --> 00:23:27,795
how do we weigh off the different trade-offs

505
00:23:27,795 --> 00:23:29,740
between different types of mistakes the classifier

506
00:23:29,740 --> 00:23:30,941
might make?

507
00:23:30,941 --> 00:23:33,065
So if you're using a squared loss,

508
00:23:33,065 --> 00:23:37,171
that sort of says that things that are very, very bad

509
00:23:37,171 --> 00:23:38,634
are now going to be squared bad

510
00:23:38,634 --> 00:23:39,992
so that's like really, really bad,

511
00:23:39,992 --> 00:23:41,511
like we don't want anything

512
00:23:41,511 --> 00:23:44,423
that's totally catastrophically misclassified,

513
00:23:44,423 --> 00:23:48,121
whereas if you're using this hinge loss,

514
00:23:48,121 --> 00:23:50,356
we don't actually care between being a little bit wrong

515
00:23:50,356 --> 00:23:53,353
and being a lot wrong, being a lot wrong kind of like,

516
00:23:53,353 --> 00:23:56,140
if an example is a lot wrong, and we increase it

517
00:23:56,140 --> 00:23:57,851
and make it a little bit less wrong,

518
00:23:57,851 --> 00:24:00,408
that's kind of the same goodness as an example

519
00:24:00,408 --> 00:24:03,403
which was only a little bit wrong and then increasing it

520
00:24:03,403 --> 00:24:05,175
to be a little bit more right.

521
00:24:05,175 --> 00:24:07,068
So that's a little bit hand wavy,

522
00:24:07,068 --> 00:24:09,585
but this idea of using a linear versus a square

523
00:24:09,585 --> 00:24:11,998
is a way to quantify how much we care

524
00:24:11,998 --> 00:24:14,397
about different categories of errors.

525
00:24:14,397 --> 00:24:16,175
And this is definitely something that you should think about

526
00:24:16,175 --> 00:24:18,738
when you're actually applying these things in practice,

527
00:24:18,738 --> 00:24:21,045
because the loss function is the way

528
00:24:21,045 --> 00:24:23,570
that you tell your algorithm what types of errors

529
00:24:23,570 --> 00:24:24,904
you care about and what types of errors

530
00:24:24,904 --> 00:24:27,067
it should trade off against.

531
00:24:27,067 --> 00:24:28,707
So that's actually super important in practice

532
00:24:28,707 --> 00:24:31,207
depending on your application.

533
00:24:33,115 --> 00:24:35,817
So here's just a little snippet of sort of vectorized code

534
00:24:35,817 --> 00:24:38,366
in numpy, and you'll end up implementing

535
00:24:38,366 --> 00:24:41,762
something like this for the first assignment,

536
00:24:41,762 --> 00:24:44,393
but this kind of gives you the sense that this sum

537
00:24:44,393 --> 00:24:45,752
is actually like pretty easy

538
00:24:45,752 --> 00:24:47,598
to implement in numpy, it only takes a couple lines

539
00:24:47,598 --> 00:24:49,568
of vectorized code.

540
00:24:49,568 --> 00:24:51,726
And you can see in practice, like one nice trick

541
00:24:51,726 --> 00:24:56,704
is that we can actually go in here and zero out the margins

542
00:24:56,704 --> 00:24:58,656
corresponding to the correct class,

543
00:24:58,656 --> 00:25:01,550
and that makes it easy to then just,

544
00:25:01,550 --> 00:25:05,145
that's sort of one nice vectorized trick to skip,

545
00:25:05,145 --> 00:25:06,869
iterate over all but one class.

546
00:25:06,869 --> 00:25:08,691
You just kind of zero out the one you want to skip

547
00:25:08,691 --> 00:25:10,654
and then compute the sum anyway, so that's a nice trick

548
00:25:10,654 --> 00:25:14,115
you might consider using on the assignment.

549
00:25:14,115 --> 00:25:17,906
So now, another question about this loss function.

550
00:25:17,906 --> 00:25:20,239
Suppose that you were lucky enough to find a W

551
00:25:20,239 --> 00:25:22,429
that has loss of zero, you're not losing at all,

552
00:25:22,429 --> 00:25:23,545
you're totally winning,

553
00:25:23,545 --> 00:25:24,939
this loss function is crushing it,

554
00:25:24,939 --> 00:25:28,619
but then there's a question, is this W unique

555
00:25:28,619 --> 00:25:29,857
or were there other Ws

556
00:25:29,857 --> 00:25:33,190
that could also have achieved zero loss?

557
00:25:34,051 --> 00:25:35,071
- [Student] There are other Ws.

558
00:25:35,071 --> 00:25:38,244
- Answer, yeah, so there are definitely other Ws.

559
00:25:38,244 --> 00:25:40,798
And in particular, because we talked a little bit

560
00:25:40,798 --> 00:25:44,776
about this thing of scaling the whole problem up or down

561
00:25:44,776 --> 00:25:47,509
depending on W, so you could actually take W

562
00:25:47,509 --> 00:25:51,676
multiplied by two and this doubled W (Is it quad U now?

563
00:25:52,784 --> 00:25:53,778
I don't know.)

564
00:25:53,778 --> 00:25:54,910
[laughing]

565
00:25:54,910 --> 00:25:57,401
This would also achieve zero loss.

566
00:25:57,401 --> 00:25:59,368
So as a concrete example of this,

567
00:25:59,368 --> 00:26:00,866
you can go back to your favorite example

568
00:26:00,866 --> 00:26:01,984
and maybe work through the numbers

569
00:26:01,984 --> 00:26:03,151
a little bit later,

570
00:26:03,151 --> 00:26:06,109
but if you're taking W and we double W,

571
00:26:06,109 --> 00:26:09,929
then the margins between the correct and incorrect scores

572
00:26:09,929 --> 00:26:11,383
will also double.

573
00:26:11,383 --> 00:26:12,990
So that means that if all these margins

574
00:26:12,990 --> 00:26:15,321
were already greater than one, and we doubled them,

575
00:26:15,321 --> 00:26:17,074
they're still going to be greater than one,

576
00:26:17,074 --> 00:26:19,657
so you'll still have zero loss.

577
00:26:20,980 --> 00:26:22,982
And this is kind of interesting,

578
00:26:22,982 --> 00:26:25,372
because if our loss function

579
00:26:25,372 --> 00:26:28,228
is the way that we tell our classifier which W we want

580
00:26:28,228 --> 00:26:29,845
and which W we care about,

581
00:26:29,845 --> 00:26:31,133
this is a little bit weird,

582
00:26:31,133 --> 00:26:32,595
now there's this inconsistency

583
00:26:32,595 --> 00:26:35,997
and how is the classifier to choose

584
00:26:35,997 --> 00:26:37,662
between these different versions of W

585
00:26:37,662 --> 00:26:39,912
that all achieve zero loss?

586
00:26:40,899 --> 00:26:43,010
And that's because what we've done here

587
00:26:43,010 --> 00:26:46,103
is written down only a loss in terms of the data,

588
00:26:46,103 --> 00:26:48,936
and we've only told our classifier

589
00:26:49,782 --> 00:26:51,248
that it should try to find the W

590
00:26:51,248 --> 00:26:53,039
that fits the training data.

591
00:26:53,039 --> 00:26:55,033
But really in practice, we don't actually care

592
00:26:55,033 --> 00:26:57,265
that much about fitting the training data,

593
00:26:57,265 --> 00:26:58,560
the whole point of machine learning

594
00:26:58,560 --> 00:27:02,353
is that we use the training data to find some classifier

595
00:27:02,353 --> 00:27:05,022
and then we'll apply that thing on test data.

596
00:27:05,022 --> 00:27:07,658
So we don't really care about the training data performance,

597
00:27:07,658 --> 00:27:09,303
we really care about the performance

598
00:27:09,303 --> 00:27:11,728
of this classifier on test data.

599
00:27:11,728 --> 00:27:13,585
So as a result, if the only thing

600
00:27:13,585 --> 00:27:15,471
we're telling our classifier to do

601
00:27:15,471 --> 00:27:16,976
is fit the training data,

602
00:27:16,976 --> 00:27:18,853
then we can lead ourselves

603
00:27:18,853 --> 00:27:21,107
into some of these weird situations sometimes,

604
00:27:21,107 --> 00:27:24,879
where the classifier might have unintuitive behavior.

605
00:27:24,879 --> 00:27:28,586
So a concrete, canonical example of this sort of thing,

606
00:27:28,586 --> 00:27:30,785
by the way, this is not linear classification anymore,

607
00:27:30,785 --> 00:27:32,226
this is a little bit of a more general

608
00:27:32,226 --> 00:27:33,718
machine learning concept,

609
00:27:33,718 --> 00:27:36,615
is that suppose we have this data set of blue points,

610
00:27:36,615 --> 00:27:39,468
and we're going to fit some curve to the training data,

611
00:27:39,468 --> 00:27:40,627
the blue points,

612
00:27:40,627 --> 00:27:43,583
then if the only thing we've told our classifier to do

613
00:27:43,583 --> 00:27:45,332
is to try and fit the training data,

614
00:27:45,332 --> 00:27:47,251
it might go in and have very wiggly curves

615
00:27:47,251 --> 00:27:48,612
to try to perfectly classify

616
00:27:48,612 --> 00:27:50,447
all of the training data points.

617
00:27:50,447 --> 00:27:53,035
But this is bad, because we don't actually care

618
00:27:53,035 --> 00:27:54,319
about this performance,

619
00:27:54,319 --> 00:27:57,129
we care about the performance on the test data.

620
00:27:57,129 --> 00:27:59,179
So now if we have some new data come in

621
00:27:59,179 --> 00:28:01,510
that sort of follows the same trend,

622
00:28:01,510 --> 00:28:03,067
then this very wiggly blue line

623
00:28:03,067 --> 00:28:04,672
is going to be totally wrong.

624
00:28:04,672 --> 00:28:06,401
And in fact, what we probably would have preferred

625
00:28:06,401 --> 00:28:08,606
the classifier to do was maybe predict

626
00:28:08,606 --> 00:28:10,134
this straight green line,

627
00:28:10,134 --> 00:28:12,483
rather than this very complex wiggly line

628
00:28:12,483 --> 00:28:15,971
to perfectly fit all the training data.

629
00:28:15,971 --> 00:28:18,621
And this is a core fundamental problem

630
00:28:18,621 --> 00:28:20,032
in machine learning,

631
00:28:20,032 --> 00:28:21,462
and the way we usually solve it,

632
00:28:21,462 --> 00:28:23,616
is this concept of regularization.

633
00:28:23,616 --> 00:28:25,959
So here we're going to add an additional term

634
00:28:25,959 --> 00:28:27,243
to the loss function.

635
00:28:27,243 --> 00:28:28,632
In addition to the data loss,

636
00:28:28,632 --> 00:28:31,055
which will tell our classifier that it should fit

637
00:28:31,055 --> 00:28:33,248
the training data, we'll also typically add

638
00:28:33,248 --> 00:28:35,109
another term to the loss function

639
00:28:35,109 --> 00:28:36,857
called a regularization term,

640
00:28:36,857 --> 00:28:41,491
which encourages the model to somehow pick a simpler W,

641
00:28:41,491 --> 00:28:43,292
where the concept of simple

642
00:28:43,292 --> 00:28:46,792
kind of depends on the task and the model.

643
00:28:48,725 --> 00:28:50,439
There's this whole idea of Occam's Razor,

644
00:28:50,439 --> 00:28:53,668
which is this fundamental idea in scientific discovery

645
00:28:53,668 --> 00:28:56,374
more broadly, which is that if you have many different

646
00:28:56,374 --> 00:28:58,513
competing hypotheses, that could explain

647
00:28:58,513 --> 00:28:59,958
your observations,

648
00:28:59,958 --> 00:29:01,839
you should generally prefer the simpler one,

649
00:29:01,839 --> 00:29:04,199
because that's the explanation that is more likely

650
00:29:04,199 --> 00:29:07,601
to generalize to new observations in the future.

651
00:29:07,601 --> 00:29:09,514
And the way we operationalize this intuition

652
00:29:09,514 --> 00:29:11,777
in machine learning is typically through some explicit

653
00:29:11,777 --> 00:29:13,338
regularization penalty

654
00:29:13,338 --> 00:29:15,921
that's often written down as R.

655
00:29:17,112 --> 00:29:19,487
So then your standard loss function

656
00:29:19,487 --> 00:29:21,209
usually has these two terms,

657
00:29:21,209 --> 00:29:23,217
a data loss and a regularization loss,

658
00:29:23,217 --> 00:29:25,930
and there's some hyper-parameter here, lambda,

659
00:29:25,930 --> 00:29:28,000
that trades off between the two.

660
00:29:28,000 --> 00:29:29,752
And we talked about hyper-parameters

661
00:29:29,752 --> 00:29:31,854
and cross-validation in the last lecture,

662
00:29:31,854 --> 00:29:34,620
so this regularization hyper-parameter lambda

66