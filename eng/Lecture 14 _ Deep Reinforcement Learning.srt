1
00:00:09,784 --> 00:00:11,867
- Okay let's get started.

2
00:00:13,038 --> 00:00:15,888
Alright, so welcome to lecture 14,

3
00:00:15,888 --> 00:00:20,884
and today we'll be talking
about reinforcement learning.

4
00:00:20,884 --> 00:00:23,222
So some administrative details first,

5
00:00:23,222 --> 00:00:30,346
update on grades. Midterm grades were released last night, so
see Piazza for more information and statistics about that.

6
00:00:30,346 --> 00:00:35,402
And we also have A2 and milestone grades
scheduled for later this week.

7
00:00:36,768 --> 00:00:40,682
Also, about your projects, all teams
must register your projects.

8
00:00:40,682 --> 00:00:47,580
So on Piazza we have a form posted, so you should go there and this is
required, every team should go and fill out this form with information

9
00:00:47,580 --> 00:00:53,214
about your project, that we'll use for
final grading and the poster session.

10
00:00:53,214 --> 00:01:01,779
And the Tiny ImageNet evaluation servers are also now online
for those of you who are doing the Tiny ImageNet challenge.

11
00:01:01,779 --> 00:01:06,193
We also have a link to a course survey on
Piazza that was released a few days ago,

12
00:01:06,193 --> 00:01:13,600
so, please fill it out if you guys haven't already. We'd love
to have your feedback and know how we can improve this class.

13
00:01:16,589 --> 00:01:19,650
Okay, so the topic of today,
reinforcement learning.

14
00:01:19,650 --> 00:01:22,544
Alright, so so far we've talked
about supervised learning,

15
00:01:22,544 --> 00:01:30,498
which is about a type of problem where we have data x and then we have
labels y and our goal is to learn a function that is mapping from x to y.

16
00:01:30,498 --> 00:01:35,067
So, for example, the classification
problem that we've been working with.

17
00:01:35,067 --> 00:01:37,753
We also talked last lecture
about unsupervised learning,

18
00:01:37,753 --> 00:01:45,362
which is the problem where we have just data and no labels, and our
goal is to learn some underlying, hidden structure of the data.

19
00:01:45,362 --> 00:01:50,528
So, an example of this is the generative
models that we talked about last lecture.

20
00:01:52,040 --> 00:01:57,370
And so today we're going to talk about a different kind
of problem set-up, the reinforcement learning problem.

21
00:01:57,370 --> 00:02:01,824
And so here we have an agent
that can take actions in its environment,

22
00:02:01,824 --> 00:02:04,352
and it can receive rewards
for for its action.

23
00:02:04,352 --> 00:02:09,959
And its goal is going to be to learn how to take
actions in a way that can maximize its reward.

24
00:02:09,959 --> 00:02:14,101
And so we'll talk about this
in a lot more detail today.

25
00:02:14,101 --> 00:02:18,116
So, the outline for today, we're going to first
talk about the reinforcement learning problem,

26
00:02:18,116 --> 00:02:20,927
and then we'll talk about
Markov decision processes,

27
00:02:20,927 --> 00:02:24,747
which is a formalism of the
reinforcement learning problem,

28
00:02:24,747 --> 00:02:31,095
and then we'll talk about two major classes of
RL algorithms, Q-learning and policy gradients.

29
00:02:32,876 --> 00:02:38,936
So, in the reinforcement learning set up, what we
have is we have an agent and we have an environment.

30
00:02:38,936 --> 00:02:43,268
And so the environment
gives the agent a state.

31
00:02:43,268 --> 00:02:46,877
In turn, the agent is
going to take an action,

32
00:02:46,877 --> 00:02:52,609
and then the environment is going to give
back a reward, as well as the next state.

33
00:02:52,609 --> 00:03:00,918
And so this is going to keep going on in this loop, on and on, until the
environment gives back a terminal state, which then ends the episode.

34
00:03:00,918 --> 00:03:03,401
So, let's see some examples of this.

35
00:03:03,401 --> 00:03:05,536
First we have here the cart-pole problem,

36
00:03:05,536 --> 00:03:11,142
which is a classic problem that some of you
may have seen, in, for example, 229 before.

37
00:03:11,142 --> 00:03:16,252
And so this objective here is that you want
to balance a pole on top of a movable cart.

38
00:03:16,252 --> 00:03:20,280
Alright, so the state that you have here
is your current description of the system.

39
00:03:20,280 --> 00:03:28,206
So, for example, angular, angular speed of your pole,
your position, and the horizontal velocity of your cart.

40
00:03:28,206 --> 00:03:33,224
And the actions you can take are horizontal
forces that you apply onto the cart, right?

41
00:03:33,224 --> 00:03:38,387
So you're basically trying to move this cart
around to try and balance this pole on top of it.

42
00:03:38,387 --> 00:03:43,990
And the reward that you're getting from this environment
is one at each time step if your pole is upright.

43
00:03:43,990 --> 00:03:48,143
So you basically want to keep this pole
balanced for as long as you can.

44
00:03:49,286 --> 00:03:52,192
Okay, so here's another example
of a classic RL problem.

45
00:03:52,192 --> 00:03:53,998
Here is robot locomotion.

46
00:03:53,998 --> 00:03:59,670
So we have here an example of a humanoid
robot, as well as an ant robot model.

47
00:03:59,670 --> 00:04:03,128
And our objective here is to
make the robot move forward.

48
00:04:03,128 --> 00:04:10,807
And so the state that we have describing our system is the
angle and the positions of all the joints of our robots.

49
00:04:10,807 --> 00:04:15,887
And then the actions that we can take are
the torques applied onto these joints,

50
00:04:15,887 --> 00:04:21,228
right, and so these are trying to make the robot
move forward and then the reward that we get is

51
00:04:21,228 --> 00:04:31,701
our forward movement as well as, I think, in the time of, in the case of the humanoid, also,
you can have something like a reward of one for each time step that this robot is upright.

52
00:04:33,521 --> 00:04:38,384
So, games are also a big class of
problems that can be formulated with RL.

53
00:04:38,384 --> 00:04:40,700
So, for example, here we have Atari games

54
00:04:40,700 --> 00:04:44,280
which are a classic success
of deep reinforcement learning

55
00:04:44,280 --> 00:04:48,574
and so here the objective is to complete these
games with the highest possible score, right.

56
00:04:48,574 --> 00:04:52,753
So, your agent is basically a player
that's trying to play these games.

57
00:04:52,753 --> 00:04:57,506
And the state that you have is going to be
the raw pixels of the game state.

58
00:04:57,506 --> 00:05:02,882
Right, so these are just the pixels on the screen
that you would see as you're playing the game.

59
00:05:02,882 --> 00:05:09,912
And then the actions that you have are your game controls, so for
example, in some games maybe moving left to right, up or down.

60
00:05:09,912 --> 00:05:12,534
And then the score that you
have is your score increase

61
00:05:12,534 --> 00:05:15,667
or decrease at each time step,
and your goal is going to be

62
00:05:15,667 --> 00:05:19,834
to maximize your total score
over the course of the game.

63
00:05:21,312 --> 00:05:24,179
And, finally, here we have
another example of a game here.

64
00:05:24,179 --> 00:05:25,587
It's

65
00:05:25,587 --> 00:05:26,587
Go, which is

66
00:05:27,573 --> 00:05:28,893
something that was a

67
00:05:28,893 --> 00:05:31,697
huge achievement of deep
reinforcement learning last year,

68
00:05:31,697 --> 00:05:34,721
when Deep Minds AlphaGo beats Lee Sedol,

69
00:05:34,721 --> 00:05:36,867
which is one of the

70
00:05:36,867 --> 00:05:38,589
best Go players of the last few years,

71
00:05:38,589 --> 00:05:41,685
and this is actually in the news again

72
00:05:41,685 --> 00:05:45,667
for, as some of you may have
seen, there's another Go

73
00:05:45,667 --> 00:05:47,529
competition going on now with

74
00:05:47,529 --> 00:05:50,919
AlphaGo versus a top-ranked Go player.

75
00:05:50,919 --> 00:05:53,495
And so the objective here is to

76
00:05:53,495 --> 00:05:56,295
win the game, and our
state is the position

77
00:05:56,295 --> 00:05:58,349
of all the pieces, the action
is where to put the next

78
00:05:58,349 --> 00:06:02,062
piece down, and the reward
is, one, if you win at the end

79
00:06:02,062 --> 00:06:03,912
of the game, and zero otherwise.

80
00:06:03,912 --> 00:06:05,032
And we'll also talk about this one

81
00:06:05,032 --> 00:06:08,411
in a little bit more detail, later.

82
00:06:08,411 --> 00:06:09,891
Okay, so

83
00:06:09,891 --> 00:06:12,046
how can we mathematically formalize

84
00:06:12,046 --> 00:06:13,330
the RL problem, right?

85
00:06:13,330 --> 00:06:15,817
This loop that we talked about earlier,

86
00:06:15,817 --> 00:06:18,051
of environments giving agents states,

87
00:06:18,051 --> 00:06:20,634
and then agents taking actions.

88
00:06:22,394 --> 00:06:24,884
So, a Markov decision process is

89
00:06:24,884 --> 00:06:28,512
the mathematical formulation
of the RL problem,

90
00:06:28,512 --> 00:06:31,447
and an MDP satisfies the Markov property,

91
00:06:31,447 --> 00:06:33,054
which is that the current state completely

92
00:06:33,054 --> 00:06:36,107
characterizes the state of the world.

93
00:06:36,107 --> 00:06:40,164
And an MDP here is defined
by tuple of objects,

94
00:06:40,164 --> 00:06:43,170
consisting of S, which is
the set of possible states.

95
00:06:43,170 --> 00:06:45,762
We have A, our set of possible actions,

96
00:06:45,762 --> 00:06:50,018
we also have R, our
distribution of our reward,

97
00:06:50,018 --> 00:06:51,694
given a state, action pair,

98
00:06:51,694 --> 00:06:53,824
so it's a function
mapping from state action

99
00:06:53,824 --> 00:06:55,323
to your reward.

100
00:06:55,323 --> 00:06:57,430
You also have P, which is
a transition probability

101
00:06:57,430 --> 00:07:00,079
distribution over your
next state, that you're

102
00:07:00,079 --> 00:07:02,940
going to transition to given
your state, action pair.

103
00:07:02,940 --> 00:07:05,718
And then finally we have a
Gamma, a discount factor,

104
00:07:05,718 --> 00:07:09,720
which is basically
saying how much we value

105
00:07:09,720 --> 00:07:12,970
rewards coming up soon versus later on.

106
00:07:14,203 --> 00:07:17,395
So, the way the Markov
Decision Process works is that

107
00:07:17,395 --> 00:07:20,053
at our initial time step t equals zero,

108
00:07:20,053 --> 00:07:21,523
the environment is going to sample some

109
00:07:21,523 --> 00:07:24,615
initial state as zero, from
the initial state distribution,

110
00:07:24,615 --> 00:07:26,363
p of s zero.

111
00:07:26,363 --> 00:07:29,271
And then, once it has that,
then from time t equals zero

112
00:07:29,271 --> 00:07:32,253
until it's done, we're going
to iterate through this loop

113
00:07:32,253 --> 00:07:35,797
where the agent is going to
select an action, a sub t.

114
00:07:35,797 --> 00:07:38,885
The environment is going to
sample a reward from here,

115
00:07:38,885 --> 00:07:41,907
so reward given your state and the

116
00:07:41,907 --> 00:07:44,032
action that you just took.

117
00:07:44,032 --> 00:07:47,640
It's also going to sample the next state,

118
00:07:47,640 --> 00:07:51,534
at time t plus one, given
your probability distribution

119
00:07:51,534 --> 00:07:54,467
and then the agent is going to receive

120
00:07:54,467 --> 00:07:56,790
the reward, as well as the
next state, and then we're

121
00:07:56,790 --> 00:07:58,707
going to through this process again,

122
00:07:58,707 --> 00:08:01,769
and keep looping; agent
will select the next action,

123
00:08:01,769 --> 00:08:05,542
and so on until the episode is over.

124
00:08:05,542 --> 00:08:06,989
Okay, so

125
00:08:06,989 --> 00:08:10,724
now based on this, we
can define a policy pi,

126
00:08:10,724 --> 00:08:13,593
which is a function from
your states to your actions

127
00:08:13,593 --> 00:08:16,651
that specifies what action
to take in each state.

128
00:08:16,651 --> 00:08:19,748
And this can be either
deterministic or stochastic.

129
00:08:19,748 --> 00:08:22,447
And our objective now is
to going to be to find

130
00:08:22,447 --> 00:08:24,727
your optimal policy pi
star, that maximizes your

131
00:08:24,727 --> 00:08:27,205
cumulative discounted reward.

132
00:08:27,205 --> 00:08:29,059
So we can see here we have our

133
00:08:29,059 --> 00:08:31,813
some of our future
rewards, which can be also

134
00:08:31,813 --> 00:08:35,509
discounted by your discount factor.

135
00:08:35,509 --> 00:08:39,327
So, let's look at an
example of a simple MDP.

136
00:08:39,327 --> 00:08:42,034
And here we have Grid World, which is this

137
00:08:42,034 --> 00:08:44,533
task where we have this grid of states.

138
00:08:44,533 --> 00:08:46,950
So you can be in any of these

139
00:08:48,112 --> 00:08:50,295
cells of your grid, which are your states.

140
00:08:50,295 --> 00:08:52,613
And you can take actions from your states,

141
00:08:52,613 --> 00:08:54,713
and so these actions are going to be

142
00:08:54,713 --> 00:08:56,527
simple movements, moving to your right,

143
00:08:56,527 --> 00:08:59,299
to your left, up or down.

144
00:08:59,299 --> 00:09:02,683
And you're going to get a
negative reward for each

145
00:09:02,683 --> 00:09:07,163
transition or each time step,
basically, that happens.

146
00:09:07,163 --> 00:09:08,859
Each movement that you take,

147
00:09:08,859 --> 00:09:11,989
and this can be something
like R equals negative one.

148
00:09:11,989 --> 00:09:13,871
And so your objective is going to be

149
00:09:13,871 --> 00:09:15,588
to reach one of the terminal states,

150
00:09:15,588 --> 00:09:17,793
which are the gray states shown here,

151
00:09:17,793 --> 00:09:20,055
in the least number of actions.

152
00:09:20,055 --> 00:09:22,249
Right, so the longer
that you take to reach

153
00:09:22,249 --> 00:09:23,522
your terminal state, you're going to keep

154
00:09:23,522 --> 00:09:26,522
accumulating these negative rewards.

155
00:09:27,625 --> 00:09:30,540
Okay, so if you look at
a random policy here,

156
00:09:30,540 --> 00:09:33,141
a random policy would
consist of, basically,

157
00:09:33,141 --> 00:09:35,305
at any given state or cell that you're in

158
00:09:35,305 --> 00:09:37,770
just sampling randomly which direction

159
00:09:37,770 --> 00:09:39,090
that you're going to move in next.

160
00:09:39,090 --> 00:09:41,843
Right, so all of these
have equal probability.

161
00:09:41,843 --> 00:09:44,115
On the other hand, an optimal policy that

162
00:09:44,115 --> 00:09:46,518
we would like to have is

163
00:09:46,518 --> 00:09:48,672
basically taking the action, the direction

164
00:09:48,672 --> 00:09:51,866
that will move us closest
to a terminal state.

165
00:09:51,866 --> 00:09:53,164
So you can see here,

166
00:09:53,164 --> 00:09:54,808
if we're right next to one of the

167
00:09:54,808 --> 00:09:56,156
terminal states we should

168
00:09:56,156 --> 00:09:57,506
always move in the direction

169
00:09:57,506 --> 00:09:59,171
that gets us to this terminal state.

170
00:09:59,171 --> 00:10:01,385
And otherwise, if you're in
one of these other states,

171
00:10:01,385 --> 00:10:03,822
you want to take the
direction that will take you

172
00:10:03,822 --> 00:10:06,405
closest to one of these states.

173
00:10:09,119 --> 00:10:11,644
Okay, so now given this

174
00:10:11,644 --> 00:10:13,745
description of our MDP, what we want to do

175
00:10:13,745 --> 00:10:17,155
is we want to find our
optimal policy pi star.

176
00:10:17,155 --> 00:10:20,755
Right, our policy that's
maximizing the sum of the rewards.

177
00:10:20,755 --> 00:10:22,955
And so this optimal policy
is going to tell us,

178
00:10:22,955 --> 00:10:25,655
given any state that we're
in, what is the action that

179
00:10:25,655 --> 00:10:27,851
we should take in order
to maximize the sum

180
00:10:27,851 --> 00:10:29,731
of the rewards that we'll get.

181
00:10:29,731 --> 00:10:32,011
And so one question is how do we

182
00:10:32,011 --> 00:10:34,091
handle the randomness in the MDP, right?

183
00:10:34,091 --> 00:10:36,459
We have randomness in

184
00:10:36,459 --> 00:10:39,073
terms of our initial
state that we're sampling,

185
00:10:39,073 --> 00:10:40,727
in therms of this transition probability

186
00:10:40,727 --> 00:10:42,303
distribution that will give us

187
00:10:42,303 --> 00:10:46,341
distribution of our
next states, and so on.

188
00:10:46,341 --> 00:10:49,292
Also what we'll do is we'll
work, then, with maximizing

189
00:10:49,292 --> 00:10:51,947
our expected sum of the rewards.

190
00:10:51,947 --> 00:10:55,451
So, formally, we can write
our optimal policy pi star

191
00:10:55,451 --> 00:10:59,129
as maximizing this expected
sum of future rewards

192
00:10:59,129 --> 00:11:02,957
over policy's pi, where
we have our initial state

193
00:11:02,957 --> 00:11:05,103
sampled from our state distribution.

194
00:11:05,103 --> 00:11:07,388
We have our actions,

195
00:11:07,388 --> 00:11:09,127
sampled from our policy, given the state.

196
00:11:09,127 --> 00:11:11,929
And then we have our next states sampled

197
00:11:11,929 --> 00:11:16,423
from our transition
probability distributions.

198
00:11:16,423 --> 00:11:17,256
Okay, so

199
00:11:18,351 --> 00:11:19,668
before we talk about

200
00:11:19,668 --> 00:11:22,143
exactly how we're going
to find this policy,

201
00:11:22,143 --> 00:11:26,787
let's first talk about a few definitions
that's going to be helpful for us in doing so.

202
00:11:26,787 --> 00:11:31,405
So, specifically, the value function
and the Q-value function.

203
00:11:31,405 --> 00:11:37,425
So, as we follow the policy, we're going to sample
trajectories or paths, right, for every episode.

204
00:11:37,426 --> 00:11:43,611
And we're going to have our initial state as zero,
a-zero, r-zero, s-one, a-one, r-one, and so on.

205
00:11:43,611 --> 00:11:49,331
We're going to have this trajectory of
states, actions, and rewards that we get.

206
00:11:49,331 --> 00:11:52,613
And so, how good is a state
that we're currently in?

207
00:11:52,613 --> 00:12:10,799
Well, the value function at any state s, is the expected cumulative reward following the policy from state s, from here
on out. Right, so it's going to be expected value of our expected cumulative reward, starting from our current state.

208
00:12:10,800 --> 00:12:13,286
And then how good is a state, action pair?

209
00:12:13,286 --> 00:12:17,370
So how good is taking action a in state s?

210
00:12:17,370 --> 00:12:20,468
And we define this using
a Q-value function,

211
00:12:20,468 --> 00:12:23,574
which is, the expected
cumulative reward from taking

212
00:12:23,574 --> 00:12:27,741
action a in state s and
then following the policy.

213
00:12:29,708 --> 00:12:32,708
Right, so then, the
optimal Q-value function

214
00:12:32,708 --> 00:12:36,404
that we can get is going to be
Q star, which is the maximum

215
00:12:36,404 --> 00:12:39,216
expected cumulative reward that we can get

216
00:12:39,216 --> 00:12:43,383
from a given state action
pair, defined here.

217
00:12:45,099 --> 00:12:48,592
So now we're going to
see one important thing

218
00:12:48,592 --> 00:12:50,018
in reinforcement learning,

219
00:12:50,018 --> 00:12:52,018
which is called the Bellman equation.

220
00:12:52,018 --> 00:12:54,485
So let's consider this a Q-value function

221
00:12:54,485 --> 00:12:57,697
from the optimal policy Q star,

222
00:12:57,697 --> 00:13:00,911
which is then going to
satisfy this Bellman equation,

223
00:13:00,911 --> 00:13:03,533
which is this identity shown here,

224
00:13:03,533 --> 00:13:05,194
and what this means is that

225
00:13:05,194 --> 00:13:08,873
given any state, action pair, s and a,

226
00:13:08,873 --> 00:13:11,748
the value of this pair
is going to be the reward

227
00:13:11,748 --> 00:13:15,092
that you're going to get, r,
plus the value of whatever

228
00:13:15,092 --> 00:13:16,517
state that you end up in.

229
00:13:16,517 --> 00:13:18,868
So, let's say, s prime.

230
00:13:18,868 --> 00:13:22,319
And since we know that we
have the optimal policy,

231
00:13:22,319 --> 00:13:24,150
then we also know that we're going to

232
00:13:24,150 --> 00:13:26,202
play the best action that we can,

233
00:13:26,202 --> 00:13:28,746
right, at our state s prime.

234
00:13:28,746 --> 00:13:31,413
And so then, the value at state s prime

235
00:13:31,413 --> 00:13:34,432
is just going to be the
maximum over our actions,

236
00:13:34,432 --> 00:13:38,626
a prime, of Q star at s prime, a prime.

237
00:13:38,626 --> 00:13:41,325
And so then we get this

238
00:13:41,325 --> 00:13:44,119
identity here, for optimal Q-value.

239
00:13:44,119 --> 00:13:46,753
Right, and then also, as always, we have

240
00:13:46,753 --> 00:13:48,075
this expectation here,

241
00:13:48,075 --> 00:13:49,880
because we have randomness over what state

242
00:13:49,880 --> 00:13:52,380
that we're going to end up in.

243
00:13:54,252 --> 00:13:56,782
And then we can also
infer, from here, that our

244
00:13:56,782 --> 00:13:58,928
optimal policy, right, is going to consist

245
00:13:58,928 --> 00:14:00,860
of taking the best action in any state,

246
00:14:00,860 --> 00:14:02,488
as specified by Q star.

247
00:14:02,488 --> 00:14:04,295
Q star is going to tell us

248
00:14:04,295 --> 00:14:05,462
of the maximum

249
00:14:06,540 --> 00:14:08,437
future reward that we can
get from any of our actions,

250
00:14:08,437 --> 00:14:09,456
so we should just

251
00:14:09,456 --> 00:14:11,356
take a policy that's following this

252
00:14:11,356 --> 00:14:13,615
and just taking the action that's

253
00:14:13,615 --> 00:14:16,863
going to lead to best reward.

254
00:14:16,863 --> 00:14:21,025
Okay, so how can we solve
for this optimal policy?

255
00:14:21,025 --> 00:14:23,381
So, one way we can solve for this is

256
00:14:23,381 --> 00:14:25,692
something called a value
iteration algorithm,

257
00:14:25,692 --> 00:14:28,046
where we're going to use
this Bellman equation

258
00:14:28,046 --> 00:14:29,527
as an iterative update.

259
00:14:29,527 --> 00:14:33,830
So at each step, we're going
to refine our approximation

260
00:14:33,830 --> 00:14:37,997
of Q star by trying to
enforce the Bellman equation.

261
00:14:39,347 --> 00:14:42,602
And so, under some
mathematical conditions,

262
00:14:42,602 --> 00:14:45,602
we also know that this sequence Q, i

263
00:14:47,008 --> 00:14:49,569
of our Q-function is going
to converge to our optimal

264
00:14:49,569 --> 00:14:52,236
Q star as i approaches infinity.

265
00:14:54,257 --> 00:14:55,579
And so this, this works well,

266
00:14:55,579 --> 00:14:58,329
but what's the problem with this?

267
00:14:59,184 --> 00:15:01,887
Well, an important problem
is that this is not scalable.

268
00:15:01,887 --> 00:15:02,720
Right?

269
00:15:02,720 --> 00:15:03,553
We have to compute

270
00:15:03,553 --> 00:15:05,793
Q of s, a here for
every state, action pair

271
00:15:05,793 --> 00:15:08,597
in order to make our iterative updates.

272
00:15:08,597 --> 00:15:10,382
Right, but then this is a problem if,

273
00:15:10,382 --> 00:15:13,049
for example, if we look at these

274
00:15:14,021 --> 00:15:15,865
the state of, for example, an Atari game

275
00:15:15,865 --> 00:15:17,519
that we had earlier, it's going to be

276
00:15:17,519 --> 00:15:18,933
your screen of pixels.

277
00:15:18,933 --> 00:15:22,229
And this is a huge state
space, and it's basically

278
00:15:22,229 --> 00:15:23,865
computationally infeasible

279
00:15:23,865 --> 00:15:27,448
to compute this for
the entire state space.

280
00:15:28,725 --> 00:15:31,200
Okay, so what's the solution to this?

281
00:15:31,200 --> 00:15:33,141
Well, we can use a function approximator

282
00:15:33,141 --> 00:15:35,908
to estimate Q of s, a

283
00:15:35,908 --> 00:15:37,620
so, for example, a neural network, right.

284
00:15:37,620 --> 00:15:40,400
So, we've seen before that
any time, if we have some

285
00:15:40,400 --> 00:15:42,367
really complex function that
don't know, that we want

286
00:15:42,367 --> 00:15:44,360
to estimate, a neural network is

287
00:15:44,360 --> 00:15:46,693
a good way to estimate this.

288
00:15:48,472 --> 00:15:51,458
Okay, so this is going to take us to our

289
00:15:51,458 --> 00:15:54,242
formulation of Q-learning
that we're going to look at.

290
00:15:54,242 --> 00:15:56,646
And so, what we're going
to do is we're going

291
00:15:56,646 --> 00:15:58,906
to use a function approximator

292
00:15:58,906 --> 00:16:02,118
in order to estimate our
action value function.

293
00:16:02,118 --> 00:16:02,951
Right?

294
00:16:02,951 --> 00:16:04,502
And if this function approximator

295
00:16:04,502 --> 00:16:06,013
is a deep neural network, which is

296
00:16:06,013 --> 00:16:08,142
what's been used recently,

297
00:16:08,142 --> 00:16:10,782
then this is going to be
called deep Q-learning.

298
00:16:10,782 --> 00:16:12,322
And so this is something that

299
00:16:12,322 --> 00:16:15,742
you'll hear around as one
of the common approaches

300
00:16:15,742 --> 00:16:20,150
to deep reinforcement
learning that's in use.

301
00:16:20,150 --> 00:16:21,259
Right, and so in this case,

302
00:16:21,259 --> 00:16:23,474
we also have our function parameters

303
00:16:23,474 --> 00:16:26,134
theta here, so our Q-value function

304
00:16:26,134 --> 00:16:28,348
is determined by these weights,

305
00:16:28,348 --> 00:16:30,765
theta, of our neural network.

306
00:16:33,050 --> 00:16:35,425
Okay, so given this
function approximation,

307
00:16:35,425 --> 00:16:37,970
how do we solve for our optimal policy?

308
00:16:37,970 --> 00:16:39,814
So remember that we want to find

309
00:16:39,814 --> 00:16:44,744
a Q-function that's satisfying
the Bellman equation.

310
00:16:44,744 --> 00:16:47,017
Right, and so we want to
enforce this Bellman equation

311
00:16:47,017 --> 00:16:50,452
to happen, so what we
can do when we have this

312
00:16:50,452 --> 00:16:54,713
neural network approximating
our Q-function is that

313
00:16:54,713 --> 00:16:56,811
we can train this where our loss function

314
00:16:56,811 --> 00:16:58,169
is going to try and minimize

315
00:16:58,169 --> 00:17:00,240
the error of our Bellman equation, right?

316
00:17:00,240 --> 00:17:03,689
Or how far q of s, a is from its target,

317
00:17:03,689 --> 00:17:06,454
which is the Y_i here,
the right hand side

318
00:17:06,454 --> 00:17:09,853
of the Bellman equation
that we saw earlier.

319
00:17:09,853 --> 00:17:12,103
So, we're basically going to take these

320
00:17:12,103 --> 00:17:13,994
forward passes of our

321
00:17:13,994 --> 00:17:16,928
loss function, trying
to minimize this error

322
00:17:16,929 --> 00:17:19,332
and then our backward
pass, our gradient update,

323
00:17:19,332 --> 00:17:20,863
is just going to be

324
00:17:20,863 --> 00:17:23,243
you just take the gradient of this

325
00:17:23,243 --> 00:17:28,182
loss, with respect to our
network parameter's theta.

326
00:17:28,183 --> 00:17:31,568
Right, and so our goal is again to

327
00:17:31,568 --> 00:17:33,752
have this effect as we're
taking gradient steps

328
00:17:33,752 --> 00:17:36,107
of iteratively trying
to make our Q-function

329
00:17:36,107 --> 00:17:38,436
closer to our target value.

330
00:17:38,436 --> 00:17:40,853
So, any questions about this?

331
00:17:42,691 --> 00:17:43,524
Okay.

332
00:17:44,537 --> 00:17:48,719
So let's look at a case
study of an example where

333
00:17:48,719 --> 00:17:50,824
one of the classic examples
of deep reinforcement learning

334
00:17:50,824 --> 00:17:53,370
where this approach was applied.

335
00:17:53,370 --> 00:17:56,174
And so we're going to look at
this problem that we saw earlier

336
00:17:56,174 --> 00:17:59,744
of playing Atari games,
where our objective was

337
00:17:59,744 --> 00:18:01,746
to complete the game
with the highest score

338
00:18:01,746 --> 00:18:04,150
and remember our state is
going to be the raw pixel

339
00:18:04,150 --> 00:18:05,460
inputs of the game state,

340
00:18:05,460 --> 00:18:07,064
and we can take these actions

341
00:18:07,064 --> 00:18:09,308
of moving left, right, up, down,

342
00:18:09,308 --> 00:18:12,964
or whatever actions of
the particular game.

343
00:18:12,964 --> 00:18:15,210
And our reward at each time
step, we're going to get

344
00:18:15,210 --> 00:18:18,509
a reward of our score
increase or decrease that we

345
00:18:18,509 --> 00:18:21,183
got at this time step, and
so our cumulative total

346
00:18:21,183 --> 00:18:24,435
reward is this total reward
that we'll usually see

347
00:18:24,435 --> 00:18:27,095
at the top of the screen.

348
00:18:27,095 --> 00:18:30,135
Okay, so the network that
we're going to use for our

349
00:18:30,135 --> 00:18:32,955
Q-function is going to
look something like this,

350
00:18:32,955 --> 00:18:37,355
right, where we have our
Q-network, with weight's theta.

351
00:18:37,355 --> 00:18:41,272
And then our input, our
state s, is going to be

352
00:18:42,259 --> 00:18:43,791
our current game screen.

353
00:18:43,791 --> 00:18:45,377
And in practice we're going to take

354
00:18:45,377 --> 00:18:49,509
a stack of the last four
frames, so we have some history.

355
00:18:49,509 --> 00:18:52,340
And so we'll take these raw pixel values,

356
00:18:52,340 --> 00:18:55,609
we'll do some, you know, RGB
to gray-scale conversions,

357
00:18:55,609 --> 00:18:57,053
some down-sampling, some cropping,

358
00:18:57,053 --> 00:18:58,609
so, some pre-processing.

359
00:18:58,609 --> 00:19:02,543
And what we'll get out of
this is this 84 by 84 by four

360
00:19:02,543 --> 00:19:04,631
stack of the last four frames.

361
00:19:04,631 --> 00:19:05,464
Yeah, question.

362
00:19:05,464 --> 00:19:09,631
[inaudible question from audience]

363
00:19:12,792 --> 00:19:14,768
Okay, so the question
is, are we saying here

364
00:19:14,768 --> 00:19:18,067
that our network is
going to approximate our

365
00:19:18,067 --> 00:19:20,809
Q-value function for
different state, action pairs,

366
00:19:20,809 --> 00:19:22,491
for example, four of these?

367
00:19:22,491 --> 00:19:24,765
Yeah, that's correct.

368
00:19:24,765 --> 00:19:25,598
We'll see,

369
00:19:25,598 --> 00:19:27,551
we'll talk about that in a few slides.

370
00:19:27,551 --> 00:19:29,935
[inaudible question from audience]

371
00:19:29,935 --> 00:19:30,768
So, no.

372
00:19:30,768 --> 00:19:32,883
So, we don't have a Softmax
layer after the connected,

373
00:19:32,883 --> 00:19:35,535
because here our goal
is to directly predict

374
00:19:35,535 --> 00:19:36,816
our Q-value functions.

375
00:19:36,816 --> 00:19:37,712
[inaudible question from audience]

376
00:19:37,712 --> 00:19:38,545
Q-values.

377
00:19:38,545 --> 00:19:40,583
[inaudible question from audience]

378
00:19:40,583 --> 00:19:44,014
Yes, so it's more doing
regression to our Q-values.

379
00:19:44,014 --> 00:19:47,549
Okay, so we have our input to this network

380
00:19:47,549 --> 00:19:51,007
and then on top of this,
we're going to have

381
00:19:51,007 --> 00:19:52,847
a couple of familiar convolutional layers,

382
00:19:52,847 --> 00:19:54,084
and a fully-connected layer,

383
00:19:54,084 --> 00:19:55,334
so here we have

384
00:19:56,191 --> 00:19:58,036
an eight-by-eight
convolutions and we have some

385
00:19:58,036 --> 00:19:59,611
four-by-four convolutions.

386
00:19:59,611 --> 00:20:01,861
Then we have a FC 256 layer,

387
00:20:01,861 --> 00:20:03,458
so this is just a standard kind of networK

388
00:20:03,458 --> 00:20:05,674
that you've seen before.

389
00:20:05,674 --> 00:20:10,382
And then, finally, our last
fully-connected layer has

390
00:20:10,382 --> 00:20:13,470
a vector of outputs, which
is corresponding to your

391
00:20:13,470 --> 00:20:16,074
Q-value for each action, right, given

392
00:20:16,074 --> 00:20:17,415
the state that you've input.

393
00:20:17,415 --> 00:20:19,565
And so, for example, if
you have four actions,

394
00:20:19,565 --> 00:20:21,770
then here we have this
four-dimensional output

395
00:20:21,770 --> 00:20:25,570
corresponding to Q of
current s, as well as a-one,

396
00:20:25,570 --> 00:20:28,685
and then a-two, a-three, and a-four.

397
00:20:28,685 --> 00:20:30,857
Right so this is going
to be one scalar value

398
00:20:30,857 --> 00:20:33,179
for each of our actions.

399
00:20:33,179 --> 00:20:35,610
And then the number of
actions that we have

400
00:20:35,610 --> 00:20:36,955
can vary between,

401
00:20:36,955 --> 00:20:41,122
for example, 4 to 18,
depending on the Atari game.

402
00:20:43,073 --> 00:20:44,839
And one nice thing here is that

403
00:20:44,839 --> 00:20:46,709
using this network structure,

404
00:20:46,709 --> 00:20:49,931
a single feedforward
pass is able to compute

405
00:20:49,931 --> 00:20:52,810
the Q-values for all functions

406
00:20:52,810 --> 00:20:54,651
from the current state.

407
00:20:54,651 --> 00:20:56,117
And so this is really efficient.

408
00:20:56,117 --> 00:20:59,158
Right, so basically we
take our current state

409
00:20:59,158 --> 00:21:03,121
in and then because we have
this output of an action

410
00:21:03,121 --> 00:21:05,946
for each, or Q-value for each
action, as our output layer,

411
00:21:05,946 --> 00:21:10,259
we're able to do one pass and
get all of these values out.

412
00:21:10,259 --> 00:21:12,235
And then in order to train this,

413
00:21:12,235 --> 00:21:15,078
we're just going to use our
loss function from before.

414
00:21:15,078 --> 00:21:17,661
Remember, we're trying to
enforce this Bellman equation

415
00:21:17,661 --> 00:21:21,329
and so, on our forward
pass, our loss function

416
00:21:21,329 --> 00:21:25,193
we're going to try and
iteratively make our Q-value

417
00:21:25,193 --> 00:21:27,987
close to our target value,

418
00:21:27,987 --> 00:21:29,315
that it should have.

419
00:21:29,315 --> 00:21:31,281
And then our backward pass is just

420
00:21:31,281 --> 00:21:34,235
directly taking the gradient of this

421
00:21:34,235 --> 00:21:37,277
loss function that we have and then taking

422
00:21:37,277 --> 00:21:39,777
a gradient step based on that.

423
00:21:40,706 --> 00:21:42,948
So one other thing that's used
here that I want to mention

424
00:21:42,948 --> 00:21:45,639
is something called experience replay.

425
00:21:45,639 --> 00:21:49,556
And so this addresses a
problem with just using

426
00:21:50,579 --> 00:21:53,440
the plain two network
that I just described,

427
00:21:53,440 --> 00:21:55,416
which is that learning from batches

428
00:21:55,416 --> 00:21:58,134
of consecutive samples is bad.

429
00:21:58,134 --> 00:21:58,967
And so the reason

430
00:21:58,967 --> 00:22:01,268
because of this, right, is so for just

431
00:22:01,268 --> 00:22:03,578
playing the game, taking samples

432
00:22:03,578 --> 00:22:06,074
of state action rewards that we have

433
00:22:06,074 --> 00:22:08,222
and just taking consecutive
samples of these

434
00:22:08,222 --> 00:22:09,410
and training with these,

435
00:22:09,410 --> 00:22:11,814
well all of these samples are correlated

436
00:22:11,814 --> 00:22:14,218
and so this leads to

437
00:22:14,218 --> 00:22:16,118
inefficient learning, first of all,

438
00:22:16,118 --> 00:22:19,014
and also, because of this,
our current Q-network

439
00:22:19,014 --> 00:22:21,456
parameters, right, this
determines the policy

440
00:22:21,456 --> 00:22:24,842
that we're going to follow,
it determines our next

441
00:22:24,842 --> 00:22:25,798
samples that we're going to get that

442
00:22:25,798 --> 00:22:27,394
we're going to use for training.

443
00:22:27,394 --> 00:22:29,578
And so this leads to problems where

444
00:22:29,578 --> 00:22:30,832
you can have bad feedback loops.

445
00:22:30,832 --> 00:22:33,920
So, for example, if
currently the maximizing

446
00:22:33,920 --> 00:22:35,468
action that's going to take left,

447
00:22:35,468 --> 00:22:37,588
well this is going to bias all of my

448
00:22:37,588 --> 00:22:39,380
upcoming training examples to be dominated

449
00:22:39,380 --> 00:22:42,297
by samples from the left-hand side.

450
00:22:43,306 --> 00:22:45,406
And so this is a problem, right?

451
00:22:45,406 --> 00:22:47,875
And so the way that we
are going to address these

452
00:22:47,875 --> 00:22:49,808
problems is by using something called

453
00:22:49,808 --> 00:22:53,098
experience replay, where
we're going to keep this

454
00:22:53,098 --> 00:22:56,469
replay memory table of
transitions of state,

455
00:22:56,469 --> 00:22:59,345
as state, action, reward, next state,

456
00:22:59,345 --> 00:23:01,353
transitions that we have, and we're going

457
00:23:01,353 --> 00:23:04,279
to continuously update this
table with new transitions

458
00:23:04,279 --> 00:23:07,185
that we're getting as
game episodes are played,

459
00:23:07,185 --> 00:23:08,773
as we're getting more experience.

460
00:23:08,773 --> 00:23:10,653
Right, and so now what we can do

461
00:23:10,653 --> 00:23:13,207
is that we can now train
our Q-network on random,

462
00:23:13,207 --> 00:23:16,335
mini-batches of transitions
from the replay memory.

463
00:23:16,335 --> 00:23:19,261
Right, so instead of
using consecutive samples,

464
00:23:19,261 --> 00:23:21,815
we're now going to sample across these

465
00:23:21,815 --> 00:23:24,827
transitions that we've accumulated
random samples of these,

466
00:23:24,827 --> 00:23:27,573
and this breaks all of the,

467
00:23:27,573 --> 00:23:31,007
these correlation problems
that we had earlier.

468
00:23:31,007 --> 00:23:33,425
And then also, as another

469
00:23:33,425 --> 00:23:36,370
side benefit is that
each of these transitions

470
00:23:36,370 --> 00:23:39,207
can also contribute to potentially
multiple weight updates.

471
00:23:39,207 --> 00:23:41,440
We're just sampling from this table and so

472
00:23:41,440 --> 00:23:43,652
we could sample one multiple times.

473
00:23:43,652 --> 00:23:44,918
And so, this is going to lead

474
00:23:44,918 --> 00:23:47,585
also to greater data efficiency.

475
00:23:50,580 --> 00:23:52,442
Okay, so let's put this all together

476
00:23:52,442 --> 00:23:54,000
and let's look at the full algorithm

477
00:23:54,000 --> 00:23:57,583
for deep Q-learning
with experience replay.

478
00:23:59,166 --> 00:24:03,940
So we're going to start off with
initializing our replay memory

479
00:24:03,940 --> 00:24:07,383
to some capacity that we
choose, N, and then we're also

480
00:24:07,383 --> 00:24:09,703
going to initialize our

481
00:24:09,703 --> 00:24:13,075
Q-network, just with our random weights

482
00:24:13,075 --> 00:24:14,830
or initial weights.

483
00:24:14,830 --> 00:24:18,688
And then we're going to play
M episodes, or full games.

484
00:24:18,688 --> 00:24:21,832
This is going to be our training episodes.

485
00:24:21,832 --> 00:24:22,998
And then what we're going to do

486
00:24:22,998 --> 00:24:26,574
is we're going to initialize our state,

487
00:24:26,574 --> 00:24:29,526
using the starting game screen pixels

488
00:24:29,526 --> 00:24:31,265
at the beginning of each episode.

489
00:24:31,265 --> 00:24:33,555
And remember, we go through
the pre-processing step

490
00:24:33,555 --> 00:24:37,814
to get to our actual input state.

491
00:24:37,814 --> 00:24:39,313
And then for each time step

492
00:24:39,313 --> 00:24:41,584
of a game that we're currently playing,

493
00:24:41,584 --> 00:24:44,236
we're going to, with a small probability,

494
00:24:44,236 --> 00:24:46,268
select a random action,

495
00:24:46,268 --> 00:24:49,819
so one thing that's
important in these algorithms

496
00:24:49,819 --> 00:24:53,141
is to have sufficient exploration,

497
00:24:53,141 --> 00:24:54,957
so we want to make sure that

498
00:24:54,957 --> 00:24:58,559
we are sampling different
parts of the state space.

499
00:24:58,559 --> 00:25:00,353
And then otherwise, we're going

500
00:25:00,353 --> 00:25:02,405
to select from the greedy action

501
00:25:02,405 --> 00:25:03,614
from the current policy.

502
00:25:03,614 --> 00:25:05,564
Right, so most of the time
we'll take the greedy action

503
00:25:05,564 --> 00:25:07,443
that we think is

504
00:25:07,443 --> 00:25:11,083
a good policy of the type of
actions that we want to take

505
00:25:11,083 --> 00:25:13,580
and states that we want to see,
and with a small probability

506
00:25:13,580 --> 00:25:16,300
we'll sample something random.

507
00:25:16,300 --> 00:25:18,429
Okay, so then we'll take this action,

508
00:25:18,429 --> 00:25:23,076
a, t, and we'll observe the
next reward and the next state.

509
00:25:23,076 --> 00:25:26,070
So r, t and s, t plus one.

510
00:25:26,070 --> 00:25:28,385
And then we'll take this and
we'll store this transition

511
00:25:28,385 --> 00:25:32,771
in our replay memory
that we're building up.

512
00:25:32,771 --> 00:25:34,354
And then we're going to take,

513
00:25:34,354 --> 00:25:35,577
we're going to train a
network a little bit.

514
00:25:35,577 --> 00:25:37,550
So we're going to do experience replay

515
00:25:37,550 --> 00:25:47,213
and we'll take a sample of a random mini-batches of transitions that we have
from the replay memory, and then we'll perform a gradient descent step on this.

516
00:25:47,214 --> 00:25:49,635
Right, so this is going to
be our full training loop.

517
00:25:49,635 --> 00:25:52,561
We're going to be
continuously playing this game

518
00:25:52,561 --> 00:25:55,774
and then also sampling

519
00:25:55,774 --> 00:25:58,431
minibatches, using
experienced replay to update

520
00:25:58,431 --> 00:26:00,100
our weights of our Q-network and then

521
00:26:00,100 --> 00:26:02,350
continuing in this fashion.

522
00:26:03,887 --> 00:26:20,910
Okay, so let's see. Let's see if I can, is this playing? Okay, so let's take a look at
this deep Q-learning algorithm from Google DeepMind, trained on an Atari game of Breakout.

523
00:26:20,911 --> 00:26:26,185
Alright, so it's saying here that our input is
just going to be our state are raw game pixels.

524
00:26:26,185 --> 00:26:29,520
And so here we're looking at what's
happening at the beginning of training.

525
00:26:29,520 --> 00:26:40,302
So we've just started training a bit. And right, so it's going to look to it's learned
to kind of hit the ball, but it's not doing a very good job of sustaining it.

526
00:26:40,303 --> 00:26:42,886
But it is looking for the ball.

527
00:26:50,969 --> 00:26:55,737
Okay, so now after some more training,
it looks like a couple hours.

528
00:27:00,946 --> 00:27:05,113
Okay, so now it's learning
to do a pretty good job here.

529
00:27:06,190 --> 00:27:16,592
So it's able to continuously follow this ball
and be able to to remove most of the blocks.

530
00:27:16,593 --> 00:27:36,203
Right, so after 240 minutes. Okay, so
here it's found the pro strategy, right?

531
00:27:36,203 --> 00:27:42,795
You want to get all the way to the top
and then have it go by itself. Okay, so

532
00:27:42,796 --> 00:27:49,500
this is an example of using deep Q-learning in order
to train an agent to be able to play Atari games.

533
00:27:49,501 --> 00:27:56,418
It's able to do this on many Atari games and
so you can check out some more of this online.

534
00:27:56,419 --> 00:28:01,149
Okay, so we've talked about Q-learning. But
there is a problem with Q-learning, right?

535
00:28:01,149 --> 00:28:07,225
It can be challenging and what's the problem? Well, the
problem can be that the Q-function is very complicated.

536
00:28:07,226 --> 00:28:12,335
Right, so we have to, we're saying that we want
to learn the value of every state action pair.

537
00:28:12,335 --> 00:28:17,275
So, if, let's say you have something, for example,
a robot grasping, wanting to grasp an object.

538
00:28:17,275 --> 00:28:19,576
Right, you're going to have a
really high dimensional state.

539
00:28:19,576 --> 00:28:26,225
You have, I mean, let's say you have all of your
even just joint, joint positions, and angles.

540
00:28:26,225 --> 00:28:35,492
Right, and so learning the exact value of every state action
pair that you have, right, can be really, really hard to do.

541
00:28:35,493 --> 00:28:38,724
But on the other hand, your
policy can be much simpler.

542
00:28:38,724 --> 00:28:44,555
Right, like what you want this robot to do maybe just to
have this simple motion of just closing your hand, right?

543
00:28:44,556 --> 00:28:48,252
Just, move your fingers in this
particular direction and keep going.

544
00:28:48,252 --> 00:28:54,142
And so, that leads to the question of
can we just learn this policy directly?

545
00:28:54,142 --> 00:28:58,306
Right, is it possible, maybe, to just find the
best policy from a collection of policies,

546
00:28:58,306 --> 00:29:06,789
without trying to go through this process of estimating
your Q-value and then using that to infer your policy.

547
00:29:06,790 --> 00:29:15,937
So, this is an approach that oh, so, okay, this is an
approach that we're going to call policy gradients.

548
00:29:15,938 --> 00:29:20,858
And so, formally, let's define a
class of parametrized policies.

549
00:29:20,858 --> 00:29:24,146
Parametrized by weights theta,

550
00:29:24,146 --> 00:29:27,791
and so for each policy
let's define the value of the policy.

551
00:29:27,791 --> 00:29:35,722
So, J, our value J, given parameters theta, is going to be, or
expected some cumulative sum of future rewards that we care about.

552
00:29:35,723 --> 00:29:38,971
So, the same reward that we've been using.

553
00:29:38,971 --> 00:29:41,879
And so our goal then, under this setup

554
00:29:41,879 --> 00:29:51,547
is that we want to find an optimal policy, theta star, which
is the maximum, right, arg max over theta of J of theta.

555
00:29:51,548 --> 00:29:56,917
So we want to find the policy, the policy
parameters that gives our best expected reward.

556
00:29:56,917 --> 00:30:01,011
So, how can we do this?
Any ideas?

557
00:30:04,993 --> 00:30:10,155
Okay, well, what we can do is just a gradient
assent on our policy parameters, right?

558
00:30:10,155 --> 00:30:15,460
We've learned that given some objective that we have,
some parameters we can just use gradient asscent

559
00:30:15,460 --> 00:30:20,762
and gradient assent in order
to continuously improve our parameters.

560
00:30:23,202 --> 00:30:29,196
And so let's talk more specifically about how we can do this,
which we're going to call here the reinforce algorithm.

561
00:30:29,196 --> 00:30:36,781
So, mathematically, we can write out our expected future
reward over trajectories, and so we're going to sample

562
00:30:36,781 --> 00:30:41,902
these trajectories of experience, right, like for example
episodes of game play that we talked about earlier.

563
00:30:41,902 --> 00:30:47,411
S-zero, a-zero, r-zero, s-one,
a-one, r-one, and so on.

564
00:30:47,411 --> 00:30:51,723
Using some policy pi of theta.

565
00:30:51,723 --> 00:30:57,739
Right, and then so, for each trajectory we
can compute a reward for that trajectory.

566
00:30:57,739 --> 00:31:01,245
It's the cumulative reward that we
got from following this trajectory.

567
00:31:01,245 --> 00:31:10,570
And then the value of a policy, pi sub theta, is going to be just the expected
reward of these trajectories that we can get from the following pi sub theta.

568
00:31:10,570 --> 00:31:16,868
So that's here, this expectation over trajectories that
we can get, sampling trajectories from our policy.

569
00:31:18,563 --> 00:31:21,288
Okay.
So, we want to do gradient ascent, right?

570
00:31:21,288 --> 00:31:22,961
So let's differentiate this.

571
00:31:22,961 --> 00:31:27,356
Once we differentiate this, then we can
just take gradient steps, like normal.

572
00:31:28,535 --> 00:31:34,300
So, the problem is that now if we try and just
differentiate this exactly, this is intractable, right?

573
00:31:34,300 --> 00:31:41,319
So, the gradient of an expectation is problematic
when p is dependent on theta here, because here

574
00:31:41,319 --> 00:31:47,661
we want to take this gradient
of p of tau, given theta,

575
00:31:47,661 --> 00:31:53,033
but this is going to be, we want to take this
integral over tau. Right, so this is intractable.

576
00:31:53,033 --> 00:31:57,327
However, we can use a trick
here to get around this.

577
00:31:57,327 --> 00:32:04,941
And this trick is taking this gradient that we want, of
p. We can rewrite this by just multiplying this by one,

578
00:32:04,941 --> 00:32:10,286
by multiplying top and bottom,
both by p of tau given theta.

579
00:32:10,286 --> 00:32:26,170
Right, and then if we look at these terms that we have now here, in the way that I've written this, on the left and the
right, this is actually going to be equivalent to p of tau times our gradient with respect to theta, of log, of p.

580
00:32:26,170 --> 00:32:32,741
Right, because the gradient of the log of p is
just going to be one over p times gradient of p.

581
00:32:33,808 --> 00:32:41,385
Okay, so if we then inject this back into our
expression that we had earlier for this gradient,

582
00:32:41,385 --> 00:32:43,426
we can see that, what this
will actually look like,

583
00:32:43,426 --> 00:32:52,187
right, because now we have a gradient of log p times our probabilities of
all of these trajectories and then taking this integral here, over tau.

584
00:32:52,187 --> 00:32:54,495
This is now going to be an expectation

585
00:32:54,495 --> 00:32:58,586
over our trajectories tau,
and so what we've done here

586
00:32:58,586 --> 00:33:02,751
is that we've taken a
gradient of an expectation

587
00:33:02,751 --> 00:33:06,823
and we've transformed it into
an expectation of gradients.

588
00:33:06,823 --> 00:33:09,156
Right, and so now we can use

589
00:33:10,051 --> 00:33:12,404
sample trajectories that we can get

590
00:33:12,404 --> 00:33:14,712
in order to estimate our gradient.

591
00:33:14,712 --> 00:33:21,260
And so we do this using Monte Carlo sampling,
and this is one of the core ideas of reinforce.

592
00:33:23,624 --> 00:33:25,846
Okay, so looking at this

593
00:33:25,846 --> 00:33:28,180
expression that we want to compute,

594
00:33:28,180 --> 00:33:30,421
can we compute these
quantities that we had here

595
00:33:30,421 --> 00:33:33,071
without knowing the
transition probabilities?

596
00:33:33,071 --> 00:33:36,643
Alright, so we have that
p of tau is going to be

597
00:33:36,643 --> 00:33:38,466
the probability of a trajectory.

598
00:33:38,466 --> 00:33:40,387
It's going to be the product of

599
00:33:40,387 --> 00:33:43,379
all of our transition
probabilities of the next state

600
00:33:43,379 --> 00:33:45,821
that we get, given our
current state and action

601
00:33:45,821 --> 00:33:49,051
as well as our probability
of the actions that

602
00:33:49,051 --> 00:33:52,232
we've taken under our policy pi.

603
00:33:52,232 --> 00:33:54,743
Right, so we're going to
multiply all of these together,

604
00:33:54,743 --> 00:33:58,441
and get our probability of our trajectory.

605
00:33:58,441 --> 00:34:03,059
So this log of p of tau
that we want to compute

606
00:34:03,059 --> 00:34:06,334
is going to be we just
take this log and this will

607
00:34:06,334 --> 00:34:08,326
separate this out into a sum

608
00:34:08,326 --> 00:34:10,389
of pushing the logs inside.

609
00:34:10,389 --> 00:34:12,383
And then here, when we differentiate this,

610
00:34:12,384 --> 00:34:14,237
we can see we want to
differentiate with respect

611
00:34:14,237 --> 00:34:18,162
to theta, but this first
term that we have here,

612
00:34:18,163 --> 00:34:20,911
log p of the state
transition probabilities

613
00:34:20,911 --> 00:34:22,850
there's no theta term here, and so

614
00:34:22,850 --> 00:34:25,292
the only place where we have
theta is the second term

615
00:34:25,292 --> 00:34:28,709
that we have, of log of pi sub theta,

616
00:34:29,675 --> 00:34:32,914
of our action, given our
state, and so this is the only

617
00:34:32,914 --> 00:34:34,139
term that we keep

618
00:34:34,139 --> 00:34:37,368
in our gradient estimate,
and so we can see here that

619
00:34:37,369 --> 00:34:39,670
this doesn't depend on the
transition probabilities,

620
00:34:39,670 --> 00:34:41,293
right, so we actually don't need to know

621
00:34:41,293 --> 00:34:44,588
our transition probabilities
in order to computer

622
00:34:44,589 --> 00:34:46,422
our gradient estimate.

623
00:34:47,257 --> 00:34:50,879
And then, so, therefore
when we're sampling these,

624
00:34:50,880 --> 00:34:55,047
for any given trajectory tau,
we can estimate J of theta

625
00:34:56,306 --> 00:34:58,524
using this gradient estimate.

626
00:34:58,524 --> 00:35:00,472
This is here shown for a single trajectory

627
00:35:00,472 --> 00:35:02,220
from what we had earlier,

628
00:35:02,220 --> 00:35:05,271
and then we can also sample
over multiple trajectories

629
00:35:05,271 --> 00:35:07,188
to get the expectation.

630
00:35:09,248 --> 00:35:12,974
Okay, so given this gradient
estimator that we've derived,

631
00:35:12,974 --> 00:35:17,141
the interpretation that we can
make from this here, is that

632
00:35:18,217 --> 00:35:21,931
if our reward for a trajectory
is high, if the reward that

633
00:35:21,931 --> 00:35:25,226
we got from taking the
sequence of actions was good,

634
00:35:25,226 --> 00:35:27,517
then let's push up the
probabilities of all

635
00:35:27,517 --> 00:35:29,434
the actions that we've seen.

636
00:35:29,434 --> 00:35:31,458
Right, we're just going to say that

637
00:35:31,458 --> 00:35:33,141
these were good actions that we took.

638
00:35:33,141 --> 00:35:35,287
And then if the reward is low,

639
00:35:35,287 --> 00:35:37,186
we want to push down these probabilities.

640
00:35:37,186 --> 00:35:38,629
We want to say these were bad actions,

641
00:35:38,629 --> 00:35:40,747
let's try and not sample these so much.

642
00:35:40,747 --> 00:35:43,568
Right and so we can see
that's what's happening here,

643
00:35:43,568 --> 00:35:47,392
where we have pi of a, given s.

644
00:35:47,392 --> 00:35:50,980
This is the likelihood of
the actions that we've taken

645
00:35:50,980 --> 00:35:53,163
and then we're going to scale
this, we're going to take the

646
00:35:53,163 --> 00:35:56,621
gradient and the gradient
is going to tell us how much

647
00:35:56,621 --> 00:36:00,555
should we change the
parameters in order to increase

648
00:36:00,555 --> 00:36:03,575
our likelihood of our action, a, right?

649
00:36:03,575 --> 00:36:06,501
And then we're going to
take this and scale it by

650
00:36:06,501 --> 00:36:09,019
how much reward we actually got from it,

651
00:36:09,019 --> 00:36:12,602
so how good were these
actions, in reality.

652
00:36:14,561 --> 00:36:16,209
Okay, so

653
00:36:16,209 --> 00:36:18,454
this might seem simplistic to say that,

654
00:36:18,454 --> 00:36:21,124
you know, if a trajectory
is good, then we're saying

655
00:36:21,124 --> 00:36:22,965
here that all of its actions were good.

656
00:36:22,965 --> 00:36:23,798
Right?

657
00:36:23,798 --> 00:36:26,356
But, in expectation, this
actually averages out.

658
00:36:26,356 --> 00:36:30,125
So we have an unbiased estimator here,

659
00:36:30,125 --> 00:36:32,580
and so if you have many samples of this,

660
00:36:32,580 --> 00:36:35,622
then we will get an accurate
estimate of our gradient.

661
00:36:35,622 --> 00:36:38,510
And this is nice because we can just take

662
00:36:38,510 --> 00:36:40,666
gradient steps and we know
that we're going to be

663
00:36:40,666 --> 00:36:42,994
improving our loss
function and getting closer

664
00:36:42,994 --> 00:36:45,976
to, at least some local optimum of our

665
00:36:45,976 --> 00:36:48,602
policy parameters theta.

666
00:36:48,602 --> 00:36:50,690
Alright, but there is a problem with this,

667
00:36:50,690 --> 00:36:52,789
and the problem is that this also suffers

668
00:36:52,789 --> 00:36:54,884
from high variance.

669
00:36:54,884 --> 00:36:57,201
Because this credit
assignment is really hard.

670
00:36:57,201 --> 00:36:58,902
Right, we're saying that

671
00:36:58,902 --> 00:37:02,283
given a reward that we
got, we're going to say

672
00:37:02,283 --> 00:37:04,412
all of the actions were good,
we're just going to hope

673
00:37:04,412 --> 00:37:06,537
that this assignment of
which actions were actually

674
00:37:06,537 --> 00:37:08,395
the best actions, that mattered,

675
00:37:08,395 --> 00:37:11,080
are going to average out over time.

676
00:37:11,080 --> 00:37:14,598
And so this is really hard
and we need a lot of samples

677
00:37:14,598 --> 00:37:17,190
in order to have a good estimate.

678
00:37:17,190 --> 00:37:19,406
Alright, so this leads to the
question of, is there anything

679
00:37:19,406 --> 00:37:21,684
that we can do to reduce the variance

680
00:37:21,684 --> 00:37:23,851
and improve the estimator?

681
00:37:26,540 --> 00:37:29,123
And so variance reduction is

682
00:37:30,164 --> 00:37:33,323
an important area of research
in policy gradients,

683
00:37:33,323 --> 00:37:36,467
and in coming up with
ways in order to improve

684
00:37:36,467 --> 00:37:39,756
the estimator and require fewer samples.

685
00:37:39,756 --> 00:37:41,445
Alright, so let's look
at a couple of ideas

686
00:37:41,445 --> 00:37:43,278
of how we can do this.

687
00:37:44,202 --> 00:37:46,764
So given our gradient estimator,

688
00:37:46,764 --> 00:37:49,017
so the first idea is that we can

689
00:37:49,017 --> 00:37:52,610
push up the probabilities of an action

690
00:37:52,610 --> 00:37:56,258
only by it's affect on future rewards

691
00:37:56,258 --> 00:37:57,091
from that state, right?

692
00:37:57,091 --> 00:37:59,312
So, now with instead of scaling

693
00:37:59,312 --> 00:38:02,066
this likelihood, or
pushing up this likelihood

694
00:38:02,066 --> 00:38:04,736
of this action by the total
reward of its trajectory,

695
00:38:04,736 --> 00:38:07,320
let's look more
specifically at just the sum

696
00:38:07,320 --> 00:38:09,876
of rewards coming from this time step

697
00:38:09,876 --> 00:38:12,108
on to the end, right?

698
00:38:12,108 --> 00:38:14,224
And so, this is basically saying that

699
00:38:14,224 --> 00:38:17,441
how good an action is, is
only specified by how much

700
00:38:17,441 --> 00:38:18,999
future reward it generates.

701
00:38:18,999 --> 00:38:20,499
Which makes sense.

702
00:38:21,811 --> 00:38:24,251
Okay, so a second idea
that we can also use

703
00:38:24,251 --> 00:38:26,931
is using a discount factor in order

704
00:38:26,931 --> 00:38:29,448
to ignore delayed effects.

705
00:38:29,448 --> 00:38:33,133
Alright so here we've added
back in this discount factor,

706
00:38:33,133 --> 00:38:36,774
that we've seen before,
which is saying that

707
00:38:36,774 --> 00:38:39,991
we are, you know, our discount
factor's going to tell us

708
00:38:39,991 --> 00:38:41,841
how much we care about just the

709
00:38:41,841 --> 00:38:44,510
rewards that are coming up soon,

710
00:38:44,510 --> 00:38:47,276
versus rewards that came much later on.

711
00:38:47,276 --> 00:38:49,462
Right, so we were going to now

712
00:38:49,462 --> 00:38:51,438
say how good or bad an action is,

713
00:38:51,438 --> 00:38:54,071
looking more at the local neighborhood

714
00:38:54,071 --> 00:38:57,489
of action set it generates
in the immediate near future

715
00:38:57,489 --> 00:39:00,880
and down weighting the the
ones that come later on.

716
00:39:00,880 --> 00:39:02,471
Okay so

717
00:39:02,471 --> 00:39:05,194
these are some straightforward ideas

718
00:39:05,194 --> 00:39:07,730
that are generally used in practice.

719
00:39:07,730 --> 00:39:11,529
So, a third idea is this idea of using

720
00:39:11,529 --> 00:39:14,597
a baseline in order to
reduce your variance.

721
00:39:14,597 --> 00:39:18,273
And so, a problem with
just using the raw value

722
00:39:18,273 --> 00:39:20,690
of your trajectories, is that

723
00:39:21,675 --> 00:39:23,869
this isn't necessarily meaningful, right?

724
00:39:23,869 --> 00:39:26,653
So, for example, if your
rewards are all positive,

725
00:39:26,653 --> 00:39:27,973
then you're just going to keep pushing

726
00:39:27,973 --> 00:39:29,835
up the probabilities of all your actions.

727
00:39:29,835 --> 00:39:32,039
And of course, you'll push
them up to various degrees,

728
00:39:32,039 --> 00:39:35,448
but what's really important
is whether a reward is better

729
00:39:35,448 --> 00:39:39,753
or worse than what you're
expecting to be getting.

730
00:39:39,753 --> 00:39:42,993
Alright, so in order to
address this, we can introduce

731
00:39:42,993 --> 00:39:46,071
a baseline function that's
dependent on the state.

732
00:39:46,071 --> 00:39:47,598
Right, so this baseline function tell us

733
00:39:47,598 --> 00:39:51,219
what's, how much we, what's
our guess and what we expect

734
00:39:51,219 --> 00:39:53,886
to get from this state, and then

735
00:39:55,515 --> 00:39:58,031
our reward or our scaling
factor that we're going to use

736
00:39:58,031 --> 00:39:59,837
to be pushing up or
down our probabilities,

737
00:39:59,837 --> 00:40:02,592
can now just be our expected
sum of future rewards,

738
00:40:02,592 --> 00:40:05,508
minus this baseline, so now
it's the relative of how

739
00:40:05,508 --> 00:40:08,939
much better or worse is
the reward that we got

740
00:40:08,939 --> 00:40:10,772
from what we expected.

741
00:40:11,870 --> 00:40:14,971
And so how can we choose this baseline?

742
00:40:14,971 --> 00:40:16,099
Well,

743
00:40:16,099 --> 00:40:19,168
a very simple baseline, the
most simple you can use,

744
00:40:19,168 --> 00:40:21,065
is just taking a moving average

745
00:40:21,065 --> 00:40:23,013
of rewards that you've experienced so far.

746
00:40:23,013 --> 00:40:25,027
So you can even do this
overall trajectories,

747
00:40:25,027 --> 00:40:28,863
and this is just an
average of what rewards

748
00:40:28,863 --> 00:40:31,431
have I been seeing as I've been training,

749
00:40:31,431 --> 00:40:34,765
and as I've been playing these episodes?

750
00:40:34,765 --> 00:40:37,549
Right, and so this gives
some idea of whether the

751
00:40:37,549 --> 00:40:41,716
reward that I currently get
was relatively better or worse.

752
00:40:42,821 --> 00:40:45,737
And so there's some variance
on this that you can use

753
00:40:45,737 --> 00:40:49,215
but so far the variance
reductions that we've seen so far

754
00:40:49,215 --> 00:40:51,588
are all used in what's typically

755
00:40:51,588 --> 00:40:54,452
called "vanilla REINFORCE" algorithm.

756
00:40:54,452 --> 00:40:56,787
Right, so looking at the
cumulative future reward,

757
00:40:56,787 --> 00:41:00,954
having a discount factor,
and some simple baselines.

758
00:41:02,601 --> 00:41:05,081
Now let's talk about how we can

759
00:41:05,081 --> 00:41:06,547
think about this idea of baseline

760
00:41:06,547 --> 00:41:08,769
and potentially choose better baselines.

761
00:41:08,769 --> 00:41:12,084
Right, so if we're going to
think about what's a better

762
00:41:12,084 --> 00:41:13,567
baseline that we can choose,

763
00:41:13,567 --> 00:41:16,569
what we want to do is we want
to push up the probability

764
00:41:16,569 --> 00:41:19,931
of an action from a state,
if the action was better than

765
00:41:19,931 --> 00:41:24,255
the expected value of what we
should get from that state.

766
00:41:24,255 --> 00:41:27,655
So, thinking about the value
of what we're going to expect

767
00:41:27,655 --> 00:41:30,163
from the state, what
does this remind you of?

768
00:41:30,163 --> 00:41:31,189
Does this remind you of anything

769
00:41:31,189 --> 00:41:34,939
that we talked about
earlier in this lecture?

770
00:41:37,023 --> 00:41:37,856
Yes.

771
00:41:37,856 --> 00:41:39,266
[inaudible from audience]

772
00:41:39,266 --> 00:41:41,297
Yeah, so the value functions, right?

773
00:41:41,297 --> 00:41:45,201
The value functions that we
talked about with Q-learning.

774
00:41:45,201 --> 00:41:46,034
So, exactly.

775
00:41:46,034 --> 00:41:47,871
So Q-functions and value functions

776
00:41:47,871 --> 00:41:50,895
and so, the intuition is that

777
00:41:50,895 --> 00:41:52,347
well,

778
00:41:52,347 --> 00:41:54,704
we're happy with an action,

779
00:41:54,704 --> 00:41:58,173
taking an action in a state s, if

780
00:41:58,173 --> 00:42:00,248
our Q-value of taking

781
00:42:00,248 --> 00:42:04,752
a specific action from
this state is larger than

782
00:42:04,752 --> 00:42:06,999
the value function or expected value

783
00:42:06,999 --> 00:42:08,406
of the cumulative future reward

784
00:42:08,406 --> 00:42:09,698
that we can get from this state.

785
00:42:09,698 --> 00:42:11,842
Right, so this means that
this action was better than

786
00:42:11,842 --> 00:42:14,416
other actions that we could've taken.

787
00:42:14,416 --> 00:42:17,896
And on the contrary, we're
unhappy if this action,

788
00:42:17,896 --> 00:42:22,063
if this value or this
difference is negative or small.

789
00:42:23,917 --> 00:42:27,299
Right, so now if we plug
this in, in order to,

790
00:42:27,299 --> 00:42:29,269
as our scaling factor of how much we want

791
00:42:29,269 --> 00:42:32,692
to push up or down, our
probabilities of our actions,

792
00:42:32,692 --> 00:42:34,868
then we can get this estimator here.

793
00:42:34,868 --> 00:42:37,452
Right, so, it's going to be

794
00:42:37,452 --> 00:42:40,168
exactly the same as before, but now where

795
00:42:40,168 --> 00:42:43,993
we've had before our
cumulative expected reward,

796
00:42:43,993 --> 00:42:46,708
with our various reduction,
variance reduction

797
00:42:46,708 --> 00:42:50,514
techniques and baselines in,
here we can just plug in now

798
00:42:50,514 --> 00:42:53,297
this difference of how much better our

799
00:42:53,297 --> 00:42:57,113
current action was,
based on our Q-function

800
00:42:57,113 --> 00:43:00,530
minus our value function from that state.

801
00:43:01,771 --> 00:43:04,148
Right, but what we talked
about so far with our

802
00:43:04,148 --> 00:43:06,993
REINFORCE algorithm, we don't know

803
00:43:06,993 --> 00:43:09,413
what Q and V actually are.

804
00:43:09,413 --> 00:43:11,313
So can we learn these?

805
00:43:11,313 --> 00:43:14,479
And the answer is yes, using Q-learning.

806
00:43:14,479 --> 00:43:16,465
What we've already talked about before.

807
00:43:16,465 --> 00:43:19,828
So we can combine policy gradients

808
00:43:19,828 --> 00:43:22,210
while we've just been talking
about, with Q-learning,

809
00:43:22,210 --> 00:43:25,982
by training both an actor,
which is the policy,

810
00:43:25,982 --> 00:43:28,784
as well as a critic, right, a Q-function,

811
00:43:28,784 --> 00:43:32,366
which is going to tell us
how good we think a state is,

812
00:43:32,366 --> 00:43:34,380
and an action in a state.

813
00:43:34,380 --> 00:43:36,964
Right, so using this in approach,

814
00:43:36,964 --> 00:43:40,633
an actor is going to
decide which action to take

815
00:43:40,633 --> 00:43:43,716
and then the critic, or
Q-function, is going to tell

816
00:43:43,716 --> 00:43:47,708
the actor how good its action
was and how it should adjust.

817
00:43:47,708 --> 00:43:51,072
And so, and this also alleviates
a little bit of the task

818
00:43:51,072 --> 00:43:53,636
of this critic compared
to the Q-learning problems

819
00:43:53,636 --> 00:43:56,694
that we talked about earlier
of having to have this

820
00:43:56,694 --> 00:43:59,958
learning a Q-value for
every state, action pair,

821
00:43:59,958 --> 00:44:01,784
because here it only has to learn this

822
00:44:01,784 --> 00:44:04,762
for the state-action pairs that
are generated by the policy.

823
00:44:04,762 --> 00:44:06,103
It only needs to know this

824
00:44:06,103 --> 00:44:10,512
where it matters for
computing this scaling factor.

825
00:44:10,512 --> 00:44:12,830
Right, and then we can also,
as we're learning this,

826
00:44:12,830 --> 00:44:15,196
incorporate all of the
Q-learning tricks that we saw

827
00:44:15,196 --> 00:44:18,188
earlier, such as experience replay.

828
00:44:18,188 --> 00:44:20,972
And so, now I'm also going to just

829
00:44:20,972 --> 00:44:24,610
define this term that we saw earlier,

830
00:44:24,610 --> 00:44:28,248
Q of s of a, how much,
how good was an action

831
00:44:28,248 --> 00:44:30,831
in a given state, minus V of s?

832
00:44:32,199 --> 00:44:35,533
Our expected value of
how good the state is

833
00:44:35,533 --> 00:44:38,172
by this term advantage function.

834
00:44:38,172 --> 00:44:41,498
Right, so the advantage
function is how much advantage

835
00:44:41,498 --> 00:44:43,568
did we get from playing this action?

836
00:44:43,568 --> 00:44:48,100
How much better the
action was than expected.

837
00:44:48,100 --> 00:44:51,709
So, using this, we can
put together our full

838
00:44:51,709 --> 00:44:53,457
actor-critic algorithm.

839
00:44:53,457 --> 00:44:56,279
And so what this looks like,
is that we're going to start

840
00:44:56,279 --> 00:45:00,326
off with by initializing
our policy parameters theta,

841
00:45:00,326 --> 00:45:03,689
and our critic parameters
that we'll call phi.

842
00:45:03,689 --> 00:45:07,522
And then for each, for
iterations of training,

843
00:45:08,401 --> 00:45:11,149
we're going to sample M trajectories,

844
00:45:11,149 --> 00:45:12,185
under the current policy.

845
00:45:12,185 --> 00:45:13,734
Right, we're going to play
our policy and get these

846
00:45:13,734 --> 00:45:18,725
trajectories as s-zero, a-zero,
r-zero, s-one and so on.

847
00:45:18,725 --> 00:45:20,359
Okay, and then we're going to compute

848
00:45:20,359 --> 00:45:21,671
the gradients that we want.

849
00:45:21,671 --> 00:45:24,977
Right, so for each of these trajectories

850
00:45:24,977 --> 00:45:26,017
and in each time step, we're going

851
00:45:26,017 --> 00:45:28,901
to compute this advantage function,

852
00:45:28,901 --> 00:45:30,818
and then we're going to

853
00:45:31,701 --> 00:45:33,465
use this advantage function, right?

854
00:45:33,465 --> 00:45:37,131
And then we're going to use
that in our gradient estimator

855
00:45:37,131 --> 00:45:40,533
that we showed earlier, and accumulate our

856
00:45:40,533 --> 00:45:42,894
gradient estimate that we have for here.

857
00:45:42,894 --> 00:45:46,017
And then we're also going to train our

858
00:45:46,017 --> 00:45:50,837
critic parameters phi
by exactly the same way,

859
00:45:50,837 --> 00:45:54,193
so as we saw earlier,
basically trying to enforce

860
00:45:54,193 --> 00:45:57,557
this value function, right,
to learn our value function,

861
00:45:57,557 --> 00:46:01,640
which is going to be pulled
into, just minimizing

862
00:46:02,638 --> 00:46:05,467
this advantage function and this will

863
00:46:05,467 --> 00:46:08,324
encourage it to be closer
to this Bellman equation

864
00:46:08,324 --> 00:46:10,347
that we saw earlier, right?

865
00:46:10,347 --> 00:46:14,347
And so, this is basically
just iterating between

866
00:46:15,197 --> 00:46:17,733
learning and optimizing
our policy function,

867
00:46:17,733 --> 00:46:20,211
as well as our critic function.

868
00:46:20,211 --> 00:46:22,311
And so then we're going to update the

869
00:46:22,311 --> 00:46:23,977
gradients and then we're
going to go through and just

870
00:46:23,977 --> 00:46:26,727
continuously repeat this process.

871
00:46:29,271 --> 00:46:31,827
Okay, so now let's look at
some examples of REINFORCE

872
00:46:31,827 --> 00:46:36,027
in action, and let's look
first here at something called

873
00:46:36,027 --> 00:46:39,464
the Recurrent Attention Model,
which is something that,

874
00:46:39,464 --> 00:46:42,805
which is a model also
referred to as hard attention,

875
00:46:42,805 --> 00:46:46,876
but you'll see a lot in,
recently, in computer vision

876
00:46:46,876 --> 00:46:49,146
tasks for various purposes.

877
00:46:49,146 --> 00:46:51,806
Right, and so the idea behind this is

878
00:46:51,806 --> 00:46:55,122
here, I've talked about the
original work on hard attention,

879
00:46:55,122 --> 00:46:59,167
which is on image
classification, and your goal is

880
00:46:59,167 --> 00:47:02,504
to still predict the image class,

881
00:47:02,504 --> 00:47:04,822
but now you're going to do
this by taking a sequence

882
00:47:04,822 --> 00:47:06,494
of glimpses around the image.

883
00:47:06,494 --> 00:47:10,300
You're going to look at local
regions around the image

884
00:47:10,300 --> 00:47:12,754
and you're basically going
to selectively focus on these

885
00:47:12,754 --> 00:47:17,141
parts and build up information
as you're looking around.

886
00:47:17,141 --> 00:47:19,382
Right, and so the reason
that we want to do this

887
00:47:19,382 --> 00:47:21,638
is, well, first of all it
has some nice inspiration

888
00:47:21,638 --> 00:47:24,551
from human perception in eye movement.

889
00:47:24,551 --> 00:47:26,869
Let's say we're looking at a complex image

890
00:47:26,869 --> 00:47:29,225
and we want to determine
what's in the image.

891
00:47:29,225 --> 00:47:31,594
Well, you know, we might,
maybe look at a low-resolution

892
00:47:31,594 --> 00:47:34,013
of it first, and then
look specifically at parts

893
00:47:34,013 --> 00:47:36,913
of the image that will give us clues about

894
00:47:36,913 --> 00:47:39,168
what's in this image.

895
00:47:39,168 --> 00:47:40,001
And then,

896
00:47:41,160 --> 00:47:45,703
this approach of just looking
at, looking around at an image

897
00:47:45,703 --> 00:47:48,533
at local regions, is also
going to help you save

898
00:47:48,533 --> 00:47:50,435
computational resources, right?

899
00:47:50,435 --> 00:47:53,293
You don't need to process the full image.

900
00:47:53,293 --> 00:47:55,366
In practice, what usually
happens is you look at a

901
00:47:55,366 --> 00:47:57,511
low-resolution image
first, of a full image,

902
00:47:57,511 --> 00:48:01,678
to decide how to get started,
and then you look at high-res

903
00:48:02,773 --> 00:48:04,671
portions of the image after that.

904
00:48:04,671 --> 00:48:06,979
And so this saves a lot
of computational resources

905
00:48:06,979 --> 00:48:09,725
and you can think about,
then, benefits of this

906
00:48:09,725 --> 00:48:11,927
to scalability, right,
being able to, let's say

907
00:48:11,927 --> 00:48:15,177
process larger images more efficiently.

908
00:48:16,164 --> 00:48:17,780
And then, finally, this
could also actually help

909
00:48:17,780 --> 00:48:20,099
with actual classification performance,

910
00:48:20,099 --> 00:48:21,855
because now you're able to

911
00:48:21,855 --> 00:48:24,760
ignore clutter and irrelevant
parts of the image.

912
00:48:24,760 --> 00:48:25,593
Right?

913
00:48:25,593 --> 00:48:27,678
Like, you know, instead
of always putting through

914
00:48:27,678 --> 00:48:29,931
your ConvNet, all the parts of your image,

915
00:48:29,931 --> 00:48:32,846
you can use this to, maybe,
first prune out what are the

916
00:48:32,846 --> 00:48:34,936
relevant parts that I
actually want to process,

917
00:48:34,936 --> 00:48:36,353
using my ConvNet.

918
00:48:37,237 --> 00:48:39,849
Okay, so what's the reinforcement learning

919
00:48:39,849 --> 00:48:41,531
formulation of this problem?

920
00:48:41,531 --> 00:48:44,711
Well, our state is going to be

921
00:48:44,711 --> 00:48:46,889
the glimpses that we've
seen so far, right?

922
00:48:46,889 --> 00:48:47,722
Our

923
00:48:48,881 --> 00:48:51,117
what's the information that we've seen?

924
00:48:51,117 --> 00:48:53,643
Our action is then going to be where

925
00:48:53,643 --> 00:48:55,228
to look next in the image.

926
00:48:55,228 --> 00:48:57,090
Right, so in practice,
this can be something like

927
00:48:57,090 --> 00:48:59,113
the x, y-coordinates,
maybe centered around some

928
00:48:59,113 --> 00:49:02,842
fixed-sized glimpse that
you want to look at next.

929
00:49:02,842 --> 00:49:05,664
And then the reward for
the classification problem

930
00:49:05,664 --> 00:49:08,256
is going to be one, at
the final time step,

931
00:49:08,256 --> 00:49:12,423
if our image is correctly
classified, and zero otherwise.

932
00:49:14,495 --> 00:49:16,162
And so, because this

933
00:49:17,373 --> 00:49:20,016
glimpsing, taking these
glimpses around the image

934
00:49:20,016 --> 00:49:21,932
is a non-differentiable operation,

935
00:49:21,932 --> 00:49:24,006
this is why we need to use

936
00:49:24,006 --> 00:49:25,761
reinforcement learning formulation,

937
00:49:25,761 --> 00:49:29,088
and learn policies for how
to take these glimpse actions

938
00:49:29,088 --> 00:49:31,792
and we can train this using REINFORCE.

939
00:49:31,792 --> 00:49:35,105
So, given the state of glimpses so far,

940
00:49:35,105 --> 00:49:37,537
the core of our model is going to be

941
00:49:37,537 --> 00:49:40,891
this RNN that we're going
to use to model the state,

942
00:49:40,891 --> 00:49:44,501
and then we're going to
use our policy parameters

943
00:49:44,501 --> 00:49:47,418
in order to output the next action.

944
00:49:49,354 --> 00:49:53,248
Okay, so what this model looks
like is we're going to take

945
00:49:53,248 --> 00:49:54,571
an input image.

946
00:49:54,571 --> 00:49:57,655
Right, and then we're going to
take a glimpse at this image.

947
00:49:57,655 --> 00:50:00,068
So here, this glimpse is the red box here,

948
00:50:00,068 --> 00:50:03,184
and this is all blank, zeroes.

949
00:50:03,184 --> 00:50:06,966
And so we'll pass what
we see so far into some

950
00:50:06,966 --> 00:50:09,388
neural network, and this can be any

951
00:50:09,388 --> 00:50:12,193
kind of network depending on your task.

952
00:50:12,193 --> 00:50:14,276
In the original experiments
that I'm showing here,

953
00:50:14,276 --> 00:50:16,138
on MNIST, this is very
simple, so you can just

954
00:50:16,138 --> 00:50:18,758
use a couple of small,
fully-connected layers,

955
00:50:18,758 --> 00:50:21,724
but you can imagine
for more complex images

956
00:50:21,724 --> 00:50:26,105
and other tasks you may want
to use fancier ConvNets.

957
00:50:26,105 --> 00:50:28,775
Right, so you've passed this
into some neural network,

958
00:50:28,775 --> 00:50:31,065
and then, remember I said
we're also going to be

959
00:50:31,065 --> 00:50:34,102
integrating our state of,
glimpses that we've seen

960
00:50:34,102 --> 00:50:36,115
so far, using a recurrent network.

961
00:50:36,115 --> 00:50:38,057
So, I'm just going to

962
00:50:38,057 --> 00:50:40,265
we'll see that later on, but
this is going to go through that,

963
00:50:40,265 --> 00:50:42,646
and then it's going to output my

964
00:50:42,646 --> 00:50:46,094
x, y-coordinates, of where
I'm going to see next.

965
00:50:46,094 --> 00:50:48,435
And in practice, this is going to be

966
00:50:48,435 --> 00:50:50,766
We want to output a
distribution over actions,

967
00:50:50,766 --> 00:50:53,385
right, and so, what this is
going to be it's going to be

968
00:50:53,385 --> 00:50:57,282
a gaussian distribution and
we're going to output the mean.

969
00:50:57,282 --> 00:50:59,084
You can also output a mean and variance

970
00:50:59,084 --> 00:51:00,545
of this distribution in practice.

971
00:51:00,545 --> 00:51:03,944
The variance can also be fixed.

972
00:51:03,944 --> 00:51:07,172
Okay, so we're going to take this

973
00:51:07,172 --> 00:51:08,496
action that we're now going to sample

974
00:51:08,496 --> 00:51:11,854
a specific x, y location
from our action distribution

975
00:51:11,854 --> 00:51:15,457
and then we're going to put
this in to get the next,

976
00:51:15,457 --> 00:51:17,777
extract the next glimpse from our image.

977
00:51:17,777 --> 00:51:19,297
Right, so here we've moved

978
00:51:19,297 --> 00:51:23,385
to the end of the two,
this tail part of the two.

979
00:51:23,385 --> 00:51:25,465
And so now we're actually
starting to get some signal

980
00:51:25,465 --> 00:51:26,745
of what we want to see, right?

981
00:51:26,745 --> 00:51:29,065
Like, what we want to do is we
want to look at the relevant

982
00:51:29,065 --> 00:51:32,724
parts of the image that are
useful for classification.

983
00:51:32,724 --> 00:51:35,354
So we pass this through, again,
our neural network layers,

984
00:51:35,354 --> 00:51:37,104
and then also through

985
00:51:38,153 --> 00:51:40,362
our recurrent network, right,
that's taking this input

986
00:51:40,362 --> 00:51:43,642
as well as this previous hidden
state, and we're going to

987
00:51:43,642 --> 00:51:45,524
use this to get a,

988
00:51:45,524 --> 00:51:47,343
so this is representing our policy,

989
00:51:47,343 --> 00:51:49,565
and then we're going to use this to output

990
00:51:49,565 --> 00:51:51,354
our distribution for the next

991
00:51:51,354 --> 00:51:54,095
location that we want to glimpse at.

992
00:51:54,095 --> 00:51:55,874
So we can continue doing this,

993
00:51:55,874 --> 00:51:57,303
you can see in this next glimpse here,

994
00:51:57,303 --> 00:51:59,903
we've moved a little bit more
toward the center of the two.

995
00:51:59,903 --> 00:52:01,723
Alright, so it's probably learning that,

996
00:52:01,723 --> 00:52:05,005
you know, once I've seen
this tail part of the two,

997
00:52:05,005 --> 00:52:08,093
that looks like this,
maybe moving in this upper

998
00:52:08,093 --> 00:52:10,794
left-hand direction will
get you more towards

999
00:52:10,794 --> 00:52:12,631
a center, which will also have a value,

1000
00:52:12,631 --> 00:52:14,543
valuable information.

1001
00:52:14,543 --> 00:52:17,473
And then we can keep doing this.

1002
00:52:17,473 --> 00:52:20,612
And then finally, at the
end, at our last time step,

1003
00:52:20,612 --> 00:52:23,412
so we can have a fixed
number of time steps here,

1004
00:52:23,412 --> 00:52:26,795
in practice something like six or eight.

1005
00:52:26,795 --> 00:52:29,359
And then at the final time
step, since we want to do

1006
00:52:29,359 --> 00:52:33,350
classification, we'll have our standard

1007
00:52:33,350 --> 00:52:36,100
Softmax layer that will produce a

1008
00:52:37,376 --> 00:52:39,363
distribution of
probabilities for each class.

1009
00:52:39,363 --> 00:52:42,111
And then here the max class was a two,

1010
00:52:42,111 --> 00:52:44,108
so we can predict that this was a two.

1011
00:52:44,108 --> 00:52:46,558
Right, and so this is going
to be the set up of our

1012
00:52:46,558 --> 00:52:50,428
model and our policy, and then we have our

1013
00:52:50,428 --> 00:52:53,079
estimate for the gradient
of this policy that we've

1014
00:52:53,079 --> 00:52:54,420
said earlier we could compute by taking

1015
00:52:54,420 --> 00:52:56,695
trajectories from here

1016
00:52:56,695 --> 00:52:59,569
and using those to do back prop.

1017
00:52:59,569 --> 00:53:02,819
And so we can just do this
in order to train this model

1018
00:53:02,819 --> 00:53:05,281
and learn the parameters
of our policy, right?

1019
00:53:05,281 --> 00:53:08,698
All of the weights that you can see here.

1020
00:53:09,953 --> 00:53:10,786
Okay, so

1021
00:53:12,239 --> 00:53:14,270
here's an example of a

1022
00:53:14,270 --> 00:53:16,710
policies trained on MNIST,

1023
00:53:16,710 --> 00:53:19,016
and so you can see that, in general,

1024
00:53:19,016 --> 00:53:20,808
from wherever it's
starting, usually learns

1025
00:53:20,808 --> 00:53:22,942
to go closer to where the digit is,

1026
00:53:22,942 --> 00:53:25,260
and then looking at the relevant
parts of the digit, right?

1027
00:53:25,260 --> 00:53:27,685
So this is pretty cool and

1028
00:53:27,685 --> 00:53:28,744
this

1029
00:53:28,744 --> 00:53:30,460
you know, follows kind of
what you would expect, right,

1030
00:53:30,460 --> 00:53:31,627
if you were to

1031
00:53:33,335 --> 00:53:34,967
choose places to look next

1032
00:53:34,967 --> 00:53:38,186
in order to most efficiently determine

1033
00:53:38,186 --> 00:53:40,108
what digit this is.

1034
00:53:40,108 --> 00:53:43,491
Right, and so this idea of hard attention,

1035
00:53:43,491 --> 00:53:45,862
of recurrent attention
models, has also been used

1036
00:53:45,862 --> 00:53:49,758
in a lot of tasks in
computer vision in the last

1037
00:53:49,758 --> 00:53:52,687
couple of years, so you'll
see this, used, for example,

1038
00:53:52,687 --> 00:53:54,790
fine-grained image recognition.

1039
00:53:54,790 --> 00:53:57,869
So, I mentioned earlier that

1040
00:53:57,869 --> 00:54:00,596
one of the useful benefits of this

1041
00:54:00,596 --> 00:54:01,763
can be also to

1042
00:54:02,975 --> 00:54:05,198
both save on computational efficiency

1043
00:54:05,198 --> 00:54:08,009
as well as to ignore
clutter and irrelevant

1044
00:54:08,009 --> 00:54:10,180
parts of the image, and
when you have fine-grained

1045
00:54:10,180 --> 00:54:11,750
image classification problems,

1046
00:54:11,750 --> 00:54:13,092
you usually want both of these.

1047
00:54:13,092 --> 00:54:17,307
You want to keep high-resolution,
so that you can look

1048
00:54:17,307 --> 00:54:19,447
at, you know, important differences.

1049
00:54:19,447 --> 00:54:23,327
And then you also want to
focus on these differences

1050
00:54:23,327 --> 00:54:25,777
and ignore irrelevant parts.

1051
00:54:25,777 --> 00:54:27,359
Yeah, question.

1052
00:54:27,359 --> 00:54:31,526
[inaudible question from audience]

1053
00:54:35,061 --> 00:54:36,789
Okay, so yeah, so the question is

1054
00:54:36,789 --> 00:54:39,061
how is there is
computational efficiency,

1055
00:54:39,061 --> 00:54:41,482
because we also have this
recurrent neural network in place.

1056
00:54:41,482 --> 00:54:45,842
So that's true, it depends
on exactly what's your,

1057
00:54:45,842 --> 00:54:47,761
what is your problem, what
is your network, and so on,

1058
00:54:47,761 --> 00:54:50,151
but you can imagine that
if you had some really

1059
00:54:50,151 --> 00:54:52,512
hi- resolution image

1060
00:54:52,512 --> 00:54:54,773
and you don't want to process
the entire parts of this

1061
00:54:54,773 --> 00:54:58,477
image with some huge ConvNet
or some huge, you know,

1062
00:54:58,477 --> 00:55:01,900
network, now you can
get some savings by just

1063
00:55:01,900 --> 00:55:04,669
focusing on specific
smaller parts of the image.

1064
00:55:04,669 --> 00:55:06,589
You only process those parts of the image.

1065
00:55:06,589 --> 00:55:08,507
But, you're right, that
it depends on exactly

1066
00:55:08,507 --> 00:55:10,924
what problem set-up you have.

1067
00:55:12,210 --> 00:55:14,530
This has also been used
in image captioning,

1068
00:55:14,530 --> 00:55:17,138
so if we're going to produce
an caption for an image,

1069
00:55:17,138 --> 00:55:20,421
we can choose, you know,
we can have the image

1070
00:55:20,421 --> 00:55:23,197
use this attention model
to generate this caption

1071
00:55:23,197 --> 00:55:26,120
and what it usually ends up
learning is these policies

1072
00:55:26,120 --> 00:55:28,999
where it'll focus on
specific parts of the image,

1073
00:55:28,999 --> 00:55:31,850
in sequence, and as it
focuses on each part,

1074
00:55:31,850 --> 00:55:34,629
it'll generate some words
or the part of the caption

1075
00:55:34,629 --> 00:55:38,341
referring to that part of the image.

1076
00:55:38,341 --> 00:55:40,170
And then it's also been used,

1077
00:55:40,170 --> 00:55:42,948
also tasks such as visual
question answering,

1078
00:55:42,948 --> 00:55:45,509
where we ask a question about the image

1079
00:55:45,509 --> 00:55:48,981
and you want the model
to output some answer

1080
00:55:48,981 --> 00:55:51,786
to your question, for
example, I don't know,

1081
00:55:51,786 --> 00:55:53,840
how many chairs are around the table?

1082
00:55:53,840 --> 00:55:58,229
And so you can see how
this attention mechanism

1083
00:55:58,229 --> 00:55:59,457
might be a good type of model

1084
00:55:59,457 --> 00:56:03,040
for learning how to
answer these questions.

1085
00:56:05,707 --> 00:56:08,475
Okay, so that was an
example of policy gradients

1086
00:56:08,475 --> 00:56:10,564
in these hard attention models.

1087
00:56:10,564 --> 00:56:13,524
And so, now I'm going to
talk about one more example,

1088
00:56:13,524 --> 00:56:16,430
that also uses policy gradients,

1089
00:56:16,430 --> 00:56:18,465
which is learning how to play Go.

1090
00:56:18,465 --> 00:56:22,006
Right, so DeepMind had this agent

1091
00:56:22,006 --> 00:56:24,497
for playing Go, called AlphGo,

1092
00:56:24,497 --> 00:56:27,297
that's been in the news a lot

1093
00:56:27,297 --> 00:56:30,708
in the past, last year and this year.

1094
00:56:30,708 --> 00:56:31,541
So, sorry?

1095
00:56:31,541 --> 00:56:32,374
[inaudible comment from audience]

1096
00:56:32,374 --> 00:56:35,291
And yesterday, yes, that's correct.

1097
00:56:36,172 --> 00:56:39,258
So this is very exciting,
recent news as well.

1098
00:56:39,258 --> 00:56:40,987
So last year,

1099
00:56:40,987 --> 00:56:43,234
a first version of AlphaGo

1100
00:56:43,234 --> 00:56:44,817
was put into a

1101
00:56:46,678 --> 00:56:49,539
competition against one
of the best Go players

1102
00:56:49,539 --> 00:56:52,609
of recent years, Lee Sedol, and the agent

1103
00:56:52,609 --> 00:56:54,927
was able to beat him

1104
00:56:54,927 --> 00:56:57,886
four to one, in a game of five matches.

1105
00:56:57,886 --> 00:57:00,541
And actually, right now, just

1106
00:57:00,541 --> 00:57:03,788
there's another match with
Ke Jie, which is current

1107
00:57:03,788 --> 00:57:07,855
world number one, and
so it's best of three

1108
00:57:07,855 --> 00:57:09,348
in China right now.

1109
00:57:09,348 --> 00:57:12,236
And so the first game was yesterday.

1110
00:57:12,236 --> 00:57:13,436
AlphaGo won.

1111
00:57:13,436 --> 00:57:16,596
I think it was by just
half a point, and so,

1112
00:57:16,596 --> 00:57:18,606
so there's two more games to watch.

1113
00:57:18,606 --> 00:57:20,657
These are all live-stream, so

1114
00:57:20,657 --> 00:57:24,276
you guys, should also go
online and watch these games.

1115
00:57:24,276 --> 00:57:28,193
It's pretty interesting
to hear the commentary.

1116
00:57:29,225 --> 00:57:32,577
But, so what is this AlphaGo
agent, right, from DeepMind?

1117
00:57:32,577 --> 00:57:34,868
And it's based on a lot
of what we've talked

1118
00:57:34,868 --> 00:57:36,466
about so far in this lecture.

1119
00:57:36,466 --> 00:57:39,687
And what it is it's a mixed
of supervised learning

1120
00:57:39,687 --> 00:57:42,045
and reinforcement learning,

1121
00:57:42,045 --> 00:57:44,657
as well as a mix of some older

1122
00:57:44,657 --> 00:57:48,573
methods for Go, Monte Carlo Tree Search,

1123
00:57:48,573 --> 00:57:51,656
as well as recent deep RL approaches.

1124
00:57:52,579 --> 00:57:56,986
So, okay, so how does AlphaGo
beat the Go world champion?

1125
00:57:56,986 --> 00:57:59,363
Well, what it first does is

1126
00:57:59,363 --> 00:58:02,449
to train AlphaGo, what it
takes as input is going to be

1127
00:58:02,449 --> 00:58:04,089
a few featurization of the board.

1128
00:58:04,089 --> 00:58:06,470
So it's basically, right,
your board and the positions

1129
00:58:06,470 --> 00:58:08,739
of the pieces on the board.

1130
00:58:08,739 --> 00:58:10,739
That's your natural state representation.

1131
00:58:10,739 --> 00:58:13,819
And what they do in order
to improve performance

1132
00:58:13,819 --> 00:58:16,638
a little bit is that
they featurize this into

1133
00:58:16,638 --> 00:58:17,958
some

1134
00:58:17,958 --> 00:58:20,510
more channels of one is all
the different stone colors,

1135
00:58:20,510 --> 00:58:21,956
so this is kind of like your

1136
00:58:21,956 --> 00:58:23,790
configuration of your board.

1137
00:58:23,790 --> 00:58:27,270
Also some channels, for
example, where, which moves

1138
00:58:27,270 --> 00:58:31,138
are legal, some bias
channels, some various things

1139
00:58:31,138 --> 00:58:33,125
and then, given this state, right,

1140
00:58:33,125 --> 00:58:35,117
it's going to first

1141
00:58:35,117 --> 00:58:36,518
train a network

1142
00:58:36,518 --> 00:58:38,867
that's initialized with
supervised training

1143
00:58:38,867 --> 00:58:40,897
from professional Go games.

1144
00:58:40,897 --> 00:58:43,147
So, given the current board configuration

1145
00:58:43,147 --> 00:58:45,627
or features, featurization of this,

1146
00:58:45,627 --> 00:58:48,495
what's the correct next action to take?

1147
00:58:48,495 --> 00:58:50,678
Alright, so given

1148
00:58:50,678 --> 00:58:52,787
examples of professional games played,

1149
00:58:52,787 --> 00:58:55,608
you know, just collected over time,

1150
00:58:55,608 --> 00:58:57,635
we can just take all of
these professional Go moves,

1151
00:58:57,635 --> 00:58:59,815
train a standard, supervised mapping,

1152
00:58:59,815 --> 00:59:02,605
from board state to action to take.

1153
00:59:02,605 --> 00:59:05,365
Alright, so they take this,
which is a pretty good start,

1154
00:59:05,365 --> 00:59:07,637
and then they're going
to use this to initialize

1155
00:59:07,637 --> 00:59:09,227
a policy network.

1156
00:59:09,227 --> 00:59:10,844
Right, so policy network,
it's just going to take

1157
00:59:10,844 --> 00:59:14,985
the exact same structure of input is your

1158
00:59:14,985 --> 00:59:16,389
board state and your output is the

1159
00:59:16,389 --> 00:59:17,778
actions that you're going to take.

1160
00:59:17,778 --> 00:59:20,887
And this was the set-up
for the policy gradients

1161
00:59:20,887 --> 00:59:21,978
that we just saw, right?

1162
00:59:21,978 --> 00:59:25,156
So now we're going to just
continue training this

1163
00:59:25,156 --> 00:59:27,130
using policy gradients.

1164
00:59:27,130 --> 00:59:30,831
And it's going to do this
reinforcement learning training

1165
00:59:30,831 --> 00:59:35,123
by playing against itself for
random, previous iterations.

1166
00:59:35,123 --> 00:59:37,384
So self play, and the
reward it's going to get

1167
00:59:37,384 --> 00:59:42,243
is one, if it wins, and a
negative one if it loses.

1168
00:59:42,243 --> 00:59:44,624
And what we're also going to
do is we're also going to learn

1169
00:59:44,624 --> 00:59:47,573
a value network, so,
something like a critic.

1170
00:59:47,573 --> 00:59:51,235
And then, the final AlphaGo
is going to be combining

1171
00:59:51,235 --> 00:59:53,982
all of these together, so
policy and value networks

1172
00:59:53,982 --> 00:59:56,075
as well as with

1173
00:59:56,075 --> 00:59:59,043
a Monte Carlo Tree Search
algorithm, in order to select

1174
00:59:59,043 --> 01:00:01,475
actions by look ahead search.

1175
01:00:01,475 --> 01:00:04,743
Right, so after putting all this together,

1176
01:00:04,743 --> 01:00:08,853
a value of a node, of
where you are in play,

1177
01:00:08,853 --> 01:00:11,590
and what you do next, is
going to be a combination

1178
01:00:11,590 --> 01:00:13,811
of your value function, as well as

1179
01:00:13,811 --> 01:00:16,552
roll at outcome that you're
computing from standard

1180
01:00:16,552 --> 01:00:19,891
Monte Carlo Tree Search roll outs.

1181
01:00:19,891 --> 01:00:22,891
Okay, so, yeah, so this is basically

1182
01:00:24,203 --> 01:00:27,453
the various, the components of AlphaGo.

1183
01:00:28,397 --> 01:00:30,314
If you're interested in
reading more about this,

1184
01:00:30,314 --> 01:00:33,814
there's a nature paper about this in 2016,

1185
01:00:34,664 --> 01:00:37,656
and they trained this, I think, over,

1186
01:00:37,656 --> 01:00:40,765
the version of AlphaGo
that's being used in these

1187
01:00:40,765 --> 01:00:45,016
matches is, like, I think
a couple thousand CPUs

1188
01:00:45,016 --> 01:00:47,953
plus a couple hundred GPUs,
putting all of this together,

1189
01:00:47,953 --> 01:00:52,120
so it's a huge amount of
training that's going on, right.

1190
01:00:55,659 --> 01:00:57,514
And yeah, so you guys should,

1191
01:00:57,514 --> 01:00:59,681
follow the game this week.

1192
01:01:01,643 --> 01:01:03,491
It's pretty exciting.

1193
01:01:03,491 --> 01:01:07,524
Okay, so in summary,
today we've talked about

1194
01:01:07,524 --> 01:01:10,858
policy gradients, right,
which are general.

1195
01:01:10,858 --> 01:01:13,025
They, you're just directly

1196
01:01:14,456 --> 01:01:18,855
taking gradient descent or
ascent on your policy parameters,

1197
01:01:18,855 --> 01:01:21,942
so this works well for a
large class of problems,

1198
01:01:21,942 --> 01:01:23,947
but it also suffers from high variance,

1199
01:01:23,947 --> 01:01:25,938
so it requires a lot of samples,

1200
01:01:25,938 --> 01:01:28,669
and your challenge here
is sample efficiency.

1201
01:01:28,669 --> 01:01:32,608
We also talked about
Q-learning, which doesn't always

1202
01:01:32,608 --> 01:01:35,349
work, it's harder to
sometimes get it to work

1203
01:01:35,349 --> 01:01:37,536
because of this problem
that we talked earlier where

1204
01:01:37,536 --> 01:01:39,702
you are trying to compute this

1205
01:01:39,702 --> 01:01:42,285
exact state, action value

1206
01:01:43,324 --> 01:01:47,769
for many, for very high
dimensions, but when it does work,

1207
01:01:47,769 --> 01:01:51,342
for problems, for example,
the Atari we saw earlier,

1208
01:01:51,342 --> 01:01:53,092
then it's usually more sample efficient

1209
01:01:53,092 --> 01:01:54,322
than policy gradients.

1210
01:01:54,322 --> 01:01:56,540
Right, and one of the
challenges in Q-learning is that

1211
01:01:56,540 --> 01:01:57,484
you want to make sure that you're

1212
01:01:57,484 --> 01:01:59,902
doing sufficient exploration.

1213
01:01:59,902 --> 01:02:00,735
Yeah?

1214
01:02:00,735 --> 01:02:04,902
[inaudible question from audience]

1215
01:02:14,313 --> 01:02:17,764
Oh, so for Q-learning can
you do this process where

1216
01:02:17,764 --> 01:02:20,684
you're, okay, where you're
trying to start this off by

1217
01:02:20,684 --> 01:02:21,753
some supervised training?

1218
01:02:21,753 --> 01:02:24,924
So, I guess the direct
approach for Q-learning doesn't

1219
01:02:24,924 --> 01:02:27,532
do that because you're
trying to regress to these

1220
01:02:27,532 --> 01:02:29,972
Q-values, right, instead of
policy gradients over this

1221
01:02:29,972 --> 01:02:32,732
distribution, but I think there
are ways in which you can,

1222
01:02:32,732 --> 01:02:34,232
like, massage this

1223
01:02:35,938 --> 01:02:37,985
type of thing to also bootstrap.

1224
01:02:37,985 --> 01:02:40,393
Because I think bootstrapping
in general or like

1225
01:02:40,393 --> 01:02:43,854
behavior cloning is a good way to

1226
01:02:43,854 --> 01:02:46,021
warm start these policies.

1227
01:02:47,454 --> 01:02:50,284
Okay, so, right, so we've
talked about policy gradients

1228
01:02:50,284 --> 01:02:54,213
and Q-learning, and just
another look at some of these,

1229
01:02:54,213 --> 01:02:55,413
some of the guarantees that you have,

1230
01:02:55,413 --> 01:02:56,752
right, with policy gradients.

1231
01:02:56,752 --> 01:02:58,622
One thing we do know
that's really nice is that

1232
01:02:58,622 --> 01:03:02,789
this will always converge to
a local minimum of J of theta,

1233
01:03:04,339 --> 01:03:06,592
because we're just directly
doing gradient ascent,

1234
01:03:06,592 --> 01:03:09,043
and so this is often,

1235
01:03:09,043 --> 01:03:12,131
and this local minimum is
often just pretty good, right.

1236
01:03:12,131 --> 01:03:14,931
And in Q-learning, on the
other hand, we don't have any

1237
01:03:14,931 --> 01:03:17,041
guarantees because here
we're trying to approximate

1238
01:03:17,041 --> 01:03:20,277
this Bellman equation with
a complicated function

1239
01:03:20,277 --> 01:03:23,358
approximator and so, in this
case, this is the problem

1240
01:03:23,358 --> 01:03:25,787
with Q-learning being a
little bit trickier to train

1241
01:03:25,787 --> 01:03:29,954
in terms of applicability
to a wide range of problems.

1242
01:03:31,849 --> 01:03:34,737
Alright, so today you got basically very,

1243
01:03:34,737 --> 01:03:37,907
brief, kind of high-level
overview of reinforcement learning

1244
01:03:37,907 --> 01:03:41,546
and some major classes
of algorithms in RL.

1245
01:03:41,546 --> 01:03:44,419
And next time we're going to have a

1246
01:03:44,419 --> 01:03:47,577
guest lecturer from, Song
Han, who's done a lot

1247
01:03:47,577 --> 01:03:51,276
of pioneering work in model compression

1248
01:03:51,276 --> 01:03:52,569
and energy efficient deep learning,

1249
01:03:52,569 --> 01:03:56,459
and so he's going to talk some
of this, about some of this.

1250
01:03:56,459 --> 01:03:58,459
Thank you.

