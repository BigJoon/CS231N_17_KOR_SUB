1
00:00:15,562 --> 00:00:22,506
- All right welcome to lecture nine. So today
we will be talking about CNN Architectures.

2
00:00:22,506 --> 00:00:28,516
And just a few administrative points before we
get started, assignment two is due Thursday.

3
00:00:28,516 --> 00:00:37,665
The mid term will be in class on Tuesday May ninth, so next week and it will
cover material through Tuesday through this coming Thursday May fourth.

4
00:00:37,665 --> 00:00:42,160
So everything up to recurrent neural
networks are going to be fair game.

5
00:00:42,160 --> 00:00:49,931
The poster session we've decided on a time, it's going to be Tuesday June
sixth from twelve to three p.m. So this is the last week of classes.

6
00:00:49,931 --> 00:00:54,638
So we have our our poster session a little bit
early during the last week so that after that,

7
00:00:54,638 --> 00:01:00,942
once you guys get feedback you still have some time to
work for your final report which will be due finals week.

8
00:01:04,135 --> 00:01:06,622
Okay, so just a quick review of last time.

9
00:01:06,622 --> 00:01:10,134
Last time we talked about different
kinds of deep learning frameworks.

10
00:01:10,134 --> 00:01:13,500
We talked about you know
PyTorch, TensorFlow, Caffe2

11
00:01:15,324 --> 00:01:19,572
and we saw that using these kinds of frameworks we
were able to easily build big computational graphs,

12
00:01:19,572 --> 00:01:26,594
for example very large neural networks and comm nets, and
be able to really easily compute gradients in these graphs.

13
00:01:26,594 --> 00:01:33,225
So to compute all of the gradients for all the intermediate
variables weights inputs and use that to train our models

14
00:01:33,225 --> 00:01:36,475
and to run all this efficiently on GPUs

15
00:01:38,468 --> 00:01:45,788
And we saw that for a lot of these frameworks the way this works is by working
with these modularized layers that you guys have been working writing with,

16
00:01:45,788 --> 00:01:50,738
in your home works as well where we have
a forward pass, we have a backward pass,

17
00:01:50,738 --> 00:01:59,214
and then in our final model architecture, all we need to do then
is to just define all of these sequence of layers together.

18
00:01:59,214 --> 00:02:05,747
So using that we're able to very easily be able
to build up very complex network architectures.

19
00:02:07,436 --> 00:02:15,330
So today we're going to talk about some specific kinds of CNN Architectures
that are used today in cutting edge applications and research.

20
00:02:15,330 --> 00:02:20,441
And so we'll go into depth in some of the most
commonly used architectures for these that are winners

21
00:02:20,441 --> 00:02:22,935
of ImageNet classification benchmarks.

22
00:02:22,935 --> 00:02:28,895
So in chronological order AlexNet,
VGG net, GoogLeNet, and ResNet.

23
00:02:28,895 --> 00:02:44,581
And so these will go into a lot of depth. And then I'll also after that, briefly go through some other architectures that are
not as prominently used these days, but are interesting either from a historical perspective, or as recent areas of research.

24
00:02:47,632 --> 00:02:51,649
Okay, so just a quick review.
We talked a long time ago about LeNet,

25
00:02:51,649 --> 00:02:56,413
which was one of the first instantiations of a
comNet that was successfully used in practice.

26
00:02:56,413 --> 00:03:06,588
And so this was the comNet that took an input image, used com filters five
by five filters applied at stride one and had a couple of conv layers,

27
00:03:06,588 --> 00:03:10,145
a few pooling layers and then some
fully connected layers at the end.

28
00:03:10,145 --> 00:03:15,130
And this fairly simple comNet was very
successfully applied to digit recognition.

29
00:03:17,840 --> 00:03:23,685
So AlexNet from 2012 which you guys have also
heard already before in previous classes,

30
00:03:23,685 --> 00:03:31,989
was the first large scale convolutional neural network
that was able to do well on the ImageNet classification

31
00:03:31,989 --> 00:03:41,421
task so in 2012 AlexNet was entered in the competition, and was able to
outperform all previous non deep learning based models by a significant margin,

32
00:03:41,421 --> 00:03:48,822
and so this was the comNet that started the
spree of comNet research and usage afterwards.

33
00:03:48,822 --> 00:03:57,237
And so the basic comNet AlexNet architecture is a conv layer
followed by pooling layer, normalization, com pool norm,

34
00:03:59,231 --> 00:04:01,816
and then a few more conv
layers, a pooling layer,

35
00:04:01,816 --> 00:04:04,232
and then several fully
connected layers afterwards.

36
00:04:04,232 --> 00:04:10,576
So this actually looks very similar to the LeNet network
that we just saw. There's just more layers in total.

37
00:04:10,576 --> 00:04:19,197
There is five of these conv layers, and two fully connected layers
before the final fully connected layer going to the output classes.

38
00:04:22,699 --> 00:04:26,740
So let's first get a sense of the
sizes involved in the AlexNet.

39
00:04:26,740 --> 00:04:33,938
So if we look at the input to the AlexNet this was trained
on ImageNet, with inputs at a size 227 by 227 by 3 images.

40
00:04:33,938 --> 00:04:44,003
And if we look at this first layer which is a conv layer for the
AlexNet, it's 11 by 11 filters, 96 of these applied at stride 4.

41
00:04:44,003 --> 00:04:50,133
So let's just think about this for a moment.
What's the output volume size of this first layer?

42
00:04:52,598 --> 00:04:54,181
And there's a hint.

43
00:04:58,579 --> 00:05:12,251
So remember we have our input size, we have our convolutional filters, ray. And we have this formula,
which is the hint over here that gives you the size of the output dimensions after applying com right?

44
00:05:12,251 --> 00:05:18,442
So remember it was the full image, minus the
filter size, divided by the stride, plus one.

45
00:05:18,442 --> 00:05:27,729
So given that that's written up here for you 55, does anyone have
a guess at what's the final output size after this conv layer?

46
00:05:27,729 --> 00:05:30,633
[student speaks off mic]

47
00:05:30,633 --> 00:05:33,776
- So I had 55 by 55 by 96, yep.
That's correct.

48
00:05:33,776 --> 00:05:38,923
Right so our spatial dimensions at the output are
going to be 55 in each dimension and then we have

49
00:05:38,923 --> 00:05:46,201
96 total filters so the depth after our conv layer
is going to be 96. So that's the output volume.

50
00:05:46,201 --> 00:05:50,296
And what's the total number
of parameters in this layer?

51
00:05:50,296 --> 00:05:53,629
So remember we have 96 11 by 11 filters.

52
00:05:55,661 --> 00:05:58,563
[student speaks off mic]

53
00:05:58,563 --> 00:06:01,563
- [Lecturer] 96 by 11 by 11, almost.

54
00:06:02,755 --> 00:06:06,107
So yes, so I had another by three,
yes that's correct.

55
00:06:06,107 --> 00:06:14,442
So each of the filters is going to see through a local region
of 11 by 11 by three, right because the input depth was three.

56
00:06:14,442 --> 00:06:19,793
And so, that's each filter size,
times we have 96 of these total.

57
00:06:19,793 --> 00:06:23,960
And so there's 35K parameters
in this first layer.

58
00:06:26,828 --> 00:06:31,043
Okay, so now if we look at the second layer
this is a pooling layer right and in this case

59
00:06:31,043 --> 00:06:34,814
we have three three by three
filters applied at stride two.

60
00:06:34,814 --> 00:06:38,981
So what's the output volume
of this layer after pooling?

61
00:06:41,511 --> 00:06:45,678
And again we have a hint, very
similar to the last question.

62
00:06:52,061 --> 00:06:57,077
Okay, 27 by 27 by 96.
Yes that's correct.

63
00:06:58,526 --> 00:07:02,338
Right so the pooling layer is basically
going to use this formula that we had here.

64
00:07:02,338 --> 00:07:17,465
Again because these are pooling applied at a stride of two so we're going to use the same formula to determine
the spatial dimensions and so the spatial dimensions are going to be 27 by 27, and pooling preserves the depth.

65
00:07:17,465 --> 00:07:22,337
So we had 96 as depth as input, and it's
still going to be 96 depth at output.

66
00:07:23,635 --> 00:07:28,937
And next question. What's the
number of parameters in this layer?

67
00:07:32,256 --> 00:07:35,164
I hear some muttering.
[student answers off mic]

68
00:07:35,164 --> 00:07:37,715
- Nothing.
Okay.

69
00:07:37,715 --> 00:07:41,611
Yes, so pooling layer has no parameters,
so, kind of a trick question.

70
00:07:43,549 --> 00:07:46,082
Okay, so we can basically, yes, question?

71
00:07:46,082 --> 00:07:48,002
[student speaks off mic]

72
00:07:48,002 --> 00:07:52,990
- The question is, why are there no
parameters in the pooling layer?

73
00:07:52,990 --> 00:07:55,361
The parameters are the weights right,
that we're trying to learn.

74
00:07:55,361 --> 00:07:57,321
And so convolutional layers
have weights that we learn

75
00:07:57,321 --> 00:08:03,046
but pooling all we do is have a rule, we look
at the pooling region, and we take the max.

76
00:08:03,046 --> 00:08:06,520
So there's no parameters that are learned.

77
00:08:06,520 --> 00:08:15,060
So we can keep on doing this and you can just repeat the process and it's kind of a good
exercise to go through this and figure out the sizes, the parameters, at every layer.

78
00:08:17,283 --> 00:08:23,498
And so if you do this all the way, you can look at
this is the final architecture that you can work with.

79
00:08:23,498 --> 00:08:32,730
There's 11 by 11 filters at the beginning, then five by five and some three
by three filters. And so these are generally pretty familiar looking sizes

80
00:08:32,730 --> 00:08:39,932
that you've seen before and then at the end we have a couple of
fully connected layers of size 4096 and finally the last layer,

81
00:08:39,933 --> 00:08:42,350
is FC8 going to the soft max,

82
00:08:43,499 --> 00:08:47,166
which is going to the
1000 ImageNet classes.

83
00:08:48,849 --> 00:08:57,162
And just a couple of details about this, it was the first use of the ReLu
non-linearity that we've talked about that's the most commonly used non-linearity.

84
00:08:57,162 --> 00:09:08,201
They used local response normalization layers basically trying to normalize the response
across neighboring channels but this is something that's not really used anymore.

85
00:09:08,201 --> 00:09:12,747
It turned out not to, other people showed
that it didn't have so much of an effect.

86
00:09:12,747 --> 00:09:22,579
There's a lot of heavy data augmentation, and so you can look in the paper for more details,
but things like flipping, jittering, jittering, color normalization all of these things

87
00:09:22,579 --> 00:09:29,537
which you'll probably find useful for you when you're working on
your projects for example, so a lot of data augmentation here.

88
00:09:29,537 --> 00:09:33,229
They also use dropout batch size of 128,

89
00:09:33,229 --> 00:09:37,993
and learned with SGD with
momentum which we talked about

90
00:09:37,993 --> 00:09:43,105
in an earlier lecture, and basically just started
with a base learning rate of 1e negative 2.

91
00:09:43,105 --> 00:09:50,955
Every time it plateaus, reduce by a factor of 10 and
then just keep going. Until they finish training

92
00:09:50,955 --> 00:09:59,822
and a little bit of weight decay and in the end, in order to get the best numbers
they also did an ensembling of models and so training multiple of these,

93
00:09:59,822 --> 00:10:03,972
averaging them together and this also
gives an improvement in performance.

94
00:10:05,215 --> 00:10:09,591
And so one other thing I want to point out is
that if you look at this AlexNet diagram up here,

95
00:10:09,591 --> 00:10:16,045
it looks kind of like the normal comNet diagrams
that we've been seeing, except for one difference,

96
00:10:16,045 --> 00:10:22,747
which is that it's, you can see it's kind of split
in these two different rows or columns going across.

97
00:10:23,987 --> 00:10:33,715
And so the reason for this is mostly historical note, so AlexNet was
trained on GTX580 GPUs older GPUs that only had three gigs of memory.

98
00:10:34,916 --> 00:10:38,065
So it couldn't actually fit
this entire network on here,

99
00:10:38,065 --> 00:10:42,583
and so what they ended up doing, was
they spread the network across two GPUs.

100
00:10:42,583 --> 00:10:47,265
So on each GPU you would have half of the
neurons, or half of the feature maps.

101
00:10:47,265 --> 00:10:52,540
And so for example if you look at this first
conv layer, we have 55 by 55 by 96 output,

102
00:10:55,199 --> 00:11:04,965
but if you look at this diagram carefully, you can zoom in later in the actual
paper, you can see that, it's actually only 48 depth-wise, on each GPU,

103
00:11:05,859 --> 00:11:09,403
and so they just spread it, the
feature maps, directly in half.

104
00:11:11,098 --> 00:11:18,177
And so what happens is that for most of these layers, for example com
one, two, four and five, the connections are only with feature maps

105
00:11:18,177 --> 00:11:30,493
on the same GPU, so you would take as input, half of the feature maps that were on the
the same GPU as before and you don't look at the full 96 feature maps for example.

106
00:11:30,493 --> 00:11:34,660
You just take as input the
48 in that first layer.

107
00:11:35,577 --> 00:11:48,506
And then there's a few layers so com three, as well as FC six, seven and eight, where here are the
GPUs do talk to each other and so there's connections with all feature maps in the preceding layer.

108
00:11:48,506 --> 00:11:55,001
so there's communication across the GPUs, and each of these neurons
are then connected to the full depth of the previous input layer.

109
00:11:55,001 --> 00:11:56,437
Question.

110
00:11:56,437 --> 00:12:02,252
- [Student] It says the full simplified
AlexNetwork architecture. [mumbles]

111
00:12:06,393 --> 00:12:10,843
- Oh okay, so the question is why does it say
full simplified AlexNet architecture here?

112
00:12:10,843 --> 00:12:19,846
It just says that because I didn't put all the details on here, so for example
this is the full set of layers in the architecture, and the strides and so on,

113
00:12:19,846 --> 00:12:26,078
but for example the normalization layer, there's
other, these details are not written on here.

114
00:12:31,447 --> 00:12:38,659
And then just one little note, if you look at the paper and
try and write out the math and architectures and so on,

115
00:12:39,668 --> 00:12:53,531
there's a little bit of an issue on the very first layer they'll say if you'll look in the figure they'll say 224 by 224 ,
but there's actually some kind of funny pattern going on and so the numbers actually work out if you look at it as 227.

116
00:12:55,792 --> 00:13:05,071
AlexNet was the winner of the ImageNet classification benchmark in
2012, you can see that it cut the error rate by quite a large margin.

117
00:13:06,056 --> 00:13:15,003
It was the first CNN base winner, and it was widely used as a base to our
architecture almost ubiquitously from then until a couple years ago.

118
00:13:16,530 --> 00:13:18,790
It's still used quite a bit.

119
00:13:18,790 --> 00:13:24,881
It's used in transfer learning for lots of different
tasks and so it was used for basically a long time,

120
00:13:24,881 --> 00:13:34,012
and it was very famous and now though there's been some more recent architectures
that have generally just had better performance and so we'll talk about these

121
00:13:34,012 --> 00:13:40,092
next and these are going to be the more common
architectures that you'll be wanting to use in practice.

122
00:13:41,663 --> 00:13:48,623
So just quickly first in 2013 the ImageNet
challenge was won by something called a ZFNet.

123
00:13:48,623 --> 00:13:49,528
Yes, question.

124
00:13:49,528 --> 00:13:53,539
[student speaks off mic]

125
00:13:53,539 --> 00:13:57,422
- So the question is intuition why AlexNet was
so much better than the ones that came before,

126
00:13:57,422 --> 00:14:05,596
DefLearning comNets [mumbles] this is just a
very different kind of approach in architecture.

127
00:14:05,596 --> 00:14:09,814
So this was the first deep learning based
approach first comNet that was used.

128
00:14:13,255 --> 00:14:19,108
So in 2013 the challenge was won by something called a
ZFNet [Zeller Fergus Net] named after the creators.

129
00:14:19,108 --> 00:14:24,559
And so this mostly was improving
hyper parameters over the AlexNet.

130
00:14:24,559 --> 00:14:36,545
It had the same number of layers, the same general structure and they made a few changes things like changing
the stride size, different numbers of filters and after playing around with these hyper parameters more,

131
00:14:36,545 --> 00:14:42,179
they were able to improve the error rate.
But it's still basically the same idea.

132
00:14:42,179 --> 00:14:50,653
So in 2014 there are a couple of architectures that were now more
significantly different and made another jump in performance,

133
00:14:50,653 --> 00:14:58,988
and the main difference with these networks
first of all was much deeper networks.

134
00:14:58,988 --> 00:15:13,131
So from the eight layer network that was in 2012 and 2013, now in 2014 we had two
very close winners that were around 19 layers and 22 layers. So significantly deeper.

135
00:15:13,131 --> 00:15:17,312
And the winner of this
was GoogleNet, from Google

136
00:15:17,312 --> 00:15:20,986
but very close behind was
something called VGGNet

137
00:15:20,986 --> 00:15:28,231
from Oxford, and on actually the localization challenge
VGG got first place in some of the other tracks.

138
00:15:28,231 --> 00:15:32,768
So these were both very,
very strong networks.

139
00:15:32,768 --> 00:15:35,473
So let's first look at VGG
in a little bit more detail.

140
00:15:35,473 --> 00:15:41,628
And so the VGG network is the idea of much
deeper networks and with much smaller filters.

141
00:15:41,628 --> 00:15:51,184
So they increased the number of layers from eight layers in AlexNet
right to now they had models with 16 to 19 layers in VGGNet.

142
00:15:53,100 --> 00:16:04,726
And one key thing that they did was they kept very small filter so only three by three conv all the way,
which is basically the smallest com filter size that is looking at a little bit of the neighboring pixels.

143
00:16:04,726 --> 00:16:12,295
And they just kept this very simple structure of three by three
convs with the periodic pooling all the way through the network.

144
00:16:12,295 --> 00:16:20,758
And it's very simple elegant network architecture, was
able to get 7.3% top five error on the ImageNet challenge.

145
00:16:23,461 --> 00:16:28,252
So first the question of
why use smaller filters.

146
00:16:28,252 --> 00:16:34,181
So when we take these small filters now we have
fewer parameters and we try and stack more of them

147
00:16:34,181 --> 00:16:40,154
instead of having larger filters, have smaller filters with
more depth instead, have more of these filters instead,

148
00:16:40,154 --> 00:16:48,012
what happens is that you end up having the same effective receptive
field as if you only have one seven by seven convolutional layer.

149
00:16:48,012 --> 00:16:56,276
So here's a question, what is the effective receptive field
of three of these three by three conv layers with stride one?

150
00:16:56,276 --> 00:17:01,999
So if you were to stack three three by three conv layers
with Stride one what's the effective receptive field,

151
00:17:01,999 --> 00:17:10,564
the total area of the input, spatial area of the input that
enure at the top layer of the three layers is looking at.

152
00:17:13,123 --> 00:17:16,797
So I heard fifteen pixels,
why fifteen pixels?

153
00:17:16,797 --> 00:17:21,419
- [Student] Okay, so the
reason given was because

154
00:17:21,419 --> 00:17:28,179
they overlap-- - Okay, so the reason given was
because they overlap. So it's on the right track.

155
00:17:28,179 --> 00:17:36,478
What actually is happening though is you have to see, at the first
layer, the receptive field is going to be three by three right?

156
00:17:36,478 --> 00:17:44,003
And then at the second layer, each of these neurons in the second
layer is going to look at three by three other first layer

157
00:17:44,003 --> 00:17:52,486
filters, but the corners of these three by three have an additional
pixel on each side, that is looking at in the original input layer.

158
00:17:52,486 --> 00:17:57,233
So the second layer is actually looking at five by
five receptive field and then if you do this again,

159
00:17:57,233 --> 00:18:04,850
the third layer is looking at three by three
in the second layer but this is going to,

160
00:18:04,850 --> 00:18:07,717
if you just draw out this pyramid is looking
at seven by seven in the input layer.

161
00:18:07,717 --> 00:18:16,836
So the effective receptive field here is going to be seven by
seven. Which is the same as one seven by seven conv layer.

162
00:18:16,836 --> 00:18:22,356
So what happens is that this has the same effective receptive
field as a seven by seven conv layer but it's deeper.

163
00:18:22,356 --> 00:18:27,011
It's able to have more non-linearities in
there, and it's also fewer parameters.

164
00:18:27,011 --> 00:18:37,346
So if you look at the total number of parameters, each of these conv filters for
the three by threes is going to have nine parameters in each conv [mumbles]

165
00:18:38,975 --> 00:18:45,458
three times three, and then times the input depth, so
three times three times C, times this total number

166
00:18:45,458 --> 00:18:51,844
of output feature maps, which is again C is we're
going to preserve the total number of channels.

167
00:18:51,844 --> 00:19:00,975
So you get three times three, times C times C for each of these layers,
and we have three layers so it's going to be three times this number,

168
00:19:00,975 --> 00:19:08,219
compared to if you had a single seven by seven layer then you
get, by the same reasoning, seven squared times C squared.

169
00:19:08,219 --> 00:19:11,842
So you're going to have fewer
parameters total, which is nice.

170
00:19:16,380 --> 00:19:24,971
So now if we look at this full network here there's a lot of numbers up
here that you can go back and look at more carefully but if we look at all

171
00:19:24,971 --> 00:19:31,526
of the sizes and number of parameters the same
way that we calculated the example for AlexNet,

172
00:19:31,526 --> 00:19:33,327
this is a good exercise to go through,

173
00:19:33,327 --> 00:19:46,644
we can see that you know going the same way we have a couple of these conv layers and a pooling layer a
couple more conv layers, pooling layer, several more conv layers and so on. And so this just keeps going up.

174
00:19:46,644 --> 00:19:53,241
And if you counted the total number of convolutional and fully
connected layers, we're going to have 16 in this case for VGG 16,

175
00:19:53,241 --> 00:20:01,288
and then VGG 19, it's just a very similar architecture,
but with a few more conv layers in there.

176
00:20:03,831 --> 00:20:06,415
And so the total memory
usage of this network,

177
00:20:06,415 --> 00:20:18,006
so just making a forward pass through counting up all of these numbers so in the
memory numbers here written in terms of the total numbers, like we calculated earlier,

178
00:20:18,006 --> 00:20:23,935
and if you look at four bytes per number,
this is going to be about 100 megs per image,

179
00:20:23,935 --> 00:20:29,537
and so this is the scale of the memory usage that's
happening and this is only for a forward pass right,

180
00:20:29,537 --> 00:20:36,280
when you do a backward pass you're going to have to
store more and so this is pretty heavy memory wise.

181
00:20:36,280 --> 00:20:45,220
100 megs per image, if you have on five gigs of total memory,
then you're only going to be able to store about 50 of these.

182
00:20:48,110 --> 00:20:56,941
And so also the total number of parameters here we have is 138 million
parameters in this network, and this compares with 60 million for AlexNet.

183
00:20:56,941 --> 00:20:58,291
Question?

184
00:20:58,291 --> 00:21:01,708
[student speaks off mic]

185
00:21:07,014 --> 00:21:10,730
- So the question is what do we mean by deeper,
is it the number of filters, number of layers?

186
00:21:10,730 --> 00:21:14,897
So deeper in this case is
always referring to layers.

187
00:21:16,415 --> 00:21:26,026
So there are two usages of the word depth which is confusing one is the depth
rate per channel, width by height by depth, you can use the word depth here,

188
00:21:27,752 --> 00:21:35,108
but in general we talk about the depth of a network, this is going to
be the total number of layers in the network, and usually in particular

189
00:21:35,108 --> 00:21:44,178
we're counting the total number of weight layers. So the total number of layers
with trainable weight, so convolutional layers and fully connected layers.

190
00:21:44,178 --> 00:21:47,678
[student mumbles off mic]

191
00:22:01,620 --> 00:22:06,984
- Okay, so the question is, within each
layer what do different filters need?

192
00:22:06,984 --> 00:22:13,853
And so we talked about this back in the comNet
lecture, so you can also go back and refer to that,

193
00:22:13,853 --> 00:22:28,426
but each filter is a set of let's say three by three convs, so each filter is looking at a, is a set
of weight looking at a three by three value input input depth, and this produces one feature map,

194
00:22:28,426 --> 00:22:32,764
one activation map of all the responses
of the different spatial locations.

195
00:22:32,764 --> 00:22:40,456
And then we have we can have as many filters as we want right so for
example 96 and each of these is going to produce a feature map.

196
00:22:40,456 --> 00:22:49,178
And so it's just like each filter corresponds to a different pattern that we're looking for
in the input that we convolve around and we see the responses everywhere in the input,

197
00:22:49,178 --> 00:22:56,991
we create a map of these and then another filter will
we convolve over the image and create another map.

198
00:22:59,571 --> 00:23:01,036
Question.

199
00:23:01,036 --> 00:23:04,453
[student speaks off mic]

200
00:23:08,275 --> 00:23:17,543
- So question is, is there intuition behind, as you go deeper into the network
we have more channel depth so more number of filters right and so you can have

201
00:23:18,486 --> 00:23:22,576
any design that you want so
you don't have to do this.

202
00:23:22,576 --> 00:23:25,151
In practice you will see this
happen a lot of the times

203
00:23:25,151 --> 00:23:31,408
and one of the reasons is people try and maintain
kind of a relatively constant level of compute,

204
00:23:31,408 --> 00:23:38,801
so as you go higher up or deeper into your network,
you're usually also using basically down sampling

205
00:23:40,416 --> 00:23:46,569
and having smaller total spatial area and then so then
they also increase now you increase by depth a little bit,

206
00:23:46,569 --> 00:23:54,177
it's not as expensive now to increase by depth because
it's spatially smaller and so, yeah that's just a reason.

207
00:23:54,177 --> 00:23:55,526
Question.

208
00:23:55,526 --> 00:23:58,943
[student speaks off mic]

209
00:24:00,682 --> 00:24:05,463
- So performance-wise is there any reason to use
SBN [mumbles] instead of SouthMax [mumbles],

210
00:24:05,463 --> 00:24:10,571
so no, for a classifier you can use either one,
and you did that earlier in the class as well,

211
00:24:10,571 --> 00:24:18,052
but in general SouthMax losses, have generally worked
well and been standard use for classification here.

212
00:24:19,319 --> 00:24:20,833
Okay yeah one more question.

213
00:24:20,833 --> 00:24:24,333
[student mumbles off mic]

214
00:24:38,712 --> 00:24:46,208
- Yes, so the question is, we don't have to store all of the memory
like we can throw away the parts that we don't need and so on?

215
00:24:46,208 --> 00:24:50,031
And yes this is true.
Some of this you don't need to keep,

216
00:24:50,031 --> 00:25:03,381
but you're also going to be doing a backwards pass through ware for the most part, when you were doing the chain rule
and so on you needed a lot of these activations as part of it and so in large part a lot of this does need to be kept.

217
00:25:04,816 --> 00:25:15,250
So if we look at the distribution of where memory is used and where parameters are,
you can see that a lot of memories in these early layers right where you still have

218
00:25:15,250 --> 00:25:24,864
spatial dimensions you're going to have more memory usage and then a lot of
the parameters are actually in the last layers, the fully connected layers

219
00:25:24,864 --> 00:25:29,647
have a huge number of parameters right, because
we have all of these dense connections.

220
00:25:29,647 --> 00:25:37,809
And so that's something just to know and then keep
in mind so later on we'll see some networks actually

221
00:25:37,809 --> 00:25:43,155
get rid of these fully connected layers and be
able to save a lot on the number of parameters.

222
00:25:43,155 --> 00:25:48,869
And then just one last thing to point out, you'll also
see different ways of calling all of these layers right.

223
00:25:48,869 --> 00:25:57,000
So here I've written out exactly what the layers are.
conv3-64 means three by three convs with 64 total filters.

224
00:25:57,000 --> 00:26:06,000
But for VGGNet on this diagram on the right here there's also
common ways that people will look at each group of filters,

225
00:26:06,000 --> 00:26:12,632
so each orange block here, as in conv1
part one, so conv1-1, conv1-2, and so on.

226
00:26:12,632 --> 00:26:15,465
So just something to keep in mind.

227
00:26:17,404 --> 00:26:22,930
So VGGNet ended up getting second place in
the ImageNet 2014 classification challenge,

228
00:26:22,930 --> 00:26:25,593
first in localization.

229
00:26:25,593 --> 00:26:29,847
They followed a very similar training
procedure as Alex Krizhevsky for the AlexNet.

230
00:26:29,847 --> 00:26:39,574
They didn't use local response normalization, so as I mentioned earlier,
they found out this didn't really help them, and so they took it out.

231
00:26:39,574 --> 00:26:50,425
You'll see VGG 16 and VGG 19 are common variants of the cycle here,
and this is just the number of layers, 19 is slightly deeper than 16.

232
00:26:50,425 --> 00:27:01,176
In practice VGG 19 works very little bit better, and there's a little bit
more memory usage, so you can use either but 16 is very commonly used.

233
00:27:02,280 --> 00:27:10,920
For best results, like AlexNet, they did ensembling in order
to average several models, and you get better results.

234
00:27:10,920 --> 00:27:20,968
And they also showed in their work that the FC7 features of the last
fully connected layer before going to the 1000 ImageNet classes.

235
00:27:20,968 --> 00:27:27,273
The 4096 size layer just before that,
is a good feature representation,

236
00:27:27,273 --> 00:27:35,865
that can even just be used as is, to extract these features
from other data, and generalized these other tasks as well.

237
00:27:35,865 --> 00:27:38,602
And so FC7 is a good
feature representation.

238
00:27:38,602 --> 00:27:39,952
Yeah question.

239
00:27:39,952 --> 00:27:45,242
[student speaks off mic]
- Sorry what was the question?

240
00:27:46,749 --> 00:27:50,846
Okay, so the question is
what is localization here?

241
00:27:50,846 --> 00:27:57,973
And so this is a task, and we'll talk about it a little bit more in
a later lecture on detection and localization so I don't want to

242
00:27:57,973 --> 00:28:04,015
go into detail here but it's basically an image, not
just classifying What's the class of the image,

243
00:28:04,015 --> 00:28:10,243
but also drawing a bounding box around
where that object is in the image.

244
00:28:10,243 --> 00:28:16,963
And the difference with detection, which is a very related task is that
detection there can be multiple instances of this object in the image

245
00:28:16,963 --> 00:28:23,481
localization we're assuming there's just one, this
classification but we just how this additional bounding box.

246
00:28:26,153 --> 00:28:33,192
So we looked at VGG which was one of the deep networks
from 2014 and then now we'll talk about GoogleNet

247
00:28:33,192 --> 00:28:37,413
which was the other one that won
the classification challenge.

248
00:28:38,422 --> 00:28:48,586
So GoogleNet again was a much deeper network with 22 layers but one of
the main insights and special things about GoogleNet is that it really

249
00:28:48,586 --> 00:28:58,676
looked at this problem of computational efficiency and it tried to design
a network architecture that was very efficient in the amount of compute.

250
00:28:58,676 --> 00:29:05,833
And so they did this using this inception module which
we'll go into more detail and basically stacking

251
00:29:05,833 --> 00:29:09,146
a lot of these inception
modules on top of each other.

252
00:29:09,146 --> 00:29:20,651
There's also no fully connected layers in this network, so they got rid of that were able to save a lot of
parameters and so in total there's only five million parameters which is twelve times less than AlexNet,

253
00:29:20,651 --> 00:29:25,118
which had 60 million even
though it's much deeper now.

254
00:29:25,118 --> 00:29:27,785
It got 6.7% top five error.

255
00:29:32,202 --> 00:29:36,173
So what's the inception module?
So the idea behind the inception module

256
00:29:36,173 --> 00:29:40,833
is that they wanted to design
a good local network typology

257
00:29:40,833 --> 00:29:53,151
and it has this idea of this local topology that's you know you can think of it as a network
within a network and then stack a lot of these local typologies one on top of each other.

258
00:29:53,151 --> 00:29:59,197
And so in this local network that they're calling an
inception module what they're doing is they're basically

259
00:29:59,197 --> 00:30:07,948
applying several different kinds of filter operations in
parallel on top of the same input coming into this same layer.

260
00:30:07,948 --> 00:30:12,706
So we have our input coming in from the previous layer and
then we're going to do different kinds of convolutions.

261
00:30:12,706 --> 00:30:26,457
So a one by one conv, right a three by three conv, five by five conv, and then they also have a pooling operation
in this case three by three pooling, and so you get all of these different outputs from these different layers,

262
00:30:26,457 --> 00:30:32,309
and then what they do is they concatenate all
these filter outputs together depth wise, and so

263
00:30:32,309 --> 00:30:39,703
then this creates one tenser output at the end
that is going tom pass on to the next layer.

264
00:30:41,830 --> 00:30:50,825
So if we look at just a naive way of doing this we just do exactly that we have all
of these different operations we get the outputs we concatenate them together.

265
00:30:50,825 --> 00:30:53,196
So what's the problem with this?

266
00:30:53,196 --> 00:30:58,527
And it turns out that computational
complexity is going to be a problem here.

267
00:30:59,792 --> 00:31:11,966
So if we look more carefully at an example, so here just for as an example I've put one by
one conv, 128 filter so three by three conv 192 filters, five by five convs and 96 filters.

268
00:31:11,966 --> 00:31:20,208
Assume everything has basically the stride that's going to maintain
the spatial dimensions, and that we have this input coming in.

269
00:31:22,151 --> 00:31:30,041
So what is the output size of the one by one filter with
128 , one by one conv with 128 filters? Who has a guess?

270
00:31:36,720 --> 00:31:40,720
OK so I heard 28 by 28,
by 128 which is correct.

271
00:31:41,798 --> 00:31:53,969
So right by one by one conv we're going to maintain spatial dimensions and then on top
of that, each conv filter is going to look through the entire 256 depth of the input,

272
00:31:53,969 --> 00:32:01,004
but then the output is going to be, we have a 28 by 28 feature
map for each of the 128 filters that we have in this conv layer.

273
00:32:01,004 --> 00:32:03,171
So we get 28 by 28 by 128.

274
00:32:06,279 --> 00:32:15,749
OK and then now if we do the same thing and we look at the filter sizes
of the output sizes sorry of all of the different filters here, after the

275
00:32:15,749 --> 00:32:21,189
three by three conv we're going to have this volume
of 28 by 28 by 192 right after five by five conv

276
00:32:21,189 --> 00:32:25,369
we have 96 filters here.
So 28 by 28 by 96,

277
00:32:25,369 --> 00:32:35,522
and then out pooling layer is just going to keep the same spatial
dimension here, so pooling layer will preserve it in depth,

278
00:32:35,522 --> 00:32:41,002
and here because of our stride, we're also
going to preserve our spatial dimensions.

279
00:32:42,035 --> 00:32:52,308
And so now if we look at the output size after filter concatenation what we're
going to get is 28 by 28, these are all 28 by 28, and we concatenating depth wise.

280
00:32:52,308 --> 00:33:00,140
So we get 28 by 28 times all of these added together, and
the total output size is going to be 28 by 28 by 672.

281
00:33:01,923 --> 00:33:11,018
So the input to our inception module was 28 by 28 by 256,
then the output from this module is 28 by 28 by 672.

282
00:33:12,276 --> 00:33:18,064
So we kept the same spatial dimensions,
and we blew up the depth.

283
00:33:18,064 --> 00:33:18,998
Question.

284
00:33:18,998 --> 00:33:22,715
[student speaks off mic]

285
00:33:22,715 --> 00:33:26,356
OK So in this case, yeah, the question is,
how are we getting 28 by 28 for everything?

286
00:33:26,356 --> 00:33:30,117
So here we're doing all the zero padding in
order to maintain the spatial dimensions,

287
00:33:30,117 --> 00:33:34,213
and that way we can do this filter
concatenation depth-wise.

288
00:33:35,205 --> 00:33:37,043
Question in the back.

289
00:33:37,043 --> 00:33:40,460
[student speaks off mic]

290
00:33:45,634 --> 00:33:48,615
- OK The question is what's
the 256 deep at the input,

291
00:33:48,615 --> 00:33:54,624
and so this is not the input to the network, this is the
input just to this local module that I'm looking at.

292
00:33:54,624 --> 00:34:01,316
So in this case 256 is the depth of the previous
inception module that came just before this.

293
00:34:01,316 --> 00:34:09,248
And so now coming out we have 28 by 28 by 672, and that's
going to be the input to the next inception module.

294
00:34:09,248 --> 00:34:10,725
Question.

295
00:34:10,726 --> 00:34:14,143
[student speaks off mic]

296
00:34:17,849 --> 00:34:23,991
- Okay the question is, how did we get 28 by
28 by 128 for the first one, the first conv,

297
00:34:23,991 --> 00:34:34,868
and this is basically it's a one by one convolution right, so we're going to take
this one by one convolution slide it across our 28 by 28 by 256 input spatially

298
00:34:36,295 --> 00:34:42,766
where it's at each location, it's going to multiply, it's going
to do a [mumbles] through the entire 256 depth, and so we do this

299
00:34:42,766 --> 00:34:47,793
one by one conv slide it over spatially and we
get a feature map out that's 28 by 28 by one.

300
00:34:47,793 --> 00:34:59,121
There's one number at each spatial location coming out, and each filter produces
one of these 28 by 28 by one maps, and we have here a total 128 filters,

301
00:35:01,860 --> 00:35:05,610
and that's going to
produce 28 by 28, by 128.

302
00:35:06,619 --> 00:35:11,213
OK so if you look at the number of operations
that are happening in the convolutional layer,

303
00:35:11,213 --> 00:35:23,363
let's look at the first one for example this one by one conv as I was just
saying at each each location we're doing a one by one by 256 dot product.

304
00:35:25,355 --> 00:35:29,168
So there's 256 multiply
operations happening here

305
00:35:29,168 --> 00:35:38,675
and then for each filter map we have 28 by 28 spatial locations, so
that's the first 28 times 28 first two numbers that are multiplied here.

306
00:35:38,675 --> 00:35:54,669
These are the spatial locations for each filter map, and so we have to do this to 25 60 multiplication
each one of these then we have 128 total filters at this layer, or we're producing 128 total feature maps.

307
00:35:54,669 --> 00:36:02,031
And so the total number of these operations here
is going to be 28 times 28 times 128 times 256.

308
00:36:02,939 --> 00:36:11,159
And so this is going to be the same for, you can think about this for the three
by three conv, and the five by five conv, that's exactly the same principle.

309
00:36:11,159 --> 00:36:17,500
And in total we're going to get 854 million
operations that are happening here.

310
00:36:18,778 --> 00:36:22,001
- [Student] And the 128,
192, and 96 are just values

311
00:36:22,941 --> 00:36:29,854
- Question the 128, 192 and 256 are values that I picked.
Yes, these are not values that I just came up with.

312
00:36:29,854 --> 00:36:36,404
They are similar to the ones that you will see
in like a particular layer of inception net,

313
00:36:36,404 --> 00:36:43,913
so in GoogleNet basically, each module has a different set of these
kinds of parameters, and I picked one that was similar to one of these.

314
00:36:45,899 --> 00:36:49,856
And so this is very expensive computationally
right, these these operations.

315
00:36:49,856 --> 00:36:56,317
And then the other thing that I also want to note is that the pooling layer
also adds to this problem because it preserves the whole feature depth.

316
00:36:57,872 --> 00:37:04,329
So at every layer your total depth can only grow
right, you're going to take the full featured depth

317
00:37:04,329 --> 00:37:11,323
from your pooling layer, as well as all the additional
feature maps from the conv layers and add these up together.

318
00:37:11,323 --> 00:37:19,770
So here our input was 256 depth and our output is 672 depth
and you're just going to keep increasing this as you go up.

319
00:37:22,730 --> 00:37:26,251
So how do we deal with this and how
do we keep this more manageable?

320
00:37:26,251 --> 00:37:36,991
And so one of the key insights that GoogleNet used was that well we can we can
address this by using bottleneck layers and try and project these feature maps

321
00:37:36,991 --> 00:37:43,984
to lower dimension before our our convolutional
operations, so before our expensive layers.

322
00:37:45,817 --> 00:37:47,452
And so what exactly does that mean?

323
00:37:47,452 --> 00:37:58,890
So reminder one by one convolution, I guess we were just going through this but it's taking your input volume,
it's performing a dot product at each spatial location and what it does is it preserves spatial dimension

324
00:38:00,951 --> 00:38:06,949
but it reduces the depth and it reduces that by
projecting your input depth to a lower dimension.

325
00:38:06,949 --> 00:38:11,325
It just takes it's basically like a linear
combination of your input feature maps.

326
00:38:13,690 --> 00:38:19,009
And so this main idea is that it's projecting
your depth down and so the inception module

327
00:38:19,009 --> 00:38:29,895
takes these one by one convs and adds these at a bunch of places in these modules
where there's going to be, in order to alleviate this expensive compute.

328
00:38:29,895 --> 00:38:36,972
So before the three by three and five by five conv
layers, it puts in one of these one by one convolutions.

329
00:38:36,972 --> 00:38:43,125
And then after the pooling layer it also
puts an additional one by one convolution.

330
00:38:44,094 --> 00:38:48,419
Right so these are the one by one
bottleneck layers that are added in.

331
00:38:49,372 --> 00:38:53,546
And so how does this change the math
that we were looking at earlier?

332
00:38:53,546 --> 00:38:59,399
So now basically what's happening is that we
still have the same input here 28 by 28 by 256,

333
00:38:59,399 --> 00:39:13,666
but these one by one convs are going to reduce the depth dimension and so you can see before the three by
three convs, if I put a one by one conv with 64 filters, my output from that is going to be, 28 by 28 by 64.

334
00:39:14,994 --> 00:39:25,964
So instead of now going into the three by three convs afterwards instead of
28 by 28 by 256 coming in, we only have a 28 by 28, by 64 block coming in.

335
00:39:25,964 --> 00:39:32,264
And so this is now reducing the smaller input
going into these conv layers, the same thing for

336
00:39:32,264 --> 00:39:41,309
the five by five conv, and then for the pooling layer, after the
pooling comes out, we're going to reduce the depth after this.

337
00:39:42,372 --> 00:39:52,024
And so, if you work out the math the same way for all of the convolutional ops here,
adding in now all these one by one convs on top of the three by threes and five by fives,

338
00:39:52,024 --> 00:40:03,309
the total number of operations is 358 million operations, so it's much less than
the 854 million that we had in the naive version, and so you can see how you

339
00:40:03,309 --> 00:40:11,248
can use this one by one conv, and the filter
size for that to control your computation.

340
00:40:11,248 --> 00:40:12,928
Yes, question in the back.

341
00:40:11,723 --> 00:40:15,112


342
00:40:12,928 --> 00:40:16,345
[student speaks off mic]

343
00:40:15,112 --> 00:40:18,195


344
00:40:18,195 --> 00:40:25,923


345
00:40:24,335 --> 00:40:27,317
- Yes, so the question
is, have you looked into

346
00:40:25,923 --> 00:40:32,658


347
00:40:27,317 --> 00:40:30,028
what information might be
lost by doing this one by one

348
00:40:30,028 --> 00:40:31,789
conv at the beginning.

349
00:40:31,789 --> 00:40:35,922
And so there might be
some information loss,

350
00:40:32,658 --> 00:40:43,333


351
00:40:35,922 --> 00:40:38,455
but at the same time if
you're doing these projections

352
00:40:38,455 --> 00:40:41,565
you're taking a linear
combination of these input

353
00:40:41,565 --> 00:40:45,065
feature maps which has redundancy in them,

354
00:40:43,333 --> 00:40:45,482


355
00:40:45,482 --> 00:40:53,487


356
00:40:46,675 --> 00:40:48,433
you're taking combinations of them,

357
00:40:48,433 --> 00:40:50,647
and you're also introducing
an additional non-linearity

358
00:40:50,647 --> 00:40:53,399
after the one by one
conv, so it also actually

359
00:40:53,399 --> 00:40:55,565
helps in that way with
adding a little bit more

360
00:40:55,565 --> 00:41:00,232
depth and so, I don't think
there's a rigorous analysis

361
00:41:00,232 --> 00:41:03,954
of this, but basically in
general this works better

362
00:41:03,954 --> 00:41:08,124
and there's reasons why it helps as well.

363
00:41:08,124 --> 00:41:11,693
OK so here we have, we're
basically using these one by one

364
00:41:11,693 --> 00:41:16,437
convs to help manage our
computational complexity,

365
00:41:16,437 --> 00:41:18,972
and then what GooleNet
does is it takes these

366
00:41:18,972 --> 00:41:20,156
inception modules and it's going to stack

367
00:41:20,156 --> 00:41:21,260
all these together.

368
00:41:21,260 --> 00:41:23,637
So this is a full inception architecture.

369
00:41:22,334 --> 00:41:24,068


370
00:41:23,637 --> 00:41:28,222
And if we look at this a
little bit more detail,

371
00:41:24,068 --> 00:41:30,176


372
00:41:28,222 --> 00:41:29,651
so here I've flipped it,

373
00:41:29,651 --> 00:41:31,701
because it's so big, it's not going to fit

374
00:41:30,176 --> 00:41:35,753


375
00:41:31,701 --> 00:41:33,583
vertically any more on the slide.

376
00:41:33,583 --> 00:41:35,868
So what we start with is
we first have this stem

377
00:41:35,868 --> 00:41:39,717
network, so this is more
the kind of vanilla plain

378
00:41:38,703 --> 00:41:40,602


379
00:41:39,717 --> 00:41:42,012
conv net that we've seen earlier [mumbles]

380
00:41:40,602 --> 00:41:42,678


381
00:41:42,012 --> 00:41:44,066
six sequence of layers.

382
00:41:42,678 --> 00:41:46,186


383
00:41:44,066 --> 00:41:46,888
So conv pool a couple
of convs in another pool

384
00:41:46,888 --> 00:41:49,380
just to get started and then after that

385
00:41:49,380 --> 00:41:52,321
we have all of our different
our multiple inception

386
00:41:52,321 --> 00:41:55,721
modules all stacked on top of each other,

387
00:41:55,721 --> 00:41:59,243
and then on top we have
our classifier output.

388
00:41:59,243 --> 00:42:01,288
And notice here that
they've really removed

389
00:42:00,004 --> 00:42:01,706


390
00:42:01,288 --> 00:42:03,002
the expensive fully connected layers

391
00:42:01,706 --> 00:42:06,047


392
00:42:03,002 --> 00:42:06,887
it turns out that the model
works great without them,

393
00:42:06,887 --> 00:42:09,792
even and you reduce a lot of parameters.

394
00:42:08,458 --> 00:42:12,784


395
00:42:09,792 --> 00:42:12,875
And then what they also have here is,

396
00:42:12,784 --> 00:42:16,284


397
00:42:14,643 --> 00:42:17,057
you can see these couple
of extra stems coming out

398
00:42:17,057 --> 00:42:19,676
and these are auxiliary
classification outputs

399
00:42:17,311 --> 00:42:21,204


400
00:42:19,676 --> 00:42:24,083
and so these are also you know
just a little mini networks

401
00:42:21,204 --> 00:42:30,586


402
00:42:24,083 --> 00:42:27,036
with an average pooling,
a one by one conv,

403
00:42:27,036 --> 00:42:30,027
a couple of fully connected
layers here going to

404
00:42:30,027 --> 00:42:33,360
the soft Max and also a 1000 way SoftMax

405
00:42:30,586 --> 00:42:35,184


406
00:42:35,218 --> 00:42:36,512
with the ImageNet classes.

407
00:42:36,512 --> 00:42:39,376
And so you're actually
using your ImageNet training

408
00:42:39,376 --> 00:42:42,160
classification loss in
three separate places here.

409
00:42:42,160 --> 00:42:46,384
The standard end of the
network, as well as in these

410
00:42:44,206 --> 00:42:53,518


411
00:42:46,384 --> 00:42:49,699
two places earlier on in
the network, and the reason

412
00:42:49,699 --> 00:42:52,562
they do that is just
this is a deep network

413
00:42:52,562 --> 00:42:54,891
and they found that having
these additional auxiliary

414
00:42:53,518 --> 00:43:00,004


415
00:42:54,891 --> 00:42:58,641
classification outputs,
you get more gradient

416
00:43:00,861 --> 00:43:02,950
training injected at the earlier layers,

417
00:43:02,950 --> 00:43:05,771
and so more just helpful signal flowing in

418
00:43:05,771 --> 00:43:08,521
because these intermediate
layers should also be

419
00:43:07,450 --> 00:43:08,593


420
00:43:08,521 --> 00:43:09,354
helpful.

421
00:43:10,323 --> 00:43:11,708
You should be able to do classification

422
00:43:11,708 --> 00:43:14,294
based off some of these as well.

423
00:43:14,294 --> 00:43:17,885
And so this is the full architecture,

424
00:43:17,885 --> 00:43:21,521
there's 22 total layers
with weights and so

425
00:43:20,502 --> 00:43:37,733


426
00:43:21,521 --> 00:43:25,166
within each of these modules
each of those one by one,

427
00:43:25,166 --> 00:43:27,622
three by three, five by
five is a weight layer,

428
00:43:27,622 --> 00:43:30,284
just including all of
these parallel layers,

429
00:43:30,284 --> 00:43:34,451
and in general it's a relatively
more carefully designed

430
00:43:38,470 --> 00:43:41,908
architecture and part of this
is based on some of these

431
00:43:40,794 --> 00:43:42,271


432
00:43:41,908 --> 00:43:44,938
intuitions that we're talking
about and part of them

433
00:43:42,271 --> 00:43:45,233


434
00:43:44,938 --> 00:43:48,387
also is just you know
Google the authors they had

435
00:43:45,233 --> 00:43:51,385


436
00:43:48,387 --> 00:43:51,046
huge clusters and they're
cross validating across

437
00:43:51,046 --> 00:43:53,826
all kinds of design
choices and this is what

438
00:43:51,385 --> 00:44:00,384


439
00:43:53,826 --> 00:43:56,321
ended up working well.

440
00:43:56,321 --> 00:43:57,915
Question?

441
00:43:57,915 --> 00:44:01,332
[student speaks off mic]

442
00:44:00,384 --> 00:44:06,130


443
00:44:06,130 --> 00:44:09,328


444
00:44:09,328 --> 00:44:16,954


445
00:44:16,954 --> 00:44:20,739


446
00:44:20,739 --> 00:44:26,734


447
00:44:25,252 --> 00:44:28,071
- Yeah so the question is,
are the auxiliary outputs

448
00:44:26,734 --> 00:44:30,040


449
00:44:28,071 --> 00:44:31,224
actually useful for the
final classification,

450
00:44:30,040 --> 00:44:36,273


451
00:44:31,224 --> 00:44:33,267
to use these as well?

452
00:44:33,267 --> 00:44:35,026
I think when they're training them

453
00:44:35,026 --> 00:44:38,214
they do average all these
for the losses coming out.

454
00:44:36,273 --> 00:44:46,667


455
00:44:38,214 --> 00:44:39,974
I think they are helpful.

456
00:44:39,974 --> 00:44:42,180
I can't remember if in
the final architecture,

457
00:44:42,180 --> 00:44:44,765
whether they average all
of these or just take one,

458
00:44:44,765 --> 00:44:47,178
it seems very possible that
they would use all of them,

459
00:44:47,178 --> 00:44:50,082
but you'll need to check on that.

460
00:44:50,082 --> 00:44:53,499
[student speaks off mic]

461
00:44:54,291 --> 00:44:58,494


462
00:44:59,162 --> 00:45:01,411
- So the question is for
the bottleneck layers,

463
00:45:01,411 --> 00:45:05,389
is it possible to use some
other types of dimensionality

464
00:45:01,966 --> 00:45:03,664


465
00:45:03,664 --> 00:45:09,653


466
00:45:05,389 --> 00:45:09,518
reduction and yes you can use
other kinds of dimensionality

467
00:45:09,518 --> 00:45:11,029
reduction.

468
00:45:11,029 --> 00:45:13,893
The benefits here of
this one by one conv is,

469
00:45:13,893 --> 00:45:16,232
you're getting this effect,
but it's all, you know

470
00:45:16,232 --> 00:45:17,948
it's a conv layer just like any other.

471
00:45:17,948 --> 00:45:19,664
You have the soul network of these,

472
00:45:19,664 --> 00:45:21,350
you just train it this full network

473
00:45:21,350 --> 00:45:23,646
back [mumbles] through everything,

474
00:45:23,646 --> 00:45:25,157
and it's learning how to combine the

475
00:45:25,157 --> 00:45:26,990
previous feature maps.

476
00:45:29,411 --> 00:45:31,540
Okay yeah, question in the back.

477
00:45:31,540 --> 00:45:34,957
[student speaks off mic]

478
00:45:36,617 --> 00:45:40,284
- Yes so, question is
are any weights shared

479
00:45:41,969 --> 00:45:43,359
or all they all separate and yeah,

480
00:45:43,359 --> 00:45:46,352
all of these layers have separate weights.

481
00:45:44,060 --> 00:45:53,036


482
00:45:46,352 --> 00:45:47,500
Question.

483
00:45:47,500 --> 00:45:50,917
[student speaks off mic]

484
00:45:53,036 --> 00:45:55,171


485
00:45:55,171 --> 00:46:07,411


486
00:45:57,594 --> 00:45:59,273
- Yes so the question is why do we have

487
00:45:59,273 --> 00:46:00,953
to inject gradients at earlier layers?

488
00:46:00,953 --> 00:46:04,703
So our classification
output at the very end,

489
00:46:06,517 --> 00:46:08,896
where we get a gradient on this, it's

490
00:46:07,411 --> 00:46:12,931


491
00:46:08,896 --> 00:46:10,409
passed all the way back
through the chain roll

492
00:46:10,409 --> 00:46:12,824
but the problem is when
you have very deep networks

493
00:46:12,824 --> 00:46:15,282
and you're going all the
way back through these,

494
00:46:15,282 --> 00:46:18,916
some of this gradient
signal can become minimized

495
00:46:17,200 --> 00:46:22,266


496
00:46:18,916 --> 00:46:21,988
and lost closer to the beginning,
and so that's why having

497
00:46:21,988 --> 00:46:25,588
these additional ones in earlier parts

498
00:46:22,266 --> 00:46:27,515


499
00:46:25,588 --> 00:46:29,187
can help provide some additional signal.

500
00:46:29,187 --> 00:46:33,477
[student mumbles off mic]

501
00:46:33,477 --> 00:46:35,486
- So the question is are you
doing back prop all the times

502
00:46:35,486 --> 00:46:36,663
for each output.

503
00:46:36,663 --> 00:46:38,824
No it's just one back
prop all the way through,

504
00:46:36,910 --> 00:46:41,370


505
00:46:38,824 --> 00:46:42,256
and you can think of these three,

506
00:46:42,256 --> 00:46:44,790
you can think of there being kind of like

507
00:46:42,784 --> 00:46:46,446


508
00:46:44,790 --> 00:46:46,754
an addition at the end
of these if you were to

509
00:46:46,754 --> 00:46:48,885
draw up your computational
graph, and so you get your

510
00:46:48,885 --> 00:46:52,364
final signal and you can
just take all of these

511
00:46:51,168 --> 00:46:55,850


512
00:46:52,364 --> 00:46:54,814
gradients and just back plot
them all the way through.

513
00:46:54,814 --> 00:46:57,719
So it's as if they were
added together at the end

514
00:46:55,850 --> 00:46:58,010


515
00:46:57,719 --> 00:46:59,780
in a computational graph.

516
00:46:58,010 --> 00:47:04,111


517
00:46:59,780 --> 00:47:02,066
OK so in the interest of
time because we still have

518
00:47:02,066 --> 00:47:06,233
a lot to get through, can
take other questions offline.

519
00:47:04,111 --> 00:47:11,911


520
00:47:08,163 --> 00:47:11,330
Okay so GoogleNet basically 22 layers.

521
00:47:12,251 --> 00:47:14,627
It has an efficient inception module,

522
00:47:14,627 --> 00:47:16,793
there's no fully connected layers.

523
00:47:16,793 --> 00:47:18,919
12 times fewer parameters than AlexNet,

524
00:47:18,919 --> 00:47:22,836
and it's the ILSVRC 2014
classification winner.

525
00:47:23,349 --> 00:47:26,841


526
00:47:26,038 --> 00:47:28,775
And so now let's look at the 2015 winner,

527
00:47:26,841 --> 00:47:30,315


528
00:47:28,775 --> 00:47:31,679
which is the ResNet network and so here

529
00:47:30,315 --> 00:47:36,426


530
00:47:31,679 --> 00:47:35,964
this idea is really, this
revolution of depth net right.

531
00:47:35,964 --> 00:47:39,149
We were starting to increase
depth in 2014, and here we've

532
00:47:36,426 --> 00:47:41,866


533
00:47:39,149 --> 00:47:43,477
just had this hugely
deeper model at 152 layers

534
00:47:41,866 --> 00:47:48,503


535
00:47:43,477 --> 00:47:46,426
was the ResNet architecture.

536
00:47:46,426 --> 00:47:49,656
And so now let's look at that
in a little bit more detail.

537
00:47:48,503 --> 00:47:52,460


538
00:47:49,656 --> 00:47:52,521
So the ResNet architecture,
is getting extremely

539
00:47:52,521 --> 00:47:55,096
deep networks, much deeper
than any other networks

540
00:47:55,096 --> 00:47:58,534
before and it's doing this using this idea

541
00:47:58,534 --> 00:48:01,289
of residual connections
which we'll talk about.

542
00:48:01,289 --> 00:48:04,968
And so, they had 152
layer model for ImageNet.

543
00:48:02,806 --> 00:48:07,704


544
00:48:04,968 --> 00:48:08,779
They were able to get 3.5
of 7% top 5 error with this

545
00:48:07,704 --> 00:48:10,958


546
00:48:08,779 --> 00:48:11,927
and the really special
thing is that they swept

547
00:48:11,927 --> 00:48:15,004
all classification and
detection contests in the

548
00:48:15,004 --> 00:48:17,862
ImageNet mart benchmark
and this other benchmark

549
00:48:17,862 --> 00:48:18,924
called COCO.

550
00:48:18,924 --> 00:48:20,189
It just basically won everything.

551
00:48:20,189 --> 00:48:24,356
So it was just clearly
better than everything else.

552
00:48:22,797 --> 00:48:28,523


553
00:48:25,865 --> 00:48:29,258
And so now let's go into a
little bit of the motivation

554
00:48:29,258 --> 00:48:31,869
behind ResNet and residual connections

555
00:48:29,797 --> 00:48:31,270


556
00:48:31,869 --> 00:48:33,348
that we'll talk about.

557
00:48:33,348 --> 00:48:36,504
And the question that they
started off by trying to answer

558
00:48:36,504 --> 00:48:39,769
is what happens when we try
and stack deeper and deeper

559
00:48:39,769 --> 00:48:42,749
layers on a plain
convolutional neural network?

560
00:48:41,717 --> 00:48:45,525


561
00:48:42,749 --> 00:48:44,584
So if we take something like VGG

562
00:48:44,584 --> 00:48:47,937
or some normal network that's
just stacks of conv and

563
00:48:45,525 --> 00:48:48,409


564
00:48:47,937 --> 00:48:50,517
pool layers on top of each
other can we just continuously

565
00:48:48,409 --> 00:48:54,607


566
00:48:50,517 --> 00:48:54,684
extend these, get deeper
layers and just do better?

567
00:48:54,607 --> 00:49:01,635


568
00:48:56,411 --> 00:48:59,231
And and the answer is no.

569
00:48:59,231 --> 00:49:00,832
So if you so if you look at what happens

570
00:49:00,832 --> 00:49:03,242
when you get deeper, so here
I'm comparing a 20 layer

571
00:49:01,635 --> 00:49:06,149


572
00:49:03,242 --> 00:49:07,409
network and a 56 layer network
and so this is just a plain

573
00:49:06,149 --> 00:49:14,186


574
00:49:10,308 --> 00:49:12,718
kind of network you'll see
that in the test error here

575
00:49:12,718 --> 00:49:15,864
on the right the 56 layer
network is doing worse

576
00:49:14,186 --> 00:49:21,276


577
00:49:15,864 --> 00:49:17,627
than the 28 layer network.

578
00:49:17,627 --> 00:49:20,581
So the deeper network was
not able to do better.

579
00:49:20,581 --> 00:49:23,748
But then the really weird thing is now

580
00:49:22,230 --> 00:49:26,829


581
00:49:24,639 --> 00:49:27,178
if you look at the training error right

582
00:49:27,178 --> 00:49:28,778
we here have again the 20 layer network

583
00:49:28,778 --> 00:49:30,490
and a 56 layer network.

584
00:49:30,490 --> 00:49:33,927
The 56 layer network, one of
the obvious problems you think,

585
00:49:32,563 --> 00:49:39,727


586
00:49:33,927 --> 00:49:38,094
I have a really deep network,
I have tons of parameters

587
00:49:39,117 --> 00:49:42,104
maybe it's probably starting
to over fit at some point.

588
00:49:39,727 --> 00:49:46,291


589
00:49:42,104 --> 00:49:44,682
But what actually happens is
that when you're over fitting

590
00:49:44,682 --> 00:49:46,276
you would expect to have very good,

591
00:49:46,276 --> 00:49:49,795
very low training error rate,
and just bad test error,

592
00:49:48,599 --> 00:49:54,530


593
00:49:49,795 --> 00:49:51,995
but what's happening here is
that in the training error

594
00:49:51,995 --> 00:49:54,571
the 56 layer network is
also doing worse than

595
00:49:54,571 --> 00:49:56,321
the 20 layer network.

596
00:49:57,643 --> 00:49:59,438
And so even though the
deeper model performs worse,

597
00:49:59,438 --> 00:50:02,355
this is not caused by over-fitting.

598
00:50:00,715 --> 00:50:07,423


599
00:50:04,272 --> 00:50:07,957
And so the hypothesis
of the ResNet creators

600
00:50:07,957 --> 00:50:11,063
is that the problem is actually
an optimization problem.

601
00:50:09,123 --> 00:50:11,112


602
00:50:11,063 --> 00:50:14,171
Deeper models are just harder to optimize,

603
00:50:14,171 --> 00:50:16,421
than more shallow networks.

604
00:50:15,874 --> 00:50:21,072


605
00:50:17,645 --> 00:50:19,317
And the reasoning was that well,

606
00:50:19,317 --> 00:50:21,489
a deeper model should be
able to perform at least

607
00:50:21,489 --> 00:50:24,073
as well as a shallower model.

608
00:50:24,073 --> 00:50:26,238
You can have actually a
solution by construction

609
00:50:26,238 --> 00:50:28,645
where you just take the learned layers

610
00:50:28,645 --> 00:50:30,486
from your shallower model, you just

611
00:50:30,486 --> 00:50:33,140
copy these over and then
for the remaining additional

612
00:50:33,140 --> 00:50:36,002
deeper layers you just
add identity mappings.

613
00:50:36,002 --> 00:50:38,902
So by construction this
should be working just as well

614
00:50:37,613 --> 00:50:44,193


615
00:50:38,902 --> 00:50:40,343
as the shallower layer.

616
00:50:40,343 --> 00:50:43,087
And your model that weren't
able to learn properly,

617
00:50:43,087 --> 00:50:47,105
it should be able to learn at least this.

618
00:50:44,193 --> 00:50:51,533


619
00:50:47,105 --> 00:50:50,688
And so motivated by
this their solution was

620
00:50:52,418 --> 00:50:56,994
well how can we make it
easier for our architecture,

621
00:50:55,933 --> 00:50:59,979


622
00:50:56,994 --> 00:50:59,896
our model to learn these
kinds of solutions,

623
00:50:59,896 --> 00:51:01,404
or at least something like this?

624
00:51:01,404 --> 00:51:06,303
And so their idea is well
instead of just stacking

625
00:51:03,512 --> 00:51:08,034


626
00:51:06,303 --> 00:51:08,675
all these layers on top
of each other and having

627
00:51:08,675 --> 00:51:12,604
every layer try and learn
some underlying mapping

628
00:51:10,290 --> 00:51:16,671


629
00:51:12,604 --> 00:51:17,564
of a desired function, lets
instead have these blocks,

630
00:51:17,564 --> 00:51:20,472
where we try and fit a residual mapping,

631
00:51:20,472 --> 00:51:22,518
instead of a direct mapping.

632
00:51:22,518 --> 00:51:24,863
And so what this looks
like is here on this right

633
00:51:24,863 --> 00:51:29,030
where the input to these block
is just the input coming in

634
00:51:26,397 --> 00:51:30,619


635
00:51:30,628 --> 00:51:34,795
and here we are going to
use our, here on the side,

636
00:51:36,770 --> 00:51:44,093


637
00:51:38,148 --> 00:51:41,051
we're going to use our
layers to try and fit

638
00:51:41,051 --> 00:51:44,218
some residual of our desire to H of X,

639
00:51:44,093 --> 00:51:50,766


640
00:51:45,142 --> 00:51:49,309
minus X instead of the desired
function H of X directly.

641
00:51:50,260 --> 00:51:53,814
And so basically at the
end of this block we take

642
00:51:50,766 --> 00:52:01,457


643
00:51:53,814 --> 00:51:56,637
the step connection on
this right here, this loop,

644
00:51:56,637 --> 00:51:59,787
where we just take our input,
we just use pass it through

645
00:51:59,787 --> 00:52:03,346
as an identity, and so if
we had no weight layers

646
00:52:01,457 --> 00:52:08,803


647
00:52:03,346 --> 00:52:05,102
in between it was just
going to be the identity

648
00:52:05,102 --> 00:52:08,051
it would be the same thing
as the output, but now we use

649
00:52:08,051 --> 00:52:10,955
our additional weight
layers to learn some delta,

650
00:52:08,803 --> 00:52:18,448


651
00:52:10,955 --> 00:52:13,372
for some residual from our X.

652
00:52:14,877 --> 00:52:16,604
And so now the output
of this is going to be

653
00:52:16,604 --> 00:52:19,937
just our original R X plus some residual

654
00:52:18,448 --> 00:52:24,072


655
00:52:20,895 --> 00:52:22,003
that we're going to call it.

656
00:52:22,003 --> 00:52:25,312
It's basically a delta
and so the idea is that

657
00:52:24,072 --> 00:52:29,780


658
00:52:25,312 --> 00:52:29,238
now the output it should
be easy for example,

659
00:52:29,238 --> 00:52:32,238
in the case where identity is ideal,

660
00:52:29,780 --> 00:52:36,642


661
00:52:33,320 --> 00:52:37,078
to just squash all of
these weights of F of X

662
00:52:37,078 --> 00:52:40,059
from our weight layers
just set it to all zero

663
00:52:37,777 --> 00:52:41,944


664
00:52:40,059 --> 00:52:42,308
for example, then we're
just going to get identity

665
00:52:42,308 --> 00:52:44,319
as the output, and we can get something,

666
00:52:44,319 --> 00:52:47,831
for example, close to this
solution by construction

667
00:52:47,831 --> 00:52:49,388
that we had earlier.

668
00:52:49,388 --> 00:52:51,314
Right, so this is just
a network architecture

669
00:52:51,314 --> 00:52:53,448
that says okay, let's try and fit this,

670
00:52:53,448 --> 00:52:57,574
learn how our weight layers
residual, and be something

671
00:52:53,910 --> 00:53:01,245


672
00:52:57,574 --> 00:53:01,772
close, that way it'll more
likely be something close to X,

673
00:53:01,772 --> 00:53:04,151
it's just modifying X,
than to learn exactly

674
00:53:04,151 --> 00:53:06,198
this full mapping of what it should be.

675
00:53:06,198 --> 00:53:09,059
Okay, any questions about this?

676
00:53:07,127 --> 00:53:10,165


677
00:53:09,059 --> 00:53:09,999
[student speaks off mic]

678
00:53:09,999 --> 00:53:13,499
- Question is is there the same dimension?

679
00:53:14,580 --> 00:53:18,413
So yes these two paths
are the same dimension.

680
00:53:15,980 --> 00:53:22,672


681
00:53:19,562 --> 00:53:22,383
In general either it's the same dimension,

682
00:53:22,383 --> 00:53:24,666
or what they actually
do is they have these

683
00:53:22,672 --> 00:53:27,785


684
00:53:24,666 --> 00:53:27,571
projections and shortcuts
and they have different ways

685
00:53:27,571 --> 00:53:31,790
of padding to make things work
out to be the same dimension.

686
00:53:27,785 --> 00:53:33,034


687
00:53:31,790 --> 00:53:33,098
Depth wise.

688
00:53:33,098 --> 00:53:34,205
Yes

689
00:53:34,205 --> 00:53:35,763
- [Student] When you use the word residual

690
00:53:35,763 --> 00:53:39,930
you were talking about [mumbles off mic]

691
00:53:43,709 --> 00:53:47,637


692
00:53:46,667 --> 00:53:49,407
- So the question is what
exactly do we mean by

693
00:53:47,637 --> 00:53:50,818


694
00:53:49,407 --> 00:53:52,968
residual this output
of this transformation

695
00:53:52,968 --> 00:53:54,448
is a residual?

696
00:53:54,448 --> 00:53:58,419
So we can think of our output
here right as this F of X

697
00:53:55,278 --> 00:54:03,460


698
00:53:58,419 --> 00:54:02,709
plus X, where F of X is the
output of our transformation

699
00:54:02,709 --> 00:54:06,346
and then X is our input,
just passed through

700
00:54:03,460 --> 00:54:12,744


701
00:54:06,346 --> 00:54:07,460
by the identity.

702
00:54:07,460 --> 00:54:10,605
So we'd like to using a plain layer,

703
00:54:10,605 --> 00:54:12,854
what we're trying to do is learn something

704
00:54:12,854 --> 00:54:16,287
like H of X, but what we saw
earlier is that it's hard

705
00:54:14,007 --> 00:54:17,246


706
00:54:16,287 --> 00:54:18,008
to learn H of X.

707
00:54:18,008 --> 00:54:21,481
It's a good H of X as we
get very deep networks.

708
00:54:18,972 --> 00:54:22,387


709
00:54:21,481 --> 00:54:23,771
And so here the idea is
let's try and break it down

710
00:54:22,387 --> 00:54:28,457


711
00:54:23,771 --> 00:54:27,301
instead of as H of X is
equal to F of X plus,

712
00:54:27,301 --> 00:54:30,248
and let's just try and learn F of X.

713
00:54:28,457 --> 00:54:34,459


714
00:54:30,248 --> 00:54:33,648
And so instead of learning
directly this H of X

715
00:54:33,648 --> 00:54:36,341
we just want to learn what
is it that we need to add

716
00:54:36,341 --> 00:54:40,551
or subtract to our input as
we move on to the next layer.

717
00:54:40,551 --> 00:54:44,160
So you can think of it as
kind of modifying this input,

718
00:54:44,160 --> 00:54:45,469
in place in a sense.

719
00:54:45,469 --> 00:54:46,699
We have--

720
00:54:46,699 --> 00:54:49,931
[interrupted by student mumbling off mic]

721
00:54:47,461 --> 00:54:54,073


722
00:54:49,931 --> 00:54:51,654
- The question is, when we're
saying the word residual

723
00:54:51,654 --> 00:54:53,248
are we talking about F of X?

724
00:54:53,248 --> 00:54:54,357
Yeah.

725
00:54:54,357 --> 00:54:56,522
So F of X is what we're
calling the residual.

726
00:54:56,522 --> 00:54:58,939
And it just has that meaning.

727
00:54:59,683 --> 00:55:05,251


728
00:55:02,287 --> 00:55:04,751
Yes another question.

729
00:55:04,751 --> 00:55:08,251
[student mumbles off mic]

730
00:55:05,251 --> 00:55:11,528


731
00:55:12,129 --> 00:55:14,334
- So the question is in
practice do we just sum

732
00:55:14,334 --> 00:55:17,480
F of X and X together, or
do we learn some weighted

733
00:55:17,480 --> 00:55:20,955
combination and you just do a direct sum.

734
00:55:18,137 --> 00:55:23,353


735
00:55:20,955 --> 00:55:23,599
Because when you do a direct sum,

736
00:55:23,599 --> 00:55:26,869
this is the idea of let
me just learn what is it

737
00:55:26,869 --> 00:55:29,619
I have to add or subtract onto X.

738
00:55:27,132 --> 00:55:32,152


739
00:55:31,462 --> 00:55:35,273
Is this clear to everybody,
the main intuition?

740
00:55:32,152 --> 00:55:37,058


741
00:55:35,273 --> 00:55:36,171
Question.

742
00:55:36,171 --> 00:55:39,588
[student speaks off mic]

743
00:55:37,058 --> 00:55:40,630


744
00:55:41,531 --> 00:55:43,865
- Yeah, so the question
is not clear why is it

745
00:55:43,865 --> 00:55:45,823
that learning the
residual should be easier

746
00:55:45,823 --> 00:55:47,909
than learning the direct mapping?

747
00:55:47,909 --> 00:55:50,609
And so this is just their hypotheses,

748
00:55:48,303 --> 00:55:54,433


749
00:55:50,609 --> 00:55:55,181
and a hypotheses is that if
we're learning the residual

750
00:55:55,181 --> 00:55:59,557
you just have to learn
what's the delta to X right?

751
00:55:59,557 --> 00:56:02,947
And if our hypotheses is that generally

752
00:56:00,654 --> 00:56:05,214


753
00:56:02,947 --> 00:56:07,169
even something like our
solution by construction,

754
00:56:05,214 --> 00:56:09,097


755
00:56:07,169 --> 00:56:10,690
where we had some number
of these shallow layers

756
00:56:09,097 --> 00:56:14,246


757
00:56:10,690 --> 00:56:13,718
that were learned and we had
all these identity mappings

758
00:56:13,718 --> 00:56:16,911
at the top this was a
solution that should have been

759
00:56:14,246 --> 00:56:23,349


760
00:56:16,911 --> 00:56:20,219
good, and so that implies that
maybe a lot of these layers,

761
00:56:20,219 --> 00:56:23,157
actually something just close to identity,

762
00:56:23,157 --> 00:56:24,795
would be a good layer

763
00:56:24,795 --> 00:56:27,092
And so because of that,
now we formulate this

764
00:56:27,092 --> 00:56:30,168
as being able to learn the identity

765
00:56:30,168 --> 00:56:31,764
plus just a little delta.

766
00:56:31,764 --> 00:56:35,125
And if really the identity
is best we just make

767
00:56:35,125 --> 00:56:37,573
F of X squashes transformation
to just be zero,

768
00:56:37,573 --> 00:56:39,662
which is something that's relatively,

769
00:56:39,662 --> 00:56:41,173
might seem easier to learn,

770
00:56:41,173 --> 00:56:43,509
also we're able to get
things that are close

771
00:56:43,509 --> 00:56:45,594
to identity mappings.

772
00:56:44,436 --> 00:56:49,816


773
00:56:45,594 --> 00:56:47,927
And so again this is not
something that's necessarily

774
00:56:47,927 --> 00:56:51,776
proven or anything it's just
the intuition and hypothesis,

775
00:56:49,816 --> 00:56:55,214


776
00:56:51,776 --> 00:56:54,848
and then we'll also see
later some works where people

777
00:56:54,848 --> 00:56:56,533
are actually trying to
challenge this and say oh maybe

778
00:56:55,214 --> 00:56:59,054


779
00:56:56,533 --> 00:56:59,518
it's not actually the residuals
that are so necessary,

780
00:56:59,518 --> 00:57:03,069
but at least this is the
hypothesis for this paper,

781
00:57:03,069 --> 00:57:06,105
and in practice using this model,

782
00:57:04,021 --> 00:57:07,273


783
00:57:06,105 --> 00:57:08,317
it was able to do very well.

784
00:57:07,273 --> 00:57:12,749


785
00:57:08,317 --> 00:57:09,620
Question.

786
00:57:09,620 --> 00:57:13,037
[student speaks off mic]

787
00:57:12,749 --> 00:57:17,017


788
00:57:17,017 --> 00:57:23,000


789
00:57:23,000 --> 00:57:28,771


790
00:57:28,771 --> 00:57:31,115


791
00:57:31,115 --> 00:57:39,810


792
00:57:39,810 --> 00:57:44,714


793
00:57:42,623 --> 00:57:45,394
- Yes so the question is
have people tried other ways

794
00:57:45,394 --> 00:57:49,938
of combining the inputs
from previous layers and yes

795
00:57:49,938 --> 00:57:52,556
so this is basically a very
active area of research

796
00:57:52,556 --> 00:57:55,018
on and how we formulate
all these connections,

797
00:57:55,018 --> 00:57:57,557
and what's connected to what
in all of these structures.

798
00:57:57,557 --> 00:58:00,174
So we'll see a few more
examples of different network

799
00:58:00,174 --> 00:58:04,505
architectures briefly later
but this is an active area

800
00:58:01,043 --> 00:58:07,051


801
00:58:04,505 --> 00:58:05,505
of research.

802
00:58:06,468 --> 00:58:10,245
OK so we basically have all
of these residual blocks

803
00:58:07,051 --> 00:58:11,672


804
00:58:10,245 --> 00:58:12,903
that are stacked on top of each other.

805
00:58:11,672 --> 00:58:17,257


806
00:58:12,903 --> 00:58:15,598
We can see the full resident architecture.

807
00:58:15,598 --> 00:58:18,750
Each of these residual blocks
has two three by three conv

808
00:58:17,257 --> 00:58:26,466


809
00:58:18,750 --> 00:58:23,285
layers as part of this block
and there's also been work

810
00:58:23,285 --> 00:58:26,479
just saying that this happens
to be a good configuration

811
00:58:26,479 --> 00:58:28,109
that works well.

812
00:58:28,109 --> 00:58:30,638
We stack all these blocks
together very deeply.

813
00:58:30,638 --> 00:58:34,161
Another thing like with
this very deep architecture

814
00:58:31,594 --> 00:58:35,817


815
00:58:34,161 --> 00:58:37,911
it's basically also
enabling up to 150 layers

816
00:58:35,817 --> 00:58:38,129


817
00:58:38,129 --> 00:58:42,233


818
00:58:42,231 --> 00:58:45,981
deep of this, and then
what we do is we stack

819
00:58:47,392 --> 00:58:49,725
all these and periodically we also double

820
00:58:49,725 --> 00:58:52,042
the number of filters
and down sample spatially

821
00:58:52,042 --> 00:58:54,792
using stride two when we do that.

822
00:58:54,676 --> 00:59:01,817


823
00:58:56,666 --> 00:58:59,319
And then we have this additional [mumbles]

824
00:58:59,319 --> 00:59:02,835
at the very beginning of our network

825
00:59:01,817 --> 00:59:11,166


826
00:59:02,835 --> 00:59:03,668
and at the end we also hear,

827
00:59:03,668 --> 00:59:04,677
don't have any fully connected layers

828
00:59:04,677 --> 00:59:06,844
and we just have a global
average pooling layer

829
00:59:06,844 --> 00:59:09,451
that's going to average
over everything spatially,

830
00:59:09,451 --> 00:59:13,618
and then be input into the
last 1000 way classification.

831
00:59:11,166 --> 00:59:18,873


832
00:59:15,504 --> 00:59:17,801
So this is the full ResNet architecture

833
00:59:17,801 --> 00:59:20,333
and it's very simple and
elegant just stacking up

834
00:59:18,873 --> 00:59:27,273


835
00:59:20,333 --> 00:59:22,745
all of these ResNet blocks
on top of each other,

836
00:59:22,745 --> 00:59:27,032
and they have total depths
of up to 34, 50, 100,

837
00:59:27,032 --> 00:59:30,199
and they tried up to 152 for ImageNet.

838
00:59:27,273 --> 00:59:33,221


839
00:59:33,221 --> 00:59:40,208


840
00:59:35,040 --> 00:59:39,369
OK so one additional
thing just to know is that

841
00:59:39,369 --> 00:59:41,869
for a very deep network,
so the ones that are more

842
00:59:40,208 --> 00:59:44,458


843
00:59:41,869 --> 00:59:44,774
than 50 layers deep, they
also use bottleneck layers

844
00:59:44,774 --> 00:59:47,473
similar to what GoogleNet did
in order to improve efficiency

845
00:59:47,473 --> 00:59:51,865
and so within each block
now you're going to,

846
00:59:51,865 --> 00:59:54,645
what they did is, have this
one by one conv filter,

847
00:59:52,233 --> 00:59:58,409


848
00:59:54,645 --> 00:59:58,005
that first projects it
down to a smaller depth.

849
00:59:58,005 --> 01:00:01,774
So again if we are looking
at let's say 28 by 28

850
00:59:58,409 --> 01:00:04,161


851
01:00:01,774 --> 01:00:05,087
by 256 implant, we do
this one by one conv,

852
01:00:05,087 --> 01:00:06,926
it's taking it's
projecting the depth down.

853
01:00:05,583 --> 01:00:07,587


854
01:00:06,926 --> 01:00:08,759
We get 28 by 28 by 64.

855
01:00:07,587 --> 01:00:11,521


856
01:00:09,917 --> 01:00:12,220
Now your convolution
your three by three conv,

857
01:00:12,220 --> 01:00:15,809
in here they only have
one, is operating over this

858
01:00:14,268 --> 01:00:18,759


859
01:00:15,809 --> 01:00:19,296
reduced step so it's going
to be less expensive,

860
01:00:19,296 --> 01:00:21,881
and then afterwards they have another

861
01:00:21,881 --> 01:00:24,621
one by one conv that
projects the depth back up

862
01:00:24,621 --> 01:00:28,180
to 256, and so, this is
the actual block that

863
01:00:27,094 --> 01:00:33,600


864
01:00:28,180 --> 01:00:30,680
you'll see in deeper networks.

865
01:00:33,831 --> 01:00:38,618
So in practice the ResNet
also uses batch normalization

866
01:00:38,618 --> 01:00:42,092
after every conv layer, they
use Xavier initialization

867
01:00:42,092 --> 01:00:46,259
with an extra scaling factor
that they helped introduce

868
01:00:45,907 --> 01:00:50,989


869
01:00:47,221 --> 01:00:51,388
to improve the initialization
trained with SGD + momentum.

870
01:00:50,989 --> 01:00:54,761


871
01:00:52,414 --> 01:00:54,622
Their learning rate they
use a similar learning rate

872
01:00:54,622 --> 01:00:57,280
type of schedule where you
decay your learning rate

873
01:00:57,280 --> 01:01:00,280
when your validation error plateaus.

874
01:01:02,561 --> 01:01:05,351
Mini batch size 256, a
little bit of weight decay

875
01:01:05,351 --> 01:01:06,684
and no drop out.

876
01:01:08,455 --> 01:01:10,990
And so experimentally they
were able to show that they

877
01:01:10,990 --> 01:01:12,710
were able to train these
very deep networks,

878
01:01:12,710 --> 01:01:14,391
without degrading.

879
01:01:13,280 --> 01:01:18,100


880
01:01:14,391 --> 01:01:17,582
They were able to have
basically good gradient flow

881
01:01:17,582 --> 01:01:19,870
coming all the way back
down through the network.

882
01:01:18,100 --> 01:01:23,369


883
01:01:19,870 --> 01:01:23,435
They tried up to 152 layers on ImageNet,

884
01:01:23,435 --> 01:01:27,442
1200 on Cifar, which is a,
you have played with it,

885
01:01:25,957 --> 01:01:33,011


886
01:01:27,442 --> 01:01:31,525
but a smaller data set
and they also saw that now

887
01:01:32,804 --> 01:01:35,556
you're deeper networks are
able to achieve lower training

888
01:01:33,011 --> 01:01:37,748


889
01:01:35,556 --> 01:01:37,113
errors as expected.

890
01:01:37,113 --> 01:01:40,228
So you don't have the same strange plots

891
01:01:37,748 --> 01:01:40,009


892
01:01:40,228 --> 01:01:43,841
that we saw earlier where the behavior

893
01:01:43,841 --> 01:01:45,353
was in the wrong direction.

894
01:01:45,353 --> 01:01:48,260
And so from here they were
able to sweep first place

895
01:01:47,214 --> 01:01:54,531


896
01:01:48,260 --> 01:01:50,344
at all of the ILSVRC competitions,

897
01:01:50,344 --> 01:01:53,570
and all of the COCO competitions in 2015

898
01:01:53,570 --> 01:01:55,653
by a significant margins.

899
01:01:54,531 --> 01:01:59,670


900
01:01:56,962 --> 01:02:01,574
Their total top five error
was 3.6 % for a classification

901
01:01:59,670 --> 01:02:06,051


902
01:02:01,574 --> 01:02:05,626
and this is actually better
than human performance

903
01:02:05,626 --> 01:02:07,459
in the ImageNet paper.

904
01:02:06,051 --> 01:02:17,765


905
01:02:09,712 --> 01:02:12,864
There was also a human
metric that came from

906
01:02:12,864 --> 01:02:17,031
actually [mumbles] our
lab Andre Kapathy spent

907
01:02:18,023 --> 01:02:21,940
like a week training
himself and then basically

908
01:02:18,606 --> 01:02:22,531


909
01:02:23,127 --> 01:02:25,540
did all of, did this task himself

910
01:02:25,540 --> 01:02:29,341
and was I think somewhere around 5-ish %,

911
01:02:26,291 --> 01:02:38,145


912
01:02:29,341 --> 01:02:31,584
and so I was basically able to do

913
01:02:31,584 --> 01:02:35,001
better than the then that human at least.

914
01:02:36,985 --> 01:02:41,072
Okay, so these are kind
of the main networks

915
01:02:38,145 --> 01:02:42,403


916
01:02:41,072 --> 01:02:42,879
that have been used recently.

917
01:02:42,879 --> 01:02:45,211
We had AlexNet starting off with first,

918
01:02:45,211 --> 01:02:48,814
VGG and GoogleNet are still very popular,

919
01:02:48,814 --> 01:02:51,967
but ResNet is the most
recent best performing model

920
01:02:51,967 --> 01:02:56,100
that if you're looking for
something training a new network

921
01:02:52,259 --> 01:02:57,053


922
01:02:56,100 --> 01:02:57,695
ResNet is available, you should try

923
01:02:57,695 --> 01:02:59,028
working with it.

924
01:03:00,964 --> 01:03:04,844
So just quickly looking at
some of this getting a better

925
01:03:04,844 --> 01:03:07,213
sense of the complexity involved.

926
01:03:07,213 --> 01:03:09,098
So here we have some
plots that are sorted by

927
01:03:09,098 --> 01:03:12,848
performance so this is
top one accuracy here,

928
01:03:14,003 --> 01:03:16,085
and higher is better.

929
01:03:16,085 --> 01:03:17,887
And so you'll see a lot
of these models that we

930
01:03:17,887 --> 01:03:19,817
talked about, as well as
some different versions

931
01:03:19,817 --> 01:03:22,350
of them so, this
GoogleNet inception thing,

932
01:03:22,350 --> 01:03:26,266
I think there's like V2,
V3 and the best one here

933
01:03:26,266 --> 01:03:29,135
is V4, which is actually
a ResNet plus inception

934
01:03:26,809 --> 01:03:33,039


935
01:03:29,135 --> 01:03:32,199
combination, so these are just kind of

936
01:03:32,199 --> 01:03:34,820
more incremental, smaller
changes that they've

937
01:03:33,039 --> 01:03:37,230


938
01:03:34,820 --> 01:03:37,555
built on top of them,
and so that's the best

939
01:03:37,555 --> 01:03:39,969
performing model here.

940
01:03:39,969 --> 01:03:42,756
And if we look on the
right, these plots of their

941
01:03:42,756 --> 01:03:46,256
computational complexity here it's sorted.

942
01:03:44,085 --> 01:03:54,530


943
01:03:48,496 --> 01:03:51,609
The Y axis is your top one accuracy

944
01:03:51,609 --> 01:03:53,123
so higher is better.

945
01:03:53,123 --> 01:03:57,223
The X axis is your operations
and so the more to the right,

946
01:03:54,530 --> 01:04:03,445


947
01:03:57,223 --> 01:03:59,749
the more ops you're doing,
the more computationally

948
01:03:59,749 --> 01:04:02,456
expensive and then the bigger the circle,

949
01:04:02,456 --> 01:04:03,884
your circle is your memory usage,

950
01:04:03,884 --> 01:04:06,097
so the gray circles are referenced here,

951
01:04:06,097 --> 01:04:08,061
but the bigger the circle
the more memory usage

952
01:04:08,061 --> 01:04:11,824
and so here we can see
that VGG these green ones

953
01:04:10,419 --> 01:04:20,131


954
01:04:11,824 --> 01:04:14,152
are kind of the least efficient.

955
01:04:14,152 --> 01:04:15,909
They have the biggest memory,

956
01:04:15,909 --> 01:04:17,016
the most operations,

957
01:04:17,016 --> 01:04:19,433
but they they do pretty well.

958
01:04:20,648 --> 01:04:23,221
GoogleNet is the most efficient here.

959
01:04:23,221 --> 01:04:25,757
It's way down on the operation side,

960
01:04:25,757 --> 01:04:30,085
as well as a small little
circle for memory usage.

961
01:04:28,931 --> 01:04:34,026


962
01:04:30,085 --> 01:04:34,059
AlexNet, our earlier
model, has lowest accuracy.

963
01:04:34,059 --> 01:04:35,985
It's relatively smaller compute, because

964
01:04:35,985 --> 01:04:38,804
it's a smaller network, but
it's also not particularly

965
01:04:38,804 --> 01:04:40,221
memory efficient.

966
01:04:39,571 --> 01:04:42,428


967
01:04:42,119 --> 01:04:47,026
And then ResNet here, we
have moderate efficiency.

968
01:04:42,428 --> 01:04:45,996


969
01:04:45,996 --> 01:04:55,450


970
01:04:47,026 --> 01:04:49,310
It's kind of in the middle,
both in terms of memory

971
01:04:49,310 --> 01:04:53,310
and operations, and it
has the highest accuracy.

972
01:04:55,450 --> 01:05:00,424


973
01:04:56,839 --> 01:04:58,838
And so here also are
some additional plots.

974
01:04:58,838 --> 01:05:01,618
You can look at these
more on your own time,

975
01:05:00,424 --> 01:05:07,759


976
01:05:01,618 --> 01:05:04,600
but this plot on the left is
showing the forward pass time

977
01:05:04,600 --> 01:05:07,825
and so this is in milliseconds
and you can up at the top

978
01:05:07,825 --> 01:05:11,253
VGG forward passes about 200
milliseconds you can get about

979
01:05:11,253 --> 01:05:13,466
five frames per second with this,

980
01:05:13,466 --> 01:05:15,678
and this is sorted in order.

981
01:05:15,678 --> 01:05:18,459
There's also this plot on
the right looking at power

982
01:05:18,459 --> 01:05:22,584
consumption and if you look
more at this paper here,

983
01:05:22,584 --> 01:05:25,693
there's further analysis of
these kinds of computational

984
01:05:25,693 --> 01:05:26,693
comparisons.

985
01:05:29,163 --> 01:05:38,624


986
01:05:31,414 --> 01:05:34,318
So these were the main
architectures that you should

987
01:05:34,318 --> 01:05:36,776
really know in-depth and be familiar with,

988
01:05:36,776 --> 01:05:39,560
and be thinking about actively using.

989
01:05:39,560 --> 01:05:41,278
But now I'm going just
to go briefly through

990
01:05:41,278 --> 01:05:43,156
some other architectures
that are just good

991
01:05:43,156 --> 01:05:46,323
to know either historical inspirations

992
01:05:45,070 --> 01:05:47,981


993
01:05:47,290 --> 01:05:50,040
or more recent areas of research.

994
01:05:47,981 --> 01:05:52,011


995
01:05:51,526 --> 01:05:53,569
So the first one Network in Network,

996
01:05:52,011 --> 01:06:00,795


997
01:05:53,569 --> 01:05:57,152
this is from 2014, and
the idea behind this

998
01:06:01,339 --> 01:06:06,334
is that we have these
vanilla convolutional layers

999
01:06:06,334 --> 01:06:10,261
but we also have these,
this introduces the idea of

1000
01:06:10,261 --> 01:06:13,493
MLP conv layers they call
it, which are micro networks

1001
01:06:10,864 --> 01:06:14,337


1002
01:06:13,493 --> 01:06:15,497
or basically network within networth, the

1003
01:06:14,337 --> 01:06:15,941


1004
01:06:15,497 --> 01:06:16,928
name of the paper.

1005
01:06:16,928 --> 01:06:21,088
Where within each conv
layer trying to stack an MLP

1006
01:06:21,088 --> 01:06:23,962
with a couple of fully
connected layers on top of

1007
01:06:23,962 --> 01:06:26,293
just the standard conv
and be able to compute

1008
01:06:26,293 --> 01:06:28,989
more abstract features for these local

1009
01:06:28,989 --> 01:06:29,977
patches right.

1010
01:06:29,977 --> 01:06:32,017
So instead of sliding
just a conv filter around,

1011
01:06:32,017 --> 01:06:36,153
it's sliding a slightly
more complex hierarchical

1012
01:06:36,153 --> 01:06:40,780
set of filters around
and using that to get the

1013
01:06:36,818 --> 01:06:40,741


1014
01:06:40,780 --> 01:06:42,785
activation maps.

1015
01:06:42,785 --> 01:06:46,092
And so, it uses these fully connected,

1016
01:06:46,092 --> 01:06:48,751
or basically one by one
conv kind of layers.

1017
01:06:48,751 --> 01:06:51,001
It's going to stack them all up like the

1018
01:06:51,001 --> 01:06:53,701
bottom diagram here where
we just have these networks

1019
01:06:52,523 --> 01:07:03,360


1020
01:06:53,701 --> 01:06:58,006
within networks stacked
in each of the layers.

1021
01:06:58,006 --> 01:07:01,475
And the main reason to know this is just

1022
01:07:01,475 --> 01:07:04,825
it was kind of a precursor
to GoogleNet and ResNet

1023
01:07:03,360 --> 01:07:05,527


1024
01:07:04,825 --> 01:07:08,584
in 2014 with this idea
of bottleneck layers

1025
01:07:06,713 --> 01:07:10,763


1026
01:07:08,584 --> 01:07:10,912
that you saw used very heavily in there.

1027
01:07:10,912 --> 01:07:13,828
And it also had a little bit
of philosophical inspiration

1028
01:07:13,828 --> 01:07:17,388
for GoogleNet for this idea
of a local network typology

1029
01:07:17,388 --> 01:07:19,963
network in network that they also used,

1030
01:07:19,963 --> 01:07:22,880
with a different kind of structure.

1031
01:07:25,048 --> 01:07:29,422
Now I'm going to talk
about a series of works,

1032
01:07:26,436 --> 01:07:33,371


1033
01:07:29,422 --> 01:07:32,125
on, or works since ResNet
that are mostly geared

1034
01:07:32,125 --> 01:07:34,655
towards improving resNet
and so this is more recent

1035
01:07:33,371 --> 01:07:39,489


1036
01:07:34,655 --> 01:07:37,569
research has been done since then.

1037
01:07:37,569 --> 01:07:38,923
I'm going to go over these pretty fast,

1038
01:07:38,923 --> 01:07:40,721
and so just at a very high level.

1039
01:07:39,489 --> 01:07:47,095


1040
01:07:40,721 --> 01:07:42,147
If you're interested in
any of these you should

1041
01:07:42,147 --> 01:07:45,564
look at the papers, to have more details.

1042
01:07:46,565 --> 01:07:50,686
So the authors of ResNet
a little bit later on

1043
01:07:47,095 --> 01:07:51,694


1044
01:07:50,686 --> 01:07:54,853
in 2016 also had this paper
where they improved the

1045
01:07:51,694 --> 01:07:55,782


1046
01:07:55,876 --> 01:07:57,552
ResNet block design.

1047
01:07:57,552 --> 01:08:01,327
And so they basically
adjusted what were the layers

1048
01:08:01,327 --> 01:08:03,825
that were in the ResNet block path,

1049
01:08:03,825 --> 01:08:07,299
and showed this new
structure was able to have

1050
01:08:07,299 --> 01:08:11,024
a more direct path in order
for propagating information

1051
01:08:11,024 --> 01:08:14,711
throughout the network,
and you want to have a good

1052
01:08:13,517 --> 01:08:23,024


1053
01:08:14,711 --> 01:08:16,883
path to propagate
information all the way up,

1054
01:08:16,883 --> 01:08:19,671
and then back up all the way down again.

1055
01:08:19,671 --> 01:08:22,244
And so they showed that this
new block was better for that

1056
01:08:22,245 --> 01:08:26,129
and was able to give better performance.

1057
01:08:24,100 --> 01:08:28,734


1058
01:08:26,129 --> 01:08:29,769
There's also a Wide Residual
networks which this paper

1059
01:08:28,734 --> 01:08:33,212


1060
01:08:29,769 --> 01:08:34,401
argued that while ResNets
made networks much deeper

1061
01:08:33,212 --> 01:08:37,215


1062
01:08:34,401 --> 01:08:36,687
as well as added these
residual connections

1063
01:08:36,687 --> 01:08:40,206
and their argument was
that residuals are really

1064
01:08:37,215 --> 01:08:44,216


1065
01:08:40,206 --> 01:08:41,038
the important factor.

1066
01:08:41,038 --> 01:08:42,334
Having this residual construction,

1067
01:08:42,335 --> 01:08:46,100
and not necessarily having
extremely deep networks.

1068
01:08:44,216 --> 01:08:52,804


1069
01:08:46,100 --> 01:08:50,277
And so what they did was they
used wider residual blocks,

1070
01:08:50,277 --> 01:08:52,702
and so what this means is
just more filters in every

1071
01:08:52,702 --> 01:08:53,604
conv layer.

1072
01:08:53,604 --> 01:08:56,806
So before we might have
F filters per layer

1073
01:08:56,807 --> 01:09:00,118
and they use these factors
of K and said well,

1074
01:08:58,238 --> 01:09:06,722


1075
01:09:00,118 --> 01:09:03,472
every layer it's going to be
F times K filters instead.

1076
01:09:03,473 --> 01:09:07,432
And so, using these
wider layers they showed

1077
01:09:07,432 --> 01:09:09,812
that their 50 layer wide
ResNet was able to out-perform

1078
01:09:07,737 --> 01:09:21,579


1079
01:09:09,812 --> 01:09:12,312
the 152 layer original ResNet,

1080
01:09:14,564 --> 01:09:17,099
and it also had the
additional advantages of

1081
01:09:17,099 --> 01:09:21,513
increasing with this,
even with the same amount

1082
01:09:21,513 --> 01:09:23,845
of parameters, tit's more
computationally efficient

1083
01:09:22,416 --> 01:09:32,277


1084
01:09:23,845 --> 01:09:26,587
because you can parallelize
these with operations

1085
01:09:26,587 --> 01:09:27,732
more easily.

1086
01:09:27,733 --> 01:09:31,143
Right just convolutions with more neurons

1087
01:09:31,143 --> 01:09:33,560
just spread across more kernels

1088
01:09:32,277 --> 01:09:37,357


1089
01:09:33,560 --> 01:09:36,430
as opposed to depth
that's more sequential,

1090
01:09:36,430 --> 01:09:39,416
so it's more computationally
efficient to increase

1091
01:09:37,358 --> 01:09:41,694


1092
01:09:39,416 --> 01:09:40,356
your width.

1093
01:09:40,356 --> 01:09:42,104
So here you can see
this work is starting to

1094
01:09:42,104 --> 01:09:44,402
trying to understand the
contributions of width

1095
01:09:44,403 --> 01:09:47,550
and depth and residual connections,

1096
01:09:46,455 --> 01:09:51,952


1097
01:09:47,550 --> 01:09:50,627
and making some arguments
for one way versus the other.

1098
01:09:50,627 --> 01:09:54,127
And this other paper around the same time,

1099
01:09:51,952 --> 01:09:57,257


1100
01:09:55,874 --> 01:09:58,935
I think maybe a little
bit later, is ResNeXt,

1101
01:09:57,257 --> 01:10:00,257


1102
01:09:58,935 --> 01:10:02,601
and so this is again,
the creators of ResNet

1103
01:10:01,177 --> 01:10:07,612


1104
01:10:02,601 --> 01:10:05,193
continuing to work on
pushing the architecture.

1105
01:10:05,193 --> 01:10:09,110
And here they also had
this idea of okay, let's

1106
01:10:07,612 --> 01:10:13,337


1107
01:10:10,509 --> 01:10:13,742
indeed tackle this width
thing more but instead of just

1108
01:10:13,742 --> 01:10:15,829
increasing the width
of this residual block

1109
01:10:15,829 --> 01:10:19,386
through more filters they have structure.

1110
01:10:19,386 --> 01:10:23,335
And so within each residual
block, multiple parallel

1111
01:10:21,694 --> 01:10:25,716


1112
01:10:23,335 --> 01:10:25,344
pathways and they're going to call

1113
01:10:25,344 --> 01:10:27,225
the total number of these
pathways the cardinality.

1114
01:10:25,716 --> 01:10:29,920


1115
01:10:27,225 --> 01:10:31,308
And so it's basically
taking the one ResNet block

1116
01:10:29,920 --> 01:10:36,377


1117
01:10:33,386 --> 01:10:36,341
with the bottlenecks and having
it be relatively thinner,

1118
01:10:36,341 --> 01:10:39,205
but having multiple of
these done in parallel.

1119
01:10:39,205 --> 01:10:43,373
And so here you can also
see that this both have some

1120
01:10:43,373 --> 01:10:45,262
relation to this idea of wide networks,

1121
01:10:45,262 --> 01:10:50,097
as well as to has some connection
to the inception module

1122
01:10:47,337 --> 01:10:56,856


1123
01:10:50,097 --> 01:10:52,461
as well right where we
have these parallel,

1124
01:10:52,461 --> 01:10:54,833
these layers operating in parallel.

1125
01:10:54,833 --> 01:10:59,000
And so now this ResNeXt has
some flavor of that as well.

1126
01:10:56,856 --> 01:10:58,732


1127
01:10:58,732 --> 01:11:05,714


1128
01:11:01,648 --> 01:11:05,653
So another approach
towards improving ResNets

1129
01:11:05,653 --> 01:11:09,445
was this idea called Stochastic
Depth and in this work

1130
01:11:09,445 --> 01:11:12,557
the motivation is well let's look more

1131
01:11:10,316 --> 01:11:14,887


1132
01:11:12,557 --> 01:11:14,688
at this depth problem.

1133
01:11:14,688 --> 01:11:18,855
Once you get deeper and
deeper the typical problems

1134
01:11:16,574 --> 01:11:23,678


1135
01:11:20,385 --> 01:11:22,347
that you're going to have
vanishing gradients right.

1136
01:11:22,347 --> 01:11:26,874
You're not able to, your
gradients will get smaller

1137
01:11:23,678 --> 01:11:31,737


1138
01:11:26,874 --> 01:11:29,077
and eventually vanish as
you're trying to back propagate

1139
01:11:29,077 --> 01:11:32,881
them over very long layers,
or a large number of layers.

1140
01:11:31,737 --> 01:11:43,594


1141
01:11:32,881 --> 01:11:36,003
And so what their motivation
is well let's try to have

1142
01:11:36,003 --> 01:11:40,418
short networks during training
and they use this idea

1143
01:11:40,418 --> 01:11:43,855
of dropping out a subset of
the layers during training.

1144
01:11:43,855 --> 01:11:46,474
And so for a subset of the
layers they just drop out

1145
01:11:46,474 --> 01:11:49,246
the weights and they just set
it to identity connection,

1146
01:11:49,246 --> 01:11:53,379
and now what you get is you
have these shorter networks

1147
01:11:49,838 --> 01:11:55,094


1148
01:11:53,379 --> 01:11:55,425
during training, you can pass back your

1149
01:11:55,425 --> 01:11:56,936
gradients better.

1150
01:11:56,936 --> 01:12:00,343
It's also a little more
efficient, and then it's

1151
01:12:00,343 --> 01:12:02,345
kind of like the drop out right.

1152
01:12:02,345 --> 01:12:04,884
It has this sort of flavor
that you've seen before.

1153
01:12:03,076 --> 01:12:07,716


1154
01:12:04,884 --> 01:12:07,251
And then at test time you want
to use the full deep network

1155
01:12:07,251 --> 01:12:08,918
that you've trained.

1156
01:12:07,716 --> 01:12:12,574


1157
01:12:11,256 --> 01:12:13,591
So these are some of the
works that looking at the

1158
01:12:12,574 --> 01:12:19,214


1159
01:12:13,591 --> 01:12:15,640
resident architecture, trying
to understand different

1160
01:12:15,640 --> 01:12:19,848
aspects of it and trying
to improve ResNet training.

1161
01:12:19,848 --> 01:12:23,241
And so there's also some
works now that are going

1162
01:12:23,241 --> 01:12:25,856
beyond ResNet that are
saying well what are some non

1163
01:12:23,471 --> 01:12:30,596


1164
01:12:25,856 --> 01:12:30,116
ResNet architectures that
maybe can also work better,

1165
01:12:30,116 --> 01:12:33,063
or comparable or better to ResNets.

1166
01:12:30,596 --> 01:12:34,078


1167
01:12:33,063 --> 01:12:36,579
And so one idea is
FractalNet, which came out

1168
01:12:34,078 --> 01:12:39,854


1169
01:12:36,579 --> 01:12:39,282
pretty recently, and the
argument in FractalNet

1170
01:12:39,282 --> 01:12:42,312
is that while residual
representations maybe

1171
01:12:39,854 --> 01:12:48,815


1172
01:12:42,312 --> 01:12:44,239
are not actually necessary,
so this goes back

1173
01:12:44,239 --> 01:12:46,083
to what we were talking about earlier.

1174
01:12:46,083 --> 01:12:48,586
What's the motivation of
residual networks and it seems

1175
01:12:48,586 --> 01:12:50,756
to make sense and there's, you know,

1176
01:12:48,815 --> 01:12:53,214


1177
01:12:50,756 --> 01:12:53,455
good reasons for why this
should help but in this paper

1178
01:12:53,455 --> 01:12:56,600
they're saying that well here
is a different architecture

1179
01:12:56,600 --> 01:12:59,217
that we're introducing, there's
no residual representations.

1180
01:12:59,217 --> 01:13:01,884
We think that the key is
more about transitioning

1181
01:13:01,884 --> 01:13:04,708
effectively from shallow to deep networks,

1182
01:13:03,054 --> 01:13:08,638


1183
01:13:04,708 --> 01:13:07,203
and so they have this fractal architecture

1184
01:13:07,203 --> 01:13:09,901
which has if you look on the right here,

1185
01:13:08,638 --> 01:13:19,934


1186
01:13:09,901 --> 01:13:14,068
these layers where they compose
it in this fractal fashion.

1187
01:13:15,579 --> 01:13:18,199
And so there's both
shallow and deep pathways

1188
01:13:18,199 --> 01:13:19,449
to your output.

1189
01:13:20,855 --> 01:13:23,269
And so they have these
different length pathways,

1190
01:13:23,269 --> 01:13:26,046
they train them with
dropping out sub paths,

1191
01:13:26,046 --> 01:13:30,378
and so again it has this
dropout kind of flavor,

1192
01:13:30,378 --> 01:13:33,151
and then at test time they'll
use the entire fractal network

1193
01:13:30,875 --> 01:13:35,956


1194
01:13:33,151 --> 01:13:35,846
and they show that this was able to

1195
01:13:35,846 --> 01:13:38,013
get very good performance.

1196
01:13:39,857 --> 01:13:42,953
There's another idea
called Densely Connected

1197
01:13:42,953 --> 01:13:45,696
convolutional Networks,
DenseNet, and this idea

1198
01:13:44,253 --> 01:13:51,817


1199
01:13:45,696 --> 01:13:48,397
is now we have these
blocks that are called

1200
01:13:48,397 --> 01:13:49,377
dense blocks.

1201
01:13:49,377 --> 01:13:51,588
And within each block
each layer is going to be

1202
01:13:51,588 --> 01:13:55,238
connected to every other layer after it,

1203
01:13:51,817 --> 01:13:53,275


1204
01:13:53,275 --> 01:13:58,974


1205
01:13:55,238 --> 01:13:56,750
in this feed forward fashion.

1206
01:13:56,750 --> 01:13:58,312
So within this block,
your input to the block

1207
01:13:58,312 --> 01:14:01,172
is also the input to
every other conv layer,

1208
01:13:58,974 --> 01:14:07,497


1209
01:14:01,172 --> 01:14:04,764
and as you compute each conv output,

1210
01:14:04,764 --> 01:14:06,609
those outputs are now connected to every

1211
01:14:06,609 --> 01:14:09,589
layer after and then,
these are all concatenated

1212
01:14:07,497 --> 01:14:11,036


1213
01:14:09,589 --> 01:14:12,500
as input to the conv
layer, and they do some

1214
01:14:11,036 --> 01:14:17,278


1215
01:14:12,500 --> 01:14:16,262
they have some other
processes for reducing

1216
01:14:16,262 --> 01:14:19,453
the dimensions and keeping efficient.

1217
01:14:17,278 --> 01:14:21,294


1218
01:14:19,453 --> 01:14:23,329
And so their main takeaway from this,

1219
01:14:21,294 --> 01:14:25,438


1220
01:14:23,329 --> 01:14:26,843
is that they argue that
this is alleviating

1221
01:14:25,438 --> 01:14:31,455


1222
01:14:26,843 --> 01:14:29,790
a vanishing gradient problem
because you have all of these

1223
01:14:29,790 --> 01:14:31,673
very dense connections.

1224
01:14:31,673 --> 01:14:35,722
It strengthens feature propagation
and then also encourages

1225
01:14:35,722 --> 01:14:38,134
future use right because
there are so many of these

1226
01:14:38,134 --> 01:14:41,807
connections each feature
map that you're learning

1227
01:14:40,174 --> 01:14:42,796


1228
01:14:41,807 --> 01:14:44,630
is input in multiple
later layers and being

1229
01:14:42,796 --> 01:14:47,358


1230
01:14:44,630 --> 01:14:46,297
used multiple times.

1231
01:14:47,358 --> 01:14:51,417


1232
01:14:48,716 --> 01:14:50,813
So these are just a
couple of ideas that are

1233
01:14:50,813 --> 01:14:55,025
you know alternatives or
what can we do that's not

1234
01:14:51,417 --> 01:14:54,410


1235
01:14:55,025 --> 01:14:57,836
ResNets and yet is still performing either

1236
01:14:57,836 --> 01:15:00,784
comparably or better to
ResNets and so this is

1237
01:14:58,596 --> 01:15:02,432


1238
01:15:00,784 --> 01:15:03,816
another very active area
of current research.

1239
01:15:02,432 --> 01:15:04,436


1240
01:15:03,816 --> 01:15:05,501
You can see that a lot of this is looking

1241
01:15:04,436 --> 01:15:09,314


1242
01:15:05,501 --> 01:15:09,057
at the way how different layers
are connected to each other

1243
01:15:09,057 --> 01:15:12,640
and how depth is managed
in these networks.

1244
01:15:09,314 --> 01:15:12,318


1245
01:15:12,318 --> 01:15:17,817


1246
01:15:14,338 --> 01:15:15,808
And so one last thing
that I wanted to mention

1247
01:15:15,808 --> 01:15:18,801
quickly, is just efficient networks.

1248
01:15:18,801 --> 01:15:21,831
So this idea of efficiency
and you saw that GoogleNet

1249
01:15:21,831 --> 01:15:24,216
was a work that was
looking into this direction

1250
01:15:24,216 --> 01:15:27,240
of how can we have efficient
networks which are important

1251
01:15:27,240 --> 01:15:30,260
for you know a lot of
practical usage both training

1252
01:15:30,260 --> 01:15:34,804
as well as especially
deployment and so this is

1253
01:15:34,804 --> 01:15:38,737
another recent network
that's called SqueezeNet

1254
01:15:38,737 --> 01:15:40,744
which is looking at
very efficient networks.

1255
01:15:40,744 --> 01:15:42,428
They have these things
called fire modules,

1256
01:15:42,428 --> 01:15:45,095
which consists of a
squeeze layer with a lot of

1257
01:15:43,534 --> 01:15:49,417


1258
01:15:45,095 --> 01:15:47,674
one by one filters and
then this feeds then into

1259
01:15:47,674 --> 01:15:50,455
an expand layer with one by
one and three by three filters,

1260
01:15:49,417 --> 01:15:53,761


1261
01:15:50,455 --> 01:15:53,490
and they're showing that with
this kind of architecture

1262
01:15:53,490 --> 01:15:57,820
they're able to get AlexNet
level accuracy on ImageNet,

1263
01:15:54,596 --> 01:15:57,252


1264
01:15:57,820 --> 01:16:00,030
but with 50 times fewer parameters,

1265
01:16:00,030 --> 01:16:03,140
and then you can further do
network compression on this

1266
01:16:03,140 --> 01:16:06,903
to get up to 500 times
smaller than AlexNet

1267
01:16:06,903 --> 01:16:10,905
and just have the whole
network just be 0.5 megs.

1268
01:16:07,852 --> 01:16:13,772


1269
01:16:10,905 --> 01:16:13,691
And so this is a direction
of how do we have

1270
01:16:13,691 --> 01:16:15,572
efficient networks model compression

1271
01:16:15,572 --> 01:16:17,955
that we'll cover more in a lecture later,

1272
01:16:17,955 --> 01:16:20,872
but just giving you a hint of that.

1273
01:16:22,666 --> 01:16:26,351
OK so today in summary we've
talked about different kinds

1274
01:16:26,351 --> 01:16:27,619
of CNN Architectures.

1275
01:16:27,619 --> 01:16:30,568
We looked in-depth at four
of the main architectures

1276
01:16:30,568 --> 01:16:32,365
that you'll see in wide usage.

1277
01:16:32,365 --> 01:16:36,363
AlexNet, one of the early,
very popular networks.

1278
01:16:32,951 --> 01:16:37,710


1279
01:16:36,363 --> 01:16:39,642
VGG and GoogleNet which
are still widely used.

1280
01:16:39,642 --> 01:16:43,695
But ResNet is kind of
taking over as the thing

1281
01:16:43,695 --> 01:16:46,716
that you should be
looking most when you can.

1282
01:16:46,716 --> 01:16:47,943
We also looked at these other networks

1283
01:16:47,943 --> 01:16:50,147
in a little bit more depth at a brief

1284
01:16:50,147 --> 01:16:51,397
level overview.

1285
01:16:52,731 --> 01:16:55,315
And so the takeaway that these
models that are available

1286
01:16:53,510 --> 01:16:58,849


1287
01:16:55,315 --> 01:16:57,974
they're in a lot of
[mumbles] so you can use them

1288
01:16:57,974 --> 01:16:59,038
when you need them.

1289
01:16:59,038 --> 01:17:01,085
There's a trend toward
extremely deep networks,

1290
01:17:01,085 --> 01:17:04,804
but there's also significant
research now around

1291
01:17:04,804 --> 01:17:07,637
the design of how do we connect layers,

1292
01:17:05,207 --> 01:17:13,190


1293
01:17:07,637 --> 01:17:10,785
skip connections, what
is connected to what,

1294
01:17:10,785 --> 01:17:14,343
and also using these to
design your architecture

1295
01:17:13,190 --> 01:17:15,809


1296
01:17:14,343 --> 01:17:16,229
to improve gradient flow.

1297
01:17:16,229 --> 01:17:19,013
There's an even more recent
trend towards examining

1298
01:17:19,013 --> 01:17:22,246
what's the necessity
of depth versus width,

1299
01:17:21,233 --> 01:17:25,649


1300
01:17:22,246 --> 01:17:23,558
residual connections.

1301
01:17:23,558 --> 01:17:25,358
Trade offs, what's
actually helping matters,

1302
01:17:25,358 --> 01:17:28,356
and so there's a lot of these recent works

1303
01:17:25,649 --> 01:17:29,969


1304
01:17:28,356 --> 01:17:30,153
in this direction that you can look into

1305
01:17:30,153 --> 01:17:32,190
some of the ones I pointed
out if you are interested.

1306
01:17:32,190 --> 01:17:34,407
And next time we'll talk about
Recurrent neural networks.

1307
01:17:34,407 --> 00:00:00,000


1308
01:17:34,710 --> 01:17:41,270


1309
01:17:41,270 --> 01:17:47,393


1310
01:17:47,393 --> 01:17:52,045


1311
01:17:53,169 --> 01:17:55,691