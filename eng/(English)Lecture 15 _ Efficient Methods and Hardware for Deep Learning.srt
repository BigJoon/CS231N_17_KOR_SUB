1
00:00:11,374 --> 00:00:14,567
- Hello everyone, welcome to CS231.

2
00:00:14,567 --> 00:00:17,618
I'm Song Han. Today I'm going to give a guest lecture

3
00:00:17,618 --> 00:00:21,468
on the efficient methods and hardware for deep learning.

4
00:00:21,468 --> 00:00:24,714
So I'm a fifth year PhD candidate here at Stanford,

5
00:00:24,714 --> 00:00:28,081
advised by Professor Bill Dally.

6
00:00:28,081 --> 00:00:31,093
So, in this course we have seen a lot of convolution neural

7
00:00:31,093 --> 00:00:33,932
networks, recurrent neural networks, or even

8
00:00:33,932 --> 00:00:37,358
since last time, the reinforcement learning.

9
00:00:37,358 --> 00:00:39,281
They are spanning a lot of applications.

10
00:00:39,281 --> 00:00:41,979
For example, the self-=driving car, machine translation,

11
00:00:41,979 --> 00:00:44,157
AlphaGo and Smart Robots.

12
00:00:44,157 --> 00:00:46,904
And it's changing our lives, but there is a recent

13
00:00:46,904 --> 00:00:50,781
trend that in order to achieve such high accuracy,

14
00:00:50,781 --> 00:00:53,652
the models are getting larger and larger.

15
00:00:53,652 --> 00:00:56,669
For example for ImageNet recognition, the winner from

16
00:00:56,669 --> 00:01:00,502
2012 to 2015, the model size increased by 16X.

17
00:01:02,519 --> 00:01:05,104
And just in one year, for Baidu's deep speech

18
00:01:05,104 --> 00:01:07,809
just in one year, the training operations, the number

19
00:01:07,809 --> 00:01:11,142
of training operations increased by 10X.

20
00:01:12,043 --> 00:01:15,651
So such large model creates lots of problems,

21
00:01:15,651 --> 00:01:18,941
for example the model size becomes larger and larger

22
00:01:18,941 --> 00:01:22,413
so it's difficult for them to be deployed either

23
00:01:22,413 --> 00:01:25,159
on those for example, on the mobile phones.

24
00:01:25,159 --> 00:01:28,232
If the item is larger than 100 megabytes, you

25
00:01:28,232 --> 00:01:30,797
cannot download until you connect to Wi-Fi.

26
00:01:30,797 --> 00:01:33,315
So those product managers and for example Baidu,

27
00:01:33,315 --> 00:01:36,280
Facebook, they are very sensitive to the size of the binary

28
00:01:36,280 --> 00:01:37,982
size of their model.

29
00:01:37,982 --> 00:01:40,358
And also for example, the self-driving car, you can only

30
00:01:40,358 --> 00:01:43,743
do those on over-the-air update for the model

31
00:01:43,743 --> 00:01:47,130
if the model is too large, it's also difficult.

32
00:01:47,130 --> 00:01:51,958
And the second challenge for those large models is

33
00:01:51,958 --> 00:01:55,272
that the training speed is extremely slow.

34
00:01:55,272 --> 00:01:58,930
For example, the ResNet152, which is only a few, less

35
00:01:58,930 --> 00:02:03,713
than 1% actually, more accurate than ResNet101.

36
00:02:03,713 --> 00:02:07,046
Takes 1.5 weeks to train on four Maxwell

37
00:02:08,839 --> 00:02:10,589
M40 GPUs for example.

38
00:02:11,703 --> 00:02:15,175
Which greatly limits either we are doing homework

39
00:02:15,175 --> 00:02:17,422
or if the researcher's designing new models is

40
00:02:17,422 --> 00:02:19,284
getting pretty slow.

41
00:02:19,284 --> 00:02:22,473
And the third challenge for those bulky model is

42
00:02:22,473 --> 00:02:24,377
the energy efficiency.

43
00:02:24,377 --> 00:02:27,730
For example, the AlphaGo beating Lee Sedol last year,

44
00:02:27,730 --> 00:02:31,563
took 2000 CPUs and 300 GPUs, which cost $3,000

45
00:02:33,090 --> 00:02:37,527
just to pay for the electric bill, which is insane.

46
00:02:37,527 --> 00:02:39,968
So either on those embedded devices, those models

47
00:02:39,968 --> 00:02:43,100
are draining your battery power for on data-center

48
00:02:43,100 --> 00:02:46,548
increases the total cost of ownership of maintaining

49
00:02:46,548 --> 00:02:48,215
a large data-center.

50
00:02:49,250 --> 00:02:51,678
For example, Google in their blog, they mentioned

51
00:02:51,678 --> 00:02:55,118
if all the users using the Google Voice Search for

52
00:02:55,118 --> 00:02:58,592
just three minutes, they have to double their data-center.

53
00:02:58,592 --> 00:03:00,509
So that's a large cost.

54
00:03:01,766 --> 00:03:04,802
So reducing such cost is very important.

55
00:03:04,802 --> 00:03:08,356
And let's see where is actually the energy consumed.

56
00:03:08,356 --> 00:03:11,024
The large model means lots of memory access.

57
00:03:11,024 --> 00:03:14,060
You have to access, load those models from the memory

58
00:03:14,060 --> 00:03:15,869
means more energy.

59
00:03:15,869 --> 00:03:19,541
If you look at how much energy is consumed by loading

60
00:03:19,541 --> 00:03:23,708
the memory versus how much is consumed by multiplications

61
00:03:24,852 --> 00:03:29,717
and add those arithmetic operations, the memory access

62
00:03:29,717 --> 00:03:33,550
is more than two or three orders of magnitude,

63
00:03:34,579 --> 00:03:38,746
more energy consuming than those arithmetic operations.

64
00:03:40,191 --> 00:03:43,996
So how to make deep learning more efficient.

65
00:03:43,996 --> 00:03:47,102
So we have to improve energy efficiency by this

66
00:03:47,102 --> 00:03:49,852
Algorithm and Hardware Co-Design.

67
00:03:50,700 --> 00:03:53,090
So this is the previous way, which is our hardware.

68
00:03:53,090 --> 00:03:57,257
For example, we have some benchmarks say Spec 2006

69
00:03:58,510 --> 00:04:01,039
and then run those benchmarks and tune your CPU

70
00:04:01,039 --> 00:04:03,956
architectures for those benchmarks.

71
00:04:06,015 --> 00:04:08,823
Now what we should do is to open up the box to see

72
00:04:08,823 --> 00:04:11,620
what can we do from algorithm side first and see what

73
00:04:11,620 --> 00:04:15,375
is the optimum question mark processing unit.

74
00:04:15,375 --> 00:04:18,733
That breaks the boundary between the algorithm

75
00:04:18,733 --> 00:04:22,316
hardware to improve the overall efficiency.

76
00:04:26,017 --> 00:04:29,779
So today's talk, I'm going to have the following agenda.

77
00:04:29,779 --> 00:04:33,910
We are going to cover four aspects: The algorithm hardware

78
00:04:33,910 --> 00:04:36,071
and inference and training.

79
00:04:36,071 --> 00:04:40,817
So they form a small two by two matrix, so includes the

80
00:04:40,817 --> 00:04:43,138
algorithm for efficient inference,

81
00:04:43,138 --> 00:04:45,291
hardware for efficient inference

82
00:04:45,291 --> 00:04:47,581
and the algorithm for efficient training,

83
00:04:47,581 --> 00:04:50,976
and lastly, the hardware for efficient training.

84
00:04:50,976 --> 00:04:53,125
For example, I'm going to cover the TPU, I'm

85
00:04:53,125 --> 00:04:54,609
going to cover the Volta.

86
00:04:54,609 --> 00:04:58,692
But before I cover those things, let's have three

87
00:04:59,741 --> 00:05:02,443
slides for Hardware 101.

88
00:05:02,443 --> 00:05:05,180
A brief introduction of the families of hardware

89
00:05:05,180 --> 00:05:06,430
in such a tree.

90
00:05:07,355 --> 00:05:11,955
So in general, we can have roughly two branches.

91
00:05:11,955 --> 00:05:14,761
One is general purpose hardware.

92
00:05:14,761 --> 00:05:18,844
It can do any applications versus the specialized

93
00:05:21,324 --> 00:05:25,249
hardware, which is tuned for a specific kind of

94
00:05:25,249 --> 00:05:29,113
applications, a domain of applications.

95
00:05:29,113 --> 00:05:31,962
So the general purpose hardware includes, the CPU

96
00:05:31,962 --> 00:05:35,621
or the GPU, and their difference is that CPU is

97
00:05:35,621 --> 00:05:38,288
latency oriented, single threaded.

98
00:05:38,288 --> 00:05:40,451
It's like a big elephant.

99
00:05:40,451 --> 00:05:43,534
While the GPU is throughput oriented.

100
00:05:44,486 --> 00:05:46,846
It has many small though weak threads, but there

101
00:05:46,846 --> 00:05:49,691
are thousands of such small weak cores.

102
00:05:49,691 --> 00:05:54,088
Like a group of small ants, where there are so many ants.

103
00:05:54,088 --> 00:05:58,255
And specialized hardware, roughly there are FPGAs and ASICs.

104
00:05:59,126 --> 00:06:03,274
So FPGA stand for Field Programmable Gate Array.

105
00:06:03,274 --> 00:06:07,748
So it is programmable, hardware programmable so its

106
00:06:07,748 --> 00:06:09,353
logic can be changed.

107
00:06:09,353 --> 00:06:13,520
So it's cheaper for you to try new ideas and do prototype,

108
00:06:14,597 --> 00:06:16,262
but it's less efficient.

109
00:06:16,262 --> 00:06:18,185
It's in the middle between the general purpose and

110
00:06:18,185 --> 00:06:19,018
pure ASIC.

111
00:06:19,965 --> 00:06:24,137
So ASIC stands for Application Specific Integrated Circuit.

112
00:06:24,137 --> 00:06:25,842
It has a fixed logic, just designed

113
00:06:25,842 --> 00:06:27,293
for a certain application.

114
00:06:27,293 --> 00:06:29,341
For example deep learning.

115
00:06:29,341 --> 00:06:34,264
And Google's TPU is a kind of ASIC and the neural networks

116
00:06:34,264 --> 00:06:37,852
we train on, the earlier GPUs is here.

117
00:06:37,852 --> 00:06:41,645
And another slide for Hardware 101 is the number

118
00:06:41,645 --> 00:06:43,657
representations.

119
00:06:43,657 --> 00:06:47,473
So in this slide, I'm going to convey you the idea that

120
00:06:47,473 --> 00:06:49,924
all the numbers in computer are not represented

121
00:06:49,924 --> 00:06:51,742
by a real number.

122
00:06:51,742 --> 00:06:54,536
It's not a real number, but they are actually discrete.

123
00:06:54,536 --> 00:06:57,977
Even for those floating point with your 32 Bit.

124
00:06:57,977 --> 00:07:02,301
Floating point numbers, their resolution is not perfect.

125
00:07:02,301 --> 00:07:06,336
It's not continuous, but it's discrete.

126
00:07:06,336 --> 00:07:10,271
So for example FP32, meaning using a 32 bit to represent

127
00:07:10,271 --> 00:07:12,147
a floating point number.

128
00:07:12,147 --> 00:07:15,296
So there are three components in the representation.

129
00:07:15,296 --> 00:07:18,907
The sign bit, the exponent bit, the mantissa,

130
00:07:18,907 --> 00:07:23,682
and the number it represents is shown by minus 1 to the S

131
00:07:23,682 --> 00:07:26,515
times 1.M times 2 to the exponent.

132
00:07:28,778 --> 00:07:32,745
So similar there is FP16, using a 16 bit to represent

133
00:07:32,745 --> 00:07:34,745
a floating point number.

134
00:07:36,616 --> 00:07:39,375
In particular, I'm going to introduce Int8, where

135
00:07:39,375 --> 00:07:43,692
the core TPU use, using an integer to represent a fixed

136
00:07:43,692 --> 00:07:44,863
point number.

137
00:07:44,863 --> 00:07:47,912
So we have a certain number of bits for the integer.

138
00:07:47,912 --> 00:07:50,827
Followed by a radix point, if we put different layers.

139
00:07:50,827 --> 00:07:54,255
And lastly, the fractional bits.

140
00:07:54,255 --> 00:07:58,088
So why do we prefer those eight bit, or 16 bit

141
00:07:59,257 --> 00:08:01,502
rather than those traditional like the

142
00:08:01,502 --> 00:08:03,844
32 bit floating point.

143
00:08:03,844 --> 00:08:04,856
That's the cost.

144
00:08:04,856 --> 00:08:08,981
So, I generated the figure from 45 nanometer technology

145
00:08:08,981 --> 00:08:13,189
about the energy cost versus the area cost for different

146
00:08:13,189 --> 00:08:14,635
operations.

147
00:08:14,635 --> 00:08:18,709
In particular, let's see here, go you from 32 bit to

148
00:08:18,709 --> 00:08:22,876
16 bit, we have about four times reduction in energy

149
00:08:24,066 --> 00:08:28,783
and also about four times reduction in the area.

150
00:08:28,783 --> 00:08:30,966
Area means money.

151
00:08:30,966 --> 00:08:33,751
Every millimeter square takes money to take out a chip

152
00:08:33,751 --> 00:08:38,593
So it's very beneficial for hardware design to go from

153
00:08:38,593 --> 00:08:40,010
32 bit to 16 bit.

154
00:08:41,801 --> 00:08:45,968
That's why you hear NVIDIA from Pascal Architecture,

155
00:08:46,894 --> 00:08:49,821
they said they're starting to support FP16.

156
00:08:49,821 --> 00:08:53,915
That's the reason why it's so beneficial.

157
00:08:53,915 --> 00:08:57,122
For example, previous battery level could last four hours,

158
00:08:57,122 --> 00:08:58,662
now it becomes 16 hours.

159
00:08:58,662 --> 00:09:00,269
That's what it means to reduce

160
00:09:00,269 --> 00:09:02,698
the energy cost by four times.

161
00:09:02,698 --> 00:09:07,160
But here still, there's a problem of large energy costs

162
00:09:07,160 --> 00:09:08,297
for reading the memory.

163
00:09:08,297 --> 00:09:11,771
And let's see how can we deal with this memory reference

164
00:09:11,771 --> 00:09:16,279
so expensive, how do we deal with this problem better?

165
00:09:16,279 --> 00:09:19,913
So let's switch gear and come to our topic directly.

166
00:09:19,913 --> 00:09:24,285
So let's first introduce algorithm for efficient inference.

167
00:09:24,285 --> 00:09:27,919
So I'm going to cover six topics, this is a really long slide.

168
00:09:27,919 --> 00:09:30,336
So I'm going to relatively fast.

169
00:09:31,796 --> 00:09:34,747
So the first idea I'm going to talk about is pruning.

170
00:09:34,747 --> 00:09:36,767
Pruning the neural networks.

171
00:09:36,767 --> 00:09:39,671
For example, this is original neural network.

172
00:09:39,671 --> 00:09:42,927
So what I'm trying to do is, can we remove some of the

173
00:09:42,927 --> 00:09:46,260
weight and still have the same accuracy?

174
00:09:47,424 --> 00:09:49,026
It's like pruning a tree, get rid

175
00:09:49,026 --> 00:09:51,838
of those redundant connections.

176
00:09:51,838 --> 00:09:55,540
This is first proposed by Professor Yann LeCun back in 1989,

177
00:09:55,540 --> 00:09:59,839
and I revisited this problem, 26 years later, on those

178
00:09:59,839 --> 00:10:03,933
modern deep neural nets to see how it works.

179
00:10:03,933 --> 00:10:06,764
So not all parameters are useful actually.

180
00:10:06,764 --> 00:10:09,388
For example, in this case, if you want to fit a single line,

181
00:10:09,388 --> 00:10:12,308
but you're using a quadratic term, apparently the

182
00:10:12,308 --> 00:10:14,808
0.01 is a redundant parameter.

183
00:10:15,977 --> 00:10:18,174
So I'm going to train the connectivity first and then

184
00:10:18,174 --> 00:10:20,611
prune some of the connections.

185
00:10:20,611 --> 00:10:22,384
And then train the remaining weights,

186
00:10:22,384 --> 00:10:24,364
and through this process, it regulates.

187
00:10:24,364 --> 00:10:28,663
And as a result, I can reduce the number of connections,

188
00:10:28,663 --> 00:10:31,908
and annex that from 16 million parameters to only

189
00:10:31,908 --> 00:10:35,278
six million parameters, which is 10 times less

190
00:10:35,278 --> 00:10:36,611
the computation.

191
00:10:37,645 --> 00:10:39,645
So this is the accuracy.

192
00:10:42,842 --> 00:10:46,224
So the x-axis is how much parameters to prune away

193
00:10:46,224 --> 00:10:49,592
and the y-axis is the accuracy you have.

194
00:10:49,592 --> 00:10:53,180
So we want to have less parameters, but we also

195
00:10:53,180 --> 00:10:55,834
want to have the same accuracy as before.

196
00:10:55,834 --> 00:10:58,424
We don't want to sacrifice accuracy,

197
00:10:58,424 --> 00:11:02,591
For example at 80%, we locked zero away left 80%

198
00:11:04,255 --> 00:11:08,257
of the parameters, but accuracy jumped by 4%.

199
00:11:08,257 --> 00:11:10,097
That's intolerable.

200
00:11:10,097 --> 00:11:12,535
But the good thing is that if we retrain the remaining

201
00:11:12,535 --> 00:11:16,285
weights, the accuracy can fully recover here.

202
00:11:18,020 --> 00:11:19,914
And if we do this process iteratively

203
00:11:19,914 --> 00:11:22,997
by pruning and retraining, pruning and retraining,

204
00:11:22,997 --> 00:11:26,938
we can fully recover the accuracy not until we are

205
00:11:26,938 --> 00:11:30,479
prune away 90% of the parameters.

206
00:11:30,479 --> 00:11:34,114
So if you go back to home and try it on your Ipad

207
00:11:34,114 --> 00:11:38,314
or notebook, just zero away 50% of the parameters say

208
00:11:38,314 --> 00:11:41,118
you went on your homework, you will astonishingly find

209
00:11:41,118 --> 00:11:44,118
that accuracy actually doesn't hurt.

210
00:11:45,087 --> 00:11:47,422
So we just mentioned convolution neural nets,

211
00:11:47,422 --> 00:11:52,301
how about RNNs and LSTMs, so I tried with this neural talk.

212
00:11:52,301 --> 00:11:55,637
Again, pruning away 90% of the rates doesn't hurt the

213
00:11:55,637 --> 00:11:56,554
blue score.

214
00:11:58,385 --> 00:12:00,007
And here are some visualizations.

215
00:12:00,007 --> 00:12:04,401
For example, the original picture, the neural talk says

216
00:12:04,401 --> 00:12:07,507
a basketball player in a white uniform is playing

217
00:12:07,507 --> 00:12:08,710
with a ball.

218
00:12:08,710 --> 00:12:12,797
Versus pruning away 90% it says, a basketball player

219
00:12:12,797 --> 00:12:16,775
in a white uniform is playing with a basketball.

220
00:12:16,775 --> 00:12:18,192
And on and so on.

221
00:12:19,155 --> 00:12:23,157
But if you're too aggressive, say you prune away

222
00:12:23,157 --> 00:12:27,324
95% of the weights, the network is going to get drunk.

223
00:12:28,766 --> 00:12:32,355
It says, a man in a red shirt and white and black shirt

224
00:12:32,355 --> 00:12:34,345
is running through a field.

225
00:12:34,345 --> 00:12:37,059
So there's really a limit, a threshold, you have to

226
00:12:37,059 --> 00:12:39,726
take care of during the pruning.

227
00:12:41,095 --> 00:12:43,395
So interestingly, after I did the work, did some

228
00:12:43,395 --> 00:12:45,788
resource and research and find actually the same

229
00:12:45,788 --> 00:12:49,524
pruning procedure actually happens to human brain

230
00:12:49,524 --> 00:12:50,357
as well.

231
00:12:50,357 --> 00:12:54,459
So when we were born, there are about 50 trillion synapses

232
00:12:54,459 --> 00:12:55,688
in the brain.

233
00:12:55,688 --> 00:13:00,162
And at one year old, this number surged into 1,000 trillion.

234
00:13:00,162 --> 00:13:04,329
And as we become adolescent, it becomes smaller actually,

235
00:13:05,201 --> 00:13:09,368
500 trillion in the end, according to the study by Nature.

236
00:13:11,803 --> 00:13:13,459
So this is very interesting.

237
00:13:13,459 --> 00:13:15,966
And also, the pruning changed the weight distribution

238
00:13:15,966 --> 00:13:18,957
because we are removing those small connections

239
00:13:18,957 --> 00:13:22,027
and after we retrain them, that's why it becomes soft

240
00:13:22,027 --> 00:13:22,944
in the end.

241
00:13:23,939 --> 00:13:25,570
Yeah, question.

242
00:13:25,570 --> 00:13:26,781
- [Student] Are you trying to mean that it terms

243
00:13:26,781 --> 00:13:29,901
of your mixed weights during the training will be

244
00:13:29,901 --> 00:13:32,259
just set at zero and just start from scratch?

245
00:13:32,259 --> 00:13:35,386
And these start from the things that are at zero.

246
00:13:35,386 --> 00:13:37,411
- Yeah. So the question is, how do we deal with those

247
00:13:37,411 --> 00:13:39,435
zero connections?

248
00:13:39,435 --> 00:13:43,602
So we force them to be zero in all the other iterations.

249
00:13:45,369 --> 00:13:46,427
Question?

250
00:13:46,427 --> 00:13:50,153
- [Student] How do you pick which rates to drop?

251
00:13:50,153 --> 00:13:53,293
- Yeah so very simple. Small weights, drop it, sort it.

252
00:13:53,293 --> 00:13:54,421
If it's small, just--

253
00:13:54,421 --> 00:13:55,709
- [Student] Any threshold that I decide?

254
00:13:55,709 --> 00:13:57,042
- Exactly, yeah.

255
00:13:59,058 --> 00:14:01,929
So the next idea, weight sharing.

256
00:14:01,929 --> 00:14:05,574
So now we have, remember our end goal is to remove

257
00:14:05,574 --> 00:14:09,703
connections so that we can have less memory footprint

258
00:14:09,703 --> 00:14:12,567
so that we can have more energy efficient deployment.

259
00:14:12,567 --> 00:14:15,361
Now we have less number of parameters by pruning.

260
00:14:15,361 --> 00:14:19,446
We want to have less number of bits per parameter

261
00:14:19,446 --> 00:14:23,204
so they're multiplied together they get a small model.

262
00:14:23,204 --> 00:14:25,287
So the idea is like this.

263
00:14:26,267 --> 00:14:28,445
Not all numbers, not all the weights

264
00:14:28,445 --> 00:14:30,977
has to be the exact number.

265
00:14:30,977 --> 00:14:35,144
For example, 2.09, 2.12 or all these four weights, you

266
00:14:36,725 --> 00:14:39,867
just put them using 2.0 to represent them.

267
00:14:39,867 --> 00:14:41,278
That's enough.

268
00:14:41,278 --> 00:14:45,445
Otherwise too accurate number is just leads to overfitting.

269
00:14:46,851 --> 00:14:50,227
So the idea is I can cluster the weights if they

270
00:14:50,227 --> 00:14:53,278
are similar, just using a centroid to represent

271
00:14:53,278 --> 00:14:57,558
the number instead of using the full precision weight.

272
00:14:57,558 --> 00:15:01,094
So that every time I do the inference, I just do inference

273
00:15:01,094 --> 00:15:03,417
on this single number.

274
00:15:03,417 --> 00:15:06,995
For example, this is a four by four weight matrix

275
00:15:06,995 --> 00:15:09,027
in a certain layer.

276
00:15:09,027 --> 00:15:12,715
And what I'm going to do is do k-means clustering by having

277
00:15:12,715 --> 00:15:15,496
the similar weight sharing the same centroid.

278
00:15:15,496 --> 00:15:19,364
For example, 2.09, 2.12, I store index of

279
00:15:19,364 --> 00:15:21,987
three pointing to here.

280
00:15:21,987 --> 00:15:25,529
So that, the good thing is we need to only store the

281
00:15:25,529 --> 00:15:29,638
two bit index rather than the 32 bit, floating point number.

282
00:15:29,638 --> 00:15:31,555
That's 16 times saving.

283
00:15:34,577 --> 00:15:37,257
And how do we train such neural network?

284
00:15:37,257 --> 00:15:41,424
They are binded together, so after we get the gradient,

285
00:15:42,372 --> 00:15:45,540
we color them in the same pattern as the weight

286
00:15:45,540 --> 00:15:48,354
and then we do a group by operation by having all

287
00:15:48,354 --> 00:15:52,604
the in that weights with the same index grouped together.

288
00:15:52,604 --> 00:15:56,034
And then we do a reduction by summing them up.

289
00:15:56,034 --> 00:15:58,106
And then multiplied by the learning rate

290
00:15:58,106 --> 00:16:00,404
subtracted from the original centroid.

291
00:16:00,404 --> 00:16:04,321
That's one iteration of the SGD for such weight

292
00:16:05,292 --> 00:16:07,125
shared neural network.

293
00:16:08,613 --> 00:16:10,826
So remember previously, after pruning this is

294
00:16:10,826 --> 00:16:14,409
what the weight distribution like and after

295
00:16:16,164 --> 00:16:18,575
weight sharing, they become discrete.

296
00:16:18,575 --> 00:16:21,215
There are only 16 different values here, meaning

297
00:16:21,215 --> 00:16:25,048
we can use four bits to represent each number.

298
00:16:26,476 --> 00:16:29,764
And by training on such weight shared neural network,

299
00:16:29,764 --> 00:16:31,986
training on such extremely shared neural network,

300
00:16:31,986 --> 00:16:34,756
these weights can adjust.

301
00:16:34,756 --> 00:16:39,146
It is the subtle changes that compensated for the

302
00:16:39,146 --> 00:16:40,563
loss of accuracy.

303
00:16:41,407 --> 00:16:44,914
So let's see, this is the number of bits we give it,

304
00:16:44,914 --> 00:16:48,581
this is the accuracy for convolution layers.

305
00:16:50,095 --> 00:16:54,884
Not until four bits, does the accuracy begin to drop

306
00:16:54,884 --> 00:16:59,073
and for those fully connected layers, very astonishingly,

307
00:16:59,073 --> 00:17:02,014
it's not until two bits, only four number, does the

308
00:17:02,014 --> 00:17:03,702
accuracy begins to drop.

309
00:17:03,702 --> 00:17:06,119
And this result is per layer.

310
00:17:08,470 --> 00:17:12,404
So we have covered two methods, pruning and weight sharing.

311
00:17:12,404 --> 00:17:15,433
What if we combine these two methods together.

312
00:17:15,433 --> 00:17:16,982
Do they work well?

313
00:17:16,982 --> 00:17:20,444
So by combining those methods, this is the compression

314
00:17:20,444 --> 00:17:22,814
ratio with the smaller on the left.

315
00:17:22,814 --> 00:17:24,684
And this is the accuracy.

316
00:17:24,684 --> 00:17:27,382
We can combine it together and make the model

317
00:17:27,382 --> 00:17:32,364
about 3% of its original size without hurting the

318
00:17:32,364 --> 00:17:33,804
accuracy at all.

319
00:17:33,804 --> 00:17:36,481
Compared with the each working individual data by

320
00:17:36,481 --> 00:17:39,492
10%, accuracy begins to drop.

321
00:17:39,492 --> 00:17:41,742
And compared with the cheap SVD method,

322
00:17:41,742 --> 00:17:44,742
this has a better compression ratio.

323
00:17:46,742 --> 00:17:50,650
And final idea is we can apply the Huffman Coding

324
00:17:50,650 --> 00:17:55,031
to use more number of bits for those infrequent numbers,

325
00:17:55,031 --> 00:17:59,061
infrequently appearing weights and less number of bits

326
00:17:59,061 --> 00:18:03,351
for those more frequently appearing weights.

327
00:18:03,351 --> 00:18:06,469
So by combining these three methods, pruning, weight

328
00:18:06,469 --> 00:18:09,709
sharing, and also Huffman Coding, we can compress the

329
00:18:09,709 --> 00:18:13,490
neural networks, state-of-the-art  neural networks,

330
00:18:13,490 --> 00:18:17,073
ranging from 10x to 49x without hurting the

331
00:18:20,159 --> 00:18:21,370
prediction accuracy.

332
00:18:21,370 --> 00:18:23,267
Sometimes a little bit better.

333
00:18:23,267 --> 00:18:25,948
But maybe that is noise.

334
00:18:25,948 --> 00:18:30,115
So the next question is, these models are just pre-trained

335
00:18:31,069 --> 00:18:33,509
models by say Google, Microsoft.

336
00:18:33,509 --> 00:18:37,479
Can we make a compact model, a pump compact model

337
00:18:37,479 --> 00:18:38,457
to begin with?

338
00:18:38,457 --> 00:18:40,874
Even before such compression?

339
00:18:42,297 --> 00:18:47,098
So SqueezeNet, you may have already worked with this

340
00:18:47,098 --> 00:18:50,015
neural network model in a homework.

341
00:18:50,978 --> 00:18:55,145
So the idea is we are having a squeeze layer here to shield

342
00:18:58,639 --> 00:19:01,198
at the three by three convolution with fewer number of

343
00:19:01,198 --> 00:19:02,031
channels.

344
00:19:03,669 --> 00:19:06,177
So that's where squeeze comes from.

345
00:19:06,177 --> 00:19:10,119
And here we have two branches, rather than four branches

346
00:19:10,119 --> 00:19:12,286
as in the inception model.

347
00:19:13,919 --> 00:19:16,668
So as a result, the model is extremely compact.

348
00:19:16,668 --> 00:19:19,370
It doesn't have any  fully connected layers.

349
00:19:19,370 --> 00:19:20,978
Everything is fully convolutional.

350
00:19:20,978 --> 00:19:23,895
The last layer is a global pooling.

351
00:19:27,338 --> 00:19:31,698
So what if we apply deep compression algorithm

352
00:19:31,698 --> 00:19:35,738
on such already compact model will it be getting even

353
00:19:35,738 --> 00:19:36,571
smaller?

354
00:19:38,069 --> 00:19:42,389
So this is AlexNet after compression, this is SqueezeNet.

355
00:19:42,389 --> 00:19:46,556
Even before compression, it's 50x smaller than AlexNet,

356
00:19:47,498 --> 00:19:49,638
but has the same accuracy.

357
00:19:49,638 --> 00:19:53,805
After compression 510x smaller, but the same accuracy

358
00:19:56,093 --> 00:19:58,676
only less than half a megabyte.

359
00:20:00,444 --> 00:20:03,544
This means it's very easy to fit such a small model

360
00:20:03,544 --> 00:20:07,705
on the cache, which is literally

361
00:20:07,705 --> 00:20:09,538
tens of megabyte SRAM.

362
00:20:11,407 --> 00:20:12,865
So what does it mean?

363
00:20:12,865 --> 00:20:15,412
It's possible to achieve speed up.

364
00:20:15,412 --> 00:20:18,964
So this is the speedup, I measured if all these fully

365
00:20:18,964 --> 00:20:23,131
connected layers only for now, on the CPU, GPU, and

366
00:20:24,447 --> 00:20:26,601
the mobile GPU, before pruning

367
00:20:26,601 --> 00:20:28,839
and after pruning the weights,

368
00:20:28,839 --> 00:20:33,081
and on average, I observed a 3x speedup in a CPU,

369
00:20:33,081 --> 00:20:35,409
about 3X speedup on the GPU,

370
00:20:35,409 --> 00:20:39,151
and roughly 5x speedup on the mobile GPU, which is a

371
00:20:39,151 --> 00:20:39,984
TK1.

372
00:20:41,511 --> 00:20:44,679
And so is the energy efficiency.

373
00:20:44,679 --> 00:20:49,528
In an average improvement from 3x to 6x on a CPU, GPU,

374
00:20:49,528 --> 00:20:50,778
and mobile GPU.

375
00:20:52,209 --> 00:20:55,876
And these ideas are used in these companies.

376
00:20:57,998 --> 00:21:00,391
Having talked about when pruning and when sharing,

377
00:21:00,391 --> 00:21:02,791
which is a non-linear quantization method

378
00:21:02,791 --> 00:21:05,598
and we're going to talk about quantization, which is, why

379
00:21:05,598 --> 00:21:08,479
do they use in the TPU design?

380
00:21:08,479 --> 00:21:12,671
All the TPU designs use at only eight bit for inference.

381
00:21:12,671 --> 00:21:15,729
And the way, how they can use that is because of the

382
00:21:15,729 --> 00:21:16,749
quantization.

383
00:21:16,749 --> 00:21:19,332
And let's see how does it work.

384
00:21:20,248 --> 00:21:24,968
So quantization has this complicated figure, but

385
00:21:24,968 --> 00:21:26,769
the intuition is very simple.

386
00:21:26,769 --> 00:21:30,351
You run the neural network and train it with the normal

387
00:21:30,351 --> 00:21:32,268
floating point numbers.

388
00:21:33,849 --> 00:21:37,677
And quantize the weight and activations by gather

389
00:21:37,677 --> 00:21:39,700
the statistics for each layer.

390
00:21:39,700 --> 00:21:42,860
For example, what is the maximum number, minimum number,

391
00:21:42,860 --> 00:21:44,863
and how many bits are enough

392
00:21:44,863 --> 00:21:47,511
to represent this dynamic range.

393
00:21:47,511 --> 00:21:51,892
Then you use that number of bits for the integer part

394
00:21:51,892 --> 00:21:54,201
and the rest of the eight bit or seven bit

395
00:21:54,201 --> 00:21:58,118
for the other part of the 8 bit representation.

396
00:22:00,241 --> 00:22:05,041
And also we can fine tune in the floating point format.

397
00:22:05,041 --> 00:22:08,281
Or we can also use feed forward with fixed point

398
00:22:08,281 --> 00:22:11,509
and back propagation with update with the floating

399
00:22:11,509 --> 00:22:12,489
point number.

400
00:22:12,489 --> 00:22:17,391
There are lots of different ideas to have better accuracy.

401
00:22:17,391 --> 00:22:21,409
And this is the result, for how many number of bits

402
00:22:21,409 --> 00:22:23,121
versus what is the accuracy.

403
00:22:23,121 --> 00:22:26,020
For example, using a fixed, 8 bit, the accuracy for

404
00:22:26,020 --> 00:22:28,871
GoogleNet doesn't drop significantly.

405
00:22:28,871 --> 00:22:33,057
And for VGG-16, it also remains pretty well for

406
00:22:33,057 --> 00:22:34,100
the accuracy.

407
00:22:34,100 --> 00:22:36,763
While circling down to a six bit, the accuracy

408
00:22:36,763 --> 00:22:39,680
begins to drop pretty dramatically.

409
00:22:41,641 --> 00:22:44,474
Next idea, low rank approximation.

410
00:22:47,500 --> 00:22:51,083
It turned out that for a convolution layer,

411
00:22:51,951 --> 00:22:55,949
you can break it into two convolution layers.

412
00:22:55,949 --> 00:22:59,521
One convolution here, followed by a one by one convolution.

413
00:22:59,521 --> 00:23:02,441
So that it's like you break a complicated problem

414
00:23:02,441 --> 00:23:05,380
into two separate small problems.

415
00:23:05,380 --> 00:23:07,401
This is for convolution layer.

416
00:23:07,401 --> 00:23:10,292
As we can see, achieving about

417
00:23:10,292 --> 00:23:14,641
2x speedup, there's almost no loss of accuracy.

418
00:23:14,641 --> 00:23:18,529
And achieving a speedup of 5x, roughly a 6%

419
00:23:18,529 --> 00:23:19,946
loss of accuracy.

420
00:23:21,260 --> 00:23:24,020
And this also works for fully connected layers.

421
00:23:24,020 --> 00:23:28,110
The simplest idea is using the SVD to break it into

422
00:23:28,110 --> 00:23:30,721
one matrix into two matrices.

423
00:23:30,721 --> 00:23:34,888
And follow this idea, this paper proposes to use the

424
00:23:36,121 --> 00:23:40,940
Tensor Tree to break down one fully connected layer into

425
00:23:40,940 --> 00:23:43,631
a tree, lots of fully connected layers.

426
00:23:43,631 --> 00:23:46,131
That's why it's called a tree.

427
00:23:49,001 --> 00:23:52,191
So going even more crazy, can we use only

428
00:23:52,191 --> 00:23:56,671
two weights or three weights to represent a neural network?

429
00:23:56,671 --> 00:23:59,601
A ternary weight or a binary weight.

430
00:23:59,601 --> 00:24:02,531
We already seen this distribution before, after pruning.

431
00:24:02,531 --> 00:24:04,911
There's some positive weights and negative weights.

432
00:24:04,911 --> 00:24:08,791
Can we just use three numbers, just use one, minus one, zero

433
00:24:08,791 --> 00:24:12,081
to represent the neural network.

434
00:24:12,081 --> 00:24:16,452
This is our recent paper clear that we maintain

435
00:24:16,452 --> 00:24:20,852
a full precision weight during training time,

436
00:24:20,852 --> 00:24:24,292
but at inference time, we only keep the scaling factor

437
00:24:24,292 --> 00:24:26,063
and the ternary weight.

438
00:24:26,063 --> 00:24:30,831
So during inference, we only need three weights.

439
00:24:30,831 --> 00:24:35,831
That's very efficient and making the model very small.

440
00:24:35,831 --> 00:24:38,332
This is the proportion of the positive zero

441
00:24:38,332 --> 00:24:41,700
and negative weights, they can change during the training.

442
00:24:41,700 --> 00:24:44,200
So is their absolute value.

443
00:24:46,092 --> 00:24:50,236
And this is the visualization of kernels

444
00:24:50,236 --> 00:24:53,809
by this trained ternary quantization.

445
00:24:53,809 --> 00:24:57,976
We can see some of them are a corner detector like here.

446
00:24:59,336 --> 00:25:00,986
And also here.

447
00:25:00,986 --> 00:25:03,856
Some of them are maybe edge detector.

448
00:25:03,856 --> 00:25:06,107
For example, this filter some of them

449
00:25:06,107 --> 00:25:09,249
are corner detector like here this filter.

450
00:25:09,249 --> 00:25:12,537
Actually we don't need such fine grain resolution.

451
00:25:12,537 --> 00:25:15,168
Just three weights are enough.

452
00:25:15,168 --> 00:25:19,335
So this is the validation accuracy on ImageNet with AlexNet.

453
00:25:21,318 --> 00:25:24,238
So the threshline is the baseline accuracy

454
00:25:24,238 --> 00:25:26,529
with floating point 32.

455
00:25:26,529 --> 00:25:29,112
And the red line is our result.

456
00:25:29,979 --> 00:25:34,979
Pretty much the same accuracy converged compared with

457
00:25:34,979 --> 00:25:37,229
the full precision weights.

458
00:25:40,390 --> 00:25:43,307
Last idea, Winograd Transformation.

459
00:25:44,470 --> 00:25:47,491
So this about how do we implement deep neural nets,

460
00:25:47,491 --> 00:25:50,001
how do we implement the convolutions.

461
00:25:50,001 --> 00:25:52,430
So this is the conventional direct

462
00:25:52,430 --> 00:25:55,190
convolution implementation method.

463
00:25:55,190 --> 00:25:58,459
The slide credited to Julien, a friend from Nvidia.

464
00:25:58,459 --> 00:26:01,959
So originally, we just do the element wise

465
00:26:03,298 --> 00:26:06,390
do a dot product for those nine elements in the filter

466
00:26:06,390 --> 00:26:10,310
and nine elements in the image and then sum it up.

467
00:26:10,310 --> 00:26:15,179
For example, for every output we need nine times C

468
00:26:15,179 --> 00:26:18,012
number of multiplication and adds.

469
00:26:19,314 --> 00:26:23,481
Winograd Convolution is another method, equivalent method.

470
00:26:27,444 --> 00:26:31,491
It's not lost, it's an equivalent method proposed at

471
00:26:31,491 --> 00:26:33,531
first through this paper, Fast Algorithms

472
00:26:33,531 --> 00:26:35,334
for Convolution Neural Networks.

473
00:26:35,334 --> 00:26:38,212
That instead of directly doing the convolution, move

474
00:26:38,212 --> 00:26:42,379
it one by one, at first it transforms the input feature

475
00:26:43,905 --> 00:26:46,155
map to another feature map.

476
00:26:47,066 --> 00:26:51,233
Which contains only the weight, contains only 1, 0.5, 2

477
00:26:53,396 --> 00:26:56,813
that can efficiently implement it with shift.

478
00:26:56,813 --> 00:27:00,980
And also transform the filter into a four by four tensor.

479
00:27:02,076 --> 00:27:06,324
So what we are going to do here is sum over c and do an element-wise

480
00:27:06,324 --> 00:27:07,824
element-wise product.

481
00:27:08,964 --> 00:27:13,564
So there are only 16 multiplications happening here.

482
00:27:13,564 --> 00:27:18,356
And then we do a inverse transform to get four outputs.

483
00:27:18,356 --> 00:27:21,175
So the transform and the inverse transform can be

484
00:27:21,175 --> 00:27:24,932
amortized and the multiplications, whether it can ignored.

485
00:27:24,932 --> 00:27:29,099
So in order to get four output, we need nine times channel

486
00:27:30,524 --> 00:27:34,444
times four, which is 36 times channel.

487
00:27:34,444 --> 00:27:39,093
Multiplications originally for the direct convolution

488
00:27:39,093 --> 00:27:42,676
but now we need 16 times C of our output

489
00:27:46,655 --> 00:27:50,822
So that is 2.25x less number of multiplications to

490
00:27:53,916 --> 00:27:57,083
perform the exact same multiplication.

491
00:27:58,306 --> 00:27:59,807
And here is a speedup.

492
00:27:59,807 --> 00:28:03,974
2.25x, so theoretically, 2.25x speedup and in real,

493
00:28:07,694 --> 00:28:10,611
from cuDNN 5 they incorporated such

494
00:28:11,570 --> 00:28:14,916
Winograd Convolution algorithm.

495
00:28:14,916 --> 00:28:19,234
This is on the VGG net I believe, the speedup is

496
00:28:19,234 --> 00:28:21,401
roughly 1.7 to 2x speedup.

497
00:28:23,735 --> 00:28:25,318
Pretty significant.

498
00:28:27,314 --> 00:28:31,147
And after cuDNN 5, the cuDNN begins to use the

499
00:28:33,564 --> 00:28:36,147
Winograd Convolution algorithm.

500
00:28:38,586 --> 00:28:43,354
Okay, so far we have covered those efficient algorithms

501
00:28:43,354 --> 00:28:45,666
for efficient inference.

502
00:28:45,666 --> 00:28:48,978
We covered pruning, weight sharing, quantization,

503
00:28:48,978 --> 00:28:52,061
and also Winograd binary and ternary.

504
00:28:53,436 --> 00:28:57,603
So now let's see what is the optimal hardware for those

505
00:28:59,196 --> 00:29:00,805
efficient inference?

506
00:29:00,805 --> 00:29:02,888
And what is a Google TPU?

507
00:29:05,018 --> 00:29:08,685
So there are a wide range of domain specific

508
00:29:09,567 --> 00:29:14,286
architectures or ASICS for deep neural networks.

509
00:29:14,286 --> 00:29:16,745
They have a common goal is to minimize the memory

510
00:29:16,745 --> 00:29:18,495
access to save power.

511
00:29:20,595 --> 00:29:24,708
For example the Eyeriss from MIT by using the RS Dataflow

512
00:29:24,708 --> 00:29:28,028
to minimize the off chip direct access.

513
00:29:28,028 --> 00:29:30,818
And DaDiannao from China Academy of Science,

514
00:29:30,818 --> 00:29:33,906
buffered all the weights on chip DRAM instead of having

515
00:29:33,906 --> 00:29:35,823
to go to off-chip DRAM.

516
00:29:37,287 --> 00:29:42,108
So the TPU from Google is using eight bit integer

517
00:29:42,108 --> 00:29:44,258
to represent the numbers.

518
00:29:44,258 --> 00:29:47,319
And at Stanford I proposed the EIE architecture

519
00:29:47,319 --> 00:29:49,496
that support those compressed and

520
00:29:49,496 --> 00:29:53,067
sparse deep neural network inference.

521
00:29:53,067 --> 00:29:56,668
So this is what the TPU looks like.

522
00:29:56,668 --> 00:30:00,835
It's actually smartly, can be put into the disk drive

523
00:30:03,267 --> 00:30:06,267
up to four cards per server.

524
00:30:06,267 --> 00:30:09,039
And this is the high-level architecture

525
00:30:09,039 --> 00:30:10,622
for the Google TPU.

526
00:30:12,386 --> 00:30:17,239
Don't be overwhelmed, it's actually, the kernel part

527
00:30:17,239 --> 00:30:21,156
here, is this giant matrix multiplication unit.

528
00:30:23,218 --> 00:30:27,218
So it's a 256 by 256 matrix multiplication unit.

529
00:30:28,698 --> 00:30:32,531
So in one single cycle, it can perform 64 kilo

530
00:30:37,177 --> 00:30:41,028
those number of multiplication and accumulate operations.

531
00:30:41,028 --> 00:30:44,861
So running 700 Megahertz, the throughput is 92

532
00:30:47,708 --> 00:30:49,208
Teraops per second

533
00:30:52,380 --> 00:30:55,319
because it's actually integer operation.

534
00:30:55,319 --> 00:30:59,486
So we just about 25x as GPU and more than 100x at the CPU.

535
00:31:01,799 --> 00:31:05,966
And notice, TPU has a really large software-managed

536
00:31:07,711 --> 00:31:09,541
on-chip buffer.

537
00:31:09,541 --> 00:31:11,124
It is 24 megabytes.

538
00:31:13,550 --> 00:31:18,375
The cache for the CPU the L3 cache is already

539
00:31:18,375 --> 00:31:19,720
16 megabytes.

540
00:31:19,720 --> 00:31:24,093
This is 24 megabytes which is pretty large.

541
00:31:24,093 --> 00:31:28,453
And it's powered by two DDR3 DRAM channels.

542
00:31:28,453 --> 00:31:32,536
So this is a little weak because the bandwidth is

543
00:31:33,783 --> 00:31:37,950
only 30 gigabytes per second compared with the most

544
00:31:39,151 --> 00:31:42,984
recent GPU that HBM, 900 Gigabytes per second.

545
00:31:47,543 --> 00:31:51,751
The DDR4 is released in 2014, so that makes sense because

546
00:31:51,751 --> 00:31:55,493
the design is a little during that day, used the DDR3.

547
00:31:55,493 --> 00:32:00,391
But if you're using DDR4 or even high-bandwidth memory,

548
00:32:00,391 --> 00:32:03,391
the performance can be even boosted.

549
00:32:05,011 --> 00:32:08,303
So this is a comparison about Google's TPU compared

550
00:32:08,303 --> 00:32:12,470
with the CPU, GPU of this K80 GPU by the way, and the TPU.

551
00:32:15,800 --> 00:32:19,743
So the area is pretty much smaller, like half the size of a

552
00:32:19,743 --> 00:32:23,910
CPU and GPU and the power consumption is roughly 75 watts.

553
00:32:28,562 --> 00:32:32,562
And see this number, the peak teraops per second

554
00:32:33,482 --> 00:32:38,103
is much higher than the CPU and GPU is, about 90

555
00:32:38,103 --> 00:32:41,520
teraops per second, which is pretty high.

556
00:32:42,602 --> 00:32:44,922
So here is a workload.

557
00:32:44,922 --> 00:32:47,983
Thanks to David sharing the slide.

558
00:32:47,983 --> 00:32:51,060
This is the workload at Google.

559
00:32:51,060 --> 00:32:54,380
They did a benchmark on these TPUs.

560
00:32:54,380 --> 00:32:58,804
So it's a little interesting that convolution neural nets

561
00:32:58,804 --> 00:33:03,711
only account for 5% of data-center workload.

562
00:33:03,711 --> 00:33:06,860
Most of them is multilayer perception,

563
00:33:06,860 --> 00:33:08,329
those fully connected layers.

564
00:33:08,329 --> 00:33:12,569
About 61% maybe for ads, I'm not sure.

565
00:33:12,569 --> 00:33:17,058
And about 29% of the workload in data-center is the

566
00:33:17,058 --> 00:33:18,369
Long Short Term Memory.

567
00:33:18,369 --> 00:33:20,391
For example, speech recognition,

568
00:33:20,391 --> 00:33:23,224
or machine translation, I suspect.

569
00:33:28,475 --> 00:33:31,129
Remember just now we have seen there are

570
00:33:31,129 --> 00:33:33,569
90 teraops per second.

571
00:33:33,569 --> 00:33:37,671
But what actually number of teraops per second

572
00:33:37,671 --> 00:33:39,239
can be achieved?

573
00:33:39,239 --> 00:33:43,449
This is a basic tool to measure the bottleneck

574
00:33:43,449 --> 00:33:45,688
of a computer system.

575
00:33:45,688 --> 00:33:49,647
Whether you are bottlenecked by the arithmetic or

576
00:33:49,647 --> 00:33:53,267
you are bottlenecked by the memory bandwidth.

577
00:33:53,267 --> 00:33:54,817
It's like if you have a bucket,

578
00:33:54,817 --> 00:33:58,548
the lowest part of the bucket determines how much

579
00:33:58,548 --> 00:34:01,087
water we can hold in the bucket.

580
00:34:01,087 --> 00:34:04,337
So in this region, you are bottlenecked

581
00:34:05,927 --> 00:34:07,977
by the memory bandwidth.

582
00:34:07,977 --> 00:34:11,477
So the x-axis is the arithmetic intensity.

583
00:34:13,945 --> 00:34:18,112
Which is number of floating point operations per byte

584
00:34:19,745 --> 00:34:22,415
the ratio between the computation and memory

585
00:34:22,415 --> 00:34:24,248
of bandwidth overhead.

586
00:34:26,047 --> 00:34:30,214
So the y-axis, is the actual attainable performance.

587
00:34:32,967 --> 00:34:36,664
Here is the peak performance for example.

588
00:34:36,664 --> 00:34:40,116
When you do a lot of operation after you fetch a single

589
00:34:40,116 --> 00:34:42,575
piece of data, if you can do a lot of operation

590
00:34:42,575 --> 00:34:46,996
on top of it, then you are bottlenecked by the arithmetic.

591
00:34:46,996 --> 00:34:51,714
But after you fetch a lot of data from the memory,

592
00:34:51,714 --> 00:34:55,916
but you just do a tiny little bit of arithmetic,

593
00:34:55,916 --> 00:35:00,054
then you will be bottlenecked by the memory bandwidth.

594
00:35:00,054 --> 00:35:04,704
So how much you can fetch from the memory determines

595
00:35:04,704 --> 00:35:08,214
how much real performance you can get.

596
00:35:08,214 --> 00:35:10,065
And remember there is a ratio.

597
00:35:10,065 --> 00:35:15,047
When it is one here, this region it happens to be the same

598
00:35:15,047 --> 00:35:17,854
as the turning point is the actual

599
00:35:17,854 --> 00:35:20,521
memory bandwidth of your system.

600
00:35:21,476 --> 00:35:24,407
So let's see what is the life for the TPU.

601
00:35:24,407 --> 00:35:26,825
The TPU's peak performance is really high,

602
00:35:26,825 --> 00:35:28,908
about 90 Tops per second.

603
00:35:30,623 --> 00:35:34,790
For those convolution nets, they are pretty much saturating

604
00:35:39,915 --> 00:35:41,825
the peak performance.

605
00:35:41,825 --> 00:35:45,644
But there are lot of neural networks that has a utlitization

606
00:35:45,644 --> 00:35:47,227
less than 10%,

607
00:35:49,905 --> 00:35:53,572
meaning that 90 T-ops per second is actually

608
00:35:54,985 --> 00:35:59,152
achieves about three to 12 T-ops per second in real case.

609
00:36:03,244 --> 00:36:05,185
But why is it like that?

610
00:36:05,185 --> 00:36:09,352
The reason is, in order to have those real-time guarantee

611
00:36:10,882 --> 00:36:14,691
that the user not wait for too long, you cannot batch

612
00:36:14,691 --> 00:36:18,002
a lot of user's images or speech voice data

613
00:36:18,002 --> 00:36:19,354
at the same time.

614
00:36:19,354 --> 00:36:22,811
So as a result, for those fully connect layers,

615
00:36:22,811 --> 00:36:26,978
they have very little reuse, so they are bottlenecked

616
00:36:28,634 --> 00:36:31,453
by the memory bandwidth.

617
00:36:31,453 --> 00:36:35,584
For those convolution neural nets, for example this one,

618
00:36:35,584 --> 00:36:39,417
this blue one, that achieve 86, which is CNN0.

619
00:36:42,333 --> 00:36:44,750
The ratio between the ops and

620
00:36:48,632 --> 00:36:51,872
the number of memory is the highest.

621
00:36:51,872 --> 00:36:56,039
It's pretty high, more than 2,000 compared with other

622
00:36:57,722 --> 00:37:00,722
multilayer perceptron or long short term memory

623
00:37:00,722 --> 00:37:02,722
the ratio is pretty low.

624
00:37:04,389 --> 00:37:08,556
So this figure compares, this is the TPU and this one is

625
00:37:09,682 --> 00:37:11,765
the CPU, this is the GPU.

626
00:37:13,021 --> 00:37:16,352
Here is memory bandwidth, the peak memory bandwidth

627
00:37:16,352 --> 00:37:17,792
at a ratio of one here.

628
00:37:17,792 --> 00:37:20,538
So TPU has the highest memory bandwidth.

629
00:37:20,538 --> 00:37:24,402
And here is where are these neural networks

630
00:37:24,402 --> 00:37:26,072
lie on this curve.

631
00:37:26,072 --> 00:37:28,538
So the asterisk is for the TPU.

632
00:37:28,538 --> 00:37:31,371
It's still higher than other dots,

633
00:37:32,890 --> 00:37:37,057
but if you're not comfortable with this log scale figure,

634
00:37:38,232 --> 00:37:42,399
this is what it's like putting it in linear roofline.

635
00:37:43,781 --> 00:37:46,819
So pretty much everything disappeared except

636
00:37:46,819 --> 00:37:48,486
for the TPU results.

637
00:37:51,562 --> 00:37:54,381
So still, all these lines, although they are higher

638
00:37:54,381 --> 00:37:57,282
than the CPU and GPU, it's still way below the

639
00:37:57,282 --> 00:38:00,532
theoretical peak operations per second.

640
00:38:06,031 --> 00:38:08,802
So as I mentioned before, it is really bottlenecked

641
00:38:08,802 --> 00:38:11,780
by the low latency requirement so that it can have

642
00:38:11,780 --> 00:38:13,402
a large batch size.

643
00:38:13,402 --> 00:38:16,762
That's why you have low operations per byte.

644
00:38:16,762 --> 00:38:18,610
And how do you solve this problem?

645
00:38:18,610 --> 00:38:21,250
You want to have less number of memory footprint

646
00:38:21,250 --> 00:38:25,417
so that it can reduce the memory bandwidth requirement.

647
00:38:27,219 --> 00:38:30,449
One solution is to compress the model and the challenge

648
00:38:30,449 --> 00:38:35,179
is how do we build a hardware that can do inference

649
00:38:35,179 --> 00:38:38,387
directly on the compressed model?

650
00:38:38,387 --> 00:38:42,238
So I'm going to introduce my design of EIE, the Efficient

651
00:38:42,238 --> 00:38:46,347
Inference Engine, which deals with those sparse

652
00:38:46,347 --> 00:38:49,755
and the compressed model to save the memory bandwidth.

653
00:38:49,755 --> 00:38:52,124
And the rule of thumb, like we mentioned before is taking

654
00:38:52,124 --> 00:38:53,995
out one bit of sparsity first.

655
00:38:53,995 --> 00:38:56,366
Anything times zero is zero.

656
00:38:56,366 --> 00:38:59,697
So don't store it, don't compute on it.

657
00:38:59,697 --> 00:39:04,286
And second idea is, you don't need that much full precision,

658
00:39:04,286 --> 00:39:06,857
but you can approximate it.

659
00:39:06,857 --> 00:39:10,279
So by taking advantage of the sparse weight, we

660
00:39:10,279 --> 00:39:15,097
get about a 10x saving in the computation, 5x less

661
00:39:15,097 --> 00:39:16,345
memory footprint.

662
00:39:16,345 --> 00:39:19,645
The 2x difference is due to index overhead.

663
00:39:19,645 --> 00:39:22,555
And by taking advantage of the sparse activation,

664
00:39:22,555 --> 00:39:26,633
meaning after bandwidth, if activation is zero, then

665
00:39:26,633 --> 00:39:27,795
ignore it.

666
00:39:27,795 --> 00:39:30,712
You save another 3x of computation.

667
00:39:32,454 --> 00:39:35,465
And then by such weight sharing mechanism,

668
00:39:35,465 --> 00:39:39,382
you can use four bits to represent each weight rather

669
00:39:39,382 --> 00:39:41,144
than 32 bit.

670
00:39:41,144 --> 00:39:45,311
That's another eight times saving in the memory footprint.

671
00:39:48,195 --> 00:39:51,894
So this is physically, logically how the weights are stored.

672
00:3