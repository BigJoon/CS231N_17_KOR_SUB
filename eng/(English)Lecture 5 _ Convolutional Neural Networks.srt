1
00:00:08,435 --> 00:00:10,602
- Okay, let's get started.

2
00:00:13,372 --> 00:00:15,936
Alright, so welcome to lecture five.

3
00:00:15,936 --> 00:00:18,693
Today we're going to be getting to the title of the class,

4
00:00:18,693 --> 00:00:21,193
Convolutional Neural Networks.

5
00:00:22,493 --> 00:00:24,134
Okay, so a couple of administrative details

6
00:00:24,134 --> 00:00:25,933
before we get started.

7
00:00:25,933 --> 00:00:27,980
Assignment one is due Thursday,

8
00:00:27,980 --> 00:00:30,563
April 20, 11:59 p.m. on Canvas.

9
00:00:31,440 --> 00:00:35,607
We're also going to be releasing assignment two on Thursday.

10
00:00:38,320 --> 00:00:40,434
Okay, so a quick review of last time.

11
00:00:40,434 --> 00:00:43,679
We talked about neural networks, and how we had

12
00:00:43,679 --> 00:00:45,755
the running example of the linear score function

13
00:00:45,755 --> 00:00:48,337
that we talked about through the first few lectures.

14
00:00:48,337 --> 00:00:50,736
And then we turned this into a neural network

15
00:00:50,736 --> 00:00:53,808
by stacking these linear layers on top of each other

16
00:00:53,808 --> 00:00:56,969
with non-linearities in between.

17
00:00:56,969 --> 00:00:58,900
And we also saw that this could help address

18
00:00:58,900 --> 00:01:01,500
the mode problem where we are able to learn

19
00:01:01,500 --> 00:01:03,807
intermediate templates that are looking for,

20
00:01:03,807 --> 00:01:06,618
for example, different types of cars, right.

21
00:01:06,618 --> 00:01:09,006
A red car versus a yellow car and so on.

22
00:01:09,006 --> 00:01:11,138
And to combine these together to come up with

23
00:01:11,138 --> 00:01:14,790
the final score function for a class.

24
00:01:14,790 --> 00:01:16,998
Okay, so today we're going to talk about

25
00:01:16,998 --> 00:01:18,438
convolutional neural networks,

26
00:01:18,438 --> 00:01:20,825
which is basically the same sort of idea,

27
00:01:20,825 --> 00:01:23,300
but now we're going to learn convolutional layers

28
00:01:23,300 --> 00:01:26,134
that reason on top of basically explicitly

29
00:01:26,134 --> 00:01:29,217
trying to maintain spatial structure.

30
00:01:31,817 --> 00:01:33,397
So, let's first talk a little bit about

31
00:01:33,397 --> 00:01:36,070
the history of neural networks, and then also

32
00:01:36,070 --> 00:01:39,067
how convolutional neural networks were developed.

33
00:01:39,067 --> 00:01:43,796
So we can go all the way back to 1957 with Frank Rosenblatt,

34
00:01:43,796 --> 00:01:46,308
who developed the Mark I Perceptron machine,

35
00:01:46,308 --> 00:01:48,688
which was the first implementation of an algorithm

36
00:01:48,688 --> 00:01:51,785
called the perceptron, which had sort of the similar idea

37
00:01:51,785 --> 00:01:55,157
of getting score functions, right, using some,

38
00:01:55,157 --> 00:01:58,437
you know, W times X plus a bias.

39
00:01:58,437 --> 00:02:02,000
But here the outputs are going to be either one or a zero.

40
00:02:02,000 --> 00:02:04,295
And then in this case we have an update rule,

41
00:02:04,295 --> 00:02:06,551
so an update rule for our weights, W,

42
00:02:06,551 --> 00:02:09,492
which also look kind of similar to the type of update rule

43
00:02:09,492 --> 00:02:12,304
that we're also seeing in backprop, but in this case

44
00:02:12,304 --> 00:02:15,889
there was no principled backpropagation technique yet,

45
00:02:15,889 --> 00:02:18,182
we just sort of took the weights and adjusted them

46
00:02:18,182 --> 00:02:22,349
in the direction towards the target that we wanted.

47
00:02:23,771 --> 00:02:26,918
So in 1960, we had Widrow and Hoff,

48
00:02:26,918 --> 00:02:29,673
who developed Adaline and Madaline, which was the first time

49
00:02:29,673 --> 00:02:33,290
that we were able to get, to start to stack

50
00:02:33,290 --> 00:02:37,457
these linear layers into multilayer perceptron networks.

51
00:02:38,986 --> 00:02:42,592
And so this is starting to now look kind of like this idea

52
00:02:42,592 --> 00:02:46,658
of neural network layers, but we still didn't have backprop

53
00:02:46,658 --> 00:02:50,992
or any sort of principled way to train this.

54
00:02:50,992 --> 00:02:53,436
And so the first time backprop was really introduced

55
00:02:53,436 --> 00:02:56,015
was in 1986 with Rumelhart.

56
00:02:56,015 --> 00:02:58,676
And so here we can start seeing, you know, these kinds of

57
00:02:58,676 --> 00:03:00,858
equations with the chain rule and the update rules

58
00:03:00,858 --> 00:03:03,906
that we're starting to get familiar with, right,

59
00:03:03,906 --> 00:03:05,318
and so this is the first time we started

60
00:03:05,318 --> 00:03:06,791
to have a principled way to train

61
00:03:06,791 --> 00:03:09,874
these kinds of network architectures.

62
00:03:11,623 --> 00:03:14,961
And so after that, you know, it still wasn't able to scale

63
00:03:14,961 --> 00:03:18,076
to very large neural networks, and so there was sort of

64
00:03:18,076 --> 00:03:20,550
a period in which there wasn't a whole lot

65
00:03:20,550 --> 00:03:24,450
of new things happening here, or a lot of popular use

66
00:03:24,450 --> 00:03:26,237
of these kinds of networks.

67
00:03:26,237 --> 00:03:28,623
And so this really started being reinvigorated

68
00:03:28,623 --> 00:03:32,790
around the 2000s, so in 2006, there was this paper

69
00:03:33,641 --> 00:03:37,623
by Geoff Hinton and Ruslan Salakhutdinov,

70
00:03:37,623 --> 00:03:39,612
which basically showed that we could train

71
00:03:39,612 --> 00:03:40,719
a deep neural network,

72
00:03:40,719 --> 00:03:43,212
and show that we could do this effectively.

73
00:03:43,212 --> 00:03:44,445
But it was still not quite

74
00:03:44,445 --> 00:03:47,428
the sort of modern iteration of neural networks.

75
00:03:47,428 --> 00:03:50,208
It required really careful initialization

76
00:03:50,208 --> 00:03:52,439
in order to be able to do backprop,

77
00:03:52,439 --> 00:03:54,350
and so what they had here was they would have

78
00:03:54,350 --> 00:03:57,601
this first pre-training stage, where you model

79
00:03:57,601 --> 00:03:59,456
each hidden layer through this kind of,

80
00:03:59,456 --> 00:04:01,805
through a restricted Boltzmann machine,

81
00:04:01,805 --> 00:04:04,180
and so you're going to get some initialized weights

82
00:04:04,180 --> 00:04:07,331
by training each of these layers iteratively.

83
00:04:07,331 --> 00:04:09,583
And so once you get all of these hidden layers

84
00:04:09,583 --> 00:04:13,898
you then use that to initialize your, you know,

85
00:04:13,898 --> 00:04:16,891
your full neural network, and then from there

86
00:04:16,891 --> 00:04:20,224
you do backprop and fine tuning of that.

87
00:04:23,057 --> 00:04:26,146
And so when we really started to get the first really strong

88
00:04:26,146 --> 00:04:30,219
results using neural networks, and what sort of really

89
00:04:30,219 --> 00:04:34,219
sparked the whole craze of starting to use these

90
00:04:35,066 --> 00:04:39,233
kinds of networks really widely was at around 2012,

91
00:04:40,268 --> 00:04:42,717
where we had first the strongest results

92
00:04:42,717 --> 00:04:44,980
using for speech recognition,

93
00:04:44,980 --> 00:04:47,921
and so this is work out of Geoff Hinton's lab

94
00:04:47,921 --> 00:04:50,606
for acoustic modeling and speech recognition.

95
00:04:50,606 --> 00:04:55,021
And then for image recognition, 2012 was the landmark paper

96
00:04:55,021 --> 00:04:58,604
from Alex Krizhevsky in Geoff Hinton's lab,

97
00:04:59,638 --> 00:05:01,919
which introduced the first convolutional neural network

98
00:05:01,919 --> 00:05:04,220
architecture that was able to do,

99
00:05:04,220 --> 00:05:06,813
get really strong results on ImageNet classification.

100
00:05:06,813 --> 00:05:10,917
And so it took the ImageNet, image classification benchmark,

101
00:05:10,917 --> 00:05:13,186
and was able to dramatically reduce

102
00:05:13,186 --> 00:05:15,519
the error on that benchmark.

103
00:05:16,793 --> 00:05:19,958
And so since then, you know, ConvNets have gotten

104
00:05:19,958 --> 00:05:24,236
really widely used in all kinds of applications.

105
00:05:24,236 --> 00:05:28,225
So now let's step back and take a look at what gave rise

106
00:05:28,225 --> 00:05:31,714
to convolutional neural networks specifically.

107
00:05:31,714 --> 00:05:34,113
And so we can go back to the 1950s,

108
00:05:34,113 --> 00:05:37,689
where Hubel and Wiesel did a series of experiments

109
00:05:37,689 --> 00:05:41,003
trying to understand how neurons

110
00:05:41,003 --> 00:05:42,538
in the visual cortex worked,

111
00:05:42,538 --> 00:05:45,579
and they studied this specifically for cats.

112
00:05:45,579 --> 00:05:48,273
And so we talked a little bit about this in lecture one,

113
00:05:48,273 --> 00:05:51,362
but basically in these experiments they put electrodes

114
00:05:51,362 --> 00:05:53,526
in the cat, into the cat brain,

115
00:05:53,526 --> 00:05:56,066
and they gave the cat different visual stimulus.

116
00:05:56,066 --> 00:05:57,888
Right, and so, things like, you know,

117
00:05:57,888 --> 00:06:01,171
different kinds of edges, oriented edges,

118
00:06:01,171 --> 00:06:03,187
different sorts of shapes, and they measured

119
00:06:03,187 --> 00:06:06,937
the response of the neurons to these stimuli.

120
00:06:09,029 --> 00:06:12,765
And so there were a couple of important conclusions

121
00:06:12,765 --> 00:06:14,993
that they were able to make, and observations.

122
00:06:14,993 --> 00:06:17,021
And so the first thing found that, you know,

123
00:06:17,021 --> 00:06:19,534
there's sort of this topographical mapping in the cortex.

124
00:06:19,534 --> 00:06:22,246
So nearby cells in the cortex also represent

125
00:06:22,246 --> 00:06:24,932
nearby regions in the visual field.

126
00:06:24,932 --> 00:06:27,767
And so you can see for example, on the right here

127
00:06:27,767 --> 00:06:31,730
where if you take kind of the spatial mapping

128
00:06:31,730 --> 00:06:34,475
and map this onto a visual cortex

129
00:06:34,475 --> 00:06:37,750
there's more peripheral regions are these blue areas,

130
00:06:37,750 --> 00:06:41,722
you know, farther away from the center.

131
00:06:41,722 --> 00:06:44,122
And so they also discovered that these neurons

132
00:06:44,122 --> 00:06:46,789
had a hierarchical organization.

133
00:06:47,634 --> 00:06:51,236
And so if you look at different types of visual stimuli

134
00:06:51,236 --> 00:06:54,828
they were able to find that at the earliest layers

135
00:06:54,828 --> 00:06:57,837
retinal ganglion cells were responsive to things

136
00:06:57,837 --> 00:07:01,601
that looked kind of like circular regions of spots.

137
00:07:01,601 --> 00:07:04,231
And then on top of that there are simple cells,

138
00:07:04,231 --> 00:07:07,999
and these simple cells are responsive to oriented edges,

139
00:07:07,999 --> 00:07:11,146
so different orientation of the light stimulus.

140
00:07:11,146 --> 00:07:13,246
And then going further, they discover that these

141
00:07:13,246 --> 00:07:15,448
were then connected to more complex cells,

142
00:07:15,448 --> 00:07:17,721
which were responsive to both light orientation

143
00:07:17,721 --> 00:07:19,923
as well as movement, and so on.

144
00:07:19,923 --> 00:07:22,145
And you get, you know, increasing complexity,

145
00:07:22,145 --> 00:07:25,452
for example, hypercomplex cells are now responsive

146
00:07:25,452 --> 00:07:28,984
to movement with kind of an endpoint, right,

147
00:07:28,984 --> 00:07:32,092
and so now you're starting to get the idea of corners

148
00:07:32,092 --> 00:07:34,175
and then blobs and so on.

149
00:07:38,143 --> 00:07:38,976
And so

150
00:07:40,298 --> 00:07:44,247
then in 1980, the neocognitron was the first example

151
00:07:44,247 --> 00:07:46,715
of a network architecture, a model,

152
00:07:46,715 --> 00:07:50,924
that had this idea of simple and complex cells

153
00:07:50,924 --> 00:07:52,454
that Hubel and Wiesel had discovered.

154
00:07:52,454 --> 00:07:55,419
And in this case Fukushima put these into

155
00:07:55,419 --> 00:07:59,038
these alternating layers of simple and complex cells,

156
00:07:59,038 --> 00:08:00,729
where you had these simple cells

157
00:08:00,729 --> 00:08:03,129
that had modifiable parameters, and then complex cells

158
00:08:03,129 --> 00:08:06,799
on top of these that performed a sort of pooling

159
00:08:06,799 --> 00:08:08,791
so that it was invariant to, you know,

160
00:08:08,791 --> 00:08:12,958
different minor modifications from the simple cells.

161
00:08:14,786 --> 00:08:17,159
And so this is work that was in the 1980s, right,

162
00:08:17,159 --> 00:08:19,242
and so by 1998 Yann LeCun

163
00:08:21,839 --> 00:08:23,445
basically showed the first example

164
00:08:23,445 --> 00:08:27,743
of applying backpropagation and gradient-based learning

165
00:08:27,743 --> 00:08:29,645
to train convolutional neural networks

166
00:08:29,645 --> 00:08:32,064
that did really well on document recognition.

167
00:08:32,064 --> 00:08:35,340
And specifically they were able to do a good job

168
00:08:35,340 --> 00:08:37,610
of recognizing digits of zip codes.

169
00:08:37,610 --> 00:08:41,028
And so these were then used pretty widely

170
00:08:41,028 --> 00:08:45,082
for zip code recognition in the postal service.

171
00:08:45,082 --> 00:08:48,320
But beyond that it wasn't able to scale yet

172
00:08:48,320 --> 00:08:51,579
to more challenging and complex data, right,

173
00:08:51,579 --> 00:08:53,837
digits are still fairly simple

174
00:08:53,837 --> 00:08:56,350
and a limited set to recognize.

175
00:08:56,350 --> 00:09:00,901
And so this is where Alex Krizhevsky, in 2012,

176
00:09:00,901 --> 00:09:04,893
gave the modern incarnation of convolutional neural networks

177
00:09:04,893 --> 00:09:08,900
and his network we sort of colloquially call AlexNet.

178
00:09:08,900 --> 00:09:11,543
But this network really didn't look so much different

179
00:09:11,543 --> 00:09:14,205
than the convolutional neural networks

180
00:09:14,205 --> 00:09:16,472
that Yann LeCun was dealing with.

181
00:09:16,472 --> 00:09:18,363
They're now, you know, they were scaled now

182
00:09:18,363 --> 00:09:21,751
to be larger and deeper and able to,

183
00:09:21,751 --> 00:09:23,753
the most important parts were that they were now able

184
00:09:23,753 --> 00:09:26,544
to take advantage of the large amount of data

185
00:09:26,544 --> 00:09:30,711
that's now available, in web images, in ImageNet data set.

186
00:09:32,078 --> 00:09:33,757
As well as take advantage

187
00:09:33,757 --> 00:09:37,724
of the parallel computing power in GPUs.

188
00:09:37,724 --> 00:09:41,033
And so we'll talk more about that later.

189
00:09:41,033 --> 00:09:43,127
But fast forwarding today, so now, you know,

190
00:09:43,127 --> 00:09:45,434
ConvNets are used everywhere.

191
00:09:45,434 --> 00:09:49,999
And so we have the initial classification results

192
00:09:49,999 --> 00:09:52,294
on ImageNet from Alex Krizhevsky.

193
00:09:52,294 --> 00:09:55,188
This is able to do a really good job of image retrieval.

194
00:09:55,188 --> 00:09:57,274
You can see that when we're trying to retrieve a flower

195
00:09:57,274 --> 00:09:59,488
for example, the features that are learned

196
00:09:59,488 --> 00:10:04,134
are really powerful for doing similarity matching.

197
00:10:04,134 --> 00:10:07,049
We also have ConvNets that are used for detection.

198
00:10:07,049 --> 00:10:10,557
So we're able to do a really good job of localizing

199
00:10:10,557 --> 00:10:14,285
where in an image is, for example, a bus, or a boat,

200
00:10:14,285 --> 00:10:17,705
and so on, and draw precise bounding boxes around that.

201
00:10:17,705 --> 00:10:21,353
We're able to go even deeper beyond that to do segmentation,

202
00:10:21,353 --> 00:10:23,145
right, and so these are now richer tasks

203
00:10:23,145 --> 00:10:26,112
where we're not looking for just the bounding box

204
00:10:26,112 --> 00:10:27,958
but we're actually going to label every pixel

205
00:10:27,958 --> 00:10:32,125
in the outline of, you know, trees, and people, and so on.

206
00:10:34,126 --> 00:10:36,868
And these kind of algorithms are used in,

207
00:10:36,868 --> 00:10:38,864
for example, self-driving cars,

208
00:10:38,864 --> 00:10:42,066
and a lot of this is powered by GPUs as I mentioned earlier,

209
00:10:42,066 --> 00:10:45,114
that's able to do parallel processing

210
00:10:45,114 --> 00:10:48,812
and able to efficiently train and run these ConvNets.

211
00:10:48,812 --> 00:10:52,406
And so we have modern powerful GPUs as well as ones

212
00:10:52,406 --> 00:10:55,634
that work in embedded systems, for example,

213
00:10:55,634 --> 00:10:59,207
that you would use in a self-driving car.

214
00:10:59,207 --> 00:11:01,695
So we can also look at some of the other applications

215
00:11:01,695 --> 00:11:03,399
that ConvNets are used for.

216
00:11:03,399 --> 00:11:06,227
So, face-recognition, right, we can put an input image

217
00:11:06,227 --> 00:11:10,394
of a face and get out a likelihood of who this person is.

218
00:11:12,626 --> 00:11:15,622
ConvNets are applied to video, and so this is an example

219
00:11:15,622 --> 00:11:19,551
of a video network that looks at both images

220
00:11:19,551 --> 00:11:21,902
as well as temporal information,

221
00:11:21,902 --> 00:11:25,951
and from there is able to classify videos.

222
00:11:25,951 --> 00:11:28,569
We're also able to do pose recognition.

223
00:11:28,569 --> 00:11:30,215
Being able to recognize, you know,

224
00:11:30,215 --> 00:11:32,770
shoulders, elbows, and different joints.

225
00:11:32,770 --> 00:11:37,577
And so here are some images of our fabulous TA, Lane,

226
00:11:37,577 --> 00:11:42,234
in various kinds of pretty non-standard human poses.

227
00:11:42,234 --> 00:11:45,791
But ConvNets are able to do a pretty good job

228
00:11:45,791 --> 00:11:48,465
of pose recognition these days.

229
00:11:48,465 --> 00:11:51,741
They're also used in game playing.

230
00:11:51,741 --> 00:11:54,296
So some of the work in reinforcement learning,

231
00:11:54,296 --> 00:11:56,509
deeper enforcement learning that you may have seen,

232
00:11:56,509 --> 00:11:58,595
playing Atari games, and Go, and so on,

233
00:11:58,595 --> 00:12:02,981
and ConvNets are an important part of all of these.

234
00:12:02,981 --> 00:12:06,656
Some other applications, so they're being used for

235
00:12:06,656 --> 00:12:10,150
interpretation and diagnosis of medical images,

236
00:12:10,150 --> 00:12:14,317
for classification of galaxies, for street sign recognition.

237
00:12:18,059 --> 00:12:19,519
There's also whale recognition,

238
00:12:19,519 --> 00:12:22,342
this is from a recent Kaggle Challenge.

239
00:12:22,342 --> 00:12:26,067
We also have examples of looking at aerial maps

240
00:12:26,067 --> 00:12:28,485
and being able to draw out where are the streets

241
00:12:28,485 --> 00:12:29,999
on these maps, where are buildings,

242
00:12:29,999 --> 00:12:33,249
and being able to segment all of these.

243
00:12:35,089 --> 00:12:39,170
And then beyond recognition of classification detection,

244
00:12:39,170 --> 00:12:41,587
these types of tasks, we also have tasks

245
00:12:41,587 --> 00:12:44,472
like image captioning, where given an image,

246
00:12:44,472 --> 00:12:46,363
we want to write a sentence description

247
00:12:46,363 --> 00:12:48,644
about what's in the image.

248
00:12:48,644 --> 00:12:49,970
And so this is something that we'll go into

249
00:12:49,970 --> 00:12:52,819
a little bit later in the class.

250
00:12:52,819 --> 00:12:57,169
And we also have, you know, really, really fancy and cool

251
00:12:57,169 --> 00:13:01,251
kind of artwork that we can do using neural networks.

252
00:13:01,251 --> 00:13:03,855
And so on the left is an example of a deep dream,

253
00:13:03,855 --> 00:13:08,022
where we're able to take images and kind of hallucinate

254
00:13:09,173 --> 00:13:12,412
different kinds of objects and concepts in the image.

255
00:13:12,412 --> 00:13:16,274
There's also neural style type work, where we take an image

256
00:13:16,274 --> 00:13:19,817
and we're able to re-render this image

257
00:13:19,817 --> 00:13:23,808
using a style of a particular artist and artwork, right.

258
00:13:23,808 --> 00:13:27,899
And so here we can take, for example, Van Gogh on the right,

259
00:13:27,899 --> 00:13:30,909
Starry Night, and use that to redraw

260
00:13:30,909 --> 00:13:33,370
our original image using that style.

261
00:13:33,370 --> 00:13:36,473
And Justin has done a lot of work in this

262
00:13:36,473 --> 00:13:38,239
and so if you guys are interested,

263
00:13:38,239 --> 00:13:42,163
these are images produced by some of his code

264
00:13:42,163 --> 00:13:46,244
and you guys should talk to him more about it.

265
00:13:46,244 --> 00:13:50,069
Okay, so basically, you know, this is just a small sample

266
00:13:50,069 --> 00:13:52,727
of where ConvNets are being used today.

267
00:13:52,727 --> 00:13:55,289
But there's really a huge amount that can be done with this,

268
00:13:55,289 --> 00:13:58,378
right, and so, you know, for you guys' projects,

269
00:13:58,378 --> 00:14:00,624
sort of, you know, let your imagination go wild

270
00:14:00,624 --> 00:14:04,605
and we're excited to see what sorts of applications

271
00:14:04,605 --> 00:14:06,465
you can come up with.

272
00:14:06,465 --> 00:14:08,031
So today we're going to talk about

273
00:14:08,031 --> 00:14:10,307
how convolutional neural networks work.

274
00:14:10,307 --> 00:14:13,233
And again, same as with neural networks, we're going to first

275
00:14:13,233 --> 00:14:16,904
talk about how they work from a functional perspective

276
00:14:16,904 --> 00:14:18,668
without any of the brain analogies.

277
00:14:18,668 --> 00:14:22,835
And then we'll talk briefly about some of these connections.

278
00:14:25,453 --> 00:14:28,361
Okay, so, last lecture, we talked about

279
00:14:28,361 --> 00:14:31,444
this idea of a fully connected layer.

280
00:14:32,878 --> 00:14:36,257
And how, you know, for a fully connected layer

281
00:14:36,257 --> 00:14:39,373
what we're doing is we operate on top of these vectors,

282
00:14:39,373 --> 00:14:43,218
right, and so let's say we have, you know, an image,

283
00:14:43,218 --> 00:14:45,726
a 3D image, 32 by 32 by three,

284
00:14:45,726 --> 00:14:48,443
so some of the images that we were looking at previously.

285
00:14:48,443 --> 00:14:51,548
We'll take that, we'll stretch all of the pixels out, right,

286
00:14:51,548 --> 00:14:55,196
and then we have this 3072 dimensional vector,

287
00:14:55,196 --> 00:14:56,787
for example in this case.

288
00:14:56,787 --> 00:14:58,944
And then we have these weights, right,

289
00:14:58,944 --> 00:15:01,741
so we're going to multiply this by a weight matrix.

290
00:15:01,741 --> 00:15:05,908
And so here for example our W we're going to say is 10 by 3072.

291
00:15:07,264 --> 00:15:10,755
And then we're going to get the activations,

292
00:15:10,755 --> 00:15:13,943
the output of this layer, right, and so in this case,

293
00:15:13,943 --> 00:15:18,056
we take each of our 10 rows and we do this dot product

294
00:15:18,056 --> 00:15:20,389
with 3072 dimensional input.

295
00:15:22,207 --> 00:15:24,835
And from there we get this one number

296
00:15:24,835 --> 00:15:27,892
that's kind of the value of that neuron.

297
00:15:27,892 --> 00:15:30,020
And so in this case we're going to have

298
00:15:30,020 --> 00:15:32,270
10 of these neuron outputs.

299
00:15:35,417 --> 00:15:38,355
And so a convolutional layer, so the main difference

300
00:15:38,355 --> 00:15:39,988
between this and the fully connected layer

301
00:15:39,988 --> 00:15:41,203
that we've been talking about

302
00:15:41,203 --> 00:15:44,165
is that here we want to preserve spatial structure.

303
00:15:44,165 --> 00:15:47,090
And so taking this 32 by 32 by three image

304
00:15:47,090 --> 00:15:49,838
that we had earlier, instead of stretching this all out

305
00:15:49,838 --> 00:15:53,186
into one long vector, we're now going to keep the structure

306
00:15:53,186 --> 00:15:57,750
of this image, right, this three dimensional input.

307
00:15:57,750 --> 00:15:59,526
And then what we're going to do is

308
00:15:59,526 --> 00:16:01,910
our weights are going to be these small filters,

309
00:16:01,910 --> 00:16:05,746
so in this case for example, a five by five by three filter,

310
00:16:05,746 --> 00:16:07,212
and we're going to take this filter

311
00:16:07,212 --> 00:16:09,679
and we're going to slide it over the image spatially

312
00:16:09,679 --> 00:16:13,153
and compute dot products at every spatial location.

313
00:16:13,153 --> 00:16:17,320
And so we're going to go into detail of exactly how this works.

314
00:16:18,668 --> 00:16:20,523
So, our filters, first of all,

315
00:16:20,523 --> 00:16:23,957
always extend the full depth of the input volume.

316
00:16:23,957 --> 00:16:28,759
And so they're going to be just a smaller spatial area,

317
00:16:28,759 --> 00:16:30,357
so in this case five by five, right,

318
00:16:30,357 --> 00:16:33,425
instead of our full 32 by 32 spatial input,

319
00:16:33,425 --> 00:16:37,536
but they're always going to go through the full depth, right,

320
00:16:37,536 --> 00:16:42,499
so here we're going to take five by five by three.

321
00:16:42,499 --> 00:16:44,619
And then we're going to take this filter

322
00:16:44,619 --> 00:16:46,996
and at a given spatial location

323
00:16:46,996 --> 00:16:49,046
we're going to do a dot product

324
00:16:49,046 --> 00:16:52,901
between this filter and then a chunk of a image.

325
00:16:52,901 --> 00:16:54,492
So we're just going to overlay this filter

326
00:16:54,492 --> 00:16:56,998
on top of a spatial location in the image,

327
00:16:56,998 --> 00:16:58,636
right, and then do the dot product,

328
00:16:58,636 --> 00:17:02,665
the multiplication of each element of that filter

329
00:17:02,665 --> 00:17:05,203
with each corresponding element in that spatial location

330
00:17:05,203 --> 00:17:07,100
that we've just plopped it on top of.

331
00:17:07,100 --> 00:17:09,733
And then this is going to give us a dot product.

332
00:17:09,733 --> 00:17:14,345
So in this case, we have five times five times three,

333
00:17:14,345 --> 00:17:16,257
this is the number of multiplications

334
00:17:16,257 --> 00:17:18,755
that we're going to do, right, plus the bias term.

335
00:17:18,755 --> 00:17:22,324
And so this is basically taking our filter W

336
00:17:22,324 --> 00:17:26,491
and basically doing W transpose times X and plus bias.

337
00:17:27,722 --> 00:17:30,299
So is that clear how this works?

338
00:17:30,299 --> 00:17:31,771
Yeah, question.

339
00:17:31,771 --> 00:17:34,521
[faint speaking]

340
00:17:35,656 --> 00:17:37,837
Yeah, so the question is, when we do the dot product

341
00:17:37,837 --> 00:17:40,722
do we turn the five by five by three into one vector?

342
00:17:40,722 --> 00:17:42,907
Yeah, in essence that's what you're doing.

343
00:17:42,907 --> 00:17:44,950
You can, I mean, you can think of it as just

344
00:17:44,950 --> 00:17:47,996
plopping it on and doing the element-wise multiplication

345
00:17:47,996 --> 00:17:50,523
at each location, but this is going to give you the same result

346
00:17:50,523 --> 00:17:53,691
as if you stretched out the filter at that point,

347
00:17:53,691 --> 00:17:56,211
stretched out the input volume that it's laid over,

348
00:17:56,211 --> 00:17:57,891
and then took the dot product,

349
00:17:57,891 --> 00:18:01,111
and that's what's written here, yeah, question.

350
00:18:01,111 --> 00:18:03,867
[faint speaking]

351
00:18:03,867 --> 00:18:05,305
Oh, this is, so the question is,

352
00:18:05,305 --> 00:18:07,997
any intuition for why this is a W transpose?

353
00:18:07,997 --> 00:18:10,476
And this was just, not really,

354
00:18:10,476 --> 00:18:12,329
this is just the notation that we have here

355
00:18:12,329 --> 00:18:15,978
to make the math work out as a dot product.

356
00:18:15,978 --> 00:18:19,045
So it just depends on whether, how you're representing W

357
00:18:19,045 --> 00:18:23,974
and whether in this case if we look at the W matrix

358
00:18:23,974 --> 00:18:26,781
this happens to be each column and so we're just taking

359
00:18:26,781 --> 00:18:29,593
the transpose to get a row out of it.

360
00:18:29,593 --> 00:18:31,989
But there's no intuition here,

361
00:18:31,989 --> 00:18:34,098
we're just taking the filters of W

362
00:18:34,098 --> 00:18:37,679
and we're stretching it out into a one D vector,

363
00:18:37,679 --> 00:18:39,067
and in order for it to be a dot product

364
00:18:39,067 --> 00:18:42,862
it has to be like a one by, one by N vector.

365
00:18:42,862 --> 00:18:45,612
[faint speaking]

366
00:18:48,263 --> 00:18:49,829
Okay, so the question is,

367
00:18:49,829 --> 00:18:53,996
is W here not five by five by three, it's one by 75.

368
00:18:55,180 --> 00:18:57,307
So that's the case, right, if we're going

369
00:18:57,307 --> 00:18:59,882
to do this dot product of W transpose times X,

370
00:18:59,882 --> 00:19:01,120
we have to stretch it out first

371
00:19:01,120 --> 00:19:02,550
before we do the dot product.

372
00:19:02,550 --> 00:19:05,312
So we take the five by five by three,

373
00:19:05,312 --> 00:19:06,462
and we just take all these values

374
00:19:06,462 --> 00:19:09,629
and stretch it out into a long vector.

375
00:19:10,913 --> 00:19:14,992
And so again, similar to the other question,

376
00:19:14,992 --> 00:19:16,706
the actual operation that we're doing here

377
00:19:16,706 --> 00:19:18,691
is plopping our filter on top of

378
00:19:18,691 --> 00:19:20,568
a spatial location in the image

379
00:19:20,568 --> 00:19:23,375
and multiplying all of the corresponding values together,

380
00:19:23,375 --> 00:19:25,906
but in order just to make it kind of an easy expression

381
00:19:25,906 --> 00:19:27,527
similar to what we've seen before

382
00:19:27,527 --> 00:19:29,702
we can also just stretch each of these out,

383
00:19:29,702 --> 00:19:32,707
make sure that dimensions are transposed correctly

384
00:19:32,707 --> 00:19:35,061
so that it works out as a dot product.

385
00:19:35,061 --> 00:19:36,311
Yeah, question.

386
00:19:37,232 --> 00:19:40,740
[faint speaking]

387
00:19:40,740 --> 00:19:41,698
Okay, the question is,

388
00:19:41,698 --> 00:19:43,797
how do we slide the filter over the image.

389
00:19:43,797 --> 00:19:46,760
We'll go into that next, yes.

390
00:19:46,760 --> 00:19:49,510
[faint speaking]

391
00:19:52,071 --> 00:19:55,068
Okay, so the question is, should we rotate the kernel

392
00:19:55,068 --> 00:19:58,111
by 180 degrees to better match the convolution,

393
00:19:58,111 --> 00:20:00,178
the definition of a convolution.

394
00:20:00,178 --> 00:20:03,172
And so the answer is that we'll also show the equation

395
00:20:03,172 --> 00:20:05,870
for this later, but we're using convolution

396
00:20:05,870 --> 00:20:09,451
as kind of a looser definition of what's happening.

397
00:20:09,451 --> 00:20:11,171
So for people from signal processing,

398
00:20:11,171 --> 00:20:13,101
what we are actually technically doing,

399
00:20:13,101 --> 00:20:14,925
if you want to call this a convolution,

400
00:20:14,925 --> 00:20:18,738
is we're convolving with the flipped version of the filter.

401
00:20:18,738 --> 00:20:21,947
But for the most part, we just don't worry about this

402
00:20:21,947 --> 00:20:24,689
and we just, yeah, do this operation

403
00:20:24,689 --> 00:20:27,983
and it's like a convolution in spirit.

404
00:20:27,983 --> 00:20:28,900
Okay, so...

405
00:20:31,890 --> 00:20:35,077
Okay, so we had a question earlier, how do we, you know,

406
00:20:35,077 --> 00:20:37,246
slide this over all the spatial locations.

407
00:20:37,246 --> 00:20:38,526
Right, so what we're going to do is

408
00:20:38,526 --> 00:20:41,826
we're going to take this filter, we're going to start

409
00:20:41,826 --> 00:20:45,237
at the upper left-hand corner and basically center

410
00:20:45,237 --> 00:20:49,975
our filter on top of every pixel in this input volume.

411
00:20:49,975 --> 00:20:53,654
And at every position, we're going to do this dot product

412
00:20:53,654 --> 00:20:55,949
and this will produce one value

413
00:20:55,949 --> 00:20:57,511
in our output activation map.

414
00:20:57,511 --> 00:21:00,927
And so then we're going to just slide this around.

415
00:21:00,927 --> 00:21:02,844
The simplest version is just at every pixel

416
00:21:02,844 --> 00:21:05,359
we're going to do this operation and fill in

417
00:21:05,359 --> 00:21:09,442
the corresponding point in our output activation.

418
00:21:10,352 --> 00:21:14,166
You can see here that the dimensions are not exactly

419
00:21:14,166 --> 00:21:15,532
what would happen, right, if you're going to do this.

420
00:21:15,532 --> 00:21:17,748
I had 32 by 32 in the input

421
00:21:17,748 --> 00:21:20,126
and I'm having 28 by 28 in the output,

422
00:21:20,126 --> 00:21:22,920
and so we'll go into examples later of the math

423
00:21:22,920 --> 00:21:26,364
of exactly how this is going to work out dimension-wise,

424
00:21:26,364 --> 00:21:29,767
but basically you have a choice

425
00:21:29,767 --> 00:21:31,393
of how you're going to slide this,

426
00:21:31,393 --> 00:21:35,129
whether you go at every pixel or whether you slide,

427
00:21:35,129 --> 00:21:39,437
let's say, you know, two input values over at a time,

428
00:21:39,437 --> 00:21:41,326
two pixels over at a time,

429
00:21:41,326 --> 00:21:42,958
and so you can get different size outputs

430
00:21:42,958 --> 00:21:44,823
depending on how you choose to slide.

431
00:21:44,823 --> 00:21:48,990
But you're basically doing this operation in a grid fashion.

432
00:21:50,180 --> 00:21:52,623
Okay, so what we just saw earlier,

433
00:21:52,623 --> 00:21:55,792
this is taking one filter, sliding it over

434
00:21:55,792 --> 00:21:58,141
all of the spatial locations in the image

435
00:21:58,141 --> 00:22:00,620
and then we're going to get this activation map out, right,

436
00:22:00,620 --> 00:22:04,731
which is the value of that filter at every spatial location.

437
00:22:04,731 --> 00:22:07,669
And so when we're dealing with a convolutional layer,

438
00:22:07,669 --> 00:22:09,778
we want to work with multiple filters, right,

439
00:22:09,778 --> 00:22:12,858
because each filter is kind of looking for a specific

440
00:22:12,858 --> 00:22:16,250
type of template or concept in the input volume.

441
00:22:16,250 --> 00:22:20,479
And so we're going to have a set of multiple filters,

442
00:22:20,479 --> 00:22:22,623
and so here I'm going to take a second filter,

443
00:22:22,623 --> 00:22:26,359
this green filter, which is again five by five by three,

444
00:22:26,359 --> 00:22:30,059
I'm going to slide this over all of the spatial locations

445
00:22:30,059 --> 00:22:33,258
in my input volume, and then I'm going to get out

446
00:22:33,258 --> 00:22:37,425
this second green activation map also of the same size.

447
00:22:40,081 --> 00:22:41,628
And we can do this for as many filters

448
00:22:41,628 --> 00:22:43,553
as we want to have in this layer.

449
00:22:43,553 --> 00:22:45,817
So for example, if we have six filters,

450
00:22:45,817 --> 00:22:47,871
six of these five by five filters,

451
00:22:47,871 --> 00:22:51,698
then we're going to get in total six activation maps out.

452
00:22:51,698 --> 00:22:54,618
All of, so we're going to get this output volume

453
00:22:54,618 --> 00:22:58,368
that's going to be basically six by 28 by 28.

454
00:23:01,607 --> 00:23:03,609
Right, and so a preview of how we're going to use

455
00:23:03,609 --> 00:23:06,689
these convolutional layers in our convolutional network

456
00:23:06,689 --> 00:23:08,644
is that our ConvNet is basically going to be

457
00:23:08,644 --> 00:23:11,152
a sequence of these convolutional layers

458
00:23:11,152 --> 00:23:13,769
stacked on top of each other, same way as what we had

459
00:23:13,769 --> 00:23:16,676
with the simple linear layers in their neural network.

460
00:23:16,676 --> 00:23:18,403
And then we're going to intersperse these

461
00:23:18,403 --> 00:23:19,474
with activation functions,

462
00:23:19,474 --> 00:23:23,057
so for example, a ReLU activation function.

463
00:23:24,503 --> 00:23:28,670
Right, and so you're going to get something like Conv, ReLU,

464
00:23:29,535 --> 00:23:31,257
and usually also some pooling layers,

465
00:23:31,257 --> 00:23:33,975
and then you're just going to get a sequence of these

466
00:23:33,975 --> 00:23:36,965
each creating an output that's now going to be

467
00:23:36,965 --> 00:23:40,465
the input to the next convolutional layer.

468
00:23:43,638 --> 00:23:46,552
Okay, and so each of these layers, as I said earlier,

469
00:23:46,552 --> 00:23:49,305
has multiple filters, right, many filters.

470
00:23:49,305 --> 00:23:52,957
And each of the filter is producing an activation map.

471
00:23:52,957 --> 00:23:55,633
And so when you look at multiple of these layers

472
00:23:55,633 --> 00:23:58,141
stacked together in a ConvNet, what ends up happening

473
00:23:58,141 --> 00:24:01,175
is you end up learning this hierarching of filters,

474
00:24:01,175 --> 00:24:04,421
where the filters at the earlier layers usually represent

475
00:24:04,421 --> 00:24:06,318
low-level features that you're looking for.

476
00:24:06,318 --> 00:24:09,257
So things kind of like edges, right.

477
00:24:09,257 --> 00:24:10,272
And then at the mid-level,

478
00:24:10,272 --> 00:24:14,128
you're going to get more complex kinds of features,

479
00:24:14,128 --> 00:24:16,478
so maybe it's looking more for things

480
00:24:16,478 --> 00:24:19,113
like corners and blobs and so on.

481
00:24:19,113 --> 00:24:20,602
And then at higher-level features,

482
00:24:20,602 --> 00:24:22,823
you're going to get things that are starting

483
00:24:22,823 --> 00:24:25,852
to more resemble concepts than blobs.

484
00:24:25,852 --> 00:24:27,905
And we'll go into more detail later in the class

485
00:24:27,905 --> 00:24:30,522
in how you can actually visualize all these features

486
00:24:30,522 --> 00:24:33,165
and try and interpret what your network,

487
00:24:33,165 --> 00:24:35,561
what kinds of features your network is learning.

488
00:24:35,561 --> 00:24:38,974
But the important thing for now is just to understand

489
00:24:38,974 --> 00:24:40,378
that what these features end up being

490
00:24:40,378 --> 00:24:42,800
when you have a whole stack of these,

491
00:24:42,800 --> 00:24:46,967
is these types of simple to more complex features.

492
00:24:48,305 --> 00:24:49,138
[faint speaking]

493
00:24:49,138 --> 00:24:49,971
Yeah.

494
00:24:50,984 --> 00:24:51,817
Oh, okay.

495
00:24:59,067 --> 00:25:01,124
Oh, okay, so the question is, what's the intuition

496
00:25:01,124 --> 00:25:03,113
for increasing the depth each time.

497
00:25:03,113 --> 00:25:06,384
So here I had three filters in the original layer

498
00:25:06,384 --> 00:25:08,814
and then six filters in the next layer.

499
00:25:08,814 --> 00:25:12,651
Right, and so this is mostly a design choice.

500
00:25:12,651 --> 00:25:14,274
You know, people in practice have found

501
00:25:14,274 --> 00:25:17,255
certain types of these configurations to work better.

502
00:25:17,255 --> 00:25:19,894
And so later on we'll go into case studies of different

503
00:25:19,894 --> 00:25:23,185
kinds of convolutional neural network architectures

504
00:25:23,185 --> 00:25:25,658
and design choices for these

505
00:25:25,658 --> 00:25:28,344
and why certain ones work better than others.

506
00:25:28,344 --> 00:25:30,516
But yeah, basically the choice of,

507
00:25:30,516 --> 00:25:31,876
you're going to have many design choices

508
00:25:31,876 --> 00:25:33,238
in a convolutional neural network,

509
00:25:33,238 --> 00:25:34,948
the size of your filter, the stride,

510
00:25:34,948 --> 00:25:36,369
how many filters you have,

511
00:25:36,369 --> 00:25:39,611
and so we'll talk about this all more later.

512
00:25:39,611 --> 00:25:41,246
Question.

513
00:25:41,246 --> 00:25:43,996
[faint speaking]

514
00:25:50,300 --> 00:25:53,691
Yeah, so the question is, as we're sliding this filter

515
00:25:53,691 --> 00:25:56,364
over the image spatially it looks like we're sampling

516
00:25:56,364 --> 00:26:00,177
the edges and corners less than the other locations.

517
00:26:00,177 --> 00:26:01,676
Yeah, that's a really good point,

518
00:26:01,676 --> 00:26:04,483
and we'll talk I think in a few slides

519
00:26:04,483 --> 00:26:07,900
about how we try and compensate for that.

520
00:26:12,009 --> 00:26:15,592
Okay, so each of these convolutional layers

521
00:26:16,870 --> 00:26:20,797
that we have stacked together, we saw how we're starting

522
00:26:20,797 --> 00:26:23,877
with more simpler features and then aggregating these

523
00:26:23,877 --> 00:26:26,228
into more complex features later on.

524
00:26:26,228 --> 00:26:28,343
And so in practice this is compatible

525
00:26:28,343 --> 00:26:32,549
with what Hubel and Wiesel noticed in their experiments,

526
00:26:32,549 --> 00:26:35,895
right, that we had these simple cells

527
00:26:35,895 --> 00:26:37,406
at the earlier stages of processing,

528
00:26:37,406 --> 00:26:39,532
followed by more complex cells later on.

529
00:26:39,532 --> 00:26:42,865
And so even though we didn't explicitly

530
00:26:44,067 --> 00:26:46,455
force our ConvNet to learn these kinds of features,

531
00:26:46,455 --> 00:26:48,295
in practice when you give it this type of

532
00:26:48,295 --> 00:26:51,623
hierarchical structure and train it using backpropagation,

533
00:26:51,623 --> 00:26:55,041
these are the kinds of filters that end up being learned.

534
00:26:55,041 --> 00:26:57,791
[faint speaking]

535
00:27:05,555 --> 00:27:07,116
Okay, so yeah, so the question is,

536
00:27:07,116 --> 00:27:10,979
what are we seeing in these visualizations.

537
00:27:10,979 --> 00:27:13,321
And so, alright so, in these visualizations, like,

538
00:27:13,321 --> 00:27:17,134
if we look at this Conv1, the first convolutional layer,

539
00:27:17,134 --> 00:27:20,975
each of these grid, each part of this grid is a one neuron.

540
00:27:20,975 --> 00:27:23,118
And so what we've visualized here

541
00:27:23,118 --> 00:27:26,701
is what the input looks like that maximizes

542
00:27:27,893 --> 00:27:29,956
the activation of that particular neuron.

543
00:27:29,956 --> 00:27:31,826
So what sort of image you would get

544
00:27:31,826 --> 00:27:34,070
that would give you the largest value,

545
00:27:34,070 --> 00:27:36,594
make that neuron fire and have the largest value.

546
00:27:36,594 --> 00:27:38,811
And so the way we do this is basically

547
00:27:38,811 --> 00:27:42,978
by doing backpropagation from a particular neuron activation

548
00:27:44,415 --> 00:27:46,570
and seeing what in the input will trigger,

549
00:27:46,570 --> 00:27:48,848
will give you the highest values of this neuron.

550
00:27:48,848 --> 00:27:50,730
And this is something that we'll talk about

551
00:27:50,730 --> 00:27:53,276
in much more depth in a later lecture

552
00:27:53,276 --> 00:27:56,280
about how we create all of these visualizations.

553
00:27:56,280 --> 00:27:59,124
But basically each element of these grids

554
00:27:59,124 --> 00:28:03,342
is showing what in the input would look like

555
00:28:03,342 --> 00:28:06,775
that basically maximizes the activation of the neuron.

556
00:28:06,775 --> 00:28:10,608
So in a sense, what is the neuron looking for?

557
00:28:13,537 --> 00:28:18,490
Okay, so here is an example of some of the activation maps

558
00:28:18,490 --> 00:28:19,835
produced by each filter, right.

559
00:28:19,835 --> 00:28:22,200
So we can visualize up here on the top

560
00:28:22,200 --> 00:28:26,025
we have this whole row of example five by five filters,

561
00:28:26,025 --> 00:28:30,407
and so this is basically a real case from a trained ConvNet

562
00:28:30,407 --> 00:28:34,490
where each of these is what a five by five filter

563
00:28:35,593 --> 00:28:38,511
looks like, and then as we convolve this over an image,

564
00:28:38,511 --> 00:28:41,197
so in this case this I think it's like a corner of a car,

565
00:28:41,197 --> 00:28:44,346
the car light, what the activation looks like.

566
00:28:44,346 --> 00:28:46,799
Right, and so here for example,

567
00:28:46,799 --> 00:28:49,449
if we look at this first one, this red filter,

568
00:28:49,449 --> 00:28:51,330
filter like with a red box around it,

569
00:28:51,330 --> 00:28:53,412
we'll see that it's looking for,

570
00:28:53,412 --> 00:28:56,432
the template looks like an edge, right, an oriented edge.

571
00:28:56,432 --> 00:28:58,050
And so if you slide it over the image,

572
00:28:58,050 --> 00:29:01,812
it'll have a high value, a more white value

573
00:29:01,812 --> 00:29:06,601
where there are edges in this type of orientation.

574
00:29:06,601 --> 00:29:10,563
And so each of these activation maps is kind of the output

575
00:29:10,563 --> 00:29:12,358
of sliding one of these filters over

576
00:29:12,358 --> 00:29:16,444
and where these filters are causing, you know,

577
00:29:16,444 --> 00:29:20,747
where this sort of template is more present in the image.

578
00:29:20,747 --> 00:29:24,869
And so the reason we call these convolutional is because

579
00:29:24,869 --> 00:29:27,221
this is related to the convolution of two signals,

580
00:29:27,221 --> 00:29:29,153
and so someone pointed out earlier

581
00:29:29,153 --> 00:29:32,982
that this is basically this convolution equation over here,

582
00:29:32,982 --> 00:29:35,333
for people who have seen convolutions before

583
00:29:35,333 --> 00:29:37,340
in signal processing, and in practice

584
00:29:37,340 --> 00:29:38,927
it's actually more like a correlation

585
00:29:38,927 --> 00:29:41,583
where we're convolving with the flipped version

586
00:29:41,583 --> 00:29:46,154
of the filter, but this is kind of a subtlety,

587
00:29:46,154 --> 00:29:50,149
it's not really important for the purposes of this class.

588
00:29:50,149 --> 00:29:52,292
But basically if you're writing out what you're doing,

589
00:29:52,292 --> 00:29:55,450
it has an expression that looks something like this,

590
00:29:55,450 --> 00:29:58,385
which is the standard definition of a convolution.

591
00:29:58,385 --> 00:30:00,402
But this is basically just taking a filter,

592
00:30:00,402 --> 00:30:02,432
sliding it spatially over the image

593
00:30:02,432 --> 00:30:06,432
and computing the dot product at every location.

594
00:30:09,088 --> 00:30:11,977
Okay, so you know, as I had mentioned earlier,

595
00:30:11,977 --> 00:30:14,208
like what our total convolutional neural network

596
00:30:14,208 --> 00:30:17,278
is going to look like is we're going to have an input image,

597
00:30:17,278 --> 00:30:19,693
and then we're going to pass it through

598
00:30:19,693 --> 00:30:21,633
this sequence of layers, right,

599
00:30:21,633 --> 00:30:23,915
where we're going to have a convolutional layer first.

600
00:30:23,915 --> 00:30:28,236
We usually have our non-linear layer after that.

601
00:30:28,236 --> 00:30:30,579
So ReLU is something that's very commonly used

602
00:30:30,579 --> 00:30:33,608
that we're going to talk about more later.

603
00:30:33,608 --> 00:30:36,791
And then we have these Conv, ReLU, Conv, ReLU layers,

604
00:30:36,791 --> 00:30:39,775
and then once in a while we'll use a pooling layer

605
00:30:39,775 --> 00:30:41,244
that we'll talk about later as well

606
00:30:41,244 --> 00:30:45,411
that basically downsamples the size of our activation maps.

607
00:30:47,300 --> 00:30:50,785
And then finally at the end of this we'll take our last

608
00:30:50,785 --> 00:30:54,403
convolutional layer output and then we're going to use

609
00:30:54,403 --> 00:30:56,872
a fully connected layer that we've seen before,

610
00:30:56,872 --> 00:31:00,316
connected to all of these convolutional outputs,

611
00:31:00,316 --> 00:31:03,011
and use that to get a final score function

612
00:31:03,011 --> 00:31:07,178
basically like what we've already been working with.

613
00:31:08,445 --> 00:31:10,931
Okay, so now let's work out some examples

614
00:31:10,931 --> 00:31:14,181
of how the spatial dimensions work out.

615
00:31:18,363 --> 00:31:23,087
So let's take our 32 by 32 by three image as before,

616
00:31:23,087 --> 00:31:25,624
right, and we have our five by five by three filter

617
00:31:25,624 --> 00:31:28,025
that we're going to slide over this image.

618
00:31:28,025 --> 00:31:29,816
And we're going to see how we're going to use that

619
00:31:29,816 --> 00:31:34,337
to produce exactly this 28 by 28 activation map.

620
00:31:34,337 --> 00:31:37,644
So let's assume that we actually have a seven by seven input

621
00:31:37,644 --> 00:31:39,104
just to be simpler, and let's assume

622
00:31:39,104 --> 00:31:41,505
we have a three by three filter.

623
00:31:41,505 --> 00:31:42,522
So what we're going to do is

624
00:31:42,522 --> 00:31:44,969
we're going to take this filter,

625
00:31:44,969 --> 00:31:47,418
plop it down in our upper left-hand corner,

626
00:31:47,418 --> 00:31:50,253
right, and we're going to multiply, do the dot product,

627
00:31:50,253 --> 00:31:53,169
multiply all these values together to get our first value,

628
00:31:53,169 --> 00:31:54,918
and this is going to go into the upper left-hand value

629
00:31:54,918 --> 00:31:56,764
of our activation map.

630
00:31:56,764 --> 00:31:58,217
Right, and then what we're going to do next

631
00:31:58,217 --> 00:32:00,475
is we're just going to take this filter,

632
00:32:00,475 --> 00:32:02,389
slide it one position to the right,

633
00:32:02,389 --> 00:32:05,535
and then we're going to get another value out from here.

634
00:32:05,535 --> 00:32:09,895
And so we can continue with this to have another value,

635
00:32:09,895 --> 00:32:12,797
another, and in the end what we're going to get

636
00:32:12,797 --> 00:32:14,528
is a five by five output, right,

637
00:32:14,528 --> 00:32:17,776
because what fit was basically sliding this filter

638
00:32:17,776 --> 00:32:22,214
a total of five spatial locations horizontally

639
00:32:22,214 --> 00:32:25,381
and five spatial locations vertically.

640
00:32:27,834 --> 00:32:29,414
Okay, so as I said before

641
00:32:29,414 --> 00:32:31,906
there's different kinds of design choices that we can make.

642
00:32:31,906 --> 00:32:34,710
Right, so previously I slid it at every single

643
00:32:34,710 --> 00:32:37,828
spatial location and the interval at which I slide

644
00:32:37,828 --> 00:32:40,326
I'm going to call the stride.

645
00:32:40,326 --> 00:32:43,093
And so previously we used the stride of one.

646
00:32:43,093 --> 00:32:44,567
And so now let's see what happens

647
00:32:44,567 --> 00:32:46,700
if we have a stride of two.

648
00:32:46,700 --> 00:32:48,625
Right, so now we're going to take our first location

649
00:32:48,625 --> 00:32:51,898
the same as before, and then we're going to skip

650
00:32:51,898 --> 00:32:55,527
this time two pixels over and we're going to get

651
00:32:55,527 --> 00:32:58,944
our next value centered at this location.

652
00:33:00,773 --> 00:33:02,938
Right, and so now if we use a stride of two,

653
00:33:02,938 --> 00:33:07,340
we have in total three of these that can fit,

654
00:33:07,340 --> 00:33:11,257
and so we're going to get a three by three output.

655
00:33:13,035 --> 00:33:15,955
Okay, and so what happens when we have a stride of three,

656
00:33:15,955 --> 00:33:18,653
what's the output size of this?

657
00:33:18,653 --> 00:33:21,924
And so in this case, right, we have three,

6