1
00:00:11,077 --> 00:00:14,258
- Okay we have a lot to cover today so let's get started.

2
00:00:14,258 --> 00:00:17,454
Today we'll be talking about Generative Models.

3
00:00:17,454 --> 00:00:20,484
And before we start, a few administrative details.

4
00:00:20,484 --> 00:00:23,522
So midterm grades will be released on Gradescope this week

5
00:00:23,522 --> 00:00:27,730
A reminder that A3 is due next Friday May 26th.

6
00:00:27,730 --> 00:00:30,376
The HyperQuest deadline for extra credit you can do this

7
00:00:30,376 --> 00:00:32,709
still until Sunday May 21st.

8
00:00:33,632 --> 00:00:37,799
And our poster session is June 6th from 12 to 3 P.M..

9
00:00:40,812 --> 00:00:43,095
Okay so an overview of what we're going to talk about today

10
00:00:43,095 --> 00:00:44,646
we're going to switch gears a little bit

11
00:00:44,646 --> 00:00:47,759
and take a look at unsupervised learning today.

12
00:00:47,759 --> 00:00:50,686
And in particular we're going to talk about generative

13
00:00:50,686 --> 00:00:54,103
models which is a type of unsupervised learning.

14
00:00:54,103 --> 00:00:57,112
And we'll look at three types of generative models.

15
00:00:57,112 --> 00:01:01,174
So pixelRNNs and pixelCNNs variational autoencoders

16
00:01:01,174 --> 00:01:04,174
and Generative Adversarial networks.

17
00:01:05,571 --> 00:01:07,847
So so far in this class we've talked a lot about supervised

18
00:01:07,847 --> 00:01:09,672
learning and different kinds

19
00:01:09,672 --> 00:01:11,168
of supervised learning problems.

20
00:01:11,168 --> 00:01:14,247
So in the supervised learning set up we have our data X and

21
00:01:14,247 --> 00:01:16,078
then we have some labels Y.

22
00:01:16,078 --> 00:01:19,063
And our goal is to learn a function that's mapping

23
00:01:19,063 --> 00:01:21,417
from our data X to our labels Y.

24
00:01:21,417 --> 00:01:26,237
And these labels can take many different types of forms.

25
00:01:26,237 --> 00:01:28,390
So for example, we've looked at classification

26
00:01:28,390 --> 00:01:30,303
where our input is an image

27
00:01:30,303 --> 00:01:34,934
and we want to output Y, a class label for the category.

28
00:01:34,934 --> 00:01:37,214
We've talked about object detection where now our input

29
00:01:37,214 --> 00:01:39,926
is still an image but here we want to output the

30
00:01:39,926 --> 00:01:44,093
bounding boxes of instances of up to multiple dogs or cats.

31
00:01:46,138 --> 00:01:48,532
We've talked about semantic segmentation where here we have

32
00:01:48,532 --> 00:01:51,069
a label for every pixel the category that every pixel

33
00:01:51,069 --> 00:01:51,986
belongs to.

34
00:01:53,572 --> 00:01:55,298
And we've also talked about image captioning

35
00:01:55,298 --> 00:01:58,961
where here our label is now a sentence

36
00:01:58,961 --> 00:02:02,961
and so it's now in the form of natural language.

37
00:02:03,998 --> 00:02:06,534
So unsupervised learning in this set up,

38
00:02:06,534 --> 00:02:08,095
it's a type of learning where here we have

39
00:02:08,095 --> 00:02:11,520
unlabeled training data and our goal now is to learn some

40
00:02:11,520 --> 00:02:15,661
underlying hidden structure of the data.

41
00:02:15,661 --> 00:02:17,439
Right, so an example of this can be something like

42
00:02:17,439 --> 00:02:20,370
clustering which you guys might have seen before

43
00:02:20,370 --> 00:02:22,534
where here the goal is to find groups within the data

44
00:02:22,534 --> 00:02:25,029
that are similar through some type of metric.

45
00:02:25,029 --> 00:02:27,187
For example, K means clustering.

46
00:02:27,187 --> 00:02:30,371
Another example of an unsupervised learning task

47
00:02:30,371 --> 00:02:32,871
is a dimensionality reduction.

48
00:02:33,777 --> 00:02:36,634
So in this problem want to find axes along which

49
00:02:36,634 --> 00:02:38,939
our training data has the most variation,

50
00:02:38,939 --> 00:02:42,298
and so these axes are part of the underlying structure

51
00:02:42,298 --> 00:02:43,537
of the data.

52
00:02:43,537 --> 00:02:45,685
And then we can use this to reduce of dimensionality

53
00:02:45,685 --> 00:02:48,918
of the data such that the data has significant variation

54
00:02:48,918 --> 00:02:51,095
among each of the remaining dimensions.

55
00:02:51,095 --> 00:02:52,938
Right, so this example here we start off with data

56
00:02:52,938 --> 00:02:56,014
in three dimensions and we're going to find two

57
00:02:56,014 --> 00:02:57,842
axes of variation in this case

58
00:02:57,842 --> 00:03:01,259
and reduce our data projected down to 2D.

59
00:03:04,205 --> 00:03:06,214
Another example of unsupervised learning

60
00:03:06,214 --> 00:03:09,964
is learning feature representations for data.

61
00:03:11,006 --> 00:03:14,137
We've seen how to do this in supervised ways before

62
00:03:14,137 --> 00:03:15,645
where we used the supervised loss,

63
00:03:15,645 --> 00:03:17,209
for example classification.

64
00:03:17,209 --> 00:03:19,743
Where we have the classification label.

65
00:03:19,743 --> 00:03:21,617
We have something like a Softmax loss

66
00:03:21,617 --> 00:03:23,671
And we can train a neural network where

67
00:03:23,671 --> 00:03:25,635
we can interpret activations for example

68
00:03:25,635 --> 00:03:27,723
our FC7 layers as some kind of future

69
00:03:27,723 --> 00:03:29,869
representation for the data.

70
00:03:29,869 --> 00:03:31,791
And in an unsupervised setting,

71
00:03:31,791 --> 00:03:34,492
for example here autoencoders which we'll talk

72
00:03:34,492 --> 00:03:35,742
more about later

73
00:03:35,742 --> 00:03:38,349
In this case our loss is now trying to

74
00:03:38,349 --> 00:03:41,962
reconstruct the input data to basically,

75
00:03:41,962 --> 00:03:44,685
you have a good reconstruction of our input data

76
00:03:44,685 --> 00:03:46,872
and use this to learn features.

77
00:03:46,872 --> 00:03:49,162
So we're learning a feature representation without

78
00:03:49,162 --> 00:03:52,245
using any additional external labels.

79
00:03:53,471 --> 00:03:56,411
And finally another example of unsupervised learning

80
00:03:56,411 --> 00:03:59,585
is density estimation where in this case we want to

81
00:03:59,585 --> 00:04:02,884
estimate the underlying distribution of our data.

82
00:04:02,884 --> 00:04:05,581
So for example in this top case over here,

83
00:04:05,581 --> 00:04:08,432
we have points in 1-d and we can try

84
00:04:08,432 --> 00:04:10,811
and fit a Gaussian into this density

85
00:04:10,811 --> 00:04:13,757
and in this bottom example over here it's 2D data

86
00:04:13,757 --> 00:04:16,605
and here again we're trying to estimate the density and

87
00:04:16,605 --> 00:04:18,750
we can model this density.

88
00:04:18,750 --> 00:04:20,989
We want to fit a model such that the density is higher

89
00:04:20,989 --> 00:04:24,239
where there's more points concentrated.

90
00:04:26,100 --> 00:04:29,377
And so to summarize the differences in unsupervised

91
00:04:29,377 --> 00:04:32,069
learning which we've looked a lot so far,

92
00:04:32,069 --> 00:04:33,657
we want to use label data to learn

93
00:04:33,657 --> 00:04:35,990
a function mapping from X to Y

94
00:04:35,990 --> 00:04:38,515
and an unsupervised learning we use no labels

95
00:04:38,515 --> 00:04:40,716
and instead we try to learn some underlying hidden

96
00:04:40,716 --> 00:04:44,124
structure of the data, whether this is grouping,

97
00:04:44,124 --> 00:04:48,291
acts as a variation or underlying density estimation.

98
00:04:49,662 --> 00:04:51,855
And unsupervised learning is a huge

99
00:04:51,855 --> 00:04:54,113
and really exciting area of research and

100
00:04:54,113 --> 00:04:57,074
and some of the reasons are that training data is really

101
00:04:57,074 --> 00:04:59,898
cheap, it doesn't use labels so we're able to learn

102
00:04:59,898 --> 00:05:04,339
from a lot of data at one time and basically utilize a lot

103
00:05:04,339 --> 00:05:07,672
more data than if we required annotating

104
00:05:07,672 --> 00:05:09,977
or finding labels for data.

105
00:05:09,977 --> 00:05:13,345
And unsupervised learning is still relatively

106
00:05:13,345 --> 00:05:15,758
unsolved research area by comparison.

107
00:05:15,758 --> 00:05:17,823
There's a lot of open problems in this,

108
00:05:17,823 --> 00:05:20,483
but it also, it holds the potential of

109
00:05:20,483 --> 00:05:22,029
if you're able to successfully learn

110
00:05:22,029 --> 00:05:24,669
and represent a lot of the underlying structure

111
00:05:24,669 --> 00:05:26,434
in the data then this also takes you a

112
00:05:26,434 --> 00:05:30,229
long way towards the Holy Grail of trying to understand the

113
00:05:30,229 --> 00:05:32,729
structure of the visual world.

114
00:05:35,026 --> 00:05:38,222
So that's a little bit of kind of a high-level big picture

115
00:05:38,222 --> 00:05:40,432
view of unsupervised learning.

116
00:05:40,432 --> 00:05:44,155
And today will focus more specifically on generative models

117
00:05:44,155 --> 00:05:46,996
which is a class of models for unsupervised

118
00:05:46,996 --> 00:05:50,369
learning where given training data our goal is to try and

119
00:05:50,369 --> 00:05:52,933
generate new samples from the same distribution.

120
00:05:52,933 --> 00:05:55,441
Right, so we have training data over here generated

121
00:05:55,441 --> 00:05:57,686
from some distribution P data

122
00:05:57,686 --> 00:06:00,769
and we want to learn a model, P model

123
00:06:01,872 --> 00:06:04,955
to generate samples from the same distribution

124
00:06:04,955 --> 00:06:09,854
and so we want to learn P model to be similar to P data.

125
00:06:09,854 --> 00:06:12,636
And generative models address density estimations.

126
00:06:12,636 --> 00:06:14,551
So this problem that we saw earlier of trying

127
00:06:14,551 --> 00:06:18,217
to estimate the underlying distribution of your

128
00:06:18,217 --> 00:06:20,093
training data which is a core problem

129
00:06:20,093 --> 00:06:22,180
in unsupervised learning.

130
00:06:22,180 --> 00:06:25,190
And we'll see that there's several flavors of this.

131
00:06:25,190 --> 00:06:28,461
We can use generative models to do explicit density

132
00:06:28,461 --> 00:06:31,270
estimation where we're going to explicitly define

133
00:06:31,270 --> 00:06:33,353
and solve for our P model

134
00:06:35,045 --> 00:06:37,610
or we can also do implicit density estimation

135
00:06:37,610 --> 00:06:40,868
where in this case we'll learn a model that can

136
00:06:40,868 --> 00:06:45,035
produce samples from P model without explicitly defining it.

137
00:06:47,700 --> 00:06:50,016
So, why do we care about generative models?

138
00:06:50,016 --> 00:06:52,584
Why is this a really interesting core problem

139
00:06:52,584 --> 00:06:54,096
in unsupervised learning?

140
00:06:54,096 --> 00:06:55,868
Well there's a lot of things that we can do

141
00:06:55,868 --> 00:06:57,451
with generative models.

142
00:06:57,451 --> 00:07:01,243
If we're able to create realistic samples from the data

143
00:07:01,243 --> 00:07:03,826
distributions that we want we can do really cool things

144
00:07:03,826 --> 00:07:04,659
with this, right?

145
00:07:04,659 --> 00:07:07,143
We can generate just beautiful samples to start

146
00:07:07,143 --> 00:07:11,334
with so on the left you can see a completely new samples of

147
00:07:11,334 --> 00:07:14,568
just generated by these generative models.

148
00:07:14,568 --> 00:07:17,387
Also in the center here generated samples of

149
00:07:17,387 --> 00:07:21,042
images we can also do tasks like super resolution,

150
00:07:21,042 --> 00:07:25,232
colorization so hallucinating or filling in these edges

151
00:07:25,232 --> 00:07:27,732
with generated ideas of colors

152
00:07:30,078 --> 00:07:32,145
and what the purse should look like.

153
00:07:32,145 --> 00:07:36,022
We can also use generative models of time series data

154
00:07:36,022 --> 00:07:39,147
for simulation and planning and so this will be useful in

155
00:07:39,147 --> 00:07:41,619
for reinforcement learning applications

156
00:07:41,619 --> 00:07:43,558
which we'll talk a bit more about reinforcement learning

157
00:07:43,558 --> 00:07:45,089
in a later lecture.

158
00:07:45,089 --> 00:07:48,190
And training generative models can also enable

159
00:07:48,190 --> 00:07:50,261
inference of latent representations.

160
00:07:50,261 --> 00:07:54,018
Learning latent features that can be useful

161
00:07:54,018 --> 00:07:57,435
as general features for downstream tasks.

162
00:07:59,059 --> 00:08:02,188
So if we look at types of generative models

163
00:08:02,188 --> 00:08:05,688
these can be organized into the taxonomy here

164
00:08:05,688 --> 00:08:08,789
where we have these two major branches that we talked about,

165
00:08:08,789 --> 00:08:13,180
explicit density models and implicit density models.

166
00:08:13,180 --> 00:08:16,202
And then we can also get down into many

167
00:08:16,202 --> 00:08:19,062
of these other sub categories.

168
00:08:19,062 --> 00:08:23,423
And well we can refer to this figure is adapted

169
00:08:23,423 --> 00:08:27,814
from a tutorial on GANs from Ian Goodfellow

170
00:08:27,814 --> 00:08:29,713
and so if you're interested in some

171
00:08:29,713 --> 00:08:32,501
of these different taxonomy and categorizations of

172
00:08:32,501 --> 00:08:35,749
generative models this is a good resource that you can take

173
00:08:35,749 --> 00:08:36,861
a look at.

174
00:08:36,861 --> 00:08:39,052
But today we're going to discuss three of the most

175
00:08:39,052 --> 00:08:43,259
popular types of generative models that are in use

176
00:08:43,259 --> 00:08:45,645
and in research today.

177
00:08:45,645 --> 00:08:49,475
And so we'll talk first briefly about pixelRNNs and CNNs

178
00:08:49,475 --> 00:08:52,162
And then we'll talk about variational autoencoders.

179
00:08:52,162 --> 00:08:55,661
These are both types of explicit density models.

180
00:08:55,661 --> 00:08:57,494
One that's using a tractable density

181
00:08:57,494 --> 00:09:01,312
and another that's using an approximate density

182
00:09:01,312 --> 00:09:05,614
And then we'll talk about generative adversarial networks,

183
00:09:05,614 --> 00:09:09,781
GANs which are a type of implicit density estimation.

184
00:09:12,152 --> 00:09:16,304
So let's first talk about pixelRNNs and CNNs.

185
00:09:16,304 --> 00:09:20,015
So these are a type of fully visible belief networks

186
00:09:20,015 --> 00:09:22,432
which are modeling a density explicitly

187
00:09:22,432 --> 00:09:25,966
so in this case what they do is we have this

188
00:09:25,966 --> 00:09:28,958
image data X that we have and we want to model the

189
00:09:28,958 --> 00:09:32,236
probability or likelihood of this image P of X.

190
00:09:32,236 --> 00:09:34,941
Right and so in this case, for these kinds of models,

191
00:09:34,941 --> 00:09:37,646
we use the chain rule to decompose this likelihood

192
00:09:37,646 --> 00:09:40,384
into a product of one dimensional distribution.

193
00:09:40,384 --> 00:09:43,493
So we have here the probability of each pixel X I

194
00:09:43,493 --> 00:09:47,871
conditioned on all previous pixels X1 through XI - 1.

195
00:09:47,871 --> 00:09:50,495
and your likelihood all right, your joint likelihood

196
00:09:50,495 --> 00:09:53,416
of all the pixels in your image is going to be the product

197
00:09:53,416 --> 00:09:55,474
of all of these pixels together,

198
00:09:55,474 --> 00:09:58,073
all of these likelihoods together.

199
00:09:58,073 --> 00:10:00,690
And then once we define this likelihood,

200
00:10:00,690 --> 00:10:04,428
in order to train this model we can just maximize

201
00:10:04,428 --> 00:10:06,688
the likelihood of our training data

202
00:10:06,688 --> 00:10:08,938
under this defined density.

203
00:10:10,980 --> 00:10:14,334
So if we look at this this distribution over pixel values

204
00:10:14,334 --> 00:10:17,319
right, we have this P of XI given all the previous

205
00:10:17,319 --> 00:10:20,833
pixel values, well this is a really complex distribution.

206
00:10:20,833 --> 00:10:22,700
So how can we model this?

207
00:10:22,700 --> 00:10:25,478
Well we've seen before that if we want to have complex

208
00:10:25,478 --> 00:10:29,042
transformations we can do these using neural networks.

209
00:10:29,042 --> 00:10:31,766
Neural networks are a good way to express complex

210
00:10:31,766 --> 00:10:32,828
transformations.

211
00:10:32,828 --> 00:10:36,189
And so what we'll do is we'll use a neural network

212
00:10:36,189 --> 00:10:40,633
to express this complex function that we have

213
00:10:40,633 --> 00:10:42,300
of the distribution.

214
00:10:43,235 --> 00:10:44,796
And one thing you'll see here is that,

215
00:10:44,796 --> 00:10:47,379
okay even if we're going to use a neural network for this

216
00:10:47,379 --> 00:10:50,379
another thing we have to take care of is how do we order

217
00:10:50,379 --> 00:10:51,212
the pixels.

218
00:10:51,212 --> 00:10:54,009
Right, I said here that we have a distribution

219
00:10:54,009 --> 00:10:56,577
for P of XI given all previous pixels

220
00:10:56,577 --> 00:10:58,886
but what does all previous the pixels mean?

221
00:10:58,886 --> 00:11:01,303
So we'll take a look at that.

222
00:11:03,336 --> 00:11:06,669
So PixelRNN was a model proposed in 2016

223
00:11:07,595 --> 00:11:11,762
that basically defines a way for setting up and optimizing

224
00:11:14,949 --> 00:11:17,657
this problem and so how this model works is

225
00:11:17,657 --> 00:11:19,479
that we're going to generate pixels starting

226
00:11:19,479 --> 00:11:21,187
in a corner of the image.

227
00:11:21,187 --> 00:11:25,767
So we can look at this grid as basically the pixels

228
00:11:25,767 --> 00:11:28,039
of your image and so what we're going to do is start

229
00:11:28,039 --> 00:11:31,050
from the pixel in the upper left-hand corner

230
00:11:31,050 --> 00:11:34,548
and then we're going to sequentially generate pixels based

231
00:11:34,548 --> 00:11:36,131
on these connections from the arrows

232
00:11:36,131 --> 00:11:37,195
that you can see here.

233
00:11:37,195 --> 00:11:39,962
And each of the dependencies on the previous pixels

234
00:11:39,962 --> 00:11:44,332
in this ordering is going to be modeled using an RNN

235
00:11:44,332 --> 00:11:47,114
or more specifically an LSTM which we've seen before

236
00:11:47,114 --> 00:11:48,092
in lecture.

237
00:11:48,092 --> 00:11:51,385
Right so using this we can basically continue to move

238
00:11:51,385 --> 00:11:55,242
forward just moving down a long is diagonal

239
00:11:55,242 --> 00:11:57,860
and generating all of these pixel values dependent

240
00:11:57,860 --> 00:12:01,244
on the pixels that they're connected to.

241
00:12:01,244 --> 00:12:03,925
And so this works really well but the drawback here

242
00:12:03,925 --> 00:12:05,908
is that this sequential generation, right,

243
00:12:05,908 --> 00:12:08,736
so it's actually quite slow to do this.

244
00:12:08,736 --> 00:12:10,869
You can imagine you know if you're going to generate a new

245
00:12:10,869 --> 00:12:13,334
image instead of all of these feed forward networks that we

246
00:12:13,334 --> 00:12:15,061
see, we've seen with CNNs.

247
00:12:15,061 --> 00:12:16,952
Here we're going to have to iteratively go through

248
00:12:16,952 --> 00:12:20,952
and generate all these images, all these pixels.

249
00:12:24,044 --> 00:12:27,499
So a little bit later, after a pixelRNN,

250
00:12:27,499 --> 00:12:30,575
another model called pixelCNN was introduced.

251
00:12:30,575 --> 00:12:34,570
And this has very similar setup as pixelCNN

252
00:12:34,570 --> 00:12:36,887
and we're still going to do this image generation

253
00:12:36,887 --> 00:12:39,801
starting from the corner of the of the image and expanding

254
00:12:39,801 --> 00:12:43,074
outwards but the difference now is that now instead of using   255 00:12:43,074 --> 00:12:45,480 an RNN to model all these dependencies

255
00:12:45,480 --> 00:12:47,752
we're going to use the CNN instead.

256
00:12:47,752 --> 00:12:52,179
And we're now going to use a CNN over a a context region

257
00:12:52,179 --> 00:12:54,761
that you can see here around in the particular pixel

258
00:12:54,761 --> 00:12:56,384
that we're going to generate now.

259
00:12:56,384 --> 00:12:58,127
Right so we take the pixels around it,

260
00:12:58,127 --> 00:13:02,843
this gray area within the region that's already been

261
00:13:02,843 --> 00:13:05,480
generated and then we can pass this through a CNN

262
00:13:05,480 --> 00:13:09,313
and use that to generate our next pixel value.

263
00:13:11,041 --> 00:13:14,466
And so what this is going to give is this is going to give

264
00:13:14,466 --> 00:13:18,055
This is a CNN, a neural network at each pixel location

265
00:13:18,055 --> 00:13:20,176
right and so the output of this is going to be a soft

266
00:13:20,176 --> 00:13:22,967
max loss over the pixel values here.

267
00:13:22,967 --> 00:13:27,443
In this case we have a 0 to 255 and then we can train this

268
00:13:27,443 --> 00:13:31,193
by maximizing the likelihood of the training images.

269
00:13:31,193 --> 00:13:35,810
Right so we say that basically we want to take a training

270
00:13:35,810 --> 00:13:38,659
image we're going to do this generation process

271
00:13:38,659 --> 00:13:43,482
and at each pixel location we have the ground truth

272
00:13:43,482 --> 00:13:45,742
training data image value that we have here

273
00:13:45,742 --> 00:13:48,541
and this is a quick basically the label

274
00:13:48,541 --> 00:13:51,384
or the the the classification label that we want

275
00:13:51,384 --> 00:13:53,976
our pixel to be which of these 255 values

276
00:13:53,976 --> 00:13:56,723
and we can train this using a Softmax loss.

277
00:13:56,723 --> 00:13:59,155
Right and so basically the effect of doing this

278
00:13:59,155 --> 00:14:01,285
is that we're going to maximize the likelihood

279
00:14:01,285 --> 00:14:05,597
of our training data pixels being generated.

280
00:14:05,597 --> 00:14:06,981
Okay any questions about this?

281
00:14:06,981 --> 00:14:08,413
Yes.

282
00:14:08,413 --> 00:14:12,159
[student's words obscured due to lack of microphone]

283
00:14:12,159 --> 00:14:14,117
Yeah, so the question is, I thought we were talking

284
00:14:14,117 --> 00:14:16,606
about unsupervised learning, why do we have basically

285
00:14:16,606 --> 00:14:18,675
a classification label here?

286
00:14:18,675 --> 00:14:22,833
The reason is that this loss, this output that we have

287
00:14:22,833 --> 00:14:24,970
is the value of the input training data.

288
00:14:24,970 --> 00:14:26,983
So we have no external labels, right?

289
00:14:26,983 --> 00:14:31,645
We didn't go and have to manually collect any labels

290
00:14:31,645 --> 00:14:34,366
for this, we're just taking our input data

291
00:14:34,366 --> 00:14:38,533
and saying that this is what we used for the last function.

292
00:14:41,199 --> 00:14:45,366
[student's words obscured due to lack of microphone]

293
00:14:47,998 --> 00:14:50,746
The question is, is this like bag of words?

294
00:14:50,746 --> 00:14:53,109
I would say it's not really bag of words,

295
00:14:53,109 --> 00:14:55,784
it's more saying that we want where we're outputting

296
00:14:55,784 --> 00:14:58,724
a distribution over pixel values at each location

297
00:14:58,724 --> 00:15:01,466
of our image right, and what we want to do

298
00:15:01,466 --> 00:15:06,444
is we want to maximize the likelihood of our input,

299
00:15:06,444 --> 00:15:10,442
our training data being produced, being generated.

300
00:15:10,442 --> 00:15:13,761
Right so, in that sense, this is why it's using our input

301
00:15:13,761 --> 00:15:15,761
data to create our loss.

302
00:15:21,006 --> 00:15:24,904
So using pixelCNN training is faster than pixelRNN

303
00:15:24,904 --> 00:15:28,275
because here now right at every pixel location

304
00:15:28,275 --> 00:15:31,249
we want to maximize the value of our,

305
00:15:31,249 --> 00:15:34,301
we want to maximize the likelihood of our training data

306
00:15:34,301 --> 00:15:38,035
showing up and so we have all of these values already right,

307
00:15:38,035 --> 00:15:40,739
just from our training data and so we can do this much

308
00:15:40,739 --> 00:15:44,340
faster but a generation time for a test time we want to

309
00:15:44,340 --> 00:15:47,296
generate a completely new image right, just starting from

310
00:15:47,296 --> 00:15:50,545
the corner and we're not, we're not trying to do any type

311
00:15:50,545 --> 00:15:52,572
of learning so in that generation time we still

312
00:15:52,572 --> 00:15:56,609
have to generate each of these pixel locations

313
00:15:56,609 --> 00:15:59,197
before we can generate the next location.

314
00:15:59,197 --> 00:16:01,695
And so generation time here it still slow even though

315
00:16:01,695 --> 00:16:03,025
training time is faster.

316
00:16:03,025 --> 00:16:04,204
Question.

317
00:16:04,204 --> 00:16:08,365
[student's words obscured due to lack of microphone]

318
00:16:08,365 --> 00:16:10,517
So the question is, is this training a sensitive

319
00:16:10,517 --> 00:16:14,077
distribution to what you pick for the first pixel?

320
00:16:14,077 --> 00:16:17,376
Yeah, so it is dependent on what you have as the initial

321
00:16:17,376 --> 00:16:20,041
pixel distribution and then everything is conditioned

322
00:16:20,041 --> 00:16:21,208
based on that.

323
00:16:23,203 --> 00:16:26,667
So again, how do you pick this distribution?

324
00:16:26,667 --> 00:16:29,428
So at training time you have these distributions

325
00:16:29,428 --> 00:16:32,171
from your training data and then at generation time

326
00:16:32,171 --> 00:16:35,305
you can just initialize this with either uniform

327
00:16:35,305 --> 00:16:38,368
or from your training data, however you want.

328
00:16:38,368 --> 00:16:40,612
And then once you have that everything else is conditioned

329
00:16:40,612 --> 00:16:42,553
based on that.

330
00:16:42,553 --> 00:16:43,912
Question.

331
00:16:43,912 --> 00:16:48,079
[student's words obscured due to lack of microphone]

332
00:17:07,415 --> 00:17:09,761
Yeah so the question is is there a way that we define

333
00:17:09,761 --> 00:17:12,469
this in this chain rule fashion instead of predicting

334
00:17:12,469 --> 00:17:14,146
all the pixels at one time?

335
00:17:14,146 --> 00:17:17,884
And so we'll see, we'll see models later that do do this,

336
00:17:17,884 --> 00:17:20,164
but what the chain rule allows us to do is it allows us

337
00:17:20,164 --> 00:17:23,701
to find this very tractable density that we can then

338
00:17:23,701 --> 00:17:27,868
basically optimize and do, directly optimizes likelihood

339
00:17:31,864 --> 00:17:34,982
Okay so these are some examples of generations

340
00:17:34,982 --> 00:17:39,606
from this model and so here on the left you can see

341
00:17:39,606 --> 00:17:42,742
generations where the training data is CIFAR-10,

342
00:17:42,742 --> 00:17:43,995
CIFAR-10 dataset.

343
00:17:43,995 --> 00:17:46,115
And so you can see that in general they are starting

344
00:17:46,115 --> 00:17:48,846
to capture statistics of natural images.

345
00:17:48,846 --> 00:17:51,931
You can see general types of blobs

346
00:17:51,931 --> 00:17:55,879
and kind of things that look like parts of natural images

347
00:17:55,879 --> 00:17:56,848
coming out.

348
00:17:56,848 --> 00:17:59,647
On the right here it's ImageNet, we can again see samples

349
00:17:59,647 --> 00:18:00,730
from here and

350
00:18:03,022 --> 00:18:05,060
these are starting to look like natural images

351
00:18:05,060 --> 00:18:09,966
but they're still not, there's still room for improvement.

352
00:18:09,966 --> 00:18:12,634
You can still see that there are differences obviously

353
00:18:12,634 --> 00:18:15,226
with regional training images and some of the semantics

354
00:18:15,226 --> 00:18:17,059
are not clear in here.

355
00:18:19,371 --> 00:18:23,508
So, to summarize this, pixelRNNs and CNNs allow you

356
00:18:23,508 --> 00:18:27,020
to explicitly compute likelihood P of X.

357
00:18:27,020 --> 00:18:29,297
It's an explicit density that we can optimize.

358
00:18:29,297 --> 00:18:31,585
And being able to do this also has another benefit

359
00:18:31,585 --> 00:18:34,043
of giving a good evaluation metric.

360
00:18:34,043 --> 00:18:36,934
You know you can kind of measure how good your samples are

361
00:18:36,934 --> 00:18:40,958
by this likelihood of the data that you can compute.

362
00:18:40,958 --> 00:18:44,009
And it's able to produce pretty good samples

363
00:18:44,009 --> 00:18:47,043
but it's still an active area of research

364
00:18:47,043 --> 00:18:50,401
and the main disadvantage of these methods is that

365
00:18:50,401 --> 00:18:53,760
the generation is sequential and so it can be pretty slow.

366
00:18:53,760 --> 00:18:56,534
And these kinds of methods have also been used

367
00:18:56,534 --> 00:18:59,324
for generating audio for example.

368
00:18:59,324 --> 00:19:02,724
And you can look online for some pretty interesting examples

369
00:19:02,724 --> 00:19:05,460
of this, but again the drawback is that it takes a long time

370
00:19:05,460 --> 00:19:08,170
to generate these samples.

371
00:19:08,170 --> 00:19:11,856
And so there's a lot of work, has been work since then

372
00:19:11,856 --> 00:19:14,565
on still on improving pixelCNN performance

373
00:19:14,565 --> 00:19:17,964
And so all kinds of different you know architecture changes

374
00:19:17,964 --> 00:19:20,641
add the loss function formulating this differently

375
00:19:20,641 --> 00:19:22,346
on different types of training tricks

376
00:19:22,346 --> 00:19:25,914
And so if you're interested in learning more about this

377
00:19:25,914 --> 00:19:29,495
you can look at some of these papers on PixelCNN

378
00:19:29,495 --> 00:19:33,115
and then other pixelCNN plus plus better improved version

379
00:19:33,115 --> 00:19:35,115
that came out this year.

380
00:19:37,455 --> 00:19:39,748
Okay so now we're going to talk about another type

381
00:19:39,748 --> 00:19:44,321
of generative models call variational autoencoders.

382
00:19:44,321 --> 00:19:48,263
And so far we saw that pixelCNNs defined a tractable

383
00:19:48,263 --> 00:19:52,204
density function, right, using this this definition

384
00:19:52,204 --> 00:19:55,365
and based on that we can optimize directly optimize

385
00:19:55,365 --> 00:19:58,365
the likelihood of the training data.

386
00:19:59,419 --> 00:20:02,409
So with variational autoencoders now we're going to define

387
00:20:02,409 --> 00:20:04,195
an intractable density function.

388
00:20:04,195 --> 00:20:06,833
We're now going to model this with an additional latent

389
00:20:06,833 --> 00:20:09,492
variable Z and we'll talk in more detail

390
00:20:09,492 --> 00:20:10,769
about how this looks.

391
00:20:10,769 --> 00:20:14,936
And so our data likelihood P of X is now basically

392
00:20:16,257 --> 00:20:17,886
has to be this integral right,

393
00:20:17,886 --> 00:20:21,422
taking the expectation over all possible values of Z.

394
00:20:21,422 --> 00:20:24,016
And so this now is going to be a problem.

395
00:20:24,016 --> 00:20:26,909
We'll see that we cannot optimize this directly.

396
00:20:26,909 --> 00:20:29,349
And so instead what we have to do is we have to derive

397
00:20:29,349 --> 00:20:33,706
and optimize a lower bound on the likelihood instead.

398
00:20:33,706 --> 00:20:34,956
Yeah, question.

399
00:20:35,864 --> 00:20:37,592
So the question is is what is Z?

400
00:20:37,592 --> 00:20:41,195
Z is a latent variable and I'll go through this

401
00:20:41,195 --> 00:20:42,862
in much more detail.

402
00:20:44,479 --> 00:20:48,538
So let's talk about some background first.

403
00:20:48,538 --> 00:20:52,071
Variational autoencoders are related to a type of

404
00:20:52,071 --> 00:20:54,733
unsupervised learning model called autoencoders.

405
00:20:54,733 --> 00:20:58,267
And so we'll talk little bit more first about autoencoders

406
00:20:58,267 --> 00:21:00,965
and what they are and then I'll explain how variational

407
00:21:00,965 --> 00:21:04,332
autoencoders are related and build off of this

408
00:21:04,332 --> 00:21:05,851
and allow you to generate data.

409
00:21:05,851 --> 00:21:09,168
So with autoencoders we don't use this to generate data,

410
00:21:09,168 --> 00:21:12,132
but it's an unsupervised approach for learning a lower

411
00:21:12,132 --> 00:21:13,769
dimensional feature representation

412
00:21:13,769 --> 00:21:15,719
from unlabeled training data.

413
00:21:15,719 --> 00:21:18,399
All right so in this case we have our input data X

414
00:21:18,399 --> 00:21:20,300
and then we're going to want to learn some features

415
00:21:20,300 --> 00:21:21,550
that we call Z.

416
00:21:22,541 --> 00:21:25,708
And then we'll have an encoder that's going to be a mapping,

417
00:21:25,708 --> 00:21:28,188
a function mapping from this input data

418
00:21:28,188 --> 00:21:29,605
to our feature Z.

419
00:21:30,911 --> 00:21:33,905
And this encoder can take many different forms right,

420
00:21:33,905 --> 00:21:37,070
they would generally use neural networks so originally

421
00:21:37,070 --> 00:21:38,981
these models have been around, autoencoders have been

422
00:21:38,981 --> 00:21:41,239
around for a long time.

423
00:21:41,239 --> 00:21:45,803
So in the 2000s we used linear layers of non-linearities,

424
00:21:45,803 --> 00:21:49,650
then later on we had fully connected deeper networks

425
00:21:49,650 --> 00:21:53,556
and then after that we moved on to using CNNs for these

426
00:21:53,556 --> 00:21:54,389
encoders.

427
00:21:55,385 --> 00:21:59,995
So we take our input data X and then we map this

428
00:21:59,995 --> 00:22:01,351
to some feature Z.

429
00:22:01,351 --> 00:22:05,249
And Z we usually have as, we usually specify this

430
00:22:05,249 --> 00:22:09,138
to be smaller than X and we perform basically dimensionality

431
00:22:09,138 --> 00:22:11,817
reduction because of that.

432
00:22:11,817 --> 00:22:16,189
So the question who has an idea of why do we want to do

433
00:22:16,189 --> 00:22:17,729
dimensionality reduction here?

434
00:22:17,729 --> 00:22:20,896
Why do we want Z to be smaller than X?

435
00:22:22,114 --> 00:22:23,415
Yeah.

436
00:22:23,415 --> 00:22:25,497
[student's words obscured due to lack of microphone]

437
00:22:25,497 --> 00:22:28,074
So the answer I heard is Z should represent the most

438
00:22:28,074 --> 00:22:31,657
important features in X and that's correct.

439
00:22:32,634 --> 00:22:36,517
So we want Z to be able to learn features that can capture

440
00:22:36,517 --> 00:22:38,758
meaningful factors of variation in the data.

441
00:22:38,758 --> 00:22:41,758
Right this makes them good features.

442
00:22:42,833 --> 00:22:46,717
So how can we learn this feature representation?

443
00:22:46,717 --> 00:22:50,570
Well the way autoencoders do this is that we train

444
00:22:50,570 --> 00:22:54,513
the model such that the features can be used to reconstruct

445
00:22:54,513 --> 00:22:55,944
our original data.

446
00:22:55,944 --> 00:22:59,563
So what we want is we want to have input data that we use

447
00:22:59,563 --> 00:23:03,730
an encoder to map it to some lower dimensional features Z.

448
00:23:05,320 --> 00:23:06,926
This is the output of the encoder network,

449
00:23:06,926 --> 00:23:09,178
and we want to be able to take these features that were

450
00:23:09,178 --> 00:23:13,125
produced based on this input data and then use a decoder

451
00:23:13,125 --> 00:23:16,554
a second network and be able to output now something

452
00:23:16,554 --> 00:23:21,466
of the same size dimensionality as X and have it be similar

453
00:23:21,466 --> 00:23:24,032
to X right so we want to be able to reconstruct the original

454
00:23:24,032 --> 00:23:24,865
data.

455
00:23:26,387 --> 00:23:31,228
And again for the decoder we are basically using same types

456
00:23:31,228 --> 00:23:33,375
of networks as encoders so it's usually a little bit

457
00:23:33,375 --> 00:23:37,083
symmetric and now we can use CNN networks

458
00:23:37,083 --> 00:23:38,583
for most of these.

459
00:23:41,675 --> 00:23:44,145
Okay so the process is going to be we're going to take

460
00:23:44,145 --> 00:23:48,720
our input data right we pass it through our encoder first

461
00:23:48,720 --> 00:23:51,045
which is going to be something for example like a four layer

462
00:23:51,045 --> 00:23:53,996
convolutional network and then we're going to pass it,

463
00:23:53,996 --> 00:23:56,698
get these features and then we're going to pass it through

464
00:23:56,698 --> 00:24:00,323
a decoder which is a four layer for example upconvolutional

465
00:24:00,323 --> 00:24:03,314
network and then get a reconstructed data out at the end

466
00:24:03,314 --> 00:24:04,196
of this.

467
00:24:04,196 --> 00:24:07,447
Right in the reason why we have a convolutional network

468
00:24:07,447 --> 00:24:09,659
for the encoder and an upconvolutional network

469
00:24:09,659 --> 00:24:14,409
for the decoder is because at the encoder we're basically

470
00:24:14,409 --> 00:24:16,890
taking it from this high dimensional input to these lower

471
00:24:16,890 --> 00:24:20,394
dimensional features and now we want to go the other way

472
00:24:20,394 --> 00:24:22,810
go from our low dimensional features back out to our

473
00:24:22,810 --> 00:24:25,893
high dimensional reconstructed input.

474
00:24:28,906 --> 00:24:33,248
And so in order to get this effect that we said we wanted

475
00:24:33,248 --> 00:24:36,602
before of being able to reconstruct our input data

476
00:24:36,602 --> 00:24:39,071
we'll use something like an L2 loss function.

477
00:24:39,071 --> 00:24:42,220
Right that basically just says let me make my pixels

478
00:24:42,220 --> 00:24:44,764
of my input data to be the same as my,

479
00:24:44,764 --> 00:24:46,723
my pixels in my reconstructed data to be the same

480
00:24:46,723 --> 00:24:49,306
as the pixels of my input data.

481
00:24:51,078 --> 00:24:53,032
An important thing to notice here,

482
00:24:53,032 --> 00:24:55,147
this relates back to a question that we had earlier,

483
00:24:55,147 --> 00:24:58,599
is that even though we have this loss function here,

484
00:24:58,599 --> 00:25:01,431
there's no, there's no external labels that are being used

485
00:25:01,431 --> 00:25:02,515
in training this.

486
00:25:02,515 --> 00:25:06,337
All we have is our training data that we're going to use

487
00:25:06,337 --> 00:25:09,361
both to pass through the network as well as to compute

488
00:25:09,361 --> 00:25:10,861
our loss function.

489
00:25:13,346 --> 00:25:17,082
So once we have this after training this model

490
00:25:17,082 --> 00:25:19,021
what we can do is we can throw away this decoder.

491
00:25:19,021 --> 00:25:22,627
All this was used was too to be able to produce our

492
00:25:22,627 --> 00:25:24,937
reconstruction input and be able to compute our loss

493
00:25:24,937 --> 00:25:26,108
function.

494
00:25:26,108 --> 00:25:29,526
And we can use the encoder that we have which produces our

495
00:25:29,526 --> 00:25:32,960
feature mapping and we can use this to initialize

496
00:25:32,960 --> 00:25:34,819
a supervised model.

497
00:25:34,819 --> 00:25:37,647
Right and so for example we can now go from this input

498
00:25:37,647 --> 00:25:42,447
to our features and then have an additional classifier

499
00:25:42,447 --> 00:25:45,773
network on top of this that now we can use to output

500
00:25:45,773 --> 00:25:49,901
a class label for example for classification problem

501
00:25:49,901 --> 00:25:52,808
we can have external labels from here

502
00:25:52,808 --> 00:25:55,601
and use our standard loss functions like Softmax.

503
00:25:55,601 --> 00:25:58,157
And so the value of this is that we basically were able

504
00:25:58,157 --> 00:26:01,046
to use a lot of unlabeled training data to try and learn

505
00:26:01,046 --> 00:26:04,449
good general feature representations.

506
00:26:04,449 --> 00:26:08,107
Right, and now we can use this to initialize a supervised

507
00:26:08,107 --> 00:26:10,834
learning problem where sometimes we don't have so much data

508
00:26:10,834 --> 00:26:12,363
we only have small data.

509
00:26:12,363 --> 00:26:16,363
And we've seen in previous homeworks and classes

510
00:26:17,336 --> 00:26:19,697
that with small data it's hard to learn a model, right?

511
00:26:19,697 --> 00:26:22,563
You can have over fitting and all kinds of problems

512
00:26:22,563 --> 00:26:25,790
and so this allows you to initialize your model first

513
00:26:25,790 --> 00:26:27,540
with better features.

514
00:26:31,371 --> 00:26:34,489
Okay so we saw that autoencoders are able to reconstruct

515
00:26:34,489 --> 00:26:38,518
data and are able to, as a result, learn features

516
00:26:38,518 --> 00:26:41,243
to initialize, that we can use to initialize a supervised

517
00:26:41,243 --> 00:26:42,329
model.

518
00:26:42,329 --> 00:26:44,453
And we saw that these features that we learned

519
00:26:44,453 --> 00:26:47,474
have this intuition of being able to capture factors

520
00:26:47,474 --> 00:26:50,133
of variation in the training data.

521
00:26:50,133 --> 00:26:53,262
All right so based on this intuition of okay these,

522
00:26:53,262 --> 00:26:56,953
we can have this latent this vector Z which has

523
00:26:56,953 --> 00:26:58,941
factors of variation in our training data.

524
00:26:58,941 --> 00:27:02,290
Now a natural question is well can we use a similar type

525
00:27:02,290 --> 00:27:04,957
of setup to generate new images?

526
00:27:06,922 --> 00:27:09,502
And so now we will talk about variational autoencoders

527
00:27:09,502 --> 00:27:11,828
which is a probabillstic spin on autoencoders that will let

528
00:27:11,828 --> 00:27:15,987
us sample from the model in order to generate new data.

529
00:27:15,987 --> 00:27:19,404
Okay any questions on autoencoders first?

530
00:27:20,796 --> 00:27:22,828
Okay, so variational autoencoders.

531
00:27:22,828 --> 00:27:26,414
All right so here we assume that our training data

532
00:27:26,414 --> 00:27:28,914
that we have X I from one to N

533
00:27:30,255 --> 00:27:32,751
is generated from some underlying, unobserved

534
00:27:32,751 --> 00:27:34,812
latent representation Z.

535
00:27:34,812 --> 00:27:38,357
Right, so it's this intuition that Z is some vector

536
00:27:38,357 --> 00:27:41,891
right which element of Z is capturing how little

537
00:27:41,891 --> 00:27:45,319
or how much of some factor of variation that we have

538
00:27:45,319 --> 00:27:47,069
in our training data.

539
00:27:48,491 --> 00:27:51,118
Right so the intuition is, you know, maybe these could

540
00:27:51,118 --> 00:27:52,971
be something like different kinds of attributes.

541
00:27:52,971 --> 00:27:54,811
Let's say we're trying to generate faces,

542
00:27:54,811 --> 00:27:57,791
it could be how much of a smile is on the face,

543
00:27:57,791 --> 00:28:00,236
it could be position of the eyebrows hair

544
00:28:00,236 --> 00:28:02,608
orientation of the head.

545
00:28:02,608 --> 00:28:07,270
These are all possible types of latent factors

546
00:28:07,270 --> 00:28:08,772
that could be learned.

547
00:28:08,772 --> 00:28:11,282
Right, and so our generation process is that we're going to

548
00:28:11,282 --> 00:28:13,901
sample from a prior over Z.

549
00:28:13,901 --> 00:28:17,299
Right so for each of these attributes for example,

550
00:28:17,299 --> 00:28:19,202
you know, how much smile that there is,

551
00:28:19,202 --> 00:28:22,172
we can have a prior over what sort of distribution

552
00:28:22,172 --> 00:28:25,014
we think that there should be for this so,

553
00:28:25,014 --> 00:28:28,035
a gaussian is something that's a natural prior

554
00:28:28,035 --> 00:28:31,571
that we can use for each of these factors of Z

555
00:28:31,571 --> 00:28:34,345
and then we're going to generate our data X

556
00:28:34,345 --> 00:28:38,416
by sampling from a conditional, conditional distribution

557
00:28:38,416 --> 00:28:40,140
P of X given Z.

558
00:28:40,140 --> 00:28:43,019
So we sample Z first, we sample a value for each of these

559
00:28:43,019 --> 00:28:46,112
latent factors and then we'll use that

560
00:28:46,112 --> 00:28:48,862
and sample our image X from here.

561
00:28:51,409 --> 00:28:54,665
And so the true parameters of this generation process

562
00:28:54,665 --> 00:28:57,667
are theta, theta star right?

563
00:28:57,667 --> 00:28:59,961
So we have the parameters of our prior

564
00:28:59,961 --> 00:29:03,158
and our conditional distributions

565
00:29:03,158 --> 00:29:06,102
and what we want to do is in order to have a generative

566
00:29:06,102 --> 00:29:07,560
model be able to generate new data

567
00:29:07,560 --> 00:29:11,727
we want to estimate these parameters of our true parameters

568
00:29:14,790 --> 00:29:16,694
Okay so let's first talk about how should we represent

569
00:29:16,694 --> 00:29:17,611
this model.

570
00:29:20,282 --> 00:29:22,252
All right, so if we're going to have a model for this generator

571
00:29:22,252 --> 00:29:25,021
process, well we've already said before that we can choose

572
00:29:25,021 --> 00:29:27,317
our prior P of Z to be something simple.

573
00:29:27,317 --> 00:29:28,919
Something like a Gaussian, right?

574
00:29:28,919 --> 00:29:30,880
And this is the reasonable thing to choose for

575
00:29:30,880 --> 00:29:32,713
for latent attributes.

576
00:29:35,696 --> 00:29:39,260
Now for our conditional distribution P of X given Z

577
00:29:39,260 --> 00:29:40,840
this is much more complex right,

578
00:29:40,840 --> 00:29:43,410
because we need to use this to generate an image

579
00:29:43,410 --> 00:29:46,918
and so for P of X given Z, well as we saw before,

580
00:29:46,918 --> 00:29:49,395
when we have some type of complex function that we want

581
00:29:49,395 --> 00:29:53,062
to represent we can represent this with a neural network.

582
00:29:53,062 --> 00:29:55,176
And so that's a natural choice for let's try and model

583
00:29:55,176 --> 00:29:58,259
P of X given Z with a neural network.

584
00:30:00,308 --> 00:30:02,345
And we're going to call this the decoder network.

585
00:30:02,345 --> 00:30:04,756
Right, so we're going to think about taking some latent

586
00:30:04,756 --> 00:30:08,327
representation and trying to decode this into the image

587
00:30:08,327 --> 00:30:10,167
that it's specifying.

588
00:30:10,167 --> 00:30:13,765
So now how can we train this model?

589
00:30:13,765 --> 00:30:15,699
Right, we want to be able to train this model so that we can

590
00:30:15,699 --> 00:30:19,419
learn an estimate of these parameters.

591
00:30:19,419 --> 00:30:21,985
So if we remember our strategy from training generative

592
00:30:21,985 --> 00:30:24,668
models, back from are fully visible belief networks,

593
00:30:24,668 --> 00:30:26,668
our pixelRNNs and CNNs,

594
00:30:28,577 --> 00:30:30,492
a straightforward natural strategy is to try

595
00:30:30,492 --> 00:30:33,809
and learn these model parameters in order to maximize

596
00:30:33,809 --> 00:30:35,498
the likelihood of the training data.

597
00:30:35,498 --> 00:30:36,850
Right, so we saw earlier that in this case,

598
00:30:36,850 --> 00:30:39,346
with our latent variable Z, we're going to have

599
00:30:39,346 --> 00:30:42,771
to write out P of X taking expectation over all possible

600
00:30:42,771 --> 00:30:45,311
values of Z which is continuous and so we get this

601
00:30:45,311 --> 00:30:46,886
expression here.

602
00:30:46,886 --> 00:30:49,884
Right so now we have it with this latent Z

603
00:30:49,884 --> 00:30:53,658
and now if we're going to, if you want to try and maximize

604
00:30:53,658 --> 00:30:55,759
its likelihood, well what's the problem?

605
00:30:55,759 --> 00:30:59,301
Can we just take this take gradients and maximize

606
00:30:59,301 --> 00:31:01,372
this likelihood?

607
00:31:01,372 --> 00:31:04,358
[student's words obscured due to lack of microphone]

608
00:31:04,358 --> 00:31:07,274
Right, so this integral is not going to be tractable,

609
00:31:07,274 --> 00:31:08,524
that's correct.

610
00:31:10,199 --> 00:31:12,547
So let's take a look at this in a little bit more detail.

611
00:31:12,547 --> 00:31:15,911
Right, so we have our data likelihood term here.

612
00:31:15,911 --> 00:31:18,772
And the first time is P of Z.

613
00:31:18,772 --> 00:31:20,921
And here we already said earlier, we can just choose this

614
00:31:20,921 --> 00:31:24,847
to be a simple Gaussian prior, so this is fine.

615
00:31:24,847 --> 00:31:26,532
P of X given Z, well we said we were going to

616
00:31:26,532 --> 00:31:29,031
specify a decoder neural network.

617
00:31:29,031 --> 00:31:32,774
So given any Z, we can get P of X given Z from here.

618
00:31:32,774 --> 00:31:35,721
It's the output of our neural network.

619
00:31:35,721 --> 00:31:38,147
But then what's the problem here?

620
00:31:38,147 --> 00:31:42,450
Okay this was supposed to be a different unhappy face

621
00:31:42,450 --> 00:31:44,495
but somehow I don't know what happened,

622
00:31:44,495 --> 00:31:45,518
in the process of translation,

623
00:31:45,518 --> 00:31:48,435
it turned into a crying black ghost

624
00:31:49,298 --> 00:31:53,465
but what this is symbolizing is that basically if we want

625
00:31:54,393 --> 00:31:55,855
to compute P of X given Z

626
00:31:55,855 --> 00:31:59,519
for every Z this is now intractable right,

627
00:31:59,519 --> 00:32:02,186
we cannot compute this integral.

628
00:32:04,794 --> 00:32:06,591
So data likelihood is intractable

629
00:32:06,591 --> 00:32:10,486
and it turns out that if we look at other terms

630
00:32:10,486 --> 00:32:12,901
in this model if we look at our posterior density,

631
00:32:12,901 --> 00:32:15,818
So P of our posterior of Z given X,

632
00:32:16,921 --> 00:32:19,639
then this is going to be P of X given Z

633
00:32:19,639 --> 00:32:23,712
times P of Z over P of X by Bayes' rule

634
00:32:23,712 --> 00:32:25,740
and this is also going to be intractable, right.

635
00:32:25,740 --> 00:32:28,230
We have P of X given Z is okay, P of Z is okay,

636
00:32:28,230 --> 00:32:31,476
but we have this P of X our likelihood

637
00:32:31,476 --> 00:32:35,143
which has the integral and it's intractable.

638
00:32:36,027 --> 00:32:37,993
So we can't directly optimizes this.

639
00:32:37,993 --> 00:32:40,493
but we'll see that a solution,

640
00:32:42,463 --> 00:32:45,230
a solution that will enable us to learn this model

641
00:32:45,230 --> 00:32:48,153
is if in addition to using a decoder network

642
00:32:48,153 --> 00:32:50,997
defining this neural network to model P of X given Z.

643
00:32:50,997 --> 00:32:54,824
If we now define an additional encoder network

644
00:32:54,824 --> 00:32:57,887
Q of Z given X we're going to call this an encoder

645
00:32:57,887 --> 00:33:01,776
because we want to turn our input X into,

646
00:33:01,776 --> 00:33:04,414
get the likelihood of Z given X,

647
00:33:04,414 --> 00:33:06,652
we're going to encode this into Z.

648
00:33:06,652 --> 00:33:08,746
And defined this network to approximate

649
00:33:08,746 --> 00:33:10,329
the P of Z given X.

650
00:33:12,388 --> 00:33:14,517
Right this was posterior density term now is also

651
00:33:14,517 --> 00:33:15,688
intractable.

652
00:33:15,688 --> 00:33:20,396
If we use this additional network to approximate this

653
00:33:20,396 --> 00:33:22,866
then we'll see that this will actually allow us to derive

654
00:33:22,866 --> 00:33:25,319
a lower bound on the data likelihood that is tractable

655
00:33:25,319 --> 00:33:27,486
and which we can optimize.

656
00:33:29,308 --> 00:33:31,229
Okay so