1
00:00:10,512 --> 00:00:12,418
- Good morning.

2
00:00:12,418 --> 00:00:15,376
So, it's 12:03 so, I want to get started.

3
00:00:15,376 --> 00:00:18,014
Welcome to Lecture 12, of CS-231N.

4
00:00:18,014 --> 00:00:20,180
Today we are going to talk about Visualizing and Understanding

5
00:00:20,180 --> 00:00:21,840
convolutional networks.

6
00:00:21,840 --> 00:00:23,490
This is always a super fun lecture to give

7
00:00:23,490 --> 00:00:25,270
because we get to look a lot of pretty pictures.

8
00:00:25,270 --> 00:00:28,375
So, it's, it's one of my favorites.

9
00:00:28,375 --> 00:00:30,354
As usual a couple administrative things.

10
00:00:30,354 --> 00:00:32,743
So, hopefully your projects are all going well,

11
00:00:32,743 --> 00:00:34,722
because as a reminder your milestones

12
00:00:34,722 --> 00:00:36,595
are due on Canvas tonight.

13
00:00:36,595 --> 00:00:38,057
It is Canvas, right?

14
00:00:38,057 --> 00:00:39,545
Okay, so want to double check, yeah.

15
00:00:39,545 --> 00:00:42,088
Due on Canvas tonight, we are working on

16
00:00:42,088 --> 00:00:43,590
furiously grading your midterms.

17
00:00:43,590 --> 00:00:46,520
So, we'll hope to have those midterms grades to you back

18
00:00:46,520 --> 00:00:49,537
by on grade scope this week.

19
00:00:49,537 --> 00:00:50,900
So, I know that was little confusion,

20
00:00:50,900 --> 00:00:53,263
you all got registration email's for grade scope

21
00:00:53,263 --> 00:00:54,988
probably in the last week.

22
00:00:54,988 --> 00:00:57,372
Something like that, we start couple of questions on piazo.

23
00:00:57,372 --> 00:00:59,530
So, we've decided to use grade scope to grade the midterms.

24
00:00:59,530 --> 00:01:02,973
So, don't be confused, if you get some emails about that.

25
00:01:02,973 --> 00:01:05,047
Another reminder is that assignment three

26
00:01:05,047 --> 00:01:07,412
was released last week on Friday.

27
00:01:07,412 --> 00:01:11,088
It will be due, a week from this Friday, on the 26th.

28
00:01:11,088 --> 00:01:12,595
This is, an assignment three,

29
00:01:12,595 --> 00:01:14,444
is almost entirely brand new this year.

30
00:01:14,444 --> 00:01:17,152
So, it we apologize for taking a little bit longer than

31
00:01:17,152 --> 00:01:18,847
expected to get it out.

32
00:01:18,847 --> 00:01:20,272
But I think it's super cool.

33
00:01:20,272 --> 00:01:22,644
A lot of that stuff, we'll talk about in today's lecture.

34
00:01:22,644 --> 00:01:25,283
You'll actually be implementing on your assignment.

35
00:01:25,283 --> 00:01:27,188
And for the assignment, you'll get the choice of either

36
00:01:27,188 --> 00:01:29,575
Pi torch or tensure flow.

37
00:01:29,575 --> 00:01:30,921
To work through these different examples.

38
00:01:30,921 --> 00:01:34,512
So, we hope that's really useful experience for you guys.

39
00:01:34,512 --> 00:01:35,822
We also saw a lot of activity

40
00:01:35,822 --> 00:01:37,273
on HyperQuest over the weekend.

41
00:01:37,273 --> 00:01:39,084
So that's, that's really awesome.

42
00:01:39,084 --> 00:01:40,549
The leader board went up yesterday.

43
00:01:40,549 --> 00:01:42,568
It seems like you guys are really trying to battle it out

44
00:01:42,568 --> 00:01:44,227
to show off your deep learning

45
00:01:44,227 --> 00:01:46,063
neural network training skills.

46
00:01:46,063 --> 00:01:47,402
So that's super cool.

47
00:01:47,402 --> 00:01:50,087
And we because due to the high interest

48
00:01:50,087 --> 00:01:52,811
in HyperQuest and due to the conflicts with the,

49
00:01:52,811 --> 00:01:55,118
with the Milestones submission time.

50
00:01:55,118 --> 00:01:56,808
We decided to extend the deadline

51
00:01:56,808 --> 00:01:58,591
for extra credit through Sunday.

52
00:01:58,591 --> 00:02:02,279
So, anyone who does at least 12 runs on HyperQuest

53
00:02:02,279 --> 00:02:04,773
by Sunday will get little bit of extra credit in the class.

54
00:02:04,773 --> 00:02:07,394
Also those of you who are, at the top of leader board

55
00:02:07,394 --> 00:02:09,176
doing really well, will get may be little bit

56
00:02:09,176 --> 00:02:11,200
extra, extra credit.

57
00:02:11,200 --> 00:02:13,081
So, I thanks for participating we got lot of

58
00:02:13,081 --> 00:02:15,935
interest and that was really cool.

59
00:02:15,935 --> 00:02:17,844
Final reminder is about the poster session.

60
00:02:17,844 --> 00:02:21,445
So, we have the poster session will be on June 6th.

61
00:02:21,445 --> 00:02:22,872
That date is finalized,

62
00:02:22,872 --> 00:02:24,940
I think that, I don't remember the exact time.

63
00:02:24,940 --> 00:02:25,932
But it is June 6th.

64
00:02:25,932 --> 00:02:27,141
So that, we have some questions

65
00:02:27,141 --> 00:02:29,310
about when exactly that poster session is

66
00:02:29,310 --> 00:02:30,297
for those of you who are traveling

67
00:02:30,297 --> 00:02:31,897
at the end of quarter or starting internships

68
00:02:31,897 --> 00:02:33,247
or something like that.

69
00:02:33,247 --> 00:02:35,497
So, it will be June 6th.

70
00:02:35,497 --> 00:02:37,210
Any questions on the admin notes.

71
00:02:39,241 --> 00:02:41,171
No, totally clear.

72
00:02:41,171 --> 00:02:42,578
So, last time we talked.

73
00:02:42,578 --> 00:02:44,254
So, last time we had a pretty

74
00:02:44,254 --> 00:02:46,259
jam packed lecture, when we talked about lot of different

75
00:02:46,259 --> 00:02:48,161
computer vision tasks, as a reminder.

76
00:02:48,161 --> 00:02:49,955
We talked about semantic segmentation

77
00:02:49,955 --> 00:02:52,035
which is this problem, where you want to sign labels

78
00:02:52,035 --> 00:02:54,318
to every pixel in the input image.

79
00:02:54,318 --> 00:02:56,131
But does not differentiate the

80
00:02:56,131 --> 00:02:58,225
object instances in those images.

81
00:02:58,225 --> 00:03:00,773
We talked about classification plus localization.

82
00:03:00,773 --> 00:03:02,558
Where in addition to a class label

83
00:03:02,558 --> 00:03:04,059
you also want to draw a box

84
00:03:04,059 --> 00:03:06,539
or perhaps several boxes in the image.

85
00:03:06,539 --> 00:03:08,041
Where the distinction here is that,

86
00:03:08,041 --> 00:03:10,130
in a classification plus localization setup.

87
00:03:10,130 --> 00:03:12,594
You have some fix number of objects that you are looking for

88
00:03:12,594 --> 00:03:14,424
So, we also saw that this type of paradigm

89
00:03:14,424 --> 00:03:16,785
can be applied to the things like pose recognition.

90
00:03:16,785 --> 00:03:18,836
Where you want to regress to different numbers of joints

91
00:03:18,836 --> 00:03:20,222
in the human body.

92
00:03:20,222 --> 00:03:22,235
We also talked about the object detection

93
00:03:22,235 --> 00:03:23,976
where you start with some fixed

94
00:03:23,976 --> 00:03:25,851
set of category labels that you are interested in.

95
00:03:25,851 --> 00:03:27,102
Like dogs and cats.

96
00:03:27,102 --> 00:03:29,460
And then the task is to draw a boxes around

97
00:03:29,460 --> 00:03:31,196
every instance of those objects

98
00:03:31,196 --> 00:03:32,769
that appear in the input image.

99
00:03:32,769 --> 00:03:35,303
And object detection is really distinct from

100
00:03:35,303 --> 00:03:37,063
classification plus localization

101
00:03:37,063 --> 00:03:38,783
because with object detection, we don't know

102
00:03:38,783 --> 00:03:40,629
ahead of time, how many object instances

103
00:03:40,629 --> 00:03:42,298
we're looking for in the image.

104
00:03:42,298 --> 00:03:44,272
And we saw that there's this whole family of methods

105
00:03:44,272 --> 00:03:48,100
based on RCNN, Fast RCNN and faster RCNN,

106
00:03:48,100 --> 00:03:49,916
as well as the single shot detection methods

107
00:03:49,916 --> 00:03:52,588
for addressing this problem of object detection.

108
00:03:52,588 --> 00:03:55,026
Then finally we talked pretty briefly about

109
00:03:55,026 --> 00:03:57,722
instance segmentation, which is kind of combining

110
00:03:57,722 --> 00:04:01,164
aspects of a semantic segmentation and object detection

111
00:04:01,164 --> 00:04:03,308
where the goal is to detect all the instances

112
00:04:03,308 --> 00:04:04,934
of the categories we care about,

113
00:04:04,934 --> 00:04:07,997
as well as label the pixels belonging to each instance.

114
00:04:07,997 --> 00:04:11,339
So, in this case, we detected two dogs and one cat

115
00:04:11,339 --> 00:04:13,093
and for each of those instances we wanted

116
00:04:13,093 --> 00:04:14,887
to label all the pixels.

117
00:04:14,887 --> 00:04:17,437
So, these are we kind of covered a lot last lecture

118
00:04:17,437 --> 00:04:19,509
but those are really interesting and exciting problems

119
00:04:19,509 --> 00:04:21,284
that you guys might consider to

120
00:04:21,284 --> 00:04:23,810
using in parts of your projects.

121
00:04:23,810 --> 00:04:25,645
But today we are going to shift gears a little bit

122
00:04:25,645 --> 00:04:27,081
and ask another question.

123
00:04:27,081 --> 00:04:28,702
Which is, what's really going on

124
00:04:28,702 --> 00:04:30,578
inside convolutional networks.

125
00:04:30,578 --> 00:04:32,445
We've seen by this point in the class

126
00:04:32,445 --> 00:04:34,120
how to train convolutional networks.

127
00:04:34,120 --> 00:04:35,916
How to stitch up different types of architectures

128
00:04:35,916 --> 00:04:37,503
to attack different problems.

129
00:04:37,503 --> 00:04:39,860
But one question that you might have had in your mind,

130
00:04:39,860 --> 00:04:42,653
is what exactly is going on inside these networks?

131
00:04:42,653 --> 00:04:44,081
How did they do the things that they do?

132
00:04:44,081 --> 00:04:46,444
What kinds of features are they looking for?

133
00:04:46,444 --> 00:04:48,612
And all this source of related questions.

134
00:04:48,612 --> 00:04:51,043
So, so far we've sort of seen

135
00:04:51,043 --> 00:04:53,399
ConvNets as a little bit of a black box.

136
00:04:53,399 --> 00:04:55,635
Where some input image of raw pixels

137
00:04:55,635 --> 00:04:57,100
is coming in on one side.

138
00:04:57,100 --> 00:04:58,816
It goes to the many layers of convulsion

139
00:04:58,816 --> 00:05:01,170
and pooling in different sorts of transformations.

140
00:05:01,170 --> 00:05:04,547
And on the outside, we end up with some set of class scores

141
00:05:04,547 --> 00:05:07,363
or some types of understandable interpretable output.

142
00:05:07,363 --> 00:05:09,865
Such as class scores or bounding box positions

143
00:05:09,865 --> 00:05:12,342
or labeled pixels or something like that.

144
00:05:12,342 --> 00:05:13,307
But the question is.

145
00:05:13,307 --> 00:05:15,933
What are all these other layers in the middle doing?

146
00:05:15,933 --> 00:05:17,685
What kinds of things in the input image

147
00:05:17,685 --> 00:05:18,567
are they looking for?

148
00:05:18,567 --> 00:05:20,857
And can we try again intuition for.

149
00:05:20,857 --> 00:05:22,023
How ConvNets are working?

150
00:05:22,023 --> 00:05:24,364
What types of things in the image they are looking for?

151
00:05:24,364 --> 00:05:25,867
And what kinds of techniques do we have

152
00:05:25,867 --> 00:05:29,327
for analyzing this internals of the network?

153
00:05:29,327 --> 00:05:32,667
So, one relatively simple thing is the first layer.

154
00:05:32,667 --> 00:05:34,522
So, we've seen, we've talked about this before.

155
00:05:34,522 --> 00:05:37,508
But recalled that, the first convolutional layer

156
00:05:37,508 --> 00:05:39,819
consists of a filters that,

157
00:05:39,819 --> 00:05:41,492
so, for example in AlexNet.

158
00:05:41,492 --> 00:05:43,262
The first convolutional layer consists

159
00:05:43,262 --> 00:05:45,193
of a number of convolutional filters.

160
00:05:45,193 --> 00:05:49,230
Each convolutional of filter has shape 3 by 11 by 11.

161
00:05:49,230 --> 00:05:51,228
And these convolutional filters gets slid

162
00:05:51,228 --> 00:05:52,268
over the input image.

163
00:05:52,268 --> 00:05:54,947
We take inner products between some chunk of the image.

164
00:05:54,947 --> 00:05:56,909
And the weights of the convolutional filter.

165
00:05:56,909 --> 00:05:58,689
And that gives us our output of the

166
00:05:58,689 --> 00:06:01,729
at, at after that first convolutional layer.

167
00:06:01,729 --> 00:06:05,074
So, in AlexNet then we have 64 of these filters.

168
00:06:05,074 --> 00:06:06,947
But now in the first layer because we are taking

169
00:06:06,947 --> 00:06:08,780
in a direct inner product between the weights

170
00:06:08,780 --> 00:06:10,175
of the convolutional layer

171
00:06:10,175 --> 00:06:11,682
and the pixels of the image.

172
00:06:11,682 --> 00:06:14,548
We can get some since for what these filters are looking for

173
00:06:14,548 --> 00:06:17,697
by simply visualizing the learned weights of these filters

174
00:06:17,697 --> 00:06:19,458
as images themselves.

175
00:06:19,458 --> 00:06:22,576
So, for each of those 11 by 11 by 3 filters

176
00:06:22,576 --> 00:06:25,027
in AlexNet, we can just visualize that filter

177
00:06:25,027 --> 00:06:28,461
as a little 11 by 11 image with a three channels

178
00:06:28,461 --> 00:06:30,201
give you the red, green and blue values.

179
00:06:30,201 --> 00:06:32,051
And then because there are 64 of these filters

180
00:06:32,051 --> 00:06:35,305
we just visualize 64 little 11 by 11 images.

181
00:06:35,305 --> 00:06:38,047
And we can repeat... So we have shown here at the.

182
00:06:38,047 --> 00:06:40,982
So, these are filters taken from the prechain models,

183
00:06:40,982 --> 00:06:42,509
in the pi torch model zoo.

184
00:06:42,509 --> 00:06:44,739
And we are looking at the convolutional filters.

185
00:06:44,739 --> 00:06:45,985
The weights of the convolutional filters.

186
00:06:45,985 --> 00:06:48,313
at the first layer of AlexNet, ResNet-18,

187
00:06:48,313 --> 00:06:51,065
ResNet-101 and DenseNet-121.

188
00:06:51,065 --> 00:06:53,753
And you can see, kind of what all these layers

189
00:06:53,753 --> 00:06:55,553
what this filters looking for.

190
00:06:55,553 --> 00:06:59,015
You see the lot of things looking for oriented edges.

191
00:06:59,015 --> 00:07:01,052
Likes bars of light and dark.

192
00:07:01,052 --> 00:07:04,487
At various angles, in various angles and various positions

193
00:07:04,487 --> 00:07:07,200
in the input, we can see opposing colors.

194
00:07:07,200 --> 00:07:09,475
Like this are green and pink.

195
00:07:09,475 --> 00:07:12,732
opposing colors or this orange and blue opposing colors.

196
00:07:12,732 --> 00:07:14,893
So, this, this kind of connects back to what we

197
00:07:14,893 --> 00:07:16,221
talked about with Hugh and Wiesel.

198
00:07:16,221 --> 00:07:17,907
All the way in the first lecture.

199
00:07:17,907 --> 00:07:19,716
That remember the human visual system

200
00:07:19,716 --> 00:07:22,271
is known to the detect things like oriented edges.

201
00:07:22,271 --> 00:07:24,978
At the very early layers of the human visual system.

202
00:07:24,978 --> 00:07:26,946
And it turns out of that these convolutional networks

203
00:07:26,946 --> 00:07:29,136
tend to do something, somewhat similar.

204
00:07:29,136 --> 00:07:31,566
At their first convolutional layers as well.

205
00:07:31,566 --> 00:07:33,153
And what's kind of interesting is that

206
00:07:33,153 --> 00:07:35,631
pretty much no matter what type of architecture you hook up

207
00:07:35,631 --> 00:07:37,920
or whatever type of training data you are train it on.

208
00:07:37,920 --> 00:07:40,594
You almost always get the first layers of your.

209
00:07:40,594 --> 00:07:42,736
The first convolutional weights of any pretty much

210
00:07:42,736 --> 00:07:44,990
any convolutional network looking at images.

211
00:07:44,990 --> 00:07:46,389
Ends up looking something like this

212
00:07:46,389 --> 00:07:48,676
with oriented edges and opposing colors.

213
00:07:48,676 --> 00:07:51,539
Looking at that input image.

214
00:07:51,539 --> 00:07:53,696
But this really only, sorry what was that question?

215
00:08:04,215 --> 00:08:06,118
Yes, these are showing the learned weights

216
00:08:06,118 --> 00:08:07,592
of the first convolutional layer.

217
00:08:15,766 --> 00:08:16,826
Oh, so that the question is.

218
00:08:16,826 --> 00:08:18,998
Why does visualizing the weights of the filters?

219
00:08:18,998 --> 00:08:21,318
Tell you what the filter is looking for.

220
00:08:21,318 --> 00:08:23,945
So this intuition comes from sort of template matching

221
00:08:23,945 --> 00:08:25,045
and inner products.

222
00:08:25,045 --> 00:08:28,389
That if you imagine you have some, some template vector.

223
00:08:28,389 --> 00:08:31,125
And then you imagine you compute a scaler output

224
00:08:31,125 --> 00:08:33,272
by taking inner product between your template vector

225
00:08:33,272 --> 00:08:35,044
and some arbitrary piece of data.

226
00:08:35,044 --> 00:08:38,321
Then, the input which maximizes that activation.

227
00:08:38,321 --> 00:08:40,289
Under a norm constraint on the input

228
00:08:40,289 --> 00:08:43,062
is exactly when those two vectors match up.

229
00:08:43,062 --> 00:08:45,564
So, in that since that, when, whenever you're taking

230
00:08:45,564 --> 00:08:48,066
inner products, the thing causes an inner product

231
00:08:48,066 --> 00:08:49,736
to excite maximally

232
00:08:49,736 --> 00:08:52,506
is a copy of the thing you are taking an inner product with.

233
00:08:52,506 --> 00:08:55,060
So, that, that's why we can actually visualize these weights

234
00:08:55,060 --> 00:08:56,323
and that, why that shows us,

235
00:08:56,323 --> 00:08:57,902
what this first layer is looking for.

236
00:09:06,008 --> 00:09:08,731
So, for these networks the first layers always

237
00:09:08,731 --> 00:09:10,052
was a convolutional layer.

238
00:09:10,052 --> 00:09:12,003
So, generally whenever you are looking at image.

239
00:09:12,003 --> 00:09:13,808
Whenever you are thinking about image data

240
00:09:13,808 --> 00:09:15,174
and training convolutional networks,

241
00:09:15,174 --> 00:09:16,525
you generally put a convolutional layer

242
00:09:16,525 --> 00:09:18,178
at the first, at the first stop.

243
00:09:28,086 --> 00:09:29,006
Yeah, so the question is,

244
00:09:29,006 --> 00:09:30,665
can we do this same type of procedure

245
00:09:30,665 --> 00:09:32,118
in the middle open network.

246
00:09:32,118 --> 00:09:33,202
That's actually the next slide.

247
00:09:33,202 --> 00:09:35,104
So, good anticipation.

248
00:09:35,104 --> 00:09:37,123
So, if we do, if we draw this exact same

249
00:09:37,123 --> 00:09:39,767
visualization for the intermediate convolutional layers.

250
00:09:39,767 --> 00:09:41,753
It's actually a lot less interpretable.

251
00:09:41,753 --> 00:09:45,081
So, this is, this is performing exact same visualization.

252
00:09:45,081 --> 00:09:49,278
So, remember for this using the tiny ConvNets demo network

253
00:09:49,278 --> 00:09:50,474
that's running on the course website

254
00:09:50,474 --> 00:09:51,890
whenever you go there.

255
00:09:51,890 --> 00:09:52,702
So, for that network,

256
00:09:52,702 --> 00:09:55,987
the first layer is 7 by 7 convulsion 16 filters.

257
00:09:55,987 --> 00:09:58,263
So, after the top visualizing the first layer weights

258
00:09:58,263 --> 00:10:00,842
for this network just like we saw in a previous slide.

259
00:10:00,842 --> 00:10:02,366
But now at the second layer weights.

260
00:10:02,366 --> 00:10:04,491
After we do a convulsion then there's some relu

261
00:10:04,491 --> 00:10:06,583
and some other non-linearity perhaps.

262
00:10:06,583 --> 00:10:08,185
But the second convolutional layer,

263
00:10:08,185 --> 00:10:10,629
now receives the 16 channel input.

264
00:10:10,629 --> 00:10:15,116
And does 7 by 7 convulsion with 20 convolutional filters.

265
00:10:15,116 --> 00:10:16,064
And we've actually,

266
00:10:16,064 --> 00:10:18,660
so the problem is that you can't really visualize

267
00:10:18,660 --> 00:10:20,495
these directly as images.

268
00:10:20,495 --> 00:10:23,846
So, you can try, so, here if you

269
00:10:23,846 --> 00:10:28,547
this 16 by, so the input is this has 16 dimensions in depth.

270
00:10:28,547 --> 00:10:30,286
And we have these convolutional filters,

271
00:10:30,286 --> 00:10:32,542
each convolutional filter is 7 by 7,

272
00:10:32,542 --> 00:10:34,388
and is extending along the full depth

273
00:10:34,388 --> 00:10:35,759
so has 16 elements.

274
00:10:35,759 --> 00:10:38,072
Then we've 20 such of these convolutional filters,

275
00:10:38,072 --> 00:10:40,924
that are producing the output planes of the next layer.

276
00:10:40,924 --> 00:10:44,035
But the problem here is that we can't, looking at the,

277
00:10:44,035 --> 00:10:45,128
looking directly at the weights

278
00:10:45,128 --> 00:10:47,498
of these filters, doesn't really tell us much.

279
00:10:47,498 --> 00:10:49,734
So, we, that's really done here is that,

280
00:10:49,734 --> 00:10:53,743
now for this single 16 by 7 by 7 convolutional filter.

281
00:10:53,743 --> 00:10:58,192
We can spread out those 167 by 7 planes of the filter

282
00:10:58,192 --> 00:11:01,782
into a 167 by 7 grayscale images.

283
00:11:01,782 --> 00:11:03,284
So, that's what we've done.

284
00:11:03,284 --> 00:11:07,095
Up here, which is these little tiny gray scale images here

285
00:11:07,095 --> 00:11:08,898
show us what is, what are the weights

286
00:11:08,898 --> 00:11:11,852
in one of the convolutional filters of the second layer.

287
00:11:11,852 --> 00:11:14,473
And now, because there are 20 outputs from this layer.

288
00:11:14,473 --> 00:11:17,534
Then this second convolutional layer, has 2o such of these

289
00:11:17,534 --> 00:11:21,046
16 by 16 or 16 by 7 by 7 filters.

290
00:11:21,046 --> 00:11:22,871
So if we visualize the weights

291
00:11:22,871 --> 00:11:24,307
of those convolutional filters

292
00:11:24,307 --> 00:11:26,709
as images, you can see that there are some

293
00:11:26,709 --> 00:11:28,638
kind of spacial structures here.

294
00:11:28,638 --> 00:11:30,897
But it doesn't really give you good intuition

295
00:11:30,897 --> 00:11:32,128
for what they are looking at.

296
00:11:32,128 --> 00:11:35,099
Because these filters are not looking, are not connected

297
00:11:35,099 --> 00:11:36,644
directly to the input image.

298
00:11:36,644 --> 00:11:39,493
Instead recall that the second layer convolutional filters

299
00:11:39,493 --> 00:11:41,851
are connected to the output of the first layer.

300
00:11:41,851 --> 00:11:44,189
So, this is giving visualization of,

301
00:11:44,189 --> 00:11:46,684
what type of activation pattern after the first

302
00:11:46,684 --> 00:11:49,331
convulsion, would cause the second layer convulsion

303
00:11:49,331 --> 00:11:50,646
to maximally activate.

304
00:11:50,646 --> 00:11:52,423
But, that's not very interpretable

305
00:11:52,423 --> 00:11:53,860
because we don't have a good sense

306
00:11:53,860 --> 00:11:55,966
for what those first layer convulsions look like

307
00:11:55,966 --> 00:11:58,490
in terms of image pixels.

308
00:11:58,490 --> 00:12:00,893
So we'll need to develop some slightly more fancy technique

309
00:12:00,893 --> 00:12:02,047
to get a sense for what is going on

310
00:12:02,047 --> 00:12:03,556
in the intermediate layers.

311
00:12:03,556 --> 00:12:04,819
Question in the back.

312
00:12:09,189 --> 00:12:10,489
Yeah. So the question is that

313
00:12:10,489 --> 00:12:13,456
for... all the visualization on this on the previous slide.

314
00:12:13,456 --> 00:12:16,552
We've had the scale the weights to the zero to 255 range.

315
00:12:16,552 --> 00:12:18,648
So in practice those weights could be unbounded.

316
00:12:18,648 --> 00:12:19,885
They could have any range.

317
00:12:19,885 --> 00:12:22,983
But to get nice visualizations we need to scale those.

318
00:12:22,983 --> 00:12:24,685
These visualizations also do not take

319
00:12:24,685 --> 00:12:26,409
in to account the bias is in these layers.

320
00:12:26,409 --> 00:12:28,162
So you should keep that in mind

321
00:12:28,162 --> 00:12:30,423
when and not take these HEPS visualizations

322
00:12:30,423 --> 00:12:31,892
to, to literally.

323
00:12:34,180 --> 00:12:35,237
Now at the last layer

324
00:12:35,237 --> 00:12:36,733
remember when we looking at the last layer

325
00:12:36,733 --> 00:12:38,391
of convolutional network.

326
00:12:38,391 --> 00:12:40,698
We have these maybe 1000 class scores

327
00:12:40,698 --> 00:12:42,891
that are telling us what are the predicted scores

328
00:12:42,891 --> 00:12:44,908
for each of the classes in our training data set

329
00:12:44,908 --> 00:12:46,676
and immediately before the last layer

330
00:12:46,676 --> 00:12:48,628
we often have some fully connected layer.

331
00:12:48,628 --> 00:12:49,962
In the case of Alex net

332
00:12:49,962 --> 00:12:53,039
we have some 4096- dimensional features representation

333
00:12:53,039 --> 00:12:55,516
of our image that then gets fed into that final

334
00:12:55,516 --> 00:12:58,328
our final layer to predict our final class scores.

335
00:12:58,328 --> 00:13:00,606
And one another, another kind of route

336
00:13:00,606 --> 00:13:02,787
for tackling the problem of visual, visualizing

337
00:13:02,787 --> 00:13:04,263
and understanding ConvNets

338
00:13:04,263 --> 00:13:06,520
is to try to understand what's happening at the last layer

339
00:13:06,520 --> 00:13:07,967
of a convolutional network.

340
00:13:07,967 --> 00:13:09,022
So what we can do

341
00:13:09,022 --> 00:13:11,230
is how to take some, some data set of images

342
00:13:11,230 --> 00:13:13,110
run a bunch of, run a bunch of images

343
00:13:13,110 --> 00:13:14,815
through our trained convolutional network

344
00:13:14,815 --> 00:13:17,174
and recorded that 4096 dimensional vector

345
00:13:17,174 --> 00:13:18,687
for each of those images.

346
00:13:18,687 --> 00:13:20,722
And now go through and try to figure out

347
00:13:20,722 --> 00:13:23,219
and visualize that last layer, that last hidden layer

348
00:13:23,219 --> 00:13:26,075
rather than those rather than the first convolutional layer.

349
00:13:26,075 --> 00:13:27,804
So, one thing you might imagine is,

350
00:13:27,804 --> 00:13:29,791
is trying a nearest neighbor approach.

351
00:13:29,791 --> 00:13:31,559
So, remember, way back in the second lecture

352
00:13:31,559 --> 00:13:33,162
we saw this graphic on the left

353
00:13:33,162 --> 00:13:36,045
where we, where we had a nearest neighbor classifier.

354
00:13:36,045 --> 00:13:37,967
Where we were looking at nearest neighbors in pixels

355
00:13:37,967 --> 00:13:40,303
space between CIFAR 10 images.

356
00:13:40,303 --> 00:13:41,996
And then when you look at nearest neighbors

357
00:13:41,996 --> 00:13:44,765
in pixel space between CIFAR 10 images

358
00:13:44,765 --> 00:13:46,500
you see that you pull up images

359
00:13:46,500 --> 00:13:48,660
that looks quite similar to the query image.

360
00:13:48,660 --> 00:13:50,777
So again on the left column here is some CIFAR 10 image

361
00:13:50,777 --> 00:13:52,350
from the CIFAR 10 data set

362
00:13:52,350 --> 00:13:54,987
and then these, these next five columns

363
00:13:54,987 --> 00:13:57,239
are showing the nearest neighbors in pixel space

364
00:13:57,239 --> 00:13:58,917
to those test set images.

365
00:13:58,917 --> 00:14:00,185
And so for example

366
00:14:00,185 --> 00:14:02,446
this white dog that you see here,

367
00:14:02,446 --> 00:14:04,523
it's nearest neighbors are in pixel space

368
00:14:04,523 --> 00:14:06,328
are these kinds of white blobby things

369
00:14:06,328 --> 00:14:08,321
that may, may or may not be dogs,

370
00:14:08,321 --> 00:14:09,885
but at least the raw pixels

371
00:14:09,885 --> 00:14:11,643
of the image are quite similar.

372
00:14:11,643 --> 00:14:14,268
So now we can do the same type of visualization

373
00:14:14,268 --> 00:14:16,937
computing and visualizing these nearest neighbor images.

374
00:14:16,937 --> 00:14:17,963
But rather than computing

375
00:14:17,963 --> 00:14:19,952
the nearest neighbors in pixel space,

376
00:14:19,952 --> 00:14:21,735
instead we can compute nearest neighbors

377
00:14:21,735 --> 00:14:24,507
in that 4096 dimensional feature space.

378
00:14:24,507 --> 00:14:27,107
Which is computed by the convolutional network.

379
00:14:27,107 --> 00:14:28,351
So here on the right

380
00:14:28,351 --> 00:14:29,987
we see some examples.

381
00:14:29,987 --> 00:14:32,069
So this, this first column shows us

382
00:14:32,069 --> 00:14:34,924
some examples of images from the test set

383
00:14:34,924 --> 00:14:38,338
of image that... Of the image net classification data set

384
00:14:38,338 --> 00:14:41,253
and now the, these subsequent columns show us

385
00:14:41,253 --> 00:14:43,614
nearest neighbors to those test set images

386
00:14:43,614 --> 00:14:46,863
in the 4096, in the 4096th dimensional features space

387
00:14:46,863 --> 00:14:48,515
computed by Alex net.

388
00:14:48,515 --> 00:14:51,010
And you can see here that this is quite different

389
00:14:51,010 --> 00:14:52,941
from the pixel space nearest neighbors,

390
00:14:52,941 --> 00:14:55,086
because the pixels are often quite different.

391
00:14:55,086 --> 00:14:57,111
between the image in it's nearest neighbors

392
00:14:57,111 --> 00:14:58,375
and feature space.

393
00:14:58,375 --> 00:15:00,751
However, the semantic content of those images

394
00:15:00,751 --> 00:15:03,031
tends to be similar in this feature space.

395
00:15:03,031 --> 00:15:05,612
So for example, if you look at this second layer

396
00:15:05,612 --> 00:15:07,243
the query image is this elephant

397
00:15:07,243 --> 00:15:08,523
standing on the left side of the image

398
00:15:08,523 --> 00:15:10,484
with a screen grass behind him.

399
00:15:10,484 --> 00:15:12,521
and now one of these, one of these...

400
00:15:12,521 --> 00:15:14,542
it's third nearest neighbor in the tough set

401
00:15:14,542 --> 00:15:15,818
is actually an elephant standing

402
00:15:15,818 --> 00:15:17,307
on the right side of the image.

403
00:15:17,307 --> 00:15:18,719
So this is really interesting.

404
00:15:18,719 --> 00:15:21,308
Because between this elephant standing on the left

405
00:15:21,308 --> 00:15:23,640
and this element stand, elephant standing on the right

406
00:15:23,640 --> 00:15:25,382
the pixels between those two images

407
00:15:25,382 --> 00:15:26,942
are almost entirely different.

408
00:15:26,942 --> 00:15:28,534
However, in the feature space

409
00:15:28,534 --> 00:15:29,967
which is learned by the network

410
00:15:29,967 --> 00:15:32,554
those two images and that being very close to each other.

411
00:15:32,554 --> 00:15:35,138
Which means that somehow this, this last their features

412
00:15:35,138 --> 00:15:37,975
is capturing some of those semantic content of these images.

413
00:15:37,975 --> 00:15:39,953
That's really cool and really exciting

414
00:15:39,953 --> 00:15:40,955
and, and in general looking

415
00:15:40,955 --> 00:15:42,822
at these kind of nearest neighbor visualizations

416
00:15:42,822 --> 00:15:44,625
is really quick and easy way to visualize

417
00:15:44,625 --> 00:15:46,192
something about what's going on here.

418
00:16:02,617 --> 00:16:04,630
Yes. So the question is that

419
00:16:04,630 --> 00:16:07,598
through the... the standard supervised learning procedure

420
00:16:07,598 --> 00:16:09,902
for classific training, classification network

421
00:16:09,902 --> 00:16:11,046
There's nothing in the loss

422
00:16:11,046 --> 00:16:13,942
encouraging these features to be close together.

423
00:16:13,942 --> 00:16:15,072
So that, that's true.

424
00:16:15,072 --> 00:16:16,477
It just kind of a happy accident

425
00:16:16,477 --> 00:16:18,033
that they end up being close to each other.

426
00:16:18,033 --> 00:16:19,800
Because we didn't tell the network during training

427
00:16:19,800 --> 00:16:21,476
these features should be close.

428
00:16:21,476 --> 00:16:24,364
However there are sometimes people do train networks

429
00:16:24,364 --> 00:16:27,499
using things called either contrastive loss

430
00:16:27,499 --> 00:16:28,746
or a triplet loss.

431
00:16:28,746 --> 00:16:31,395
Which actually explicitly make...

432
00:16:31,395 --> 00:16:33,287
assumptions and constraints on the network

433
00:16:33,287 --> 00:16:34,792
such that those last their features

434
00:16:34,792 --> 00:16:37,253
end up having some metric space interpretation.

435
00:16:37,253 --> 00:16:39,907
But Alex net at least was not trained specifically for that.

436
00:16:44,931 --> 00:16:46,060
The question is, what is the nearest...

437
00:16:46,060 --> 00:16:47,434
What is this nearest neighbor thing

438
00:16:47,434 --> 00:16:48,875
have to do at the last layer?

439
00:16:48,875 --> 00:16:50,120
So we're taking this image

440
00:16:50,120 --> 00:16:51,432
we're running it through the network

441
00:16:51,432 --> 00:16:53,560
and then the, the second to last

442
00:16:53,560 --> 00:16:55,888
like the last hidden layer of the network

443
00:16:55,888 --> 00:16:57,670
is of 4096th dimensional vector.

444
00:16:57,670 --> 00:16:58,791
Because there's this, this is...

445
00:16:58,791 --> 00:17:00,681
This is there, there are these fully connected layers

446
00:17:00,681 --> 00:17:01,797
at the end of the network.

447
00:17:01,797 --> 00:17:03,096
So we are doing is...

448
00:17:03,096 --> 00:17:05,620
We're writing down that 4096th dimensional vector

449
00:17:05,620 --> 00:17:06,894
for each of the images

450
00:17:06,894 --> 00:17:08,382
and then we are computing nearest neighbors

451
00:17:08,382 --> 00:17:10,922
according to that 4096th dimensional vector.

452
00:17:10,922 --> 00:17:12,966
Which is computed by, computed by the network.

453
00:17:17,012 --> 00:17:19,171
Maybe, maybe we can chat offline.

454
00:17:19,171 --> 00:17:21,048
So another, another, another

455
00:17:21,048 --> 00:17:22,398
another angle that we might have

456
00:17:22,398 --> 00:17:24,989
for visualizing what's going on in this last layer

457
00:17:24,989 --> 00:17:28,435
is by some concept of dimensionality reduction.

458
00:17:28,435 --> 00:17:31,220
So those of you who have taken CS229 for example

459
00:17:31,220 --> 00:17:33,220
you've seen something like PCA.

460
00:17:33,220 --> 00:17:35,561
Which let's you take some high dimensional representation

461
00:17:35,561 --> 00:17:37,710
like these 4096th dimensional features

462
00:17:37,710 --> 00:17:39,841
and then compress it down to two-dimensions.

463
00:17:39,841 --> 00:17:43,183
So then you can visualize that feature space more directly.

464
00:17:43,183 --> 00:17:45,631
So, Principle Component Analysis or PCA

465
00:17:45,631 --> 00:17:47,203
is kind of one way to do that.

466
00:17:47,203 --> 00:17:50,227
But there's real another really powerful algorithm

467
00:17:50,227 --> 00:17:51,321
called t-SNE.

468
00:17:51,321 --> 00:17:54,656
Standing for t-distributed stochastic neighbor embeddings.

469
00:17:54,656 --> 00:17:57,042
Which is slightly more powerful method.

470
00:17:57,042 --> 00:17:59,746
Which is a non-linear dimensionality reduction method

471
00:17:59,746 --> 00:18:03,137
that people in deep often use for visualizing features.

472
00:18:03,137 --> 00:18:07,264
So here as an, just an example of what t-SNE can do.

473
00:18:07,264 --> 00:18:09,363
This visualization here is, is showing

474
00:18:09,363 --> 00:18:13,231
a t-SNE dimensionality reduction on the emnest data set.

475
00:18:13,231 --> 00:18:15,118
So, emnest remember is this date set

476
00:18:15,118 --> 00:18:17,521
of hand written digits between zero and nine.

477
00:18:17,521 --> 00:18:19,484
Each image is a gray scale image

478
00:18:19,484 --> 00:18:22,226
20... 28 by 28 gray scale image

479
00:18:22,226 --> 00:18:23,954
and now we're... So that

480
00:18:23,954 --> 00:18:25,268
Now we've, we've used t-SNE

481
00:18:25,268 --> 00:18:28,336
to take that 28 times 28 dimensional features space

482
00:18:28,336 --> 00:18:29,901
of the raw pixels for m-nest

483
00:18:29,901 --> 00:18:32,020
and now compress it down to two- dimensions

484
00:18:32,020 --> 00:18:34,275
ans then visualize each of those m-nest digits

485
00:18:34,275 --> 00:18:37,096
in this compress two-dimensional representation

486
00:18:37,096 --> 00:18:38,711
and when you do, when you run t-SNE

487
00:18:38,711 --> 00:18:40,596
on the raw pixels and m-nest

488
00:18:40,596 --> 00:18:42,653
You can see these natural clusters appearing.

489
00:18:42,653 --> 00:18:45,373
Which corresponds to the, the digits of these m-nest

490
00:18:45,373 --> 00:18:47,532
of, of these m-nest data set.

491
00:18:47,532 --> 00:18:49,764
So now we can do a similar type of visualization.

492
00:18:49,764 --> 00:18:52,994
Where we apply this t-SNE dimensionality reduction technique

493
00:18:52,994 --> 00:18:55,007
to the features from the last layer

494
00:18:55,007 --> 00:18:57,348
of our trained image net classifier.

495
00:18:57,348 --> 00:18:59,544
So...To be a little bit more concrete here

496
00:18:59,544 --> 00:19:00,369
what we've done

497
00:19:00,369 --> 00:19:03,087
is that we take, a large set of images

498
00:19:03,087 --> 00:19:05,073
we run them off convolutional network.

499
00:19:05,073 --> 00:19:08,187
We record that final 4096th dimensional feature vector

500
00:19:08,187 --> 00:19:10,865
for, from the last layer of each of those images.

501
00:19:10,865 --> 00:19:12,437
Which gives us large collection

502
00:19:12,437 --> 00:19:14,756
of 4096th dimensional vectors.

503
00:19:14,756 --> 00:19:17,509
Now we apply t-SNE dimensionality reduction

504
00:19:17,509 --> 00:19:20,848
to compute, sort of compress that 4096the dimensional

505
00:19:20,848 --> 00:19:24,277
features space down into a two-dimensional feature space

506
00:19:24,277 --> 00:19:27,454
and now we, layout a grid in that

507
00:19:27,454 --> 00:19:29,778
compressed two-dimensional feature space

508
00:19:29,778 --> 00:19:32,391
and visualize what types of images appear

509
00:19:32,391 --> 00:19:34,115
at each location in the grid

510
00:19:34,115 --> 00:19:36,415
in this two-dimensional feature space.

511
00:19:36,415 --> 00:19:39,454
So by doing this you get some very close rough sense

512
00:19:39,454 --> 00:19:41,196
of what the geometry

513
00:19:41,196 --> 00:19:43,417
of this learned feature space looks like.

514
00:19:43,417 --> 00:19:45,426
So these images are little bit hard to see.

515
00:19:45,426 --> 00:19:46,504
So I'd encourage you to check out

516
00:19:46,504 --> 00:19:48,620
the high resolution versions online.

517
00:19:48,620 --> 00:19:51,409
But at least maybe on the left you can see that

518
00:19:51,409 --> 00:19:53,398
there's sort of one cluster in the bottom here

519
00:19:53,398 --> 00:19:56,451
of, of green things, is a different kind of flowers

520
00:19:56,451 --> 00:19:57,855
and there's other types of clusters

521
00:19:57,855 --> 00:19:59,374
for different types of dog breeds

522
00:19:59,374 --> 00:20:01,800
and another types of animals and, and locations.

523
00:20:01,800 --> 00:20:05,020
So there's sort of discontinuous semantic notion

524
00:20:05,020 --> 00:20:06,192
in this feature space.

525
00:20:06,192 --> 00:20:08,123
Which we can explore by looking through

526
00:20:08,123 --> 00:20:10,080
this t-SNE dimensionality reduction

527
00:20:10,080 --> 00:20:11,597
version of the, of the features.

528
00:20:11,597 --> 00:20:12,604
Is there question?

529
00:20:23,716 --> 00:20:25,849
Yeah. So the basic idea is that we're

530
00:20:25,849 --> 00:20:26,887
we, we have an image

531
00:20:26,887 --> 00:20:28,599
so now we end up with three different pieces

532
00:20:28,599 --> 00:20:29,793
of information about each image.

533
00:20:29,793 --> 00:20:31,308
We have the pixels of the image.

534
00:20:31,308 --> 00:20:33,353
We have the 4096th dimensional vector.

535
00:20:33,353 --> 00:20:36,312
Then we use t-SNE to convert the 4096th dimensional vector

536
00:20:36,312 --> 00:20:38,109
into a two-dimensional coordinate

537
00:20:38,109 --> 00:20:40,430
and then we take the original pixels of the image

538
00:20:40,430 --> 00:20:42,511
and place that at the two-dimensional coordinate

539
00:20:42,511 --> 00:20:44,532
corresponding to the dimensionality reduced version

540
00:20:44,532 --> 00:20:48,090
of the 4096th dimensional feature.

541
00:20:48,090 --> 00:20:49,547
Yeah, little bit involved here.

542
00:20:49,547 --> 00:20:50,348
Question in the front.

543
00:20:55,864 --> 00:20:56,459
The question is

544
00:20:56,459 --> 00:20:59,255
Roughly how much variants do these two-dimension explain?

545
00:20:59,255 --> 00:21:00,993
Well, I'm not sure of the exact number

546
00:21:00,993 --> 00:21:02,185
and I get little bit muddy

547
00:21:02,185 --> 00:21:03,550
when you're talking about t-SNE,

548
00:21:03,550 --> 00:21:04,555
because it's a non-linear

549
00:21:04,555 --> 00:21:06,080
dimensionality reduction technique.

550
00:21:06,080 --> 00:21:07,668
So, I'd have to look offline

551
00:21:07,668 --> 00:21:10,259
and I'm not sure of exactly how much it explains.

552
00:21:10,259 --> 00:21:14,377
Question?

553
00:21:14,377 --> 00:21:16,143
Question is, can you do the same analysis

554
00:21:16,143 --> 00:21:17,038
of upper layers of the network?

555
00:21:17,038 --> 00:21:18,358
And yes, you can. But no,

556
00:21:18,358 --> 00:21:21,384
I don't have those visualizations here. Sorry.

557
00:21:21,384 --> 00:21:24,603
Question?

558
00:21:35,559 --> 00:21:37,434
The question is, Shouldn't we have overlaps

559
00:21:37,434 --> 00:21:39,482
of images once we do this dimensionality reduction?

560
00:21:39,482 --> 00:21:40,902
And yes, of course, you would.

561
00:21:40,902 --> 00:21:43,118
So this is just kind of taking a,

562
00:21:43,118 --> 00:21:45,119
nearest neighbor in our, in our regular grid

563
00:21:45,119 --> 00:21:47,537
and then picking an image close to that grid point.

564
00:21:47,537 --> 00:21:49,397
So, so... they, yeah.

565
00:21:49,397 --> 00:21:52,875
this is not showing you the kind of density

566
00:21:52,875 --> 00:21:54,792
in different parts of the feature space.

567
00:21:54,792 --> 00:21:56,985
So that's, that's another thing to look at

568
00:21:56,985 --> 00:21:58,753
and again at the link you, there's a couple more

569
00:21:58,753 --> 00:22:00,584
visualizations of this nature that,

570
00:22:00,584 --> 00:22:03,122
that address that a little bit.

571
00:22:03,122 --> 00:22:04,779
Okay. So another, another thing

572
00:22:04,779 --> 00:22:07,713
that you can do for some of these intermediate features

573
00:22:07,713 --> 00:22:09,656
is, so we talked a couple of slides ago

574
00:22:09,656 --> 00:22:12,157
that visualizing the weights of these intermediate layers

575
00:22:12,157 --> 00:22:13,856
is not so interpretable.

576
00:22:13,856 --> 00:22:17,260
But actually visualizing the activation maps of those

577
00:22:17,260 --> 00:22:18,430
intermediate layers

578
00:22:18,430 --> 00:22:20,846
is kind of interpretable in some cases.

579
00:22:20,846 --> 00:22:24,152
So for, so I, again an example of Alex Net.

580
00:22:24,152 --> 00:22:26,777
Remember the, the conv5 layers of Alex Net.

581
00:22:26,777 --> 00:22:28,603
Gives us this 128 by...

582
00:22:28,603 --> 00:22:31,640
The for...The conv5 features for any image

583
00:22:31,640 --> 00:22:35,668
is now 128 by 13 by 13 dimensional tensor.

584
00:22:35,668 --> 00:22:36,998
But we can think of that

585
00:22:36,998 --> 00:22:39,799
as 128 different

586
00:22:39,799 --> 00:22:42,386
13 by 132-D grids.

587
00:22:42,386 --> 00:22:44,347
So now we can actually go and visualize

588
00:22:44,347 --> 00:22:47,285
each of those 13 by 13 elements slices

589
00:22:47,285 --> 00:22:49,741
of the feature map as a grayscale image

590
00:22:49,741 --> 00:22:52,733
and this gives us some sense for what types of things

591
00:22:52,733 --> 00:22:55,759
in the input are each of those features

592
00:22:55,759 --> 00:22:58,501
in that convolutional layer looking for.

593
00:22:58,501 --> 00:23:00,621
So this is a, a really cool interactive tool

594
00:23:00,621 --> 00:23:03,306
by Jason Yasenski you can just download.

595
00:23:03,306 --> 00:23:05,137
So it's run, so I don't have the video,

596
00:23:05,137 --> 00:23:06,598
it has a video on his website.

597
00:23:06,598 --> 00:23:08,349
But it's running a convolutional network

598
00:23:08,349 --> 00:23:10,059
on the inputs stream of webcam

599
00:23:10,059 --> 00:23:12,213
and then visualizing in real time

600
00:23:12,213 --> 00:23:15,348
each of those slices of that intermediate feature map

601
00:23:15,348 --> 00:23:17,279
give you a sense of what it's looking for

602
00:23:17,279 --> 00:23:19,328
and you can see that, so here the input image

603
00:23:19,328 --> 00:23:21,509
is this, this picture up in, settings...

604
00:23:21,509 --> 00:23:23,931
of this picture of a person in front of the camera

605
00:23:23,931 --> 00:23:26,026
and most of these intermediate features

606
00:23:26,026 --> 00:23:28,192
are kind of noisy, not much going on.

607
00:23:28,192 --> 00:23:30,190
But there's a, but there's this one highlighted

608
00:23:30,190 --> 00:23:31,527
intermediate feature

609
00:23:31,527 --> 00:23:34,277
where that is also shown larger here

610
00:23:34,277 --> 00:23:35,911
that seems that it's activating

611
00:23:35,911 --> 00:23:37,827
on the portions of the feature map

612
00:23:37,827 --> 00:23:39,815
corresponding to the person's face.

613
00:23:39,815 --> 00:23:41,103
Which is really interesting

614
00:23:41,103 --> 00:23:43,719
and that kind of, suggests that maybe this,

615
00:23:43,719 --> 00:23:45,863
this particular slice of the feature map

616
00:23:45,863 --> 00:23:47,896
of this layer of this particular network

617
00:23:47,896 --> 00:23:51,045
is maybe looking for human faces or something like that.

618
00:23:51,045 --> 00:23:53,036
Which is kind of a nice, kind of a nice

619
00:23:53,036 --> 00:23:54,132
and cool finding.

620
00:23:54,132 --> 00:23:55,517
Question?

621
00:23:59,038 --> 00:24:02,649
The question is, Are the black activations dead relu's?

622
00:24:02,649 --> 00:24:04,957
So you got to be... a little careful with terminology.

623
00:24:04,957 --> 00:24:07,043
We usually say dead relu to mean

624
00:24:07,043 --> 00:24:09,539
something that's dead over the entire training data set.

625
00:24:09,539 --> 00:24:12,571
Here I would say that it's a relu, that, it's not active

626
00:24:12,571 --> 00:24:14,701
for this particular input.

627
00:24:14,701 --> 00:24:15,702
Question?

628
00:24:19,457 --> 00:24:21,338
The question is, If there's no humans in image net

629
00:24:21,338 --> 00:24:22,538
how can it recognize a human face?

630
00:24:22,538 --> 00:24:24,182
There definitely are humans in image net

631
00:24:24,182 --> 00:24:26,033
I don't think it's, it's one of the cat...

632
00:24:26,033 --> 00:24:27,793
I don't think it's one of the thousand categories

633
00:24:27,793 --> 00:24:29,020
for the classification challenge.

634
00:24:29,020 --> 00:24:31,068
But people definitely appear in a lot of these images

635
00:24:31,068 --> 00:24:32,684
and that can be useful signal for detecting

636
00:24:32,684 --> 00:24:34,906
other types of things.

637
00:24:34,906 --> 00:24:36,698
So that's actually kind of nice results

638
00:24:36,698 --> 00:24:39,148
because that shows that, it's sort of can learn features

639
00:24:39,148 --> 00:24:41,617
that are useful for the classification task at hand.

640
00:24:41,617 --> 00:24:42,977
That are even maybe a little bit different

641
00:24:42,977 --> 00:24:44,516
from the explicit classification task

642
00:24:44,516 --> 00:24:45,792
that we told it to perform.

643
00:24:45,792 --> 00:24:47,483
So it's actually really cool results.

644
00:24:50,346 --> 00:24:51,929
Okay, question?

645
00:24:55,192 --> 00:24:57,147
So at each layer in the convolutional network

646
00:24:57,147 --> 00:25:01,171
our input image is of three, it's like 3 by 224 by 224

647
00:25:01,171 --> 00:25:03,334
and then it goes through many stages of convolution.

648
00:25:03,334 --> 00:25:05,428
And then, it, after each convolutional layer

649
00:25:05,428 --> 00:25:07,731
is some three dimensional chunk of numbers.

650
00:25:07,731 --> 00:25:09,157
Which are the outputs from that layer

651
00:25:09,157 --> 00:25:10,476
of the convolutional network.

652
00:25:10,476 --> 00:25:13,651
And that into the entire three dimensional chunk of numbers

653
00:25:13,651 --> 00:25:15,648
which are the output of the previous convolutional layer,

654
00:25:15,648 --> 00:25:18,155
we call, we call, like an activation volume

655
00:25:18,155 --> 00:25:20,235
and then one of those, one of those slices

656
00:25:20,235 --> 00:25:22,156
is a, it's an activation map.

657
00:25:34,426 --> 00:25:36,458
So the question is, If the image is K by K

658
00:25:36,458 --> 00:25:38,513
will the activation map be K by K?

659
00:25:38,513 --> 00:25:40,375
Not always because there can be sub sampling

660
00:25:40,375 --> 00:25:42,489
due to pool, straight convolution and pooling.

661
00:25:42,489 --> 00:25:44,943
But in general, the, the size of each activation map

662
00:25:44,943 --> 00:25:47,756
will be linear in the size of the input image.

663
00:25:50,492 --> 00:25:52,762
So another, another kind of useful thing we can do

664
00:25:52,762 --> 00:25:55,625
for visualizing intermediate features is...

665
00:25:55,625 --> 00:25:58,850
Visualizing what types of patches from input images

666
00:25:58,850 --> 00:26:01,293
cause maximal activation in different,

667
00:26:01,293 --> 00:26:03,453
different features, different neurons.

668
00:26:03,453 --> 00:26:06,052
So what we've done here is that, we pick...

669
00:26:06,052 --> 00:26:08,605
Maybe again the con five layer from Alex Net?

670
00:26:08,605 --> 00:26:10,926
And remember each of these activation volumes

671
00:26:10,926 --> 00:26:12,936
at the con, at the con five in Alex net gives us

672
00:26:12,936 --> 00:26:15,738
a 128 by 13 by 13 chunk of numbers.

673
00:26:15,738 --> 00:26:17,977
Then we'll pick one of those 128 channels.

674
00:26:17,977 --> 00:26:19,644
Maybe channel 17

675
00:26:19,644 --> 00:26:21,972
and now what we'll do is run many images

676
00:26:21,972 --> 00:26:23,749
through this convolutional network.

677
00:26:23,749 --> 00:26:25,689
And then, for each of those images

678
00:26:25,689 --> 00:26:27,456
record the con five features

679
00:26:27,456 --> 00:26:30,917
and then look at the...

680
00:26:30,917 --> 00:26:32,684
Right, so, then, then look at the, the...

681
00:26:32,684 --> 00:26:35,001
The parts of that 17th feature map

682
00:26:35,001 --> 00:26:37,925
that are maximally activated over our data set of images.

683
00:26:37,925 --> 00:26:40,398
And now, because again this is a convolutional layer

684
00:26:40,398 --> 00:26:42,875
each of those neurons in the convolutional layer

685
00:26:42,875 --> 00:26:45,161
has some small receptive field in the input.

686
00:26:45,161 --> 00:26:47,242
Each of those neurons is not looking at the whole image.

687
00:26:47,242 --> 00:26:49,239
They're only looking at the sub set of the image.

688