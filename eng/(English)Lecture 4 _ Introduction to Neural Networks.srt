1
00:00:06,804 --> 00:00:10,610
[students murmuring]

2
00:00:10,610 --> 00:00:15,375
- Okay, so good afternoon everyone, let's get started.

3
00:00:15,375 --> 00:00:18,257
So hi, so for those of you who I haven't met yet,

4
00:00:18,257 --> 00:00:21,470
my name is Serena Yeung and I'm the third

5
00:00:21,470 --> 00:00:24,853
and final instructor for this class,

6
00:00:24,853 --> 00:00:28,307
and I'm also a PhD student in Fei-Fei's group.

7
00:00:28,307 --> 00:00:31,292
Okay, so today we're going to talk about backpropagation

8
00:00:31,292 --> 00:00:33,843
and neural networks, and so now we're really starting

9
00:00:33,843 --> 00:00:37,346
to get to some of the core material in this class.

10
00:00:37,346 --> 00:00:39,929
Before we begin, let's see, oh.

11
00:00:42,124 --> 00:00:44,166
So a few administrative details,

12
00:00:44,166 --> 00:00:47,602
so assignment one is due Thursday, April 20th,

13
00:00:47,602 --> 00:00:51,522
so a reminder, we shifted the date back by a little bit

14
00:00:51,522 --> 00:00:55,105
and it's going to be due 11:59 p.m. on Canvas.

15
00:00:56,722 --> 00:00:59,316
So you should start thinking about your projects,

16
00:00:59,316 --> 00:01:02,327
there are TA specialties listed on the Piazza website

17
00:01:02,327 --> 00:01:05,432
so if you have questions about a specific project topic

18
00:01:05,432 --> 00:01:08,958
you're thinking about, you can go and try and find

19
00:01:08,958 --> 00:01:12,682
the TAs that might be most relevant.

20
00:01:12,682 --> 00:01:15,910
And then also for Google Cloud, so all students are going

21
00:01:15,910 --> 00:01:19,130
to get $100 in credits to use for Google Cloud

22
00:01:19,130 --> 00:01:21,268
for their assignments and project,

23
00:01:21,268 --> 00:01:22,630
so you should be receiving an email

24
00:01:22,630 --> 00:01:24,285
for that this week, I think.

25
00:01:24,285 --> 00:01:25,666
A lot of you may have already, and then

26
00:01:25,666 --> 00:01:28,227
for those of you who haven't, they're going to come,

27
00:01:28,227 --> 00:01:31,060
should be by the end of this week.

28
00:01:32,544 --> 00:01:35,721
Okay so where we are, so far we've talked about

29
00:01:35,721 --> 00:01:39,207
how to define a classifier using a function f,

30
00:01:39,207 --> 00:01:41,669
parameterized by weights W, and this function f

31
00:01:41,669 --> 00:01:45,836
is going to take data x as input, and output a vector of scores

32
00:01:48,027 --> 00:01:51,432
for each of the classes that you want to classify.

33
00:01:51,432 --> 00:01:54,778
And so from here we can also define

34
00:01:54,778 --> 00:01:57,314
a loss function, so for example, the SVM loss function

35
00:01:57,314 --> 00:02:00,445
that we've talked about which basically quantifies

36
00:02:00,445 --> 00:02:02,053
how happy or unhappy we are with

37
00:02:02,053 --> 00:02:04,167
the scores that we've produced, right,

38
00:02:04,167 --> 00:02:07,727
and then we can use that to define a total loss term.

39
00:02:07,727 --> 00:02:10,757
So L here, which is a combination of this data term,

40
00:02:10,757 --> 00:02:14,617
combined with a regularization term that expresses

41
00:02:14,617 --> 00:02:16,601
how simple our model is, and we have a preference

42
00:02:16,601 --> 00:02:20,201
for simpler models, for better generalization.

43
00:02:20,201 --> 00:02:23,295
And so now we want to find the parameters W

44
00:02:23,295 --> 00:02:25,209
that correspond to our lowest loss, right?

45
00:02:25,209 --> 00:02:27,092
We want to minimize the loss function,

46
00:02:27,092 --> 00:02:28,497
and so to do that we want to find

47
00:02:28,497 --> 00:02:31,497
the gradient of L with respect to W.

48
00:02:33,275 --> 00:02:35,829
So last lecture we talked about how we can do this

49
00:02:35,829 --> 00:02:37,735
using optimization, and we're going

50
00:02:37,735 --> 00:02:39,976
to iteratively take steps in the direction

51
00:02:39,976 --> 00:02:42,817
of steepest descent, which is the negative of the gradient,

52
00:02:42,817 --> 00:02:44,819
in order to walk down this loss landscape

53
00:02:44,819 --> 00:02:47,291
and get to the point of lowest loss, right?

54
00:02:47,291 --> 00:02:51,947
And we saw how this gradient descent can basically take

55
00:02:51,947 --> 00:02:54,770
this trajectory, looking like this image on the right,

56
00:02:54,770 --> 00:02:59,293
getting to the bottom of your loss landscape.

57
00:02:59,293 --> 00:03:00,126
Oh!

58
00:03:02,247 --> 00:03:04,376
Okay, and so we also talked about different ways

59
00:03:04,376 --> 00:03:06,154
for computing a gradient, right?

60
00:03:06,154 --> 00:03:08,538
We can compute this numerically

61
00:03:08,538 --> 00:03:10,696
using finite difference approximation

62
00:03:10,696 --> 00:03:13,444
which is slow and approximate, but at the same time

63
00:03:13,444 --> 00:03:14,725
it's really easy to write out,

64
00:03:14,725 --> 00:03:17,589
you know you can always get the gradient this way.

65
00:03:17,589 --> 00:03:21,064
We also talked about how to use the analytic gradient

66
00:03:21,064 --> 00:03:23,113
and computing this is, it's fast

67
00:03:23,113 --> 00:03:25,227
and exact once you've gotten the expression for

68
00:03:25,227 --> 00:03:27,499
the analytic gradient, but at the same time you have

69
00:03:27,499 --> 00:03:29,670
to do all the math and the calculus to derive this,

70
00:03:29,670 --> 00:03:32,734
so it's also, you know, easy to make mistakes, right?

71
00:03:32,734 --> 00:03:34,925
So in practice what we want to do is we want to derive

72
00:03:34,925 --> 00:03:37,620
the analytic gradient and use this,

73
00:03:37,620 --> 00:03:39,837
but at the same time check our implementation using

74
00:03:39,837 --> 00:03:41,267
the numerical gradient to make sure

75
00:03:41,267 --> 00:03:44,600
that we've gotten all of our math right.

76
00:03:46,032 --> 00:03:48,518
So today we're going to talk about how to compute

77
00:03:48,518 --> 00:03:52,261
the analytic gradient for arbitrarily complex functions,

78
00:03:52,261 --> 00:03:55,824
using a framework that I'm going to call computational graphs.

79
00:03:55,824 --> 00:03:58,506
And so basically what a computational graph is,

80
00:03:58,506 --> 00:04:00,555
is that we can use this kind of graph in order

81
00:04:00,555 --> 00:04:04,010
to represent any function, where the nodes of the graph

82
00:04:04,010 --> 00:04:06,611
are steps of computation that we go through.

83
00:04:06,611 --> 00:04:07,952
So for example, in this example,

84
00:04:07,952 --> 00:04:11,306
the linear classifier that we've talked about,

85
00:04:11,306 --> 00:04:14,737
the inputs here are x and W, right,

86
00:04:14,737 --> 00:04:18,181
and then this multiplication node represents

87
00:04:18,181 --> 00:04:21,322
the matrix multiplier, the multiplication of

88
00:04:21,322 --> 00:04:25,038
the parameters W with our data x that we have,

89
00:04:25,038 --> 00:04:27,478
outputting our vector of scores.

90
00:04:27,478 --> 00:04:29,469
And then we have another computational node

91
00:04:29,469 --> 00:04:31,948
which represents our hinge loss, right,

92
00:04:31,948 --> 00:04:35,113
computing our data loss term, Li.

93
00:04:35,113 --> 00:04:38,198
And we also have this regularization term at

94
00:04:38,198 --> 00:04:39,340
the bottom right, so this node

95
00:04:39,340 --> 00:04:42,964
which computes our regularization term,

96
00:04:42,964 --> 00:04:44,877
and then our total loss here at the end, L,

97
00:04:44,877 --> 00:04:49,044
is the sum of the regularization term and the data term.

98
00:04:50,624 --> 00:04:52,325
And the advantage is that once we can express

99
00:04:52,325 --> 00:04:54,675
a function using a computational graph,

100
00:04:54,675 --> 00:04:58,103
then we can use a technique that we call backpropagation

101
00:04:58,103 --> 00:05:00,261
which is going to recursively use the chain rule

102
00:05:00,261 --> 00:05:02,227
in order to compute the gradient

103
00:05:02,227 --> 00:05:05,568
with respect to every variable in the computational graph,

104
00:05:05,568 --> 00:05:09,830
and so we're going to see how this is done.

105
00:05:09,830 --> 00:05:11,675
And this becomes very useful when we start working

106
00:05:11,675 --> 00:05:13,413
with really complex functions,

107
00:05:13,413 --> 00:05:15,502
so for example, convolutional neural networks

108
00:05:15,502 --> 00:05:17,836
that we're going to talk about later in this class.

109
00:05:17,836 --> 00:05:20,900
We have here the input image at the top,

110
00:05:20,900 --> 00:05:22,364
we have our loss at the bottom,

111
00:05:22,364 --> 00:05:24,023
and the input has to go through many layers

112
00:05:24,023 --> 00:05:26,269
of transformations in order to get all

113
00:05:26,269 --> 00:05:29,102
the way down to the loss function.

114
00:05:30,896 --> 00:05:33,165
And this can get even crazier with things like,

115
00:05:33,165 --> 00:05:35,447
the, you know, like a neural turing machine,

116
00:05:35,447 --> 00:05:37,869
which is another kind of deep learning model,

117
00:05:37,869 --> 00:05:39,915
and in this case you can see that the computational graph

118
00:05:39,915 --> 00:05:43,413
for this is really insane, and especially,

119
00:05:43,413 --> 00:05:45,897
we end up, you know, unrolling this over time.

120
00:05:45,897 --> 00:05:47,562
It's basically completely impractical

121
00:05:47,562 --> 00:05:49,901
if you want to compute the gradients

122
00:05:49,901 --> 00:05:53,234
for any of these intermediate variables.

123
00:05:55,560 --> 00:05:59,139
Okay, so how does backpropagation work?

124
00:05:59,139 --> 00:06:01,838
So we're going to start off with a simple example,

125
00:06:01,838 --> 00:06:03,672
where again, our goal is that we have a function.

126
00:06:03,672 --> 00:06:06,228
So in this case, f of x, y, z

127
00:06:06,228 --> 00:06:08,614
equals x plus y times z,

128
00:06:08,614 --> 00:06:10,746
and we want to find the gradients of the output of

129
00:06:10,746 --> 00:06:13,572
the function with respect to any of the variables.

130
00:06:13,572 --> 00:06:15,365
So the first step, always, is we want

131
00:06:15,365 --> 00:06:17,329
to take our function f, and we want

132
00:06:17,329 --> 00:06:20,623
to represent it using a computational graph.

133
00:06:20,623 --> 00:06:23,339
Right, so here our computational graph is on the right,

134
00:06:23,339 --> 00:06:25,957
and you can see that we have our,

135
00:06:25,957 --> 00:06:27,892
first we have the plus node, so x plus y,

136
00:06:27,892 --> 00:06:30,044
and then we have this multiplication node, right,

137
00:06:30,044 --> 00:06:34,060
for the second computation that we're doing.

138
00:06:34,060 --> 00:06:36,201
And then, now we're going to do a forward pass

139
00:06:36,201 --> 00:06:38,732
of this network, so given the values of

140
00:06:38,732 --> 00:06:40,782
the variables that we have, so here,

141
00:06:40,782 --> 00:06:42,944
x equals negative two, y equals five

142
00:06:42,944 --> 00:06:46,251
and z equals negative four, I'm going to fill these all in

143
00:06:46,251 --> 00:06:49,417
in our computational graph, and then here we can compute

144
00:06:49,417 --> 00:06:52,965
an intermediate value, so x plus y gives three,

145
00:06:52,965 --> 00:06:55,045
and then finally we pass it through again,

146
00:06:55,045 --> 00:06:56,990
through the last node, the multiplication,

147
00:06:56,990 --> 00:07:00,823
to get our final node of f equals negative 12.

148
00:07:04,310 --> 00:07:08,784
So here we want to give every intermediate variable a name.

149
00:07:08,784 --> 00:07:10,438
So here I've called this intermediate variable after

150
00:07:10,438 --> 00:07:14,997
the plus node q, and we have q equals x plus y,

151
00:07:14,997 --> 00:07:18,609
and then f equals q times z, using this intermediate node.

152
00:07:18,609 --> 00:07:21,341
And I've also written out here, the gradients

153
00:07:21,341 --> 00:07:24,763
of q with respect to x and y, which are just one

154
00:07:24,763 --> 00:07:28,131
because of the addition, and then the gradients of f

155
00:07:28,131 --> 00:07:31,653
with respect to q and z, which is z and q respectively

156
00:07:31,653 --> 00:07:34,607
because of the multiplication rule.

157
00:07:34,607 --> 00:07:36,598
And so what we want to find, is we want to find

158
00:07:36,598 --> 00:07:40,431
the gradients of f with respect to x, y and z.

159
00:07:43,356 --> 00:07:46,822
So what backprop is, it's a recursive application of

160
00:07:46,822 --> 00:07:49,182
the chain rule, so we're going to start at the back,

161
00:07:49,182 --> 00:07:51,311
the very end of the computational graph,

162
00:07:51,311 --> 00:07:53,360
and then we're going to work our way backwards

163
00:07:53,360 --> 00:07:56,373
and compute all the gradients along the way.

164
00:07:56,373 --> 00:07:59,015
So here if we start at the very end, right,

165
00:07:59,015 --> 00:08:01,217
we want to compute the gradient of the output

166
00:08:01,217 --> 00:08:04,674
with respect to the last variable, which is just f.

167
00:08:04,674 --> 00:08:08,591
And so this gradient is just one, it's trivial.

168
00:08:10,065 --> 00:08:12,604
So now, moving backwards, we want the gradient

169
00:08:12,604 --> 00:08:15,687
with respect to z, right, and we know

170
00:08:16,637 --> 00:08:19,137
that df over dz is equal to q.

171
00:08:19,993 --> 00:08:22,636
So the value of q is just three,

172
00:08:22,636 --> 00:08:26,386
and so we have here, df over dz equals three.

173
00:08:28,819 --> 00:08:31,688
And so next if we want to do df over dq,

174
00:08:31,688 --> 00:08:33,855
what is the value of that?

175
00:08:36,732 --> 00:08:37,957
What is df over dq?

176
00:08:37,957 --> 00:08:42,040
So we have here, df over dq is equal to z, right,

177
00:08:46,012 --> 00:08:49,012
and the value of z is negative four.

178
00:08:49,963 --> 00:08:54,130
So here we have df over dq is equal to negative four.

179
00:08:57,485 --> 00:09:00,802
Okay, so now continuing to move backwards to the graph,

180
00:09:00,802 --> 00:09:03,793
we want to find df over dy, right,

181
00:09:03,793 --> 00:09:06,251
but here in this case, the gradient with respect to y,

182
00:09:06,251 --> 00:09:08,986
y is not connected directly to f, right?

183
00:09:08,986 --> 00:09:12,838
It's connected through an intermediate node of z,

184
00:09:12,838 --> 00:09:14,756
and so the way we're going to do this

185
00:09:14,756 --> 00:09:17,998
is we can leverage the chain rule which says

186
00:09:17,998 --> 00:09:21,748
that df over dy can be written as df over dq,

187
00:09:22,746 --> 00:09:26,754
times dq over dy, and so the intuition of this

188
00:09:26,754 --> 00:09:30,707
is that in order to get to find the effect of y on f,

189
00:09:30,707 --> 00:09:33,348
this is actually equivalent to if we take

190
00:09:33,348 --> 00:09:37,416
the effect of q times q on f, which we already know, right?

191
00:09:37,416 --> 00:09:41,330
df over dq is equal to negative four,

192
00:09:41,330 --> 00:09:45,497
and we compound it with the effect of y on q, dq over dy.

193
00:09:46,604 --> 00:09:50,986
So what's dq over dy equal to in this case?

194
00:09:50,986 --> 00:09:51,819
- [Student] One.

195
00:09:51,819 --> 00:09:52,980
- One, right. Exactly.

196
00:09:52,980 --> 00:09:55,916
So dq over dy is equal to one, which means, you know,

197
00:09:55,916 --> 00:09:57,984
if we change y by a little bit,

198
00:09:57,984 --> 00:09:59,408
q is going to change by approximately

199
00:09:59,408 --> 00:10:01,627
the same amount right, this is the effect,

200
00:10:01,627 --> 00:10:05,585
and so what this is doing is this is saying,

201
00:10:05,585 --> 00:10:07,474
well if I change y by a little bit,

202
00:10:07,474 --> 00:10:10,807
the effect of y on q is going to be one,

203
00:10:13,249 --> 00:10:16,882
and then the effect of q on f is going to be approximately

204
00:10:16,882 --> 00:10:18,628
a factor of negative four, right?

205
00:10:18,628 --> 00:10:20,405
So then we multiply these together

206
00:10:20,405 --> 00:10:24,053
and we get that the effect of y on f

207
00:10:24,053 --> 00:10:26,470
is going to be negative four.

208
00:10:30,887 --> 00:10:33,249
Okay, so now if we want to do the same thing for

209
00:10:33,249 --> 00:10:35,632
the gradient with respect to x, right,

210
00:10:35,632 --> 00:10:38,074
we can do the, we can follow the same procedure,

211
00:10:38,074 --> 00:10:41,191
and so what is this going to be?

212
00:10:41,191 --> 00:10:42,711
[students speaking away from microphone]

213
00:10:42,711 --> 00:10:44,746
- I heard the same.

214
00:10:44,746 --> 00:10:48,746
Yeah exactly, so in this case we want to, again,

215
00:10:49,636 --> 00:10:51,051
apply the chain rule, right?

216
00:10:51,051 --> 00:10:55,882
We know the effect of q on f is negative four,

217
00:10:55,882 --> 00:10:59,167
and here again, since we have also the same addition node,

218
00:10:59,167 --> 00:11:01,958
dq over dx is equal to one, again,

219
00:11:01,958 --> 00:11:04,593
we have negative four times one, right, and the gradient

220
00:11:04,593 --> 00:11:08,760
with respect to x is going to be negative four.

221
00:11:11,467 --> 00:11:13,214
Okay, so what we're doing is, in backprop,

222
00:11:13,214 --> 00:11:15,389
is we basically have all of these nodes

223
00:11:15,389 --> 00:11:17,846
in our computational graph, but each node

224
00:11:17,846 --> 00:11:20,724
is only aware of its immediate surroundings, right?

225
00:11:20,724 --> 00:11:23,874
So we have, at each node, we have the local inputs

226
00:11:23,874 --> 00:11:25,579
that are connected to this node,

227
00:11:25,579 --> 00:11:27,312
the values that are flowing into the node,

228
00:11:27,312 --> 00:11:28,981
and then we also have the output

229
00:11:28,981 --> 00:11:32,432
that is directly outputted from this node.

230
00:11:32,432 --> 00:11:36,599
So here our local inputs are x and y, and the output is z.

231
00:11:39,777 --> 00:11:43,469
And at this node we also know the local gradient, right,

232
00:11:43,469 --> 00:11:46,607
we can compute the gradient of z with respect to x,

233
00:11:46,607 --> 00:11:48,905
and the gradient of z with respect to y,

234
00:11:48,905 --> 00:11:51,217
and these are usually really simple operations, right?

235
00:11:51,217 --> 00:11:53,115
Each node is going to be something like

236
00:11:53,115 --> 00:11:54,821
the addition or the multiplication

237
00:11:54,821 --> 00:11:56,786
that we had in that earlier example,

238
00:11:56,786 --> 00:11:58,276
which is something where we can just write down

239
00:11:58,276 --> 00:12:00,158
the gradient, and we don't have to, you know,

240
00:12:00,158 --> 00:12:04,665
go through very complex calculus in order to find this.

241
00:12:04,665 --> 00:12:07,513
- [Student] Can you go back and explain why

242
00:12:07,513 --> 00:12:11,699
more in the last slide was different than planning

243
00:12:11,699 --> 00:12:15,313
the first part of it using just normal calculus?

244
00:12:15,313 --> 00:12:18,683
- Yeah, so basically if we go back,

245
00:12:18,683 --> 00:12:20,183
hold on, let me...

246
00:12:21,740 --> 00:12:24,472
So if we go back here, we could exactly write out,

247
00:12:24,472 --> 00:12:26,565
find all of these using just calculus,

248
00:12:26,565 --> 00:12:30,216
so we could say, you know, we want df over dx, right,

249
00:12:30,216 --> 00:12:32,572
and we can probably expand out this expression

250
00:12:32,572 --> 00:12:35,338
and see that it's just going to be z,

251
00:12:35,338 --> 00:12:37,056
but we can do this for, in this case,

252
00:12:37,056 --> 00:12:39,526
because it's simple, but we'll see examples later on

253
00:12:39,526 --> 00:12:42,667
where once this becomes a really complicated expression,

254
00:12:42,667 --> 00:12:44,916
you don't want to have to use calculus

255
00:12:44,916 --> 00:12:47,487
to derive, right, the gradient for something,

256
00:12:47,487 --> 00:12:49,302
for a super-complicated expression,

257
00:12:49,302 --> 00:12:52,094
and instead, if you use this formalism

258
00:12:52,094 --> 00:12:55,018
and you break it down into these computational nodes,

259
00:12:55,018 --> 00:12:58,001
then you can only ever work with gradients

260
00:12:58,001 --> 00:13:01,612
of very simple computations, right,

261
00:13:01,612 --> 00:13:04,925
at the level of, you know, additions, multiplications,

262
00:13:04,925 --> 00:13:06,938
exponentials, things as simple as you want them,

263
00:13:06,938 --> 00:13:08,743
and then you just use the chain rule

264
00:13:08,743 --> 00:13:10,038
to multiply all these together,

265
00:13:10,038 --> 00:13:12,404
and get your, the value of your gradient

266
00:13:12,404 --> 00:13:16,571
without having to ever derive the entire expression.

267
00:13:18,562 --> 00:13:20,508
Does that make sense?

268
00:13:20,508 --> 00:13:21,367
[student murmuring]

269
00:13:21,367 --> 00:13:25,034
Okay, so we'll see an example of this later.

270
00:13:28,751 --> 00:13:30,449
And so, was there another question, yeah?

271
00:13:30,449 --> 00:13:32,240
[student speaking away from microphone]

272
00:13:32,240 --> 00:13:33,778
- [Student] What's the negative four

273
00:13:33,778 --> 00:13:36,146
next to the z representing?

274
00:13:36,146 --> 00:13:38,408
- Negative, okay yeah, so the negative four,

275
00:13:38,408 --> 00:13:39,884
these were the, the green values on top

276
00:13:39,884 --> 00:13:43,831
were all the values of the function as we passed

277
00:13:43,831 --> 00:13:46,623
it forward through the computational graph, right?

278
00:13:46,623 --> 00:13:49,835
So we said up here that x is equal to negative two,

279
00:13:49,835 --> 00:13:52,591
y is equal to five, and z equals negative four,

280
00:13:52,591 --> 00:13:55,621
so we filled in all of these values, and then we just wanted

281
00:13:55,621 --> 00:13:59,286
to compute the value of this function.

282
00:13:59,286 --> 00:14:04,008
Right, so we said this value of q is going to be x plus y,

283
00:14:04,008 --> 00:14:06,118
it's going to be negative two plus five, it is going

284
00:14:06,118 --> 00:14:09,289
to be three, and we have z is equal to negative four

285
00:14:09,289 --> 00:14:12,285
so we fill that in here, and then we multiplied q

286
00:14:12,285 --> 00:14:14,192
and z together, negative four times three

287
00:14:14,192 --> 00:14:16,886
in order to get the final value of f, right?

288
00:14:16,886 --> 00:14:18,175
And then the red values underneath

289
00:14:18,175 --> 00:14:20,540
were as we were filling in the gradients

290
00:14:20,540 --> 00:14:22,957
as we were working backwards.

291
00:14:24,927 --> 00:14:25,760
Okay.

292
00:14:29,418 --> 00:14:33,356
Okay, so right, so we said that, you know,

293
00:14:33,356 --> 00:14:34,845
we have these local, these nodes,

294
00:14:34,845 --> 00:14:38,969
and each node basically gets its local inputs coming in

295
00:14:38,969 --> 00:14:40,886
and the output that it sees directly passing on

296
00:14:40,886 --> 00:14:44,222
to the next node, and we also have these local gradients

297
00:14:44,222 --> 00:14:46,302
that we computed, right, the gradient of

298
00:14:46,302 --> 00:14:48,476
the immediate output of the node

299
00:14:48,476 --> 00:14:51,175
with respect to the inputs coming in.

300
00:14:51,175 --> 00:14:55,155
And so what happens during backprop is we have these,

301
00:14:55,155 --> 00:14:56,750
we'll start from the back of the graph, right,

302
00:14:56,750 --> 00:14:58,471
and then we work our way from the end

303
00:14:58,471 --> 00:15:00,341
all the way back to the beginning,

304
00:15:00,341 --> 00:15:03,116
and when we reach each node, at each node we have

305
00:15:03,116 --> 00:15:05,593
the upstream gradients coming back, right,

306
00:15:05,593 --> 00:15:08,980
with respect to the immediate output of the node.

307
00:15:08,980 --> 00:15:11,626
So by the time we reach this node in backprop,

308
00:15:11,626 --> 00:15:13,503
we've already computed the gradient

309
00:15:13,503 --> 00:15:17,808
of our final loss l, with respect to z, right?

310
00:15:17,808 --> 00:15:20,310
And so now what we want to find next

311
00:15:20,310 --> 00:15:22,508
is we want to find the gradients with respect

312
00:15:22,508 --> 00:15:26,675
to just before the node, to the values of x and y.

313
00:15:27,679 --> 00:15:30,962
And so as we saw earlier, we do this using the chain rule,

314
00:15:30,962 --> 00:15:33,053
right, we have from the chain rule,

315
00:15:33,053 --> 00:15:34,857
that the gradient of this loss function

316
00:15:34,857 --> 00:15:36,690
with respect to x is going to be

317
00:15:36,690 --> 00:15:41,482
the gradient with respect to z times, compounded by

318
00:15:41,482 --> 00:15:44,987
this gradient, local gradient of z with respect to x.

319
00:15:44,987 --> 00:15:46,369
Right, so in the chain rule we always take

320
00:15:46,369 --> 00:15:48,422
this upstream gradient coming down,

321
00:15:48,422 --> 00:15:50,650
and we multiply it by the local gradient

322
00:15:50,650 --> 00:15:55,071
in order to get the gradient with respect to the input.

323
00:15:55,071 --> 00:15:57,349
- [Student] So, sorry, is it,

324
00:15:57,349 --> 00:15:59,731
it's different because this would never work

325
00:15:59,731 --> 00:16:01,949
to get a general formula into the,

326
00:16:01,949 --> 00:16:04,478
or general symbolic formula for the gradient.

327
00:16:04,478 --> 00:16:06,700
It only works with instantaneous values,

328
00:16:06,700 --> 00:16:07,533
where you like.

329
00:16:07,533 --> 00:16:08,423
[student coughing]

330
00:16:08,423 --> 00:16:11,896
Or passing a little constant value as a symbolic.

331
00:16:11,896 --> 00:16:16,335
- So the question is whether this only works

332
00:16:16,335 --> 00:16:19,496
because we're working with the current values of

333
00:16:19,496 --> 00:16:22,579
the function, and so it works, right,

334
00:16:23,842 --> 00:16:25,745
given the current values of the function that we plug in,

335
00:16:25,745 --> 00:16:27,611
but we can write an expression for this,

336
00:16:27,611 --> 00:16:29,819
still in terms of the variables, right?

337
00:16:29,819 --> 00:16:33,635
So we'll see that gradient of L with respect to z

338
00:16:33,635 --> 00:16:36,357
is going to be some expression, and gradient of z

339
00:16:36,357 --> 00:16:39,463
with respect to x is going to be another expression, right?

340
00:16:39,463 --> 00:16:43,422
But we plug in these, we plug in the values

341
00:16:43,422 --> 00:16:45,481
of these numbers at the time in order to get

342
00:16:45,481 --> 00:16:48,708
the value of the gradient with respect to x.

343
00:16:48,708 --> 00:16:51,958
So what you could do is you could recursively plug in

344
00:16:51,958 --> 00:16:55,203
all of these expressions, right?

345
00:16:55,203 --> 00:16:58,239
Gradient with respect, z with respect to x

346
00:16:58,239 --> 00:17:01,442
is going to be a simple, simple expression, right?

347
00:17:01,442 --> 00:17:03,708
So in this case, if we have a multiplication node,

348
00:17:03,708 --> 00:17:05,739
gradient of z with respect to x is just going

349
00:17:05,739 --> 00:17:08,612
to be y, right, we know that,

350
00:17:08,612 --> 00:17:10,899
but the gradient of L with respect to z,

351
00:17:10,899 --> 00:17:12,812
this is probably a complex part of

352
00:17:12,812 --> 00:17:17,355
the graph in itself, right, so here's where we want to just,

353
00:17:17,355 --> 00:17:20,748
in this case, have this numerical, right?

354
00:17:20,748 --> 00:17:23,105
So as you said, basically this is going to be just

355
00:17:23,105 --> 00:17:25,423
a number coming down, right, a value,

356
00:17:25,423 --> 00:17:26,540
and then we just multiply it with

357
00:17:26,540 --> 00:17:30,719
the expression that we have for the local gradient.

358
00:17:30,719 --> 00:17:32,064
And I think this will be more clear when we go through

359
00:17:32,064 --> 00:17:35,647
a more complicated example in a few slides.

360
00:17:38,225 --> 00:17:40,685
Okay, so now the gradient of L with respect to y,

361
00:17:40,685 --> 00:17:44,284
we have exactly the same idea, where again,

362
00:17:44,284 --> 00:17:45,966
we use the chain rule, we have gradient of L

363
00:17:45,966 --> 00:17:48,169
with respect to z, times the gradient of z

364
00:17:48,169 --> 00:17:50,661
with respect to y, right, we use the chain rule,

365
00:17:50,661 --> 00:17:54,411
multiply these together and get our gradient.

366
00:17:55,848 --> 00:17:57,910
And then once we have these, we'll pass these on to

367
00:17:57,910 --> 00:18:00,816
the node directly before, or connected to this node.

368
00:18:00,816 --> 00:18:03,484
And so the main thing to take away from this

369
00:18:03,484 --> 00:18:06,920
is that at each node we just want to have our local gradient

370
00:18:06,920 --> 00:18:09,047
that we compute, just keep track of this,

371
00:18:09,047 --> 00:18:12,282
and then during backprop as we're receiving, you know,

372
00:18:12,282 --> 00:18:16,127
numerical values of gradients coming from upstream,

373
00:18:16,127 --> 00:18:18,271
we just take what that is, multiply it by

374
00:18:18,271 --> 00:18:20,077
the local gradient, and then this is

375
00:18:20,077 --> 00:18:23,741
what we then send back to the connected nodes,

376
00:18:23,741 --> 00:18:27,175
the next nodes going backwards, without having to care

377
00:18:27,175 --> 00:18:31,664
about anything else besides these immediate surroundings.

378
00:18:31,664 --> 00:18:33,795
So now we're going to go through another example,

379
00:18:33,795 --> 00:18:35,353
this time a little bit more complex,

380
00:18:35,353 --> 00:18:39,239
so we can see more why backprop is so useful.

381
00:18:39,239 --> 00:18:43,893
So in this case, our function is f of w and x,

382
00:18:43,893 --> 00:18:46,626
which is equal to one over one plus e

383
00:18:46,626 --> 00:18:49,576
to the negative of w-zero times x-zero

384
00:18:49,576 --> 00:18:52,619
plus w-one x-one, plus w-two, right?

385
00:18:52,619 --> 00:18:54,978
So again, the first step always is we want

386
00:18:54,978 --> 00:18:57,525
to write this out as a computational graph.

387
00:18:57,525 --> 00:18:59,798
So in this case we can see that in this graph, right,

388
00:18:59,798 --> 00:19:02,863
first we multiply together the w and x terms that we have,

389
00:19:02,863 --> 00:19:06,697
w-zero with x-zero, w-one with x-one,

390
00:19:06,697 --> 00:19:10,388
and w-two, then we add all of these together, right?

391
00:19:10,388 --> 00:19:13,471
Then we do, scale it by negative one,

392
00:19:14,525 --> 00:19:17,372
we take the exponential, we add one,

393
00:19:17,372 --> 00:19:21,372
and then finally we do one over this whole term.

394
00:19:22,533 --> 00:19:25,170
And then here I've also filled in values of these,

395
00:19:25,170 --> 00:19:27,581
so let's say given values that we have

396
00:19:27,581 --> 00:19:30,726
for the ws and xs, right, we can make a forward pass

397
00:19:30,726 --> 00:19:32,338
and basically compute what the value is

398
00:19:32,338 --> 00:19:35,171
at every stage of the computation.

399
00:19:37,091 --> 00:19:40,056
And here I've also written down here at the bottom

400
00:19:40,056 --> 00:19:42,986
the values, the expressions for some derivatives

401
00:19:42,986 --> 00:19:44,427
that are going to be helpful later on,

402
00:19:44,427 --> 00:19:49,339
so same as we did before with the simple example.

403
00:19:49,339 --> 00:19:51,820
Okay, so now then we're going to do backprop through here,

404
00:19:51,820 --> 00:19:53,327
right, so again, we're going to start at

405
00:19:53,327 --> 00:19:56,654
the very end of the graph, and so here again

406
00:19:56,654 --> 00:20:00,736
the gradient of the output with respect to the last variable

407
00:20:00,736 --> 00:20:04,074
is just one, it's just trivial,

408
00:20:04,074 --> 00:20:07,323
and so now moving backwards one step, right?

409
00:20:07,323 --> 00:20:10,768
So what's the gradient with respect to

410
00:20:10,768 --> 00:20:13,405
the input just before one over x?

411
00:20:13,405 --> 00:20:18,250
Well, so in this case, we know that the upstream gradient

412
00:20:18,250 --> 00:20:21,592
that we have coming down, right, is this red one, right?

413
00:20:21,592 --> 00:20:24,153
This is the upstream gradient that we have flowing down,

414
00:20:24,153 --> 00:20:26,938
and then now we need to find the local gradient, right,

415
00:20:26,938 --> 00:20:28,358
and the local gradient of this node,

416
00:20:28,358 --> 00:20:30,181
this node is one over x, right,

417
00:20:30,181 --> 00:20:33,117
so we have f of x equals one over x here in red,

418
00:20:33,117 --> 00:20:35,871
and the local gradient of this df over dx

419
00:20:35,871 --> 00:20:39,935
is equal to negative one over x-squared, right?

420
00:20:39,935 --> 00:20:43,700
So here we're going to take negative one over x-squared,

421
00:20:43,700 --> 00:20:45,845
and plug in the value of x that we had during

422
00:20:45,845 --> 00:20:50,012
this forward pass, 1.37, and so our final gradient

423
00:20:51,325 --> 00:20:52,805
with respect to this variable is going

424
00:20:52,805 --> 00:20:56,638
to be negative one over 1.37 squared times one

425
00:20:58,184 --> 00:20:59,934
equals negative 0.53.

426
00:21:04,382 --> 00:21:06,769
So moving back to the next node,

427
00:21:06,769 --> 00:21:09,023
we're going to go through the exact same process, right?

428
00:21:09,023 --> 00:21:12,868
So here, the gradient flowing from upstream

429
00:21:12,868 --> 00:21:16,007
is going to be negative 0.53, right,

430
00:21:16,007 --> 00:21:20,365
and here the local gradient, the node here is a plus one,

431
00:21:20,365 --> 00:21:25,203
and so now looking at our reference of derivatives at

432
00:21:25,203 --> 00:21:29,287
the bottom, we have that for a constant plus x,

433
00:21:29,287 --> 00:21:31,729
the local gradient is just one, right?

434
00:21:31,729 --> 00:21:34,209
So what's the gradient with respect

435
00:21:34,209 --> 00:21:37,376
to this variable using the chain rule?

436
00:21:42,883 --> 00:21:44,842
So it's going to be the upstream gradient

437
00:21:44,842 --> 00:21:48,925
of negative 0.53 times our local gradient of one,

438
00:21:50,021 --> 00:21:52,688
which is equal to negative 0.53.

439
00:21:55,849 --> 00:21:59,604
So let's keep moving backwards one more step.

440
00:21:59,604 --> 00:22:02,098
So here we have the exponential, right?

441
00:22:02,098 --> 00:22:05,022
So what's the upstream gradient coming down?

442
00:22:05,022 --> 00:22:08,536
[student speaking away from microphone]

443
00:22:08,536 --> 00:22:11,775
Right, so the upstream gradient is negative 0.53,

444
00:22:11,775 --> 00:22:14,358
what's the local gradient here?

445
00:22:15,402 --> 00:22:18,002
It's going to be the local gradient of e to the x, right?

446
00:22:18,002 --> 00:22:22,169
This is an exponential node, and so our chain rule

447
00:22:23,590 --> 00:22:25,555
is going to tell us that our gradient

448
00:22:25,555 --> 00:22:29,722
is going to be negative 0.53 times e to the power of x,

449
00:22:30,869 --> 00:22:32,495
which in this case is negative one,

450
00:22:32,495 --> 00:22:34,670
from our forward pass, and this is going to give us

451
00:22:34,670 --> 00:22:37,587
our final gradient of negative 0.2.

452
00:22:40,215 --> 00:22:42,882
Okay, so now one more node here,

453
00:22:44,234 --> 00:22:46,706
the next node is, that we reach, is going to be

454
00:22:46,706 --> 00:22:48,912
a multiplication with negative one, right?

455
00:22:48,912 --> 00:22:52,729
So here, what's the upstream gradient coming down?

456
00:22:52,729 --> 00:22:54,090
- [Student] Negative 0.2?

457
00:22:54,090 --> 00:22:56,565
- [Serena] Negative 0.2, right, and what's going to be

458
00:22:56,565 --> 00:23:01,510
the local gradient, can look at the reference sheet.

459
00:23:01,510 --> 00:23:03,049
It's going to be, what was it?

460
00:23:03,049 --> 00:23:03,889
I think I heard it.

461
00:23:03,889 --> 00:23:05,205
- [Student] That's minus one?

462
00:23:05,205 --> 00:23:09,680
- It's going to be minus one, exactly, yeah,

463
00:23:09,680 --> 00:23:13,282
because our local gradient says it's going to be,

464
00:23:13,282 --> 00:23:15,976
df over dx is a, right, and the value of a

465
00:23:15,976 --> 00:23:19,220
that we scaled x by is negative one here.

466
00:23:19,220 --> 00:23:21,806
So we have here that the gradient

467
00:23:21,806 --> 00:23:24,575
is negative one times negative 0.2,

468
00:23:24,575 --> 00:23:26,825
and so our gradient is 0.2.

469
00:23:29,169 --> 00:23:32,925
Okay, so now we've reached an addition node,

470
00:23:32,925 --> 00:23:35,816
and so in this case we have these two branches

471
00:23:35,816 --> 00:23:37,269
both connected to it, right?

472
00:23:37,269 --> 00:23:39,288
So what's the upstream gradient here?

473
00:23:39,288 --> 00:23:43,286
It's going to be 0.2, right, just as everything else,

474
00:23:43,286 --> 00:23:46,186
and here now the gradient with respect

475
00:23:46,186 --> 00:23:50,122
to each of these branches, it's an addition, right,

476
00:23:50,122 --> 00:23:52,586
and we saw from before in our simple example

477
00:23:52,586 --> 00:23:54,199
that when we have an addition node,

478
00:23:54,199 --> 00:23:56,226
the gradient with respect to each of the inputs

479
00:23:56,226 --> 00:23:59,266
to the addition is just going to be one, right?

480
00:23:59,266 --> 00:24:03,661
So here, our local gradient for looking at our top stream

481
00:24:03,661 --> 00:24:06,406
is going to be one times the upstream gradient

482
00:24:06,406 --> 00:24:10,573
of 0.2, which is going to give a total gradient of 0.2, right?

483
00:24:12,016 --> 00:24:14,219
And then we, for our bottom branch we'd do

484
00:24:14,219 --> 00:24:18,400
the same thing, right, our upstream gradient is 0.2,

485
00:24:18,400 --> 00:24:20,269
our local gradient is one again,

486
00:24:20,269 --> 00:24:23,277
and the total gradient is 0.2.

487
00:24:23,277 --> 00:24:26,110
So is everything clear about this?

488
00:24:27,581 --> 00:24:28,414
Okay.

489
00:24:30,649 --> 00:24:32,758
So we have a few more gradients to fill out,

490
00:24:32,758 --> 00:24:37,648
so moving back now we've reached w-zero and x-zero,

491
00:24:37,648 --> 00:24:41,086
and so here we have a multiplication node, right,

492
00:24:41,086 --> 00:24:44,169
so we saw the multiplication node from before,

493
00:24:44,169 --> 00:24:45,698
it just, the gradient with respect

494
00:24:45,698 --> 00:24:49,506
to one of the inputs just is the value of the other input.

495
00:24:49,506 --> 00:24:51,171
And so in this case, what's the gradient

496
00:24:51,171 --> 00:24:53,088
with respect to w-zero?

497
00:24:56,927 --> 00:24:58,795
- [Student] Minus 0.2.

498
00:24:58,795 --> 00:25:02,599
- Minus, I'm hearing minus 0.2, exactly.

499
00:25:02,599 --> 00:25:04,366
Yeah, so with respect to w-zero,

500
00:25:04,366 --> 00:25:07,866
we have our upstream gradient, 0.2, right,

501
00:25:09,747 --> 00:25:11,481
times our, this is the bottom one,

502
00:25:11,481 --> 00:25:13,781
times our value of x, which is negative one,

503
00:25:13,781 --> 00:25:16,472
we get negative 0.2 and we can do the same thing

504
00:25:16,472 --> 00:25:18,800
for our gradient with respect to x-zero.

505
00:25:18,800 --> 00:25:22,008
It's going to be 0.2 times the value of w-zero

506
00:25:22,008 --> 00:25:24,425
which is two, and we get 0.4.

507
00:25:26,525 --> 00:25:30,692
Okay, so here we've filled out most of these gradients,

508
00:25:32,200 --> 00:25:35,805
and so there was the question earlier

509
00:25:35,805 --> 00:25:40,128
about why this is simpler than just computing,

510
00:25:40,128 --> 00:25:43,071
deriving the analytic gradient, the expression with respect

511
00:25:43,071 --> 00:25:44,558
to any of these variables, right?

512
00:25:44,558 --> 00:25:47,166
And so you can see here, all we ever dealt with

513
00:25:47,166 --> 00:25:50,164
was expressions for local gradients

514
00:25:50,164 --> 00:25:52,165
that we had to write out, so once we had these expressions

515
00:25:52,165 --> 00:25:54,034
for local gradients, all we did

516
00:25:54,034 --> 00:25:56,850
was plug in the values for each of these that we have,

517
00:25:56,850 --> 00:25:59,656
and use the chain rule to numerically multiply this all

518
00:25:59,656 --> 00:26:01,532
the way backwards and get the gradients

519
00:26:01,532 --> 00:26:04,615
with respect to all of the variables.

520
00:26:08,779 --> 00:26:11,644
And so, you know, we can also fill out

521
00:26:11,644 --> 00:26:14,490
the gradients with respect to w-one and x-one here

522
00:26:14,490 --> 00:26:16,535
in exactly the same way, and so one thing

523
00:26:16,535 --> 00:26:19,763
that I want to note is that right when we're creating

524
00:26:19,763 --> 00:26:21,996
these computational graphs, we can define

525
00:26:21,996 --> 00:26:25,598
the computational nodes at any granularity that we want to.

526
00:26:25,598 --> 00:26:27,896
So in this case, we broke it down into

527
00:26:27,896 --> 00:26:29,813
the absolute simplest that we could, right,

528
00:26:29,813 --> 00:26:33,320
we broke it down into additions and multiplications,

529
00:26:33,320 --> 00:26:36,340
you know, it basically can't get any simpler than that,

530
00:26:36,340 --> 00:26:38,883
but in practice, right, we can group some of

531
00:26:38,883 --> 00:26:42,706
these nodes together into more complex nodes if we want.

532
00:26:42,706 --> 00:26:44,600
As long as we're able to write down

533
00:26:44,600 --> 00:26:47,245
the local gradient for that node, right?

534
00:26:47,245 --> 00:26:51,412
And so as an example, if we look at a sigmoid function,

535
00:26:53,153 --> 00:26:55,691
so I've defined the sigmoid function in

536
00:26:55,691 --> 00:26:57,861
the upper-right here, of a sigmoid of x

537
00:26:57,861 --> 00:27:00,854
is equal to one over one plus e to the negative x,

538
00:27:00,854 --> 00:27:03,404
and this is something that's a really common function

539
00:27:03,404 --> 00:27:05,996
that you'll see a lot in the rest of this class,

540
00:27:05,996 --> 00:27:09,904
and we can compute the gradient for this,

541
00:27:09,904 --> 00:27:12,686
we can write it out, and if we do actually go through

542
00:27:12,686 --> 00:27:16,427
the math of doing this analytically,

543
00:27:16,427 --> 00:27:18,173
we can get a nice expression at the end.

544
00:27:18,173 --> 00:27:22,598
So in this case it's equal to one minus sigma of x,

545
00:27:22,598 --> 00:27:26,609
so the output of this function times sigma of x, right?

546
00:27:26,609 --> 00:27:29,534
And so in cases where we have something like this,

547
00:27:29,534 --> 00:27:33,210
we could just take all the computations

548
00:27:33,210 --> 00:27:35,730
that we had in our graph that made up this sigmoid,

549
00:27:35,730 --> 00:27:37,038
and we could just replace it

550
00:27:37,038 --> 00:27:39,837
with one big node that's a sigmoid, right,

551
00:27:39,837 --> 00:27:43,655
because we do know the local gradient for this gate,

552
00:27:43,655 --> 00:27:48,564
it's this expression, d of the sigmoid of x over dx, right?

553
00:27:48,564 --> 00:27:51,260
So basically the important thing here is

554
00:27:51,260 --> 00:27:54,795
that you can, group any nodes that you want

555
00:27:54,795 --> 00:27:58,791
to make any sorts of a little bit more complex nodes,

556
00:27:58,791 --> 00:28:01,645
as long as you can write down the local gradient for this.

557
00:28:01,645 --> 00:28:04,268
And so all this is is basically a trade-off between,

558
00:28:04,268 --> 00:28:06,433
you know, how much math that you want to do

559
00:28:06,433 --> 00:28:11,043
in order to get a more, kind of concise and simpler graph,

560
00:28:11,043 --> 00:28:13,793
right, versus how simple you want

561
00:28:14,913 --> 00:28:16,849
each of your gradients to be, right?

562
00:28:16,849 --> 00:28:19,290
And then you can write out as complex of

563
00:28:19,290 --> 00:28:21,616
a computational graph that you want.

564
00:28:21,616 --> 00:28:23,358
Yeah, question?

565
00:28:23,358 --> 00:28:25,278
- [Student] This is a question on the graph itself,

566
00:28:25,278 --> 00:28:28,149
is there a reason that the first two multiplication nodes

567
00:28:28,149 --> 00:28:32,055
and the weights are not connected to a single addition node?

568
00:28:32,055 --> 00:28:33,486
- So they could also be connected into

569
00:28:33,486 --> 00:28:36,771
a single addition node, so the question was,

570
00:28:36,771 --> 00:28:39,835
is there a reason why w-zero and x-zero

571
00:28:39,835 --> 00:28:41,868
are not connected with w-two?

572
00:28:41,868 --> 00:28:44,167
All of these additions just connected together,

573
00:28:44,167 --> 00:28:46,179
and yeah, so the reason, the answer is

574
00:28:46,179 --> 00:28:48,432
that you can do that if you want,

575
00:28:48,432 --> 00:28:50,479
and in practice, maybe you would actually want to do that

576
00:28:50,479 --> 00:28:52,821
because this is still a very simple node, right?

577
00:28:52,821 --> 00:28:56,658
So in this case I just wrote this out into as simple

578
00:28:56,658 --> 00:29:01,414
as possible, where each node only had up to two inputs,

579
00:29:01,414 --> 00:29:04,499
but yeah, you could definitely do that.

580
00:29:04,499 --> 00:29:07,082
Any other questions about this?

581
00:29:08,682 --> 00:29:11,399
Okay, so the one thing that I really like

582
00:29:11,399 --> 00:29:13,681
about thinking about this like a computational graph

583
00:29:13,681 --> 00:29:15,406
is that I feel very comforted, right,

584
00:29:15,406 --> 00:29:17,934
like anytime I have to take a gradient,

585
00:29:17,934 --> 00:29:20,253
find gradients of something, even if the expression

586
00:29:20,253 --> 00:29:22,960
that I want to compute gradients of is really hairy,

587
00:29:22,960 --> 00:29:24,950
and really scary, you know, whether it's something like

588
00:29:24,950 --> 00:29:26,939
this sigmoid or something worse,

589
00:29:26,939 --> 00:29:30,702
I know that, you know, I could derive this if I want to,

590
00:29:30,702 --> 00:29:33,214
but really, if I just sit down and write it out

591
00:29:33,214 --> 00:29:35,024
in terms of a computational graph,

592
00:29:35,024 --> 00:29:37,033
I can go as simple as I need to

593
00:29:37,033 --> 00:29:40,405
to always be able to apply backprop and the chain rule,

594
00:29:40,405 --> 00:29:44,058
and be able to compute all the gradients that I need.

595
00:29:44,058 --> 00:29:46,623
And so this is something that you guys should think about

596
00:29:46,623 --> 00:29:51,204
when you're doing your homeworks, as basically, you know,

597
00:29:51,204 --> 00:29:53,438
anytime you're having trouble finding gradients of something

598
00:29:53,438 --> 00:29:55,474
just think about it as a computational graph,

599
00:29:55,474 --> 00:29:57,285
break it down into all of these parts,

600
00:29:57,285 --> 00:29:59,618
and then use the chain rule.

601
00:30:00,884 --> 00:30:04,040
Okay, and so, you know, so we talked about

602
00:30:04,040 --> 00:30:06,747
how we could group these set of nodes together

603
00:30:06,747 --> 00:30:10,558
into a sigmoid gate, and just to confirm, like,

604
00:30:10,558 --> 00:30:12,465
that this is actually exactly equivalent,

605
00:30:12,465 --> 00:30:14,098
we can plug this in, right?

606
00:30:14,098 --> 00:30:18,182
So we have that our input here to the sigmoid gate

607
00:30:18,182 --> 00:30:21,717
is going to be one, in green, and then we have

608
00:30:21,717 --> 00:30:25,800
that the output is going to be here, 0.73, right,

609
00:30:26,745 --> 00:30:28,208
and this'll work out if you plug

610
00:30:28,208 --> 00:30:29,943
it in to the sigmoid function.

611
00:30:29,943 --> 00:30:33,012
And so now if we want to do, if we want to take

612
00:30:33,012 --> 00:30:36,315
the gradient, and we want to treat this entire sigmoid

613
00:30:36,315 --> 00:30:39,525
as one node, now what we should do

614
00:30:39,525 --> 00:30:41,219
is we need to use this local gradient

615
00:30:41,219 --> 00:30:42,833
that we've derived up here, right?

616
00:30:42,833 --> 00:30:45,962
One minus sigmoid of x times the sigmoid of x.

617
00:30:45,962 --> 00:30:48,894
So if we plug this in, and here we know

618
00:30:48,894 --> 00:30:51,612
that the value of sigmoid of x was 0.73,

619
00:30:51,612 --> 00:30:54,267
so if we plug this value in we'll see that this,

620
00:30:54,267 --> 00:30:57,543
the value of this gradient is equal to 0.2, right,

621
00:30:57,543 --> 00:31:00,206
and so the value of this local gradient is 0.2,

622
00:31:00,206 --> 00:31:03,466
we multiply it by the x upstream gradient which is one,

623
00:31:03,466 --> 00:31:07,044
and we're going to get out exactly the same value

624
00:31:07,044 --> 00:31:09,267
of the gradient with respect to before the sigmoid gate,

625
00:31:09,267 --> 00:31:13,434
as if we broke it down into all of the smaller computations.

626
00:31:17,191 --> 00:31:19,325
Okay, and so as we're looking at what's happening, right,

627
00:31:19,325 --> 00:31:23,714
as we're taking these gradients going backwards

628
00:31:23,714 --> 00:31:25,168
through our computational graph,

629
00:31:25,168 --> 00:31:28,790
there's some patterns that you'll notice

630
00:31:28,790 --> 00:31:31,228
where there's some intuitive interpretation

631
00:31:31,228 --> 00:31:33,058
that we can give these, right?

632
00:31:33,058 --> 00:31:37,214
So we saw that the add gate is a gradient distributor right,

633
00:31:37,214 --> 00:31:41,129
when we passed through this addition gate here,

634
00:31:41,129 --> 00:31:43,930
which had two branches coming out of it,

635
00:31:43,930 --> 00:31:46,103
it took the gradient, the upstream gradient

636
00:31:46,103 --> 00:31:48,586
and it just distributed it, passed the exact same thing

637
00:31:48,586 --> 00:31:51,947
to both of the branches that were connected.

638
00:31:51,947 --> 00:31:55,156
So here's a couple more that we can think about.

639
00:31:55,156 --> 00:31:57,739
So what's a max gate look like?

640
00:31:59,214 --> 00:32:01,310
So we have a max gate here at the bottom, right,

641
00:32:01,310 --> 00:32:04,825
where the input's coming in are z and w,

642
00:32:04,825 --> 00:32:08,665
z has a value of two, w has a value of negative one,

643
00:32:08,665 --> 00:32:11,444
and then we took the max of this, which is two, right,

644
00:32:11,444 --> 00:32:14,251
and so we pass this down into the remainder

645
00:32:14,251 --> 00:32:16,877
of our computational graph.

646
00:32:16,877 --> 00:32:20,068
So now if we're taking the gradients with respect to this,

647
00:32:20,068 --> 00:32:23,223
the upstream gradient is, let's say two coming back, right,

648
00:32:23,223 --> 00:32:26,890
and what does this local gradient look like?

649
00:32:30,057 --> 00:32:31,753
So anyone, yes?

650
00:32:31,753 --> 00:32:35,011
- [Student] It'll be zero for one, and one for the other?

651
00:32:35,011 --> 00:32:35,862
- Right.

652
00:32:35,862 --> 00:32:39,435
[student speaking away from microphone]

653
00:32:39,435 --> 00:32:42,607
Exactly, so the answer that was given

654
00:32:42,607 --> 00:32:45,873
is that z will have a gradient of two,

655
00:32:45,873 --> 00:32:49,073
w will have a value, a gradient of zero,

656
00:32:49,073 --> 00:32:50,931
and so one of these is going to get

657
00:32:50,931 --> 00:32:53,112
the full value of the gradient just passed back,

658
00:32:53,112 --> 00:32:57,140
and routed to that variable, and then the other one

659
00:32:57,140 --> 00:33:00,223
will have a gradient of zero, and so,

660
00:33:01,328 --> 00:33:03,472
so we can think of this as kind of a gradient router, right,

661
00:33:03,472 --> 00:33:05,949
