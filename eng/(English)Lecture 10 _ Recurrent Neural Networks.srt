1
00:00:07,961 --> 00:00:08,794
- Okay.

2
00:00:08,794 --> 00:00:10,231
Can everyone hear me?

3
00:00:10,231 --> 00:00:11,075
Okay.

4
00:00:11,075 --> 00:00:12,153
Sorry for the delay.

5
00:00:12,153 --> 00:00:13,563
I had a bit of technical difficulty.

6
00:00:13,563 --> 00:00:15,891
Today was the first time I was trying to use my

7
00:00:15,891 --> 00:00:17,843
new touch bar Mac book pro for presenting,

8
00:00:17,843 --> 00:00:19,433
and none of the adapters are working.

9
00:00:19,433 --> 00:00:21,814
So, I had to switch laptops at the last minute.

10
00:00:21,814 --> 00:00:22,731
So, thanks.

11
00:00:24,003 --> 00:00:25,353
Sorry about that.

12
00:00:25,353 --> 00:00:27,451
So, today is lecture 10.

13
00:00:27,451 --> 00:00:30,420
We're talking about recurrent neural networks.

14
00:00:30,420 --> 00:00:33,003
So, as of, as usual, a couple administrative notes.

15
00:00:33,003 --> 00:00:34,835
So, We're working hard

16
00:00:35,891 --> 00:00:37,353
on assignment one grading.

17
00:00:37,353 --> 00:00:38,913
Those grades will probably be out

18
00:00:38,913 --> 00:00:40,921
sometime later today.

19
00:00:40,921 --> 00:00:42,531
Hopefully, they can get out

20
00:00:42,531 --> 00:00:44,382
before the A2 deadline.

21
00:00:44,382 --> 00:00:46,251
That's what I'm hoping for.

22
00:00:46,251 --> 00:00:50,361
On a related note, Assignment two is due today at 11:59 p.m.

23
00:00:50,361 --> 00:00:53,111
so, who's done with that already?

24
00:00:55,280 --> 00:00:56,633
About half you guys.

25
00:00:56,633 --> 00:00:58,273
So, you remember, I did warn you

26
00:00:58,273 --> 00:00:59,251
when the assignment went out

27
00:00:59,251 --> 00:01:01,091
that it was quite long, to start early.

28
00:01:01,091 --> 00:01:03,811
So, you were warned about that.

29
00:01:03,811 --> 00:01:06,561
But, hopefully, you guys have some late days left.

30
00:01:06,561 --> 00:01:08,160
Also, as another reminder,

31
00:01:08,160 --> 00:01:10,531
the midterm will be in class on Tuesday.

32
00:01:10,531 --> 00:01:12,321
If you kind of look around the lecture hall,

33
00:01:12,321 --> 00:01:13,782
there are not enough seats in this room

34
00:01:13,782 --> 00:01:15,881
to seat all the enrolled students in the class.

35
00:01:15,881 --> 00:01:18,113
So, we'll actually be having the midterm

36
00:01:18,113 --> 00:01:20,062
in several other lecture halls across campus.

37
00:01:20,062 --> 00:01:21,702
And we'll be sending out some more details

38
00:01:21,702 --> 00:01:26,099
on exactly where to go in the next couple of days.

39
00:01:26,099 --> 00:01:28,179
So a bit of a, another bit of announcement.

40
00:01:28,179 --> 00:01:29,161
We've been working on this sort of

41
00:01:29,161 --> 00:01:31,169
fun bit of extra credit thing for you to play with

42
00:01:31,169 --> 00:01:33,249
that we're calling the training game.

43
00:01:33,249 --> 00:01:34,950
This is this cool browser based experience,

44
00:01:34,950 --> 00:01:36,159
where you can go in

45
00:01:36,159 --> 00:01:37,480
and interactively train neural networks

46
00:01:37,480 --> 00:01:39,927
and tweak the hyper parameters during training.

47
00:01:39,927 --> 00:01:42,289
And this should be a really cool interactive way

48
00:01:42,289 --> 00:01:43,219
for you to practice

49
00:01:43,219 --> 00:01:45,395
some of these hyper parameter tuning skills

50
00:01:45,395 --> 00:01:47,646
that we've been talking about the last couple of lectures.

51
00:01:47,646 --> 00:01:48,681
So this is not required,

52
00:01:48,681 --> 00:01:51,629
but this, I think, will be a really useful experience

53
00:01:51,629 --> 00:01:53,190
to gain a little bit more intuition

54
00:01:53,190 --> 00:01:54,961
into how some of these hyper parameters work

55
00:01:54,961 --> 00:01:57,481
for different types of data sets in practice.

56
00:01:57,481 --> 00:01:59,339
So we're still working on getting

57
00:01:59,339 --> 00:02:00,950
all the bugs worked out of this setup,

58
00:02:00,950 --> 00:02:02,319
and we'll probably send out

59
00:02:02,319 --> 00:02:03,459
some more instructions

60
00:02:03,459 --> 00:02:04,499
on exactly how this will work

61
00:02:04,499 --> 00:02:05,790
in the next couple of days.

62
00:02:05,790 --> 00:02:07,209
But again, not required.

63
00:02:07,209 --> 00:02:08,280
But please do check it out.

64
00:02:08,280 --> 00:02:09,161
I think it'll be really fun

65
00:02:09,161 --> 00:02:11,009
and a really cool thing for you to play with.

66
00:02:11,009 --> 00:02:12,529
And will give you a bit of extra credit

67
00:02:12,529 --> 00:02:13,779
if you do some,

68
00:02:14,783 --> 00:02:15,897
if you end up working with this

69
00:02:15,897 --> 00:02:18,208
and doing a couple of runs with it.

70
00:02:18,208 --> 00:02:20,041
So, we'll again send out some more details about this

71
00:02:20,041 --> 00:02:23,458
soon once we get all the bugs worked out.

72
00:02:24,720 --> 00:02:25,819
As a reminder,

73
00:02:25,819 --> 00:02:28,139
last time we were talking about CNN Architectures.

74
00:02:28,139 --> 00:02:29,990
We kind of walked through the time line

75
00:02:29,990 --> 00:02:31,150
of some of the various winners

76
00:02:31,150 --> 00:02:33,081
of the image net classification challenge,

77
00:02:33,081 --> 00:02:35,006
kind of the breakthrough result.

78
00:02:35,006 --> 00:02:37,659
As we saw was the AlexNet architecture in 2012,

79
00:02:37,659 --> 00:02:39,979
which was a nine layer convolutional network.

80
00:02:39,979 --> 00:02:41,331
It did amazingly well,

81
00:02:41,331 --> 00:02:42,401
and it sort of kick started

82
00:02:42,401 --> 00:02:44,921
this whole deep learning revolution in computer vision,

83
00:02:44,921 --> 00:02:46,361
and kind of brought a lot of these models

84
00:02:46,361 --> 00:02:48,081
into the mainstream.

85
00:02:48,081 --> 00:02:50,038
Then we skipped ahead a couple years,

86
00:02:50,038 --> 00:02:52,470
and saw that in 2014 image net challenge,

87
00:02:52,470 --> 00:02:54,278
we had these two really interesting models,

88
00:02:54,278 --> 00:02:55,111
VGG and GoogLeNet,

89
00:02:55,111 --> 00:02:56,699
which were much deeper.

90
00:02:56,699 --> 00:02:57,532
So VGG was,

91
00:02:57,532 --> 00:02:59,870
they had a 16 and a 19 layer model,

92
00:02:59,870 --> 00:03:01,150
and GoogLeNet was, I believe,

93
00:03:01,150 --> 00:03:02,930
a 22 layer model.

94
00:03:02,930 --> 00:03:04,750
Although one thing that is kind of interesting

95
00:03:04,750 --> 00:03:05,811
about these models

96
00:03:05,811 --> 00:03:08,121
is that the 2014 image net challenge

97
00:03:08,121 --> 00:03:11,230
was right before batch normalization was invented.

98
00:03:11,230 --> 00:03:12,411
So at this time,

99
00:03:12,411 --> 00:03:13,979
before the invention of batch normalization,

100
00:03:13,979 --> 00:03:15,870
training these relatively deep models

101
00:03:15,870 --> 00:03:18,761
of roughly twenty layers was very challenging.

102
00:03:18,761 --> 00:03:20,510
So, in fact, both of these two models

103
00:03:20,510 --> 00:03:22,310
had to resort to a little bit of hackery

104
00:03:22,310 --> 00:03:24,869
in order to get their deep models to converge.

105
00:03:24,869 --> 00:03:28,579
So for VGG, they had the 16 and 19 layer models,

106
00:03:28,579 --> 00:03:31,870
but actually they first trained an 11 layer model,

107
00:03:31,870 --> 00:03:34,107
because that was what they could get to converge.

108
00:03:34,107 --> 00:03:36,600
And then added some extra random layers in the middle

109
00:03:36,600 --> 00:03:37,771
and then continued training,

110
00:03:37,771 --> 00:03:40,059
actually training the 16 and 19 layer models.

111
00:03:40,059 --> 00:03:42,361
So, managing this training process

112
00:03:42,361 --> 00:03:44,058
was very challenging in 2014

113
00:03:44,058 --> 00:03:46,539
before the invention of batch normalization.

114
00:03:46,539 --> 00:03:47,801
Similarly, for GoogLeNet,

115
00:03:47,801 --> 00:03:50,251
we saw that GoogLeNet has these auxiliary classifiers

116
00:03:50,251 --> 00:03:52,539
that were stuck into lower layers of the network.

117
00:03:52,539 --> 00:03:54,959
And these were not really needed for the class to,

118
00:03:54,959 --> 00:03:56,539
to get good classification performance.

119
00:03:56,539 --> 00:03:59,390
This was just sort of a way to cause

120
00:03:59,390 --> 00:04:01,201
extra gradient to be injected

121
00:04:01,201 --> 00:04:03,430
directly into the lower layers of the network.

122
00:04:03,430 --> 00:04:05,315
And this sort of,

123
00:04:05,315 --> 00:04:08,110
this again was before the invention of batch normalization

124
00:04:08,110 --> 00:04:09,430
and now once you have these networks

125
00:04:09,430 --> 00:04:10,411
with batch normalization,

126
00:04:10,411 --> 00:04:13,321
then you no longer need these slightly ugly hacks

127
00:04:13,321 --> 00:04:17,321
in order to get these deeper models to converge.

128
00:04:17,321 --> 00:04:20,801
Then we also saw in the 2015 image net challenge

129
00:04:20,801 --> 00:04:22,979
was this really cool model called ResNet,

130
00:04:22,979 --> 00:04:24,350
these residual networks

131
00:04:24,350 --> 00:04:26,321
that now have these shortcut connections

132
00:04:26,321 --> 00:04:28,310
that actually have these little residual blocks

133
00:04:28,310 --> 00:04:30,721
where we're going to take our input,

134
00:04:30,721 --> 00:04:32,280
pass it through the residual blocks,

135
00:04:32,280 --> 00:04:33,989
and then add that output of the,

136
00:04:33,989 --> 00:04:36,569
then add our input to the block,

137
00:04:36,569 --> 00:04:39,110
to the output from these convolutional layers.

138
00:04:39,110 --> 00:04:40,790
This is kind of a funny architecture,

139
00:04:40,790 --> 00:04:43,308
but it actually has two really nice properties.

140
00:04:43,308 --> 00:04:45,691
One is that if we just set all the weights

141
00:04:45,691 --> 00:04:47,481
in this residual block to zero,

142
00:04:47,481 --> 00:04:49,531
then this block is competing the identity.

143
00:04:49,531 --> 00:04:50,451
So in some way,

144
00:04:50,451 --> 00:04:52,881
it's relatively easy for this model

145
00:04:52,881 --> 00:04:55,681
to learn not to use the layers that it doesn't need.

146
00:04:55,681 --> 00:04:57,179
In addition, it kind of adds this

147
00:04:57,179 --> 00:05:00,110
interpretation to L2 regularization

148
00:05:00,110 --> 00:05:02,171
in the context of these neural networks,

149
00:05:02,171 --> 00:05:03,771
cause once you put L2 regularization,

150
00:05:03,771 --> 00:05:04,881
remember, on your,

151
00:05:04,881 --> 00:05:06,059
on the weights of your network,

152
00:05:06,059 --> 00:05:08,321
that's going to drive all the parameters towards zero.

153
00:05:08,321 --> 00:05:10,211
And maybe your standard convolutional architecture

154
00:05:10,211 --> 00:05:11,379
is driving towards zero.

155
00:05:11,379 --> 00:05:12,739
Maybe it doesn't make sense.

156
00:05:12,739 --> 00:05:14,481
But in the context of a residual network,

157
00:05:14,481 --> 00:05:16,561
if you drive all the parameters towards zero,

158
00:05:16,561 --> 00:05:18,051
that's kind of encouraging the model

159
00:05:18,051 --> 00:05:20,510
to not use layers that it doesn't need,

160
00:05:20,510 --> 00:05:21,891
because it will just drive those,

161
00:05:21,891 --> 00:05:23,579
the residual blocks towards the identity,

162
00:05:23,579 --> 00:05:26,310
whether or not needed for classification.

163
00:05:26,310 --> 00:05:28,538
The other really useful property of these residual networks

164
00:05:28,538 --> 00:05:31,371
has to do with the gradient flow in the backward paths.

165
00:05:31,371 --> 00:05:32,971
If you remember what happens at these addition gates

166
00:05:32,971 --> 00:05:34,361
in the backward pass,

167
00:05:34,361 --> 00:05:35,451
when upstream gradient is coming in

168
00:05:35,451 --> 00:05:37,110
through an addition gate,

169
00:05:37,110 --> 00:05:37,943
then it will split

170
00:05:37,943 --> 00:05:39,881
and fork along these two different paths.

171
00:05:39,881 --> 00:05:42,750
So then, when upstream gradient comes in,

172
00:05:42,750 --> 00:05:46,361
it'll take one path through these convolutional blocks,

173
00:05:46,361 --> 00:05:48,611
but it will also have a direct connection of the gradient

174
00:05:48,611 --> 00:05:50,811
through this residual connection.

175
00:05:50,811 --> 00:05:51,920
So then when you look at,

176
00:05:51,920 --> 00:05:53,211
when you imagine stacking many of these

177
00:05:53,211 --> 00:05:55,630
residual blocks on top of each other,

178
00:05:55,630 --> 00:05:57,091
and our network ends up with hundreds of,

179
00:05:57,091 --> 00:05:59,150
potentially hundreds of layers.

180
00:05:59,150 --> 00:06:00,578
Then, these residual connections

181
00:06:00,578 --> 00:06:02,841
give a sort of gradient super highway

182
00:06:02,841 --> 00:06:04,361
for gradients to flow backward

183
00:06:04,361 --> 00:06:05,561
through the entire network.

184
00:06:05,561 --> 00:06:08,299
And this allows it to train much easier

185
00:06:08,299 --> 00:06:09,630
and much faster.

186
00:06:09,630 --> 00:06:11,499
And actually allows these things to converge

187
00:06:11,499 --> 00:06:12,459
reasonably well,

188
00:06:12,459 --> 00:06:15,738
even when the model is potentially hundreds of layers deep.

189
00:06:15,738 --> 00:06:19,122
And this idea of managing gradient flow in your models

190
00:06:19,122 --> 00:06:21,550
is actually super important everywhere in machine learning.

191
00:06:21,550 --> 00:06:23,880
And super prevalent in recurrent networks as well.

192
00:06:23,880 --> 00:06:26,481
So we'll definitely revisit this idea of gradient flow

193
00:06:26,481 --> 00:06:28,564
later in today's lecture.

194
00:06:31,148 --> 00:06:34,377
So then, we kind of also saw a couple other more exotic,

195
00:06:34,377 --> 00:06:36,131
more recent CNN architectures last time,

196
00:06:36,131 --> 00:06:38,068
including DenseNet and FractalNet,

197
00:06:38,068 --> 00:06:39,771
and once you think about these architectures

198
00:06:39,771 --> 00:06:41,321
in terms of gradient flow,

199
00:06:41,321 --> 00:06:43,070
they make a little bit more sense.

200
00:06:43,070 --> 00:06:45,019
These things like DenseNet and FractalNet

201
00:06:45,019 --> 00:06:46,761
are adding these additional shortcut

202
00:06:46,761 --> 00:06:48,619
or identity connections inside the model.

203
00:06:48,619 --> 00:06:50,241
And if you think about what happens

204
00:06:50,241 --> 00:06:52,011
in the backwards pass for these models,

205
00:06:52,011 --> 00:06:53,411
these additional funny topologies

206
00:06:53,411 --> 00:06:55,251
are basically providing direct paths

207
00:06:55,251 --> 00:06:56,550
for gradients to flow

208
00:06:56,550 --> 00:06:58,139
from the loss at the end of the network

209
00:06:58,139 --> 00:07:00,571
more easily into all the different layers of the network.

210
00:07:00,571 --> 00:07:01,870
So I think that,

211
00:07:01,870 --> 00:07:04,491
again, this idea of managing gradient flow properly

212
00:07:04,491 --> 00:07:06,579
in your CNN Architectures

213
00:07:06,579 --> 00:07:07,771
is something that we've really seen

214
00:07:07,771 --> 00:07:09,760
a lot more in the last couple of years.

215
00:07:09,760 --> 00:07:11,721
And will probably see more moving forward

216
00:07:11,721 --> 00:07:15,221
as more exotic architectures are invented.

217
00:07:16,257 --> 00:07:18,189
We also saw this kind of nice plot,

218
00:07:18,189 --> 00:07:19,939
plotting performance of

219
00:07:19,939 --> 00:07:22,081
the number of flops versus the number of parameters

220
00:07:22,081 --> 00:07:24,331
versus the run time of these various models.

221
00:07:24,331 --> 00:07:25,790
And there's some interesting characteristics

222
00:07:25,790 --> 00:07:27,971
that you can dive in and see from this plot.

223
00:07:27,971 --> 00:07:31,110
One idea is that VGG and AlexNet

224
00:07:31,110 --> 00:07:32,801
have a huge number of parameters,

225
00:07:32,801 --> 00:07:34,291
and these parameters actually come

226
00:07:34,291 --> 00:07:35,961
almost entirely from the fully connected layers

227
00:07:35,961 --> 00:07:37,119
of the models.

228
00:07:37,119 --> 00:07:39,959
So AlexNet has something like roughly 62 million parameters,

229
00:07:39,959 --> 00:07:42,470
and if you look at that last fully connected layer,

230
00:07:42,470 --> 00:07:44,761
the final fully connected layer in AlexNet

231
00:07:44,761 --> 00:07:47,771
is going from an activation volume of six by six by 256

232
00:07:47,771 --> 00:07:51,190
into this fully connected vector of 496.

233
00:07:51,190 --> 00:07:53,310
So if you imagine what the weight matrix

234
00:07:53,310 --> 00:07:54,851
needs to look like at that layer,

235
00:07:54,851 --> 00:07:56,851
the weight matrix is gigantic.

236
00:07:56,851 --> 00:07:58,630
It's number of entries is six by six,

237
00:07:58,630 --> 00:08:01,921
six times six times 256 times 496.

238
00:08:01,921 --> 00:08:03,390
And if you multiply that out,

239
00:08:03,390 --> 00:08:04,491
you see that that single layer

240
00:08:04,491 --> 00:08:06,370
has 38 million parameters.

241
00:08:06,370 --> 00:08:07,977
So more than half of the parameters

242
00:08:07,977 --> 00:08:09,219
of the entire AlexNet model

243
00:08:09,219 --> 00:08:11,859
are just sitting in that last fully connected layer.

244
00:08:11,859 --> 00:08:13,339
And if you add up all the parameters

245
00:08:13,339 --> 00:08:15,971
in just the fully connected layers of AlexNet,

246
00:08:15,971 --> 00:08:17,739
including these other fully connected layers,

247
00:08:17,739 --> 00:08:20,721
you see something like 59 of the 62 million

248
00:08:20,721 --> 00:08:21,841
parameters in AlexNet

249
00:08:21,841 --> 00:08:24,241
are sitting in these fully connected layers.

250
00:08:24,241 --> 00:08:26,439
So then when we move other architectures,

251
00:08:26,439 --> 00:08:27,601
like GoogLeNet and ResNet,

252
00:08:27,601 --> 00:08:29,739
they do away with a lot of these large

253
00:08:29,739 --> 00:08:31,110
fully connected layers

254
00:08:31,110 --> 00:08:32,611
in favor of global average pooling

255
00:08:32,611 --> 00:08:33,699
at the end of the network.

256
00:08:33,699 --> 00:08:35,201
And this allows these networks to really cut,

257
00:08:35,201 --> 00:08:36,971
these nicer architectures,

258
00:08:36,971 --> 00:08:39,019
to really cut down the parameter count

259
00:08:39,019 --> 00:08:40,936
in these architectures.

260
00:08:44,463 --> 00:08:46,641
So that was kind of our brief recap

261
00:08:46,641 --> 00:08:48,771
of the CNN architectures that we saw last lecture,

262
00:08:48,771 --> 00:08:49,604
and then today,

263
00:08:49,604 --> 00:08:50,462
we're going to move to

264
00:08:50,462 --> 00:08:52,891
one of my favorite topics to talk about,

265
00:08:52,891 --> 00:08:56,321
which is recurrent neural networks.

266
00:08:56,321 --> 00:08:57,502
So, so far in this class,

267
00:08:57,502 --> 00:08:59,342
we've seen, what I like to think of

268
00:08:59,342 --> 00:09:01,759
as kind of a vanilla feed forward network,

269
00:09:01,759 --> 00:09:03,222
all of our network architectures have this flavor,

270
00:09:03,222 --> 00:09:05,160
where we receive some input

271
00:09:05,160 --> 00:09:07,353
and that input is a fixed size object,

272
00:09:07,353 --> 00:09:08,593
like an image or vector.

273
00:09:08,593 --> 00:09:11,691
That input is fed through some set of hidden layers

274
00:09:11,691 --> 00:09:13,850
and produces a single output,

275
00:09:13,850 --> 00:09:14,851
like a classifications,

276
00:09:14,851 --> 00:09:16,793
like a set of classifications scores

277
00:09:16,793 --> 00:09:18,876
over a set of categories.

278
00:09:20,071 --> 00:09:22,193
But in some context in machine learning,

279
00:09:22,193 --> 00:09:23,921
we want to have more flexibility

280
00:09:23,921 --> 00:09:25,942
in the types of data that our models can process.

281
00:09:25,942 --> 00:09:29,193
So once we move to this idea of recurrent neural networks,

282
00:09:29,193 --> 00:09:31,121
we have a lot more opportunities

283
00:09:31,121 --> 00:09:33,571
to play around with the types of input and output data

284
00:09:33,571 --> 00:09:35,313
that our networks can handle.

285
00:09:35,313 --> 00:09:37,329
So once we have recurrent neural networks,

286
00:09:37,329 --> 00:09:41,009
we can do what we call these one to many models.

287
00:09:41,009 --> 00:09:44,161
Or where maybe our input is some object of fixed size,

288
00:09:44,161 --> 00:09:45,032
like an image,

289
00:09:45,032 --> 00:09:47,500
but now our output is a sequence of variable length,

290
00:09:47,500 --> 00:09:48,721
such as a caption.

291
00:09:48,721 --> 00:09:49,801
Where different captions

292
00:09:49,801 --> 00:09:51,433
might have different numbers of words,

293
00:09:51,433 --> 00:09:54,081
so our output needs to be variable in length.

294
00:09:54,081 --> 00:09:56,491
We also might have many to one models,

295
00:09:56,491 --> 00:09:58,622
where our input could be variably sized.

296
00:09:58,622 --> 00:10:01,001
This might be something like a piece of text,

297
00:10:01,001 --> 00:10:03,782
and we want to say what is the sentiment of that text,

298
00:10:03,782 --> 00:10:06,161
whether it's positive or negative in sentiment.

299
00:10:06,161 --> 00:10:07,771
Or in a computer vision context,

300
00:10:07,771 --> 00:10:09,401
you might imagine taking as input a video,

301
00:10:09,401 --> 00:10:12,512
and that video might have a variable number of frames.

302
00:10:12,512 --> 00:10:14,921
And now we want to read this entire video

303
00:10:14,921 --> 00:10:16,401
of potentially variable length.

304
00:10:16,401 --> 00:10:17,439
And then at the end,

305
00:10:17,439 --> 00:10:18,742
make a classification decision

306
00:10:18,742 --> 00:10:20,142
about maybe what kind of activity or action

307
00:10:20,142 --> 00:10:22,721
is going on in that video.

308
00:10:22,721 --> 00:10:26,702
We also have a, we might also have problems

309
00:10:26,702 --> 00:10:28,091
where we want both the inputs

310
00:10:28,091 --> 00:10:29,931
and the output to be variable in length.

311
00:10:29,931 --> 00:10:31,491
We might see something like this

312
00:10:31,491 --> 00:10:33,433
in machine translation,

313
00:10:33,433 --> 00:10:35,870
where our input is some, maybe, sentence in English,

314
00:10:35,870 --> 00:10:37,302
which could have a variable length,

315
00:10:37,302 --> 00:10:39,393
and our output is maybe some sentence in French,

316
00:10:39,393 --> 00:10:41,633
which also could have a variable length.

317
00:10:41,633 --> 00:10:44,032
And crucially, the length of the English sentence

318
00:10:44,032 --> 00:10:46,801
might be different from the length of the French sentence.

319
00:10:46,801 --> 00:10:48,710
So we need some models that have the capacity

320
00:10:48,710 --> 00:10:50,782
to accept both variable length sequences

321
00:10:50,782 --> 00:10:53,931
on the input and on the output.

322
00:10:53,931 --> 00:10:56,331
Finally, we might also consider problems where

323
00:10:56,331 --> 00:10:58,153
our input is variably length,

324
00:10:58,153 --> 00:10:59,891
like something like a video sequence

325
00:10:59,891 --> 00:11:01,553
with a variable number of frames.

326
00:11:01,553 --> 00:11:02,920
And now we want to make a decision

327
00:11:02,920 --> 00:11:04,771
for each element of that input sequence.

328
00:11:04,771 --> 00:11:06,721
So in the context of videos,

329
00:11:06,721 --> 00:11:09,201
that might be making some classification decision

330
00:11:09,201 --> 00:11:11,891
along every frame of the video.

331
00:11:11,891 --> 00:11:13,281
And recurrent neural networks

332
00:11:13,281 --> 00:11:15,171
are this kind of general paradigm

333
00:11:15,171 --> 00:11:17,401
for handling variable sized sequence data

334
00:11:17,401 --> 00:11:19,302
that allow us to pretty naturally capture

335
00:11:19,302 --> 00:11:23,469
all of these different types of setups in our models.

336
00:11:24,349 --> 00:11:26,662
So recurring neural networks are actually important,

337
00:11:26,662 --> 00:11:30,030
even for some problems that have a fixed size input

338
00:11:30,030 --> 00:11:31,414
and a fixed size output.

339
00:11:31,414 --> 00:11:33,752
Recurrent neural networks can still be pretty useful.

340
00:11:33,752 --> 00:11:34,763
So in this example,

341
00:11:34,763 --> 00:11:35,982
we might want to do, for example,

342
00:11:35,982 --> 00:11:38,793
sequential processing of our input.

343
00:11:38,793 --> 00:11:40,721
So here, we're receiving a fixed size input

344
00:11:40,721 --> 00:11:41,774
like an image,

345
00:11:41,774 --> 00:11:43,732
and we want to make a classification decision

346
00:11:43,732 --> 00:11:46,227
about, like, what number is being shown in this image?

347
00:11:46,227 --> 00:11:48,854
But now, rather than just doing a single feed forward pass

348
00:11:48,854 --> 00:11:50,393
and making the decision all at once,

349
00:11:50,393 --> 00:11:52,702
this network is actually looking around the image

350
00:11:52,702 --> 00:11:55,553
and taking various glimpses of different parts of the image.

351
00:11:55,553 --> 00:11:58,233
And then after making some series of glimpses,

352
00:11:58,233 --> 00:11:59,882
then it makes its final decision

353
00:11:59,882 --> 00:12:01,742
as to what kind of number is present.

354
00:12:01,742 --> 00:12:03,233
So here, we had one,

355
00:12:03,233 --> 00:12:05,462
So here, even though our input and outputs,

356
00:12:05,462 --> 00:12:06,622
our input was an image,

357
00:12:06,622 --> 00:12:09,054
and our output was a classification decision,

358
00:12:09,054 --> 00:12:10,243
even this context,

359
00:12:10,243 --> 00:12:11,761
this idea of being able to handle

360
00:12:11,761 --> 00:12:14,121
variably length processing with recurrent neural networks

361
00:12:14,121 --> 00:12:17,473
can lead to some really interesting types of models.

362
00:12:17,473 --> 00:12:20,273
There's a really cool paper that I like

363
00:12:20,273 --> 00:12:22,140
that applied this same type of idea

364
00:12:22,140 --> 00:12:23,923
to generating new images.

365
00:12:23,923 --> 00:12:27,083
Where now, we want the model to synthesize brand new images

366
00:12:27,083 --> 00:12:29,723
that look kind of like the images it saw in training,

367
00:12:29,723 --> 00:12:32,489
and we can use a recurrent neural network architecture

368
00:12:32,489 --> 00:12:34,222
to actually paint these output images

369
00:12:34,222 --> 00:12:36,254
sort of one piece at a time in the output.

370
00:12:36,254 --> 00:12:37,342
You can see that,

371
00:12:37,342 --> 00:12:40,260
even though our output is this fixed size image,

372
00:12:40,260 --> 00:12:42,942
we can have these models that are working over time

373
00:12:42,942 --> 00:12:46,380
to compute parts of the output one at a time sequentially.

374
00:12:46,380 --> 00:12:48,577
And we can use recurrent neural networds

375
00:12:48,577 --> 00:12:51,662
for that type of setup as well.

376
00:12:51,662 --> 00:12:54,054
So after this sort of cool pitch

377
00:12:54,054 --> 00:12:55,854
about all these cool things that RNNs can do,

378
00:12:55,854 --> 00:12:58,785
you might wonder, like what exactly are these things?

379
00:12:58,785 --> 00:13:00,353
So in general, a recurrent neural network

380
00:13:00,353 --> 00:13:04,163
is this little, has this little recurrent core cell

381
00:13:04,163 --> 00:13:06,272
and it will take some input x,

382
00:13:06,272 --> 00:13:08,734
feed that input into the RNN,

383
00:13:08,734 --> 00:13:11,382
and that RNN has some internal hidden state,

384
00:13:11,382 --> 00:13:14,198
and that internal hidden state will be updated

385
00:13:14,198 --> 00:13:17,641
every time that the RNN reads a new input.

386
00:13:17,641 --> 00:13:19,654
And that internal hidden state

387
00:13:19,654 --> 00:13:21,243
will be then fed back to the model

388
00:13:21,243 --> 00:13:23,980
the next time it reads an input.

389
00:13:23,980 --> 00:13:26,334
And frequently, we will want our RNN"s

390
00:13:26,334 --> 00:13:28,822
to also produce some output at every time step,

391
00:13:28,822 --> 00:13:31,043
so we'll have this pattern where it will read an input,

392
00:13:31,043 --> 00:13:32,219
update its hidden state,

393
00:13:32,219 --> 00:13:34,469
and then produce an output.

394
00:13:35,814 --> 00:13:37,213
So then the question is

395
00:13:37,213 --> 00:13:39,862
what is the functional form of this recurrence relation

396
00:13:39,862 --> 00:13:40,961
that we're computing?

397
00:13:40,961 --> 00:13:42,923
So inside this little green RNN block,

398
00:13:42,923 --> 00:13:45,062
we're computing some recurrence relation,

399
00:13:45,062 --> 00:13:46,443
with a function f.

400
00:13:46,443 --> 00:13:49,094
So this function f will depend on some weights, w.

401
00:13:49,094 --> 00:13:52,822
It will accept the previous hidden state, h t - 1,

402
00:13:52,822 --> 00:13:55,374
as well as the input at the current state, x t,

403
00:13:55,374 --> 00:13:56,683
and this will output

404
00:13:56,683 --> 00:13:59,782
the next hidden state, or the updated hidden state,

405
00:13:59,782 --> 00:14:01,420
that we call h t.

406
00:14:01,420 --> 00:14:02,253
And now,

407
00:14:02,253 --> 00:14:04,382
then as we read the next input,

408
00:14:04,382 --> 00:14:05,283
this hidden state,

409
00:14:05,283 --> 00:14:06,894
this new hidden state, h t,

410
00:14:06,894 --> 00:14:08,774
will then just be passed into the same function

411
00:14:08,774 --> 00:14:11,552
as we read the next input, x t plus one.

412
00:14:11,552 --> 00:14:13,894
And now, if we wanted to produce some output

413
00:14:13,894 --> 00:14:15,273
at every time step of this network,

414
00:14:15,273 --> 00:14:20,203
we might attach some additional fully connected layers

415
00:14:20,203 --> 00:14:21,797
that read in this h t at every time step.

416
00:14:21,797 --> 00:14:23,407
And make that decision based

417
00:14:23,407 --> 00:14:27,327
on the hidden state at every time step.

418
00:14:27,327 --> 00:14:29,018
And one thing to note is that

419
00:14:29,018 --> 00:14:31,087
we use the same function, f w,

420
00:14:31,087 --> 00:14:32,495
and the same weights, w,

421
00:14:32,495 --> 00:14:35,662
at every time step of the computation.

422
00:14:36,921 --> 00:14:39,706
So then kind of the simplest function form

423
00:14:39,706 --> 00:14:40,634
that you can imagine

424
00:14:40,634 --> 00:14:43,434
is what we call this vanilla recurrent neural network.

425
00:14:43,434 --> 00:14:45,826
So here, we have this same functional form

426
00:14:45,826 --> 00:14:46,866
from the previous slide,

427
00:14:46,866 --> 00:14:49,191
where we're taking in our previous hidden state

428
00:14:49,191 --> 00:14:50,145
and our current input

429
00:14:50,145 --> 00:14:52,483
and we need to produce the next hidden state.

430
00:14:52,483 --> 00:14:54,954
And the kind of simplest thing you might imagine

431
00:14:54,954 --> 00:14:57,724
is that we have some weight matrix, w x h,

432
00:14:57,724 --> 00:15:00,124
that we multiply against the input, x t,

433
00:15:00,124 --> 00:15:03,162
as well as another weight matrix, w h h,

434
00:15:03,162 --> 00:15:05,615
that we multiply against the previous hidden state.

435
00:15:05,615 --> 00:15:07,226
So we make these two multiplications

436
00:15:07,226 --> 00:15:08,494
against our two states,

437
00:15:08,494 --> 00:15:09,327
add them together,

438
00:15:09,327 --> 00:15:10,924
and squash them through a tanh,

439
00:15:10,924 --> 00:15:13,514
so we get some kind of non linearity in the system.

440
00:15:13,514 --> 00:15:15,287
You might be wondering why we use a tanh here

441
00:15:15,287 --> 00:15:17,312
and not some other type of non-linearity?

442
00:15:17,312 --> 00:15:18,824
After all that we've said negative

443
00:15:18,824 --> 00:15:20,594
about tanh's in previous lectures,

444
00:15:20,594 --> 00:15:22,424
and I think we'll return a little bit to that

445
00:15:22,424 --> 00:15:23,257
later on when we talk about

446
00:15:23,257 --> 00:15:26,507
more advanced architectures, like lstm.

447
00:15:27,346 --> 00:15:28,695
So then, this,

448
00:15:28,695 --> 00:15:30,872
So then, in addition in this architecture,

449
00:15:30,872 --> 00:15:33,394
if we wanted to produce some y t at every time step,

450
00:15:33,394 --> 00:15:35,284
you might have another weight matrix, w,

451
00:15:35,284 --> 00:15:38,055
you might have another weight matrix

452
00:15:38,055 --> 00:15:39,055
that accepts this hidden state

453
00:15:39,055 --> 00:15:40,375
and then transforms it to some y

454
00:15:40,375 --> 00:15:42,724
to produce maybe some class score predictions

455
00:15:42,724 --> 00:15:44,826
at every time step.

456
00:15:44,826 --> 00:15:46,723
And when I think about recurrent neural networks,

457
00:15:46,723 --> 00:15:48,775
I kind of think about, you can also,

458
00:15:48,775 --> 00:15:50,634
you can kind of think of recurrent neural networks

459
00:15:50,634 --> 00:15:51,487
in two ways.

460
00:15:51,487 --> 00:15:53,713
One is this concept of having a hidden state

461
00:15:53,713 --> 00:15:57,095
that feeds back at itself, recurrently.

462
00:15:57,095 --> 00:15:59,135
But I find that picture a little bit confusing.

463
00:15:59,135 --> 00:16:01,073
And sometimes, I find it clearer

464
00:16:01,073 --> 00:16:04,546
to think about unrolling this computational graph

465
00:16:04,546 --> 00:16:05,914
for multiple time steps.

466
00:16:05,914 --> 00:16:08,382
And this makes the data flow of the hidden states

467
00:16:08,382 --> 00:16:09,914
and the inputs and the outputs and the weights

468
00:16:09,914 --> 00:16:11,786
maybe a little bit more clear.

469
00:16:11,786 --> 00:16:13,095
So then at the first time step,

470
00:16:13,095 --> 00:16:15,494
we'll have some initial hidden state h zero.

471
00:16:15,494 --> 00:16:18,914
This is usually initialized to zeros for most context,

472
00:16:18,914 --> 00:16:22,415
in most contexts, an then we'll have some input, x t.

473
00:16:22,415 --> 00:16:25,084
This initial hidden state, h zero,

474
00:16:25,084 --> 00:16:26,906
and our current input, x t,

475
00:16:26,906 --> 00:16:28,324
will go into our f w function.

476
00:16:28,324 --> 00:16:32,604
This will produce our next hidden state, h one.

477
00:16:32,604 --> 00:16:34,034
And then, we'll repeat this process

478
00:16:34,034 --> 00:16:36,154
when we receive the next input.

479
00:16:36,154 --> 00:16:38,295
So now our current h one and our x one,

480
00:16:38,295 --> 00:16:40,036
will go into that same f w,

481
00:16:40,036 --> 00:16:42,847
to produce our next output, h two.

482
00:16:42,847 --> 00:16:45,866
And this process will repeat over and over again,

483
00:16:45,866 --> 00:16:48,365
as we consume all of the input, x ts,

484
00:16:48,365 --> 00:16:50,866
in our sequence of inputs.

485
00:16:50,866 --> 00:16:52,116
And now, one thing to note, is that

486
00:16:52,116 --> 00:16:54,756
we can actually make this even more explicit

487
00:16:54,756 --> 00:16:58,036
and write the w matrix in our computational graph.

488
00:16:58,036 --> 00:16:59,058
And here you can see that

489
00:16:59,058 --> 00:17:01,434
we're re-using the same w matrix

490
00:17:01,434 --> 00:17:03,415
at every time step of the computation.

491
00:17:03,415 --> 00:17:06,145
So now every time that we have this little f w block,

492
00:17:06,145 --> 00:17:08,725
it's receiving a unique h and a unique x,

493
00:17:08,725 --> 00:17:11,007
but all of these blocks are taking the same w.

494
00:17:11,007 --> 00:17:12,236
And if you remember,

495
00:17:12,236 --> 00:17:16,116
we talked about how gradient flows in back propagation,

496
00:17:16,116 --> 00:17:17,058
when you re-use the same,

497
00:17:17,058 --> 00:17:19,218
when you re-use the same node multiple times

498
00:17:19,218 --> 00:17:20,786
in a computational graph,

499
00:17:20,786 --> 00:17:22,257
then remember during the backward pass,

500
00:17:22,257 --> 00:17:24,047
you end up summing the gradients

501
00:17:24,047 --> 00:17:25,257
into the w matrix

502
00:17:25,257 --> 00:17:28,218
when you're computing a d los d w.

503
00:17:28,218 --> 00:17:30,557
So, if you kind of think about

504
00:17:30,557 --> 00:17:32,526
the back propagation for this model,

505
00:17:32,526 --> 00:17:34,555
then you'll have a separate gradient for w

506
00:17:34,555 --> 00:17:36,506
flowing from each of those time steps,

507
00:17:36,506 --> 00:17:38,276
and then the final gradient for w

508
00:17:38,276 --> 00:17:39,586
will be the sum of all of those

509
00:17:39,586 --> 00:17:42,503
individual per time step gradiants.

510
00:17:43,615 --> 00:17:46,073
We can also write to this y t explicitly

511
00:17:46,073 --> 00:17:47,727
in this computational graph.

512
00:17:47,727 --> 00:17:50,668
So then, this output, h t, at every time step

513
00:17:50,668 --> 00:17:52,967
might feed into some other little neural network

514
00:17:52,967 --> 00:17:54,858
that can produce a y t,

515
00:17:54,858 --> 00:17:57,256
which might be some class scores, or something like that,

516
00:17:57,256 --> 00:17:59,087
at every time step.

517
00:17:59,087 --> 00:18:00,738
We can also make the loss more explicit.

518
00:18:00,738 --> 00:18:03,167
So in many cases, you might imagine producing,

519
00:18:03,167 --> 00:18:05,778
you might imagine that you have some ground truth label

520
00:18:05,778 --> 00:18:07,588
at every time step of your sequence,

521
00:18:07,588 --> 00:18:08,978
and then you'll compute some loss,

522
00:18:08,978 --> 00:18:10,487
some individual loss,

523
00:18:10,487 --> 00:18:11,596
at every time step of

524
00:18:11,596 --> 00:18:14,068
these outputs, y t's.

525
00:18:14,068 --> 00:18:14,915
And this loss might,

526
00:18:14,915 --> 00:18:17,556
it will frequently be something like soft max loss,

527
00:18:17,556 --> 00:18:19,497
in the case where you have, maybe,

528
00:18:19,497 --> 00:18:22,497
a ground truth label at every time step of the sequence.

529
00:18:22,497 --> 00:18:24,076
And now the final loss for the entire,

530
00:18:24,076 --> 00:18:25,395
for this entire training stop,

531
00:18:25,395 --> 00:18:27,887
will be the sum of these individual losses.

532
00:18:27,887 --> 00:18:30,485
So now, we had a scaler loss at every time step?

533
00:18:30,485 --> 00:18:32,818
And we just summed them up to get our final

534
00:18:32,818 --> 00:18:34,196
scaler loss at the top of the network.

535
00:18:34,196 --> 00:18:36,207
And now, if you think about,

536
00:18:36,207 --> 00:18:37,898
again, back propagation through this thing,

537
00:18:37,898 --> 00:18:38,896
we need, in order to train the model,

538
00:18:38,896 --> 00:18:40,215
we need to compute the gradient

539
00:18:40,215 --> 00:18:42,098
of the loss with respect to w.

540
00:18:42,098 --> 00:18:44,498
So, we'll have loss flowing from that final loss

541
00:18:44,498 --> 00:18:46,178
into each of these time steps.

542
00:18:46,178 --> 00:18:47,708
And then each of those time steps

543
00:18:47,708 --> 00:18:49,840
will compute a local gradient on the weights, w,

544
00:18:49,840 --> 00:18:52,010
which will all then be summed to give us our final

545
00:18:52,010 --> 00:18:54,343
gradient for the weights, w.

546
00:18:55,597 --> 00:18:58,268
Now if we have a, sort of, this many to one situation,

547
00:18:58,268 --> 00:19:01,188
where maybe we want to do something like sentiment analysis,

548
00:19:01,188 --> 00:19:03,348
then we would typically make that decision

549
00:19:03,348 --> 00:19:05,799
based on the final hidden state of this network.

550
00:19:05,799 --> 00:19:07,450
Because this final hidden state

551
00:19:07,450 --> 00:19:08,988
kind of summarizes all of the context

552
00:19:08,988 --> 00:19:11,868
from the entire sequence.

553
00:19:11,868 --> 00:19:14,788
Also, if we have a kind of a one to many situation,

554
00:19:14,788 --> 00:19:17,319
where we want to receive a fix sized input

555
00:19:17,319 --> 00:19:19,319
and then produce a variably sized output.

556
00:19:19,319 --> 00:19:22,698
Then you'll commonly use that fixed size input

557
00:19:22,698 --> 00:19:23,988
to initialize, somehow,

558
00:19:23,988 --> 00:19:26,050
the initial hidden state of the model,

559
00:19:26,050 --> 00:19:27,986
and now the recurrent network will tick

560
00:19:27,986 --> 00:19:30,079
for each cell in the output.

561
00:19:30,079 --> 00:19:32,748
And now, as you produce your variably sized output,

562
00:19:32,748 --> 00:19:36,915
you'll unroll the graph for each element in the output.

563
00:19:38,490 --> 00:19:41,370
So this, when we talk about the sequence to sequence models

564
00:19:41,370 --> 00:19:44,308
where you might do something like machine translation,

565
00:19:44,308 --> 00:19:46,099
where you take a variably sized input

566
00:19:46,099 --> 00:19:47,648
and a variably sized output.

567
00:19:47,648 --> 00:19:49,300
You can think of this as a combination

568
00:19:49,300 --> 00:19:50,719
of the many to one,

569
00:19:50,719 --> 00:19:52,398
plus a one to many.

570
00:19:52,398 --> 00:19:54,889
So, we'll kind of proceed in two stages,

571
00:19:54,889 --> 00:19:56,900
what we call an encoder and a decoder.

572
00:19:56,900 --> 00:19:58,119
So if you're the encoder,

573
00:19:58,119 --> 00:20:00,119
we'll receive the variably sized input,

574
00:20:00,119 --> 00:20:02,159
which might be your sentence in English,

575
00:20:02,159 --> 00:20:04,311
and then summarize that entire sentence

576
00:20:04,311 --> 00:20:08,110
using the final hidden state of the encoder network.

577
00:20:08,110 --> 00:20:11,894
And now we're in this many to one situation

578
00:20:11,894 --> 00:20:14,396
where we've summarized this entire variably sized input

579
00:20:14,396 --> 00:20:15,769
in this single vector,

580
00:20:15,769 --> 00:20:17,449
and now, we have a second decoder network,

581
00:20:17,449 --> 00:20:19,409
which is a one to many situation,

582
00:20:19,409 --> 00:20:21,740
which will input that single vector

583
00:20:21,740 --> 00:20:23,111
summarizing the input sentence

584
00:20:23,111 --> 00:20:25,487
and now produce this variably sized output,

585
00:20:25,487 --> 00:20:28,969
which might be your sentence in another language.

586
00:20:28,969 --> 00:20:30,980
And now in this variably sized output,

587
00:20:30,980 --> 00:20:32,820
we might make some predictions at every time step,

588
00:20:32,820 --> 00:20:34,609
maybe about what word to use.

589
00:20:34,609 --> 00:20:36,631
And you can imagine kind of training this entire thing

590
00:20:36,631 --> 00:20:38,199
by unrolling this computational graph

591
00:20:38,199 --> 00:20:40,608
summing the losses at the output sequence

592
00:20:40,608 --> 00:20:44,692
and just performing back propagation, as usual.

593
00:20:44,692 --> 00:20:46,391
So as a bit of a concrete example,

594
00:20:46,391 --> 00:20:47,729
one thing that we frequently use

595
00:20:47,729 --> 00:20:48,961
recurrent neural networks for,

596
00:20:48,961 --> 00:20:50,940
is this problem called language modeling.

597
00:20:50,940 --> 00:20:52,990
So in the language modeling problem,

598
00:20:52,990 --> 00:20:55,060
we want to read some sequence of,

599
00:20:55,060 --> 00:20:58,151
we want to have our network, sort of, understand

600
00:20:58,151 --> 00:21:00,908
how to produce natural language.

601
00:21:00,908 --> 00:21:04,071
So in the, so this, this might happen at the character level

602
00:21:04,071 --> 00:21:06,601
where our model will produce characters one at a time.

603
00:21:06,601 --> 00:21:08,389
This might also happen at the word level

604
00:21:08,389 --> 00:21:10,769
where our model will produce words one at a time.

605
00:21:10,769 --> 00:21:12,129
But in a very simple example,

606
00:21:12,129 --> 00:21:14,740
you can imagine this character level language model

607
00:21:14,740 --> 00:21:15,980
where we want,

608
00:21:15,980 --> 00:21:18,081
where the network will read some sequence of characters

609
00:21:18,081 --> 00:21:19,388
and then it needs to predict,

610
00:21:19,388 --> 00:21:22,780
what will the next character be in this stream of text?

611
00:21:22,780 --> 00:21:24,700
So in this example,

612
00:21:24,700 --> 00:21:27,460
we have this very small vocabulary of four letters,

613
00:21:27,460 --> 00:21:31,213
h, e, l, and o, and we have this example training sequence

614
00:21:31,213 --> 00:21:33,884
of the word hello, h, e, l, l, o.

615
00:21:33,884 --> 00:21:34,911
So during training,

616
00:21:34,911 --> 00:21:37,586
when we're training this language model,

617
00:21:37,586 --> 00:21:42,100
we will feed the characters of this training sequence

618
00:21:42,100 --> 00:21:46,119
as inputs, as x ts, to out input of our,

619
00:21:46,119 --> 00:21:49,689
we'll feed the characters of our training sequence,

620
00:21:49,689 --> 00:21:52,191
these will be the x ts that we feed in as the inputs

621
00:21:52,191 --> 00:21:53,980
to our recurrent neural network.

622
00:21:53,980 --> 00:21:56,168
And then, each of these inputs,

623
00:21:56,168 --> 00:21:57,678
it's a letter,

624
00:21:57,678 --> 00:21:58,890
and we need to figure out a way

625
00:21:58,890 --> 00:22:01,039
to represent letters in our network.

626
00:22:01,039 --> 00:22:02,759
So what we'll typically do is figure out

627
00:22:02,759 --> 00:22:05,100
what is our total vocabulary.

628
00:22:05,100 --> 00:22:07,460
In this case, our vocabulary has four elements.

629
00:22:07,460 --> 00:22:09,967
And each letter will be represented by a vector

630
00:22:09,967 --> 00:22:12,589
that has zeros in every slot but one,

631
00:22:12,589 --> 00:22:15,031
and a one for the slot in the vocabulary

632
00:22:15,031 --> 00:22:16,628
corresponding to that letter.

633
00:22:16,628 --> 00:22:18,092
In this little example,

634
00:22:18,092 --> 00:22:20,751
since our vocab has the four letters, h, e, l, o,

635
00:22:20,751 --> 00:22:22,324
then our input sequence,

636
00:22:22,324 --> 00:22:25,374
the h is represented by a four element vector

637
00:22:25,374 --> 00:22:26,535
with a one in the first slot

638
00:22:26,535 --> 00:22:28,684
and zero's in the other three slots.

639
00:22:28,684 --> 00:22:29,876
And we use the same sort of pattern

640
00:22:29,876 --> 00:22:31,306
to represent all the different letters

641
00:22:31,306 --> 00:22:33,139
in the input sequence.

642
00:22:34,914 --> 00:22:37,021
Now, during this forward pass

643
00:22:37,021 --> 00:22:38,575
of what this network is doing,

644
00:22:38,575 --> 00:22:39,965
at the first time step,

645
00:22:39,965 --> 00:22:41,874
it will receive the input letter h.

646
00:22:41,874 --> 00:22:45,285
That will go into the first RNN,

647
00:22:45,285 --> 00:22:46,714
to the RNN cell,

648
00:22:46,714 --> 00:22:48,594
and then we'll produce this output, y t,

649
00:22:48,594 --> 00:22:50,285
which is the network making predictions

650
00:22:50,285 --> 00:22:52,525
about for each letter in the vocabulary,

651
00:22:52,525 --> 00:22:54,411
which letter does it think is most likely

652
00:22:54,411 --> 00:22:56,024
going to come next.

653
00:22:56,024 --> 00:22:57,245
In this example,

654
00:22:57,245 --> 00:22:59,330
the correct output letter was e

655
00:22:59,330 --> 00:23:01,405
because our training sequence was hello,

656
00:23:01,405 --> 00:23:04,488
but the model is actually predicting,

657
00:23:05,477 --> 00:23:06,310
I think it's actually

658
00:23:06,310 --> 00:23:07,850
predicting o as the most likely letter.

659
00:23:07,850 --> 00:23:09,690
So in this case, this prediction was wrong

660
00:23:09,690 --> 00:23:11,210
and we would use softmaxt loss

661
00:23:11,210 --> 00:23:13,889
to quantify our unhappiness with these predictions.

662
00:23:13,889 --> 00:23:15,192
The next time step,

663
00:23:15,192 --> 00:23:16,680
we would feed in the second letter

664
00:23:16,680 --> 00:23:18,031
in the training sequence, e,

665
00:23:18,031 --> 00:23:19,741
and this process will repeat.

666
00:23:19,741 --> 00:23:22,181
We'll now represent e as a vector.

667
00:23:22,181 --> 00:23:24,232
Use that input vector together

668
00:23:24,232 --> 00:23:25,649
with the previous hidden state

669
00:23:25,649 --> 00:23:27,271
to produce a new hidden state

670
00:23:27,271 --> 00:23:28,792
and now use the second hidden state

671
00:23:28,792 --> 00:23:30,032
to, again, make predictions

672
00:23:30,032 --> 00:23:31,912
over every letter in the vocabulary.

673
00:23:31,912 --> 00:23:34,232
In this case, because our training sequence was hello,

674
00:23:34,232 --> 00:23:35,712
after the letter e,

675
00:23:35,712 --> 00:23:36,810
we want our model to predict l.

676
00:23:36,810 --> 00:23:37,893
In this case,

677
00:23:40,183 --> 00:23:41,832
our model may have very low predictions

678
00:23:41,832 --> 00:23:44,244
for the letter l, so we would incur high loss.

679
00:23:44,244 --> 00:23:46,794
And you kind of repeat this process over and over,

680
00:23:46,794 --> 00:23:50,343
and if you train this model with many different sequences,

681
00:23:50,343 --> 00:23:52,023
then eventually it should learn

682
00:23:52,023 --> 00:23:54,303
how to predict the next character in a sequence

683
00:23:54,303 --> 00:23:56,763
based on the context of all the previous characters

684
00:23:56,763 --> 00:23:58,596
that it's seen before.

685
00:23:59,893 --> 00:24:01,655
And now, if you think about what happens at test time,

686
00:24:01,655 --> 00:24:03,388
after we train this model,

687
00:24:03,388 --> 00:24:05,033
one thing that we might want to do with it

688
00:24:05,033 --> 00:24:07,594
is a sample from the model,

689
00:24:07,594 --> 00:24:09,673
and actually use this trained neural network model

690
00:24:09,673 --> 00:24:11,764
to synthesize new text

691
00:24:11,764 --> 00:24:13,592
that kind of looks similar in spirit

692
00:24:13,592 --> 00:24:15,103
to the text that it was trained on.

693
00:24:15,103 --> 00:24:16,312
The way that this will work

694
00:24:16,312 --> 00:24:17,833
is we'll typically see the model

695
00:24:17,833 --> 00:24:19,634
with some input prefix of text.

696
00:24:19,634 --> 00:24:22,716
In this case, the prefix is just the single letter h,

697
00:24:22,716 --> 00:24:24,327
and now we'll feed that letter h

698
00:24:24,327 --> 00:24:27,295
through the first time step of our recurrent neural network.

699
00:24:27,295 --> 00:24:30,335
It will product this distribution of scores

700
00:24:30,335 --> 00:24:32,916
over all the characters in the vocabulary.

701
00:24:32,916 --> 00:24:35,592
Now, at training time, we'll use these scores

702
00:24:35,592 --> 00:24:37,501
to actually sample from it.

703
00:24:37,501 --> 00:24:38,893
So we'll use a softmaxt function

704
00:24:38,893 --> 00:24:41,421
to convert those scores into a probability distribution

705
00:24:41,421 --> 00:24:44,162
and th