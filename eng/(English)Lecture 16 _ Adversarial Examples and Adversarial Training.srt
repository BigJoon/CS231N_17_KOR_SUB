1
00:00:08,407 --> 00:00:10,321
- Okay, sounds like it is.

2
00:00:10,321 --> 00:00:12,450
I'll be telling you about adversarial examples

3
00:00:12,450 --> 00:00:15,292
and adversarial training today.

4
00:00:15,292 --> 00:00:16,125
Thank you.

5
00:00:18,100 --> 00:00:20,606
As an overview, I will start off by telling you

6
00:00:20,606 --> 00:00:22,871
what adversarial examples are,

7
00:00:22,871 --> 00:00:26,017
and then I'll explain why they happen,

8
00:00:26,017 --> 00:00:28,670
why it's possible for them to exist.

9
00:00:28,670 --> 00:00:31,026
I'll talk a little bit about how adversarial examples

10
00:00:31,026 --> 00:00:33,580
pose real world security threats,

11
00:00:33,580 --> 00:00:36,248
that they can actually be used to compromise

12
00:00:36,248 --> 00:00:38,514
systems built on machine learning.

13
00:00:38,514 --> 00:00:41,302
I'll tell you what the defenses are so far,

14
00:00:41,302 --> 00:00:43,986
but mostly defenses are an open research problem

15
00:00:43,986 --> 00:00:47,586
that I hope some of you will move on to tackle.

16
00:00:47,586 --> 00:00:49,075
And then finally I'll tell you

17
00:00:49,075 --> 00:00:50,644
how to use adversarial examples

18
00:00:50,644 --> 00:00:53,156
to improve other machine learning algorithms

19
00:00:53,156 --> 00:00:56,020
even if you want to build a machine learning algorithm

20
00:00:56,020 --> 00:00:59,270
that won't face a real world adversary.

21
00:01:00,989 --> 00:01:05,273
Looking at the big picture and the context for this lecture,

22
00:01:05,273 --> 00:01:07,511
I think most of you are probably here

23
00:01:07,511 --> 00:01:10,390
because you've heard how incredibly powerful

24
00:01:10,390 --> 00:01:12,692
and successful machine learning is,

25
00:01:12,692 --> 00:01:14,478
that very many different tasks

26
00:01:14,478 --> 00:01:17,130
that could not be solved with software before

27
00:01:17,130 --> 00:01:20,188
are now solvable thanks to deep learning

28
00:01:20,188 --> 00:01:23,785
and convolutional networks and gradient descent.

29
00:01:23,785 --> 00:01:27,138
All of these technologies that are working really well.

30
00:01:27,138 --> 00:01:28,661
Until just a few years ago,

31
00:01:28,661 --> 00:01:30,988
these technologies didn't really work.

32
00:01:30,988 --> 00:01:33,868
In about 2013, we started to see

33
00:01:33,868 --> 00:01:37,036
that deep learning achieved human level performance

34
00:01:37,036 --> 00:01:39,018
at a lot of different tasks.

35
00:01:39,018 --> 00:01:40,993
We saw that convolutional nets

36
00:01:40,993 --> 00:01:43,228
could recognize objects and images

37
00:01:43,228 --> 00:01:47,165
and score about the same as people in those benchmarks,

38
00:01:47,165 --> 00:01:49,638
with the caveat that part of the reason that

39
00:01:49,638 --> 00:01:51,306
algorithms score as well as people

40
00:01:51,306 --> 00:01:52,761
is that people can't tell

41
00:01:52,761 --> 00:01:55,410
Alaskan Huskies from Siberian Huskies very well,

42
00:01:55,410 --> 00:01:58,559
but modulo the strangeness of the benchmarks

43
00:01:58,559 --> 00:02:01,781
deep learning caught up to about human level performance

44
00:02:01,781 --> 00:02:05,243
for object recognition in about 2013.

45
00:02:05,243 --> 00:02:08,547
That same year, we also saw that object recognition

46
00:02:08,547 --> 00:02:12,458
applied to human faces caught up to about human level.

47
00:02:12,458 --> 00:02:14,709
That suddenly we had computers

48
00:02:14,709 --> 00:02:17,874
that could recognize faces about as well as

49
00:02:17,874 --> 00:02:21,728
you or I could recognize faces of strangers.

50
00:02:21,728 --> 00:02:24,642
You can recognize the faces of your friends and family

51
00:02:24,642 --> 00:02:27,537
better than a computer, but when you're dealing

52
00:02:27,537 --> 00:02:30,152
with people that you haven't had a lot of experience with

53
00:02:30,152 --> 00:02:34,306
the computer caught up to us in about 2013.

54
00:02:34,306 --> 00:02:36,108
We also saw that computers caught up

55
00:02:36,108 --> 00:02:40,275
to humans for reading type written fonts in photos

56
00:02:41,183 --> 00:02:42,987
in about 2013.

57
00:02:42,987 --> 00:02:46,401
It even got the point that we could no longer use CAPTCHAs

58
00:02:46,401 --> 00:02:50,634
to tell whether a user of a webpage is human or not

59
00:02:50,634 --> 00:02:52,439
because the convolutional network

60
00:02:52,439 --> 00:02:56,496
is better at reading obfuscated text than a human is.

61
00:02:56,496 --> 00:02:58,406
So with this context today

62
00:02:58,406 --> 00:03:00,095
of deep learning working really well

63
00:03:00,095 --> 00:03:02,019
especially for computer vision

64
00:03:02,019 --> 00:03:05,136
it's a little bit unusual to think about

65
00:03:05,136 --> 00:03:07,800
the computer making a mistake.

66
00:03:07,800 --> 00:03:10,409
Before about 2013, nobody was ever surprised

67
00:03:10,409 --> 00:03:12,250
if the computer made a mistake.

68
00:03:12,250 --> 00:03:14,659
That was the rule not the exception,

69
00:03:14,659 --> 00:03:16,767
and so today's topic which is all about

70
00:03:16,767 --> 00:03:20,132
unusual mistakes that deep learning algorithms make

71
00:03:20,132 --> 00:03:24,000
this topic wasn't really a serious avenue of study

72
00:03:24,000 --> 00:03:28,099
until the algorithms started to work well most of the time,

73
00:03:28,099 --> 00:03:31,555
and now people study the way that they break

74
00:03:31,555 --> 00:03:36,412
now that that's actually the exception rather than the rule.

75
00:03:36,412 --> 00:03:39,168
An adversarial example is an example

76
00:03:39,168 --> 00:03:43,382
that has been carefully computed to be misclassified.

77
00:03:43,382 --> 00:03:45,864
In a lot of cases we're able to make the new image

78
00:03:45,864 --> 00:03:48,331
indistinguishable to a human observer

79
00:03:48,331 --> 00:03:50,226
from the original image.

80
00:03:50,226 --> 00:03:52,833
Here, I show you one where we start with a panda.

81
00:03:52,833 --> 00:03:54,528
On the left this is a panda

82
00:03:54,528 --> 00:03:57,297
that has not been modified in any way,

83
00:03:57,297 --> 00:03:59,855
and the convolutional network trained on the image

84
00:03:59,855 --> 00:04:03,849
in that dataset is able to recognize it as being a panda.

85
00:04:03,849 --> 00:04:05,524
One interesting thing is that the model

86
00:04:05,524 --> 00:04:08,064
doesn't have a whole lot of confidence in that decision.

87
00:04:08,064 --> 00:04:10,656
It assigns about 60% probability

88
00:04:10,656 --> 00:04:13,411
to this image being a panda.

89
00:04:13,411 --> 00:04:16,055
If we then compute exactly the way

90
00:04:16,055 --> 00:04:17,947
that we could modify the image

91
00:04:17,947 --> 00:04:20,624
to cause the convolutional network to make a mistake

92
00:04:20,624 --> 00:04:23,006
we find that the optimal direction

93
00:04:23,006 --> 00:04:27,017
to move all the pixels is given by this image in the middle.

94
00:04:27,017 --> 00:04:29,625
To a human it looks a lot like noise.

95
00:04:29,625 --> 00:04:31,176
It's not actually noise.

96
00:04:31,176 --> 00:04:33,244
It's carefully computed as a function

97
00:04:33,244 --> 00:04:34,883
of the parameters of the network.

98
00:04:34,883 --> 00:04:36,880
There's actually a lot of structure there.

99
00:04:36,880 --> 00:04:41,053
If we multiply that image of the structured attack

100
00:04:41,053 --> 00:04:45,373
by a very small coefficient and add it to the original panda

101
00:04:45,373 --> 00:04:48,131
we get an image that a human can't tell

102
00:04:48,131 --> 00:04:49,806
from the original panda.

103
00:04:49,806 --> 00:04:52,811
In fact, on this slide there is no difference

104
00:04:52,811 --> 00:04:54,447
between the panda on the left

105
00:04:54,447 --> 00:04:56,286
and the panda on the right.

106
00:04:56,286 --> 00:04:58,753
When we present the image to convolutional network

107
00:04:58,753 --> 00:05:01,921
we use 32-bit floating point values.

108
00:05:01,921 --> 00:05:05,142
The monitor here can only display eight bits

109
00:05:05,142 --> 00:05:07,712
of color resolution, and we have made a change

110
00:05:07,712 --> 00:05:09,279
that's just barely too small

111
00:05:09,279 --> 00:05:12,613
to affect the smallest of those eight bits,

112
00:05:12,613 --> 00:05:14,411
but it effects the other 24

113
00:05:14,411 --> 00:05:17,345
of the 32-bit floating point representation,

114
00:05:17,345 --> 00:05:19,342
and that little tiny change is enough

115
00:05:19,342 --> 00:05:21,198
to fool the convolutional network

116
00:05:21,198 --> 00:05:25,365
into recognizing this image of a panda as being a gibbon.

117
00:05:26,420 --> 00:05:28,056
Another interesting thing is that

118
00:05:28,056 --> 00:05:29,857
it doesn't just change the class.

119
00:05:29,857 --> 00:05:32,881
It's not that we just barely found the decision boundary

120
00:05:32,881 --> 00:05:34,734
and just barely stepped across it.

121
00:05:34,734 --> 00:05:37,702
The convolutional network actually has much more confidence

122
00:05:37,702 --> 00:05:40,172
in its incorrect prediction,

123
00:05:40,172 --> 00:05:42,097
that the image on the right is a gibbon,

124
00:05:42,097 --> 00:05:45,891
than it had for the original being a panda.

125
00:05:45,891 --> 00:05:47,588
On the right, it believes that the image

126
00:05:47,588 --> 00:05:50,752
is a gibbon with 99.9% probability,

127
00:05:50,752 --> 00:05:53,848
so before it thought that there was about

128
00:05:53,848 --> 00:05:57,341
1/3 chance that it was something other than a panda,

129
00:05:57,341 --> 00:06:00,238
and now it's about as certain as it can possibly be

130
00:06:00,238 --> 00:06:02,417
that it's a gibbon.

131
00:06:02,417 --> 00:06:05,585
As a little bit of history, people have studied ways

132
00:06:05,585 --> 00:06:07,942
of computing attacks to fool

133
00:06:07,942 --> 00:06:09,656
different machine learning models

134
00:06:09,656 --> 00:06:13,596
since at least about 2004, and maybe earlier.

135
00:06:13,596 --> 00:06:15,305
For a long time this was done in the context

136
00:06:15,305 --> 00:06:17,772
of fooling spam detectors.

137
00:06:17,772 --> 00:06:21,406
In about 2013, Battista Biggio found

138
00:06:21,406 --> 00:06:24,161
that you could fool neural networks in this way,

139
00:06:24,161 --> 00:06:27,080
and around the same time my colleague, Christian Szegedy,

140
00:06:27,080 --> 00:06:29,311
found that you could make this kind of attack

141
00:06:29,311 --> 00:06:30,948
against deep neural networks

142
00:06:30,948 --> 00:06:33,147
just by using an optimization algorithm

143
00:06:33,147 --> 00:06:36,368
to search on the input of the image.

144
00:06:36,368 --> 00:06:37,952
A lot of what I'll be telling you about today

145
00:06:37,952 --> 00:06:40,111
is my own follow-up work on this topic,

146
00:06:40,111 --> 00:06:43,496
but I've spent a lot of my career over the past few years

147
00:06:43,496 --> 00:06:46,539
understanding why these attacks are possible

148
00:06:46,539 --> 00:06:50,706
and why it's so easy to fool these convolutional networks.

149
00:06:52,279 --> 00:06:54,129
When my colleague, Christian,

150
00:06:54,129 --> 00:06:57,104
first discovered this phenomenon

151
00:06:57,104 --> 00:07:01,237
independently from Battista Biggio but around the same time,

152
00:07:01,237 --> 00:07:04,404
he found that it was actually a result

153
00:07:05,652 --> 00:07:08,206
of a visualization he was trying to make.

154
00:07:08,206 --> 00:07:10,260
He wasn't studying security.

155
00:07:10,260 --> 00:07:12,401
He wasn't studying how to fool a neural network.

156
00:07:12,401 --> 00:07:14,686
Instead, he had a convolutional network

157
00:07:14,686 --> 00:07:16,736
that could recognize objects very well,

158
00:07:16,736 --> 00:07:19,134
and he wants to understand how it worked,

159
00:07:19,134 --> 00:07:23,301
so he thought that maybe he could take an image of a scene,

160
00:07:24,156 --> 00:07:26,350
for example a picture of a ship,

161
00:07:26,350 --> 00:07:28,784
and he could gradually transform that image

162
00:07:28,784 --> 00:07:31,428
into something that the network would recognize

163
00:07:31,428 --> 00:07:33,622
as being an airplane.

164
00:07:33,622 --> 00:07:35,513
Over the course of that transformation,

165
00:07:35,513 --> 00:07:38,844
he could see how the features of the input change.

166
00:07:38,844 --> 00:07:40,860
You might expect that maybe the background                                                                             167 00:07:34,360 --> 00:07:37,692 would turn blue to look like the sky behind an airplane,

167
00:07:44,192 --> 00:07:46,424
or you might expect that the ship

168
00:07:46,424 --> 00:07:48,883
would grow wings to look more like an airplane.

169
00:07:48,883 --> 00:07:51,209
You could conclude from that that the convolution

170
00:07:51,209 --> 00:07:56,124
uses the blue sky or uses the wings to recognize airplanes.

171
00:07:56,124 --> 00:07:59,019
That's actually not really what happened at all.

172
00:07:59,019 --> 00:08:01,212
Each of these panels here shows an animation

173
00:08:01,212 --> 00:08:03,737
that you read left to right, top to bottom.

174
00:08:03,737 --> 00:08:06,848
Each panel is another step of gradient ascent

175
00:08:06,848 --> 00:08:11,441
on the log probability that the input is an airplane

176
00:08:11,441 --> 00:08:14,067
according to a convolutional net model,

177
00:08:14,067 --> 00:08:18,833
and then we follow the gradient on the input to the image.

178
00:08:18,833 --> 00:08:20,585
You're probably used to following the gradient

179
00:08:20,585 --> 00:08:22,222
on the parameters of a model.

180
00:08:22,222 --> 00:08:23,840
You can use the back propagation algorithm

181
00:08:23,840 --> 00:08:26,182
to compute the gradient on the input image

182
00:08:26,182 --> 00:08:28,001
using exactly the same procedure

183
00:08:28,001 --> 00:08:29,816
that you would use to compute the gradient

184
00:08:29,816 --> 00:08:31,976
on the parameters.

185
00:08:31,976 --> 00:08:34,803
In this animation of the ship in the upper left,

186
00:08:34,803 --> 00:08:37,918
we see five panels that all look basically the same.

187
00:08:37,918 --> 00:08:39,339
Gradient descent doesn't seem

188
00:08:39,339 --> 00:08:40,793
to have moved the image at all,

189
00:08:40,793 --> 00:08:43,496
but by the last panel the network is completely confident

190
00:08:43,496 --> 00:08:45,287
that this is an airplane.

191
00:08:45,287 --> 00:08:47,580
When you first code up this kind of experiment,

192
00:08:47,580 --> 00:08:49,433
especially if you don't know what's going to happen,

193
00:08:49,433 --> 00:08:51,881
it feels a little bit like you have a bug in your script

194
00:08:51,881 --> 00:08:52,937
and you're just displaying

195
00:08:52,937 --> 00:08:54,761
the same image over and over again.

196
00:08:54,761 --> 00:08:55,952
The first time I did it,

197
00:08:55,952 --> 00:08:58,419
I couldn't believe it was happening,

198
00:08:58,419 --> 00:09:00,540
and I had to open up the images in NumPy,

199
00:09:00,540 --> 00:09:02,355
and take the difference of them,

200
00:09:02,355 --> 00:09:03,813
and make sure that there was actually

201
00:09:03,813 --> 00:09:07,359
a non-zero difference in there, but there is.

202
00:09:07,359 --> 00:09:09,250
I show several different animations here

203
00:09:09,250 --> 00:09:12,333
of a ship, a car, a cat, and a truck.

204
00:09:13,172 --> 00:09:15,817
The only one where I actually see any change at all

205
00:09:15,817 --> 00:09:18,250
is the image of the cat.

206
00:09:18,250 --> 00:09:21,038
The color of the cat's face changes a little bit,

207
00:09:21,038 --> 00:09:23,646
and maybe it becomes a little bit more

208
00:09:23,646 --> 00:09:25,969
like the color of a metal airplane.

209
00:09:25,969 --> 00:09:28,470
Other than that, I don't see any changes

210
00:09:28,470 --> 00:09:29,895
in any of these animations,

211
00:09:29,895 --> 00:09:33,908
and I don't see anything very suggestive of an airplane.

212
00:09:33,908 --> 00:09:36,985
So gradient descent, rather than turning the input

213
00:09:36,985 --> 00:09:39,240
into an example of an airplane,

214
00:09:39,240 --> 00:09:42,818
has found an image that fools the network

215
00:09:42,818 --> 00:09:45,519
into thinking that the input is an airplane.

216
00:09:45,519 --> 00:09:47,050
And if we were malicious attackers

217
00:09:47,050 --> 00:09:49,567
we didn't even have to work very hard to figure out

218
00:09:49,567 --> 00:09:51,102
how to fool the network.

219
00:09:51,102 --> 00:09:52,234
We just asked the network

220
00:09:52,234 --> 00:09:53,837
to give us an image of an airplane,

221
00:09:53,837 --> 00:09:56,516
and it gave us something that fools it into thinking

222
00:09:56,516 --> 00:09:59,016
that the input is an airplane.

223
00:10:00,310 --> 00:10:02,727
When Christian first published this work,

224
00:10:02,727 --> 00:10:05,175
a lot of articles came out with titles like,

225
00:10:05,175 --> 00:10:07,210
The Flaw Looking At Every Deep Neural Network,

226
00:10:07,210 --> 00:10:10,590
or Deep Learning has Deep Flaws.

227
00:10:10,590 --> 00:10:12,577
It's important to remember that these vulnerabilities

228
00:10:12,577 --> 00:10:15,903
apply to essentially every machine learning algorithm

229
00:10:15,903 --> 00:10:18,625
that we've studied so far.

230
00:10:18,625 --> 00:10:20,458
Some of them like RBF networks

231
00:10:20,458 --> 00:10:22,906
and partisan density estimators

232
00:10:22,906 --> 00:10:24,942
are able to resist this effect somewhat,

233
00:10:24,942 --> 00:10:27,908
but even very simple machine learning algorithms

234
00:10:27,908 --> 00:10:32,069
are highly vulnerable to adversarial examples.

235
00:10:32,069 --> 00:10:33,870
In this image, I show an animation

236
00:10:33,870 --> 00:10:37,038
of what happens when we attack a linear model,

237
00:10:37,038 --> 00:10:38,890
so it's not a deep algorithm at all.

238
00:10:38,890 --> 00:10:41,370
It's just a shallow softmax model.

239
00:10:41,370 --> 00:10:45,440
You multiply by a matrix, you add a vector of bias terms,

240
00:10:45,440 --> 00:10:47,223
you apply the softmax function,

241
00:10:47,223 --> 00:10:48,846
and you've got your probability distribution

242
00:10:48,846 --> 00:10:51,249
over the 10 MNIST classes.

243
00:10:51,249 --> 00:10:54,022
At the upper left, I start with an image of a nine,

244
00:10:54,022 --> 00:10:57,161
and then as we move left to right, top to bottom,

245
00:10:57,161 --> 00:11:00,141
I gradually transform it to be a zero.

246
00:11:00,141 --> 00:11:02,053
Where I've drawn the yellow box,

247
00:11:02,053 --> 00:11:05,640
the model assigns high probability to it being a zero.

248
00:11:05,640 --> 00:11:08,323
I forget exactly what my threshold was for high probability,

249
00:11:08,323 --> 00:11:11,856
but I think it was around 0.9 or so.

250
00:11:11,856 --> 00:11:13,503
Then as we move to the second row,

251
00:11:13,503 --> 00:11:15,462
I transform it into a one,

252
00:11:15,462 --> 00:11:17,136
and the second yellow box indicates

253
00:11:17,136 --> 00:11:18,932
where we've successfully fooled the model

254
00:11:18,932 --> 00:11:21,663
into thinking it's a one with high probability.

255
00:11:21,663 --> 00:11:23,878
And then as you read the rest of the yellow boxes

256
00:11:23,878 --> 00:11:25,250
left to right, top to bottom,

257
00:11:25,250 --> 00:11:27,691
we go through the twos, threes, fours, and so on,

258
00:11:27,691 --> 00:11:29,646
until finally at the lower right

259
00:11:29,646 --> 00:11:31,855
we have a nine that has a yellow box around it,

260
00:11:31,855 --> 00:11:33,794
and it actually looks like a nine,

261
00:11:33,794 --> 00:11:35,001
but in this case the only reason

262
00:11:35,001 --> 00:11:36,185
it actually looks like a nine

263
00:11:36,185 --> 00:11:39,369
is that we started the whole process with a nine.

264
00:11:39,369 --> 00:11:43,042
We successfully swept through all 10 classes of MNIST

265
00:11:43,042 --> 00:11:46,892
without substantially changing the image of the digit

266
00:11:46,892 --> 00:11:50,578
in any way that would interfere with human recognition.

267
00:11:50,578 --> 00:11:54,745
This linear model was actually extremely easy to fool.

268
00:11:55,879 --> 00:11:57,791
Besides linear models, we've also seen

269
00:11:57,791 --> 00:12:01,480
that we can fool many different kinds of linear models

270
00:12:01,480 --> 00:12:04,588
including logistic regression and SVMs.

271
00:12:04,588 --> 00:12:07,118
We've also found that we can fool decision trees,

272
00:12:07,118 --> 00:12:11,285
and to a lesser extent, nearest neighbors classifiers.

273
00:12:13,049 --> 00:12:16,605
We wanted to explain exactly why this happens.

274
00:12:16,605 --> 00:12:20,122
Back in about 2014, after we'd published the original paper

275
00:12:20,122 --> 00:12:22,934
where we'd said that these problems exist,

276
00:12:22,934 --> 00:12:25,929
we were trying to figure out why they happen.

277
00:12:25,929 --> 00:12:27,394
When we wrote our first paper,

278
00:12:27,394 --> 00:12:30,517
we thought that basically this is a form of overfitting,

279
00:12:30,517 --> 00:12:34,087
that you have a very complicated deep neural network,

280
00:12:34,087 --> 00:12:36,086
it learns to fit the training set,

281
00:12:36,086 --> 00:12:39,604
its behavior on the test set is somewhat undefined,

282
00:12:39,604 --> 00:12:41,858
and then it makes random mistakes

283
00:12:41,858 --> 00:12:44,023
that an attacker can exploit.

284
00:12:44,023 --> 00:12:45,778
Let's walk through what that story looks like

285
00:12:45,778 --> 00:12:47,650
somewhat concretely.

286
00:12:47,650 --> 00:12:50,885
I have here a training set of three blue X's

287
00:12:50,885 --> 00:12:53,105
and three green O's.

288
00:12:53,105 --> 00:12:54,364
We want to make a classifier

289
00:12:54,364 --> 00:12:57,435
that can recognize X's and recognize O's.

290
00:12:57,435 --> 00:12:59,806
We have a very complicated classifier

291
00:12:59,806 --> 00:13:01,972
that can easily fit the training set,

292
00:13:01,972 --> 00:13:03,633
so we represent everywhere it believes

293
00:13:03,633 --> 00:13:06,486
X's should be with blobs of blue color,

294
00:13:06,486 --> 00:13:08,369
and I've drawn a blob of blue

295
00:13:08,369 --> 00:13:10,629
around all of the training set X's,

296
00:13:10,629 --> 00:13:13,157
so it correctly classifies the training set.

297
00:13:13,157 --> 00:13:17,840
It also has a blob of green mass showing where the O's are,

298
00:13:17,840 --> 00:13:21,360
and it successfully fits all of the green training set O's,

299
00:13:21,360 --> 00:13:24,482
but then because this is a very complicated function

300
00:13:24,482 --> 00:13:26,850
and it has just way more parameters

301
00:13:26,850 --> 00:13:29,998
than it actually needs to represent the training task,

302
00:13:29,998 --> 00:13:33,168
it throws little blobs of probability mass

303
00:13:33,168 --> 00:13:35,680
around the rest of space randomly.

304
00:13:35,680 --> 00:13:37,566
On the left there's a blob of green space

305
00:13:37,566 --> 00:13:40,121
that's kind of near the training set X's,

306
00:13:40,121 --> 00:13:42,032
and I've drawn a red X there to show

307
00:13:42,032 --> 00:13:43,740
that maybe this would be an adversarial example

308
00:13:43,740 --> 00:13:46,441
where we expect the classification to be X,

309
00:13:46,441 --> 00:13:48,570
but the model assigns O.

310
00:13:48,570 --> 00:13:51,663
On the right, I've shown that there's a red O

311
00:13:51,663 --> 00:13:53,826
where we have another adversarial example.

312
00:13:53,826 --> 00:13:55,655
We're very near the other O's.

313
00:13:55,655 --> 00:13:58,175
We might expect the model to assign this class to be an O,

314
00:13:58,175 --> 00:14:00,375
and yet because it's drawn blue mass there

315
00:14:00,375 --> 00:14:04,060
it's actually assigning it to be an X.

316
00:14:04,060 --> 00:14:05,614
If overfitting is really the story

317
00:14:05,614 --> 00:14:09,105
then each adversarial example is more or less

318
00:14:09,105 --> 00:14:12,877
the result of bad luck and also more or less unique.

319
00:14:12,877 --> 00:14:14,455
If we fit the model again

320
00:14:14,455 --> 00:14:16,378
or we fit a slightly different model

321
00:14:16,378 --> 00:14:19,137
we would expect to make different random mistakes

322
00:14:19,137 --> 00:14:22,338
on this points that are off the training set,

323
00:14:22,338 --> 00:14:25,131
but that was actually not what we found at all.

324
00:14:25,131 --> 00:14:28,017
We found that many different models would misclassify

325
00:14:28,017 --> 00:14:30,533
the same adversarial examples,

326
00:14:30,533 --> 00:14:33,271
and they would assign the same class to them.

327
00:14:33,271 --> 00:14:36,191
We also found that if we took the difference

328
00:14:36,191 --> 00:14:40,429
between an original example and an adversarial example

329
00:14:40,429 --> 00:14:43,226
then we had a direction in input space

330
00:14:43,226 --> 00:14:46,719
and we could add that same offset vector

331
00:14:46,719 --> 00:14:49,234
to any clean example, and we would almost always

332
00:14:49,234 --> 00:14:52,067
get an adversarial example as a result.

333
00:14:52,067 --> 00:14:52,935
So we started to realize

334
00:14:52,935 --> 00:14:55,283
that there was systematic effect going on here,

335
00:14:55,283 --> 00:14:57,842
not just a random effect.

336
00:14:57,842 --> 00:14:59,368
That led us to another idea

337
00:14:59,368 --> 00:15:01,317
which is that adversarial examples

338
00:15:01,317 --> 00:15:03,537
might actually be more like underfitting

339
00:15:03,537 --> 00:15:05,538
rather than overfitting.

340
00:15:05,538 --> 00:15:09,141
They might actually come from the model being too linear.

341
00:15:09,141 --> 00:15:11,267
Here I draw the same task again

342
00:15:11,267 --> 00:15:13,655
where we have the same manifold of O's

343
00:15:13,655 --> 00:15:15,929
and the same line of X's,

344
00:15:15,929 --> 00:15:19,205
and this time I fit a linear model to the data set

345
00:15:19,205 --> 00:15:23,772
rather than fitting a high capacity, non-linear model to it.

346
00:15:23,772 --> 00:15:26,103
We see that we get a dividing hyperplane

347
00:15:26,103 --> 00:15:29,082
running in between the two classes.

348
00:15:29,082 --> 00:15:30,877
This hyperplane doesn't really capture

349
00:15:30,877 --> 00:15:33,803
the true structure of the classes.

350
00:15:33,803 --> 00:15:37,167
The O's are clearly arranged in a C-shaped manifold.

351
00:15:37,167 --> 00:15:40,310
If we keep walking past the end of the O's,

352
00:15:40,310 --> 00:15:43,734
we've crossed the decision boundary and we've drawn a red O

353
00:15:43,734 --> 00:15:46,432
where even though we're very near the decision boundary

354
00:15:46,432 --> 00:15:49,688
and near other O's we believe that it is now an X.

355
00:15:49,688 --> 00:15:53,036
Similarly we can take steps that go from near X's

356
00:15:53,036 --> 00:15:57,646
to just over the line that are classified as O's.

357
00:15:57,646 --> 00:15:59,638
Another thing that's somewhat unusual about this plot

358
00:15:59,638 --> 00:16:03,208
is that if we look at the lower left or upper right corners

359
00:16:03,208 --> 00:16:05,428
these corners are very confidently classified

360
00:16:05,428 --> 00:16:09,538
as being X's on the lower left or O's on the upper right

361
00:16:09,538 --> 00:16:12,498
even though we've never seen any data over there at all.

362
00:16:12,498 --> 00:16:14,710
The linear model family forces the model

363
00:16:14,710 --> 00:16:17,604
to have very high confidence in these regions

364
00:16:17,604 --> 00:16:21,354
that are very far from the decision boundary.

365
00:16:22,757 --> 00:16:25,923
We've seen that linear models can actually assign

366
00:16:25,923 --> 00:16:28,478
really unusual confidence as you move very far

367
00:16:28,478 --> 00:16:30,016
from the decision boundary,

368
00:16:30,016 --> 00:16:31,828
even if there isn't any data there,

369
00:16:31,828 --> 00:16:34,106
but are deep neural networks actually

370
00:16:34,106 --> 00:16:36,326
anything like linear models?

371
00:16:36,326 --> 00:16:38,598
Could linear models actually explain anything

372
00:16:38,598 --> 00:16:41,190
about how it is that deep neural nets fail?

373
00:16:41,190 --> 00:16:43,114
It turns out that modern deep neural nets

374
00:16:43,114 --> 00:16:45,482
are actually very piecewise linear,

375
00:16:45,482 --> 00:16:47,648
so rather than being a single linear function

376
00:16:47,648 --> 00:16:49,162
they are piecewise linear

377
00:16:49,162 --> 00:16:52,412
with maybe not that many linear pieces.

378
00:16:53,588 --> 00:16:55,378
If we use rectified linear units

379
00:16:55,378 --> 00:16:59,545
then the mapping from the input image to the output logits

380
00:17:00,460 --> 00:17:03,662
is literally a piecewise linear function.

381
00:17:03,662 --> 00:17:06,750
By the logits I mean the un-normalized log probabilities

382
00:17:06,750 --> 00:17:11,701
before we apply the softmax op at the output of the model.

383
00:17:11,701 --> 00:17:13,161
There are other neural networks

384
00:17:13,161 --> 00:17:14,956
like maxout networks that are also

385
00:17:14,956 --> 00:17:17,146
literally piecewise linear.

386
00:17:17,146 --> 00:17:19,915
And then there are several that become very close to it.

387
00:17:19,915 --> 00:17:22,627
Before rectified linear units became popular

388
00:17:22,627 --> 00:17:27,019
most people used to use sigmoid units of one form or another

389
00:17:27,019 --> 00:17:30,369
either logistic sigmoid or hyperbolic tangent units.

390
00:17:30,369 --> 00:17:33,624
These sigmoidal units have to be carefully tuned,

391
00:17:33,624 --> 00:17:35,715
especially at initialization

392
00:17:35,715 --> 00:17:37,936
so that you spend most of your time

393
00:17:37,936 --> 00:17:40,396
near the center of the sigmoid

394
00:17:40,396 --> 00:17:43,527
where the sigmoid is approximately linear.

395
00:17:43,527 --> 00:17:46,578
Then finally, the LSTM, a kind of recurrent network

396
00:17:46,578 --> 00:17:49,641
that is one of the most popular recurrent networks today,

397
00:17:49,641 --> 00:17:52,769
uses addition from one time step to the next

398
00:17:52,769 --> 00:17:56,859
in order to accumulate and remember information over time.

399
00:17:56,859 --> 00:18:00,021
Addition is a particularly simple form of linearity,

400
00:18:00,021 --> 00:18:01,501
so we can see that the interaction

401
00:18:01,501 --> 00:18:06,055
from a very distant time step in the past and the present

402
00:18:06,055 --> 00:18:09,330
is highly linear within an LSTM.

403
00:18:09,330 --> 00:18:11,647
Now to be clear, I'm speaking about the mapping

404
00:18:11,647 --> 00:18:14,417
from the input of the model to the output of the model.

405
00:18:14,417 --> 00:18:17,155
That's what I'm saying is close to being linear

406
00:18:17,155 --> 00:18:21,128
or is piecewise linear with relatively few pieces.

407
00:18:21,128 --> 00:18:23,351
The mapping from the parameters of the network

408
00:18:23,351 --> 00:18:26,125
to the output of the network is non-linear

409
00:18:26,125 --> 00:18:29,345
because the weight matrices at each layer of the network

410
00:18:29,345 --> 00:18:31,394
are multiplied together.

411
00:18:31,394 --> 00:18:34,249
So we actually get extremely non-linear reactions

412
00:18:34,249 --> 00:18:36,434
between parameters and the output.

413
00:18:36,434 --> 00:18:39,348
That's what makes training a neural network so difficult.

414
00:18:39,348 --> 00:18:42,315
But the mapping from the input to the output

415
00:18:42,315 --> 00:18:45,177
is much more linear and predictable,

416
00:18:45,177 --> 00:18:47,347
and it means that optimization problems

417
00:18:47,347 --> 00:18:50,938
that aim to optimize the input to the model

418
00:18:50,938 --> 00:18:53,600
are much easier than optimization problems

419
00:18:53,600 --> 00:18:57,169
that aim to optimize the parameters.

420
00:18:57,169 --> 00:18:59,631
If we go and look for this happening in practice

421
00:18:59,631 --> 00:19:01,870
we can take a convolutional network

422
00:19:01,870 --> 00:19:04,273
and trace out a one-dimensional path

423
00:19:04,273 --> 00:19:07,013
through its input space.

424
00:19:07,013 --> 00:19:09,818
So what we're doing here is we're choosing a clean example.

425
00:19:09,818 --> 00:19:12,763
It's an image of a white car on a red background,

426
00:19:12,763 --> 00:19:14,856
and we are choosing a direction

427
00:19:14,856 --> 00:19:16,623
that will travel through space.

428
00:19:16,623 --> 00:19:19,403
We are going to have a coefficient epsilon

429
00:19:19,403 --> 00:19:21,273
that we multiply by this direction.

430
00:19:21,273 --> 00:19:22,848
When epsilon is negative 30,

431
00:19:22,848 --> 00:19:24,544
like at the left end of the plot,

432
00:19:24,544 --> 00:19:28,266
we're subtracting off a lot of this unit vector direction.

433
00:19:28,266 --> 00:19:30,945
When epsilon is zero, like in the middle of the plot,

434
00:19:30,945 --> 00:19:33,964
we're visiting the original image from the data set,

435
00:19:33,964 --> 00:19:36,074
and when epsilon is positive 30,

436
00:19:36,074 --> 00:19:37,645
like at the right end of the plot,

437
00:19:37,645 --> 00:19:41,228
we're adding this direction onto the input.

438
00:19:42,622 --> 00:19:45,079
In the panel on the left, I show you an animation

439
00:19:45,079 --> 00:19:47,666
where we move from epsilon equals negative 30

440
00:19:47,666 --> 00:19:50,820
as up to epsilon equals positive 30.

441
00:19:50,820 --> 00:19:53,581
You read the animation left to right, top to bottom,

442
00:19:53,581 --> 00:19:56,031
and everywhere that there's a yellow box

443
00:19:56,031 --> 00:20:00,198
the input has correctly recognized as being a car.

444
00:20:01,379 --> 00:20:04,354
On the upper left, you see that it looks mostly blue.

445
00:20:04,354 --> 00:20:07,817
On the lower right, it's hard to tell what's going on.

446
00:20:07,817 --> 00:20:10,381
It's kind of reddish and so on.

447
00:20:10,381 --> 00:20:13,772
In the middle row, just after where the yellow boxes end

448
00:20:13,772 --> 00:20:14,995
you can see pretty clearly

449
00:20:14,995 --> 00:20:17,324
that it's a car on a red background,

450
00:20:17,324 --> 00:20:20,747
though the image is small on these slides.

451
00:20:20,747 --> 00:20:23,780
What's interesting to look at here is the logits

452
00:20:23,780 --> 00:20:25,168
that the model outputs.

453
00:20:25,168 --> 00:20:30,115
This is a deep convolutional rectified linear unit network.

454
00:20:30,115 --> 00:20:32,326
Because it uses rectified linear units,

455
00:20:32,326 --> 00:20:36,160
we know that the output is a piecewise linear function

456
00:20:36,160 --> 00:20:38,559
of the input to the model.

457
00:20:38,559 --> 00:20:40,835
The main question we're asking by making this plot

458
00:20:40,835 --> 00:20:42,820
is how many different pieces

459
00:20:42,820 --> 00:20:45,628
does this piecewise linear function have

460
00:20:45,628 --> 00:20:48,552
if we look at one particular cross section.

461
00:20:48,552 --> 00:20:50,835
You might think that maybe a deep net

462
00:20:50,835 --> 00:20:52,135
is going to represent some extremely

463
00:20:52,135 --> 00:20:54,749
wiggly complicated function with lots and lots

464
00:20:54,749 --> 00:20:58,326
of linear pieces no matter which cross section you look in.

465
00:20:58,326 --> 00:21:01,408
Or we might find that it has more or less two pieces

466
00:21:01,408 --> 00:21:03,825
for each function we look at.

467
00:21:04,667 --> 00:21:07,201
Each of the different curves on this plot

468
00:21:07,201 --> 00:21:10,245
is the logits for a different class.

469
00:21:10,245 --> 00:21:13,864
We see that out at the tails of the plot

470
00:21:13,864 --> 00:21:16,528
that the frog class is the most likely,

471
00:21:16,528 --> 00:21:18,846
and the frog class basically looks like

472
00:21:18,846 --> 00:21:20,846
a big v-shaped function.

473
00:21:21,928 --> 00:21:24,193
The logits for the frog class become very high

474
00:21:24,193 --> 00:21:27,270
when epsilon is negative 30 or positive 30,

475
00:21:27,270 --> 00:21:29,253
and they drop down and become a little bit negative

476
00:21:29,253 --> 00:21:31,003
when epsilon is zero.

477
00:21:32,833 --> 00:21:36,250
The car class, listed as automobile here,

478
00:21:37,764 --> 00:21:39,856
it's actually high in the middle,

479
00:21:39,856 --> 00:21:42,950
and the car is correctly recognized.

480
00:21:42,950 --> 00:21:44,944
As we sweep out to very negative epsilon,

481
00:21:44,944 --> 00:21:47,397
the logits for the car class do increase,

482
00:21:47,397 --> 00:21:49,033
but they don't increase nearly as quickly

483
00:21:49,033 --> 00:21:51,553
as the logits for the frog class.

484
00:21:51,553 --> 00:21:52,811
So, we've found a direction

485
00:21:52,811 --> 00:21:54,793
that's associated with the frog class

486
00:21:54,793 --> 00:21:59,041
and as we follow it out to a relatively large perturbation,

487
00:21:59,041 --> 00:22:02,334
we find that the model extrapolates linearly

488
00:22:02,334 --> 00:22:04,873
and begins to make a very unreasonable prediction

489
00:22:04,873 --> 00:22:07,984
that the frog class is extremely likely

490
00:22:07,984 --> 00:22:09,971
just because we've moved for a long time

491
00:22:09,971 --> 00:22:12,073
in this direction that was locally associated

492
00:22:12,073 --> 00:22:15,240
with the frog class being more likely.

493
00:22:17,550 --> 00:22:20,694
When we actually go and construct adversarial examples,

494
00:22:20,694 --> 00:22:23,200
we need to remember that we're able to get

495
00:22:23,200 --> 00:22:24,784
quite a large perturbation

496
00:22:24,784 --> 00:22:26,829
without changing the image very much

497
00:22:26,829 --> 00:22:29,912
as far as a human being is concerned.

498
00:22:30,882 --> 00:22:33,852
So here I show you a handwritten digit three,

499
00:22:33,852 --> 00:22:36,395
and I'm going to change it in several different ways,

500
00:22:36,395 --> 00:22:37,923
and all of these changes have

501
00:22:37,923 --> 00:22:40,806
the same L2 norm perturbation.

502
00:22:40,806 --> 00:22:44,421
In the top row, I'm going to change the three into a seven

503
00:22:44,421 --> 00:22:47,752
just by looking for the nearest seven in the training set.

504
00:22:47,752 --> 00:22:49,518
The difference between those two

505
00:22:49,518 --> 00:22:53,527
is this image that looks a little bit like the seven

506
00:22:53,527 --> 00:22:55,187
wrapped in some black lines.

507
00:22:55,187 --> 00:22:57,813
So here white pixels in the middle image

508
00:22:57,813 --> 00:22:59,808
in the perturbation column,

509
00:22:59,808 --> 00:23:02,184
the white pixels represent adding something

510
00:23:02,184 --> 00:23:04,830
and black pixels represent subtracting something

511
00:23:04,830 --> 00:23:08,142
as you move from the left column to the right column.

512
00:23:08,142 --> 00:23:11,401
So when we take the three and we apply this perturbation

513
00:23:11,401 --> 00:23:13,417
that transforms it into a seven,

514
00:23:13,417 --> 00:23:16,531
we can measure the L2 norm of that perturbation.

515
00:23:16,531 --> 00:23:20,236
And it turns out to have an L2 norm of 3.96.

516
00:23:20,236 --> 00:23:21,818
That gives you kind of a reference

517
00:23:21,818 --> 00:23:24,790
for how big these perturbations can be.

518
00:23:24,790 --> 00:23:26,521
In the middle row, we apply a perturbation

519
00:23:26,521 --> 00:23:28,302
of exactly the same size,

520
00:23:28,302 --> 00:23:30,500
but with the direction chosen randomly.

521
00:23:30,500 --> 00:23:32,065
In this case we don't actually change

522
00:23:32,065 --> 00:23:33,720
the class of the three at all,

523
00:23:33,720 --> 00:23:35,377
we just get some random noise

524
00:23:35,377 --> 00:23:37,825
that didn't really change the class.

525
00:23:37,825 --> 00:23:41,373
A human could still easily read it as being a three.

526
00:23:41,373 --> 00:23:44,285
And then finally at the very bottom row,

527
00:23:44,285 --> 00:23:46,230
we take the three and we just erase a piece of it

528
00:23:46,230 --> 00:23:48,011
with a perturbation of the same norm

529
00:23:48,011 --> 00:23:50,334
and we turn it into something

530
00:23:50,334 --> 00:23:52,422
that doesn't have any class at all.

531
00:23:52,422 --> 00:23:53,714
It's not a three, it's not a seven,

532
00:23:53,714 --> 00:23:56,254
it's just a defective input.

533
00:23:56,254 --> 00:23:57,568
All of these changes can happen

534
00:23:57,568 --> 00:24:00,664
with the same L2 norm perturbation.

535
00:24:00,664 --> 00:24:03,025
And actually a lot of the time with adversarial examples,

536
00:24:03,025 --> 00:24:06,011
you make perturbations that have an even larger L2 norm.

537
00:24:06,011 --> 00:24:07,216
What's going on is that

538
00:24:07,216 --> 00:24:09,143
there are several different pixels in the image,

539
00:24:09,143 --> 00:24:12,131
and so small changes to individual pixels

540
00:24:12,131 --> 00:24:15,227
can add up to relatively large vectors.

541
00:24:15,227 --> 00:24:17,566
For larger datasets like ImageNet,

542
00:24:17,566 --> 00:24:18,990
where there's even more pixels,

543
00:24:18,990 --> 00:24:21,184
you can make very small changes to each pixel

544
00:24:21,184 --> 00:24:24,174
that travel very far in vector space

545
00:24:24,174 --> 00:24:26,368
as measured by the L2 norm.

546
00:24:26,368 --> 00:24:28,505
That means that you can actually make changes

547
00:24:28,505 --> 00:24:30,093
that are almost imperceptible

548
00:24:30,093 --> 00:24:31,605
but actually move you really far

549
00:24:31,605 --> 00:24:33,477
and get a large dot product

550
00:24:33,477 --> 00:24:36,137
with the coefficients of the linear function

551
00:24:36,137 --> 00:24:38,695
that the model represents.

552
00:24:38,695 --> 00:24:39,832
It also means that when

553
00:24:39,832 --> 00:24:41,467
we're constructing adversarial examples,

554
00:24:41,467 --> 00:24:44,838
we need to make sure that the adversarial example procedure

555
00:24:44,838 --> 00:24:46,022
isn't able to do what happened

556
00:24:46,022 --> 00:24:48,240
in the top row of this slide here.

557
00:24:48,240 --> 00:24:49,627
So in the top row of this slide,

558
00:24:49,627 --> 00:24:50,756
we took the three and we actually

559
00:24:50,756 --> 00:24:52,454
just changed it into a seven.

560
00:24:52,454 --> 00:24:53,856
So when the model says that the image

561
00:24:53,856 --> 00:24:56,232
in the upper right is a seven, it's not a mistake.

562
00:24:56,232 --> 00:24:59,145
We actually just changed the input class.

563
00:24:59,145 --> 00:25:00,499
When we build adversarial examples,

564
00:25:00,499 --> 00:25:02,928
we want to make sure that we're measuring real mistakes.

565
00:25:02,928 --> 00:25:04,459
If we're experimenters studying

566
00:25:04,459 --> 00:25:06,259
how easy a network is to fool,

567
00:25:06,259 --> 00:25:08,146
we want to make sure that we're actually fooling it

568
00:25:08,146 --> 00:25:11,515
and not just changing the input class.

569
00:25:11,515 --> 00:25:13,535
And if we're an attacker, we actually want to make sure

570
00:25:13,535 --> 00:25:17,457
that we're causing misbehavior in the system.

571
00:25:17,457 --> 00:25:19,689
To do that, when we build adversarial examples,

572
00:25:19,689 --> 00:25:24,134
we use the maxnorm to constrain the perturbation.

573
00:25:24,134 --> 00:25:26,726
Basically this says that no pixel can change

574
00:25:26,726 --> 00:25:28,812
by more than some amount epsilon.

575
00:25:28,812 --> 00:25:30,991
So the L2 norm can get really big,

576
00:25:30,991 --> 00:25:33,335
but you can't concentrate all the changes

577
00:25:33,335 --> 00:25:35,908
for that L2 norm to erase pieces of the digit,

578
00:25:35,908 --> 00:25:39,701
like in the bottom row here we erased the top of a three.

579
00:25:39,701 --> 00:25:42,604
One very fast way to build an adversarial example

580
00:25:42,604 --> 00:25:45,503
is just to take the gradient of the cost

581
00:25:45,503 --> 00:25:47,140
that you used to train the network

582
00:25:47,140 --> 00:25:48,663
with respect to the input,

583
00:25:48,663 --> 00:25:51,312
and then take the sign of that gradient.

584
00:25:51,312 --> 00:25:55,708
The sign is essentially enforcing the maxnorm constraint.

585
00:25:55,708 --> 00:25:58,550
You're only allowed to change the input by

586
00:25:58,550 --> 00:26:00,690
up to epsilon at each pixel,

587
00:26:00,690 --> 00:26:02,381
so if you just take the sign it tells you

588
00:26:02,381 --> 00:26:04,761
whether you want to add epsilon or subtract epsilon

589
00:26:04,761 --> 00:26:07,010
in order to hurt the network.

590
00:26:07,010 --> 00:26:08,844
You can view this as taking the observation

591
00:26:08,844 --> 00:26:10,790
that the network is more or less linear,

592
00:26:10,790 --> 00:26:12,211
as we showed on this slide,

593
00:26:12,211 --> 00:26:14,265
and using that to motivate

594
00:26:14,265 --> 00:26:17,918
building a first order Taylor series approximation

595
00:26:17,918 --> 00:26:21,105
of the neural network's cost.

596
00:26:21,105 --> 00:26:24,508
And then subject to that Taylor series approximation,

597
00:26:24,508 --> 00:26:26,106
we want to maximize the cost

598
00:26:26,106 --> 00:26:28,898
following this maxnorm constraint.

599
00:26:28,898 --> 00:26:30,590
And that gives us this technique that we call

600
00:26:30,590 --> 00:26:32,785
the fast gradient sign method.

601
00:26:32,785 --> 00:26:34,350
If you want to just get your hands dirty

602
00:26:34,350 --> 00:26:36,835
and start making adversarial examples really quickly,

603
00:26:36,835 --> 00:26:38,764
or if you have an algorithm where you want to train

604
00:26:38,764 --> 00:26:41,534
on adversarial examples in the inner loop of learning,

605
00:26:41,534 --> 00:26:43,402
this method will make adversarial examples for you

606
00:26:43,402 --> 00:26:45,134
very, very quickly.

607
00:26:45,134 --> 00:26:47,942
In practice you should also use other methods,

608
00:26:47,942 --> 00:26:50,353
like Nicholas Carlini's attack based on

609
00:26:50,353 --> 00:26:52,660
multiple steps of the Adam optimizer,

610
00:26:52,660 --> 00:26:55,212
to make sure that you have a very strong attack

611
00:26:55,212 --> 00:26:57,359
that you bring out when you think you have a model

612
00:26:57,359 --> 00:26:59,678
that might be more powerful.

613
00:26:59,678 --> 00:27:02,145
A lot of the time people find that they can defeat

614
00:27:02,145 --> 00:27:03,460
the fast gradient sign method

615
00:27:03,460 --> 00:27:05,740
and think that they've built a successful defense,

616
00:27:05,740 --> 00:27:08,769
but then when you bring out a more powerful method

617
00:27:08,769 --> 00:27:10,444
that takes longer to evaluate,

618
00:27:10,444 --> 00:27:12,566
they find that they can't overcome

619
00:27:12,566 --> 00:27:16,066
the more computationally expensive attack.

620
00:27:18,043 --> 00:27:20,090
I've told you that adversarial examples happen

621
00:27:20,090 --> 00:27:22,036
because the model is very linear.

622
00:27:22,036 --> 00:27:23,529
And then I told you that we could

623
00:27:23,529 --> 00:27:25,132
use this linearity assumption

624
00:27:25,132 --> 00:27:28,694
to build this attack, the fast gradient sign method.

625
00:27:28,694 --> 00:27:31,900
This method, when applied to a regular neural network

626
00:27:31,900 --> 00:27:34,079
that doesn't have any special defenses,

627
00:27:34,079 --> 00:27:38,328
will get over a 99% attack success rate.

628
00:27:38,328 --> 00:27:40,377
So that seems to confirm, somewhat,

629
00:27:40,377 --> 00:27:42,936
this hypothesis that adversarial examples

630
00:27:42,936 --> 00:27:45,054
come from the model being far too linear

631
00:27:45,054 --> 00:27:48,964
and extrapolating in linear fashions when it shouldn't.

632
00:27:48,964 --> 00:27:51,514
Well we can actually go looking for some more evidence.

633
00:27:51,514 --> 00:27:54,417
My friend David Warde-Farley and I built these maps

634
00:27:54,417 --> 00:27:57,172
of the decision boundaries of neural networks.

635
00:27:57,172 --> 00:27:58,809
And we found that they are consistent

636
00:27:58,809 --> 00:28:02,140
with the linearity hypothesis.

637
00:28:02,140 --> 00:28:04,478
So the FGSM is that attack method

638
00:28:04,478 --> 00:28:06,244
that I described in the previous slide,

639
00:28:06,244 --> 00:28:08,260
where we take the sign of the gradient.

640
00:28:08,260 --> 00:28:09,537
We'd like to build a map

641
00:28:09,537 --> 00:28:13,353
of a two-dimensional cross section of input space

642
00:28:13,353 --> 00:28:15,760
and show which classes are assigned

643
00:28:15,760 --> 00:28:18,556
to the data at each point.

644
00:28:18,556 --> 00:28:21,397
In the grid on the right, each different cell,

645
00:28:21,397 --> 00:28:23,308
each little square within the grid,

646
00:28:23,308 --> 00:28:27,715
is a map of a CIFAR-10 classifier's decision boundary,

647
00:28:27,715 --> 00:28:29,932
with each cell corresponding to a different

648
00:28:29,932 --> 00:28:32,668
CIFAR-10 testing sample.

649
00:28:32,668 --> 00:28:34,624
On the left I show you a little legend

650
00:28:34,624 --> 00:28:37,867
where you can understand what each cell means.

651
00:28:37,867 --> 00:28:40,927
The very center of each cell corresponds to

652
00:28:40,927 --> 00:28:43,338
the original example from the CIFAR-10 dataset

653
00:28:43,338 --> 00:28:45,590
with no modification.

654
00:28:45,590 --> 00:28:47,534
As we move left to right in the cell,

655
00:28:47,534 --> 00:28:48,561
we're moving in the direction

656
00:28:48,561 --> 00:28:50,918
of the fast gradient sign method attack.

657
00:28:50,918 --> 00:28:53,076
So just the sign of the gradient.

658
00:28:53,076 --> 00:28:54,897
As we move up and down within the cell,

659
00:28:54,897 --> 00:28:58,243
we're moving in a random direction that's orthogonal to

660
00:28:58,243 --> 00:29:00,907
the fast gradient sign method direction.

661
00:29:00,907 --> 00:29:04,204
So we get to see a cross section, a 2D cross section

662
00:29:04,204 --> 00:29:06,454
of CIFAR-10 decision space.

663
00:29:07,455 --> 00:29:09,604
At each pixel within this map,

664
00:29:09,604 --> 00:29:13,291
we plot a color that tells us which class is assigned there.

665
00:29:13,291 --> 00:29:15,199
We use white pixels to indicate that

666
00:29:15,199 --> 00:29:17,174
the correct class was chosen,

667
00:29:17,174 --> 00:29:19,538
and then we used different colors to represent

668
00:29:19,538 --> 00:29:21,931
all of the other incorrect classes.

669
00:29:21,931 --> 00:29:23,908
You can see that in nearly all

670
00:29:23,908 --> 00:29:25,641
of the grid cells on the right,

671
00:29:25,641 --> 00:29:29,222
roughly the left half of the image is white.

672
00:29:29,222 --> 00:29:31,564
So roughly the left half of the image

673
00:29:31,564 --> 00:29:33,648
has been correctly classified.

674
00:29:33,648 --> 00:29:36,761
As we move to the right, we see that there is usually

675
00:29:36,761 --> 00:29:39,537
a different color on the right half.

676
00:29:39,537 --> 00:29:41,441
And the boundaries between these regions

677
