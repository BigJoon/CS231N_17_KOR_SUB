0
00:00:00,000 --> 00:00:06,000
Translated by visionNoob, KNU
https://github.com/insurgent92/CS231N_17_KOR_SUB

1
00:00:07,961 --> 00:00:12,153
잘 들리시나요? 좋습니다. 
잠시 지연이 있었습니다.

2
00:00:12,153 --> 00:00:24,081
잠시 기술적인 문제가 있었습니다.

3
00:00:25,353 --> 00:00:30,420
오늘은 10강입니다. 
Recurrent Neural Networks(RNN)에 대해 배워보겠습니다.

4
00:00:30,420 --> 00:00:33,003
그 전에 공지사항을 전달하겠습니다.

5
00:00:33,003 --> 00:00:37,353
현재 과제1을 채점 중에 있습니다.

6
00:00:37,353 --> 00:00:46,251
최종 성적이 아마도 오늘 발표될 예정입니다. 
과제2 deadline 전에는 과제1 성적이 나왔으면 좋겠네요

7
00:00:46,251 --> 00:00:50,361
과제2의 제출기한은 금일 11:59 p.m. 까지입니다.

8
00:00:50,361 --> 00:00:56,633
과제 2를 다 마치신 분 있으신가요? 
절반정도 되는군요

9
00:00:56,633 --> 00:01:03,811
제가 예전에 과제2는 상당히 오래걸릴 수 있으니
미리 시작하는 것이 좋다고 말씀드렸습니다.

10
00:01:03,811 --> 00:01:06,561
late dates가 남아있길 바랍니다.

11
00:01:06,561 --> 00:01:10,531
화요일 수업시간에는 중간고사가 있습니다.

12
00:01:10,531 --> 00:01:15,881
아마 강의실이 비좁아서 모두 시험을 치룰 수 없을 것입니다.

13
00:01:15,881 --> 00:01:20,062
따라서 다른 강의실들도 함께 빌려서 진행할 예정입니다.

14
00:01:20,062 --> 00:01:26,099
향후 세부사항은 다시한번 공지하겠습니다.

15
00:01:26,099 --> 00:01:28,179
또 한가지 공지사항이 있습니다.

16
00:01:28,179 --> 00:01:34,950
현재 Train Game 이라는 사이트를 개설하고있습니다.
Train Game은 웹 브라우저 기반이며

17
00:01:34,950 --> 00:01:39,927
여러분 누구나 접속해서 딥러닝 모델을 학습시키고 
학습 도중 하이퍼파라미터를 조정하실 수 있습니다.

18
00:01:39,927 --> 00:01:47,646
앞서 강의들에서 배웠던 다양한 하이퍼파라미터들을
손으로 직접 만져볼 수 있는 아주 좋은 툴입니다.

19
00:01:47,646 --> 00:01:53,190
Train Game를 무조건 하실 필요는 없습니다. 
하지만, Train Game는 실제로 다양한 데이터를 다뤄보며

20
00:01:53,190 --> 00:01:57,481
데이터 타입에 따라 하이퍼파라미터가 어떤 영향을 미치는지에 
직관을 얻을 수 있는 아주 좋은 기회가 될것입니다.

21
00:01:57,481 --> 00:02:05,790
현재 몇 가지 버그를 수정하고 있습니다. 조만간 여러분에게 
사용법과 함께 공개하도록 하겠습니다.

22
00:02:05,790 --> 00:02:11,008
다시한번 말씀드리지만, 필수는 아닙니다. 하지만 해보시면
도움이 많이 될 것입니다.

23
00:02:11,008 --> 00:02:17,204
Train Game을 써보신 분들에게는
Extra credit을 지급할 예정입니다.

24
00:02:18,208 --> 00:02:23,458
아무튼 버그만 다 잡으면 
다시 한번 공지드리겠습니다.

25
00:02:24,720 --> 00:02:28,139
지난 강의 복습을 해봅시다. 
CNN 아키텍쳐들을 배웠습니다.

26
00:02:28,139 --> 00:02:35,006
ImageNet Classification Challenge를
중심으로 연대순 우승자들을 알아봤습니다.

27
00:02:35,006 --> 00:02:41,331
2012년에는 AlexNet이 있었습니다. 
9-layer CNN인데, 아주 잘 동작했습니다.

28
00:02:41,331 --> 00:02:48,081
AlexNet이 Computer Vision의 진화를 촉발시켰으며
딥러닝 시대의 서막을 열었습니다.

29
00:02:48,081 --> 00:02:56,699
2014년으로 넘어가봅시다. 이전의 모델들보다 훨씬 더
깊어진 두 모델이 있었습니다. VGG와 GoogLeNet 입니다.

30
00:02:56,699 --> 00:03:02,930
VGGNet은 16/19 레이어를, GoogLeNet은 22 레이어를
가진 모델이었습니다.

31
00:03:02,930 --> 00:03:11,230
2014년 모델들에서 흥미로운 점은 2014년에는
Batch normalization 이 발명되기 이전이라는 점입니다.

32
00:03:11,230 --> 00:03:18,761
Batch Norm이 없던 시절이라서 레이어가 깊을 모델을
학습시키는 일은 상당히 어려웠습습니다.

33
00:03:18,761 --> 00:03:24,869
이 두 모델은 깊은 모델을 수렴시키기 위해서 
각종 테크닉(hackery)을 써야했습니다.

34
00:03:24,869 --> 00:03:28,579
가령 VGGNet는 16/19 레이어가 있습니다.

35
00:03:28,579 --> 00:03:34,107
하지만 처음에는 11레이어 모델을 학습시켰습니다.
11 레이어가 모델이 잘 수렴하는 한계였습니다.

36
00:03:34,107 --> 00:03:40,059
그리고 나서 11레이어 중간에 레이어를 무작위로 추가해서 
VGG-16, VGG-19 를 학습시킨 것이죠

37
00:03:40,059 --> 00:03:46,539
이렇게, Bath norm이 없던 2014년에는
학습 자체가 상당히 어려운 문제였습니다.

38
00:03:46,539 --> 00:03:52,539
유사한 사례로 GoogLeNet의 경우에는 
auxiliary classifiers를 도입했습니다.

39
00:03:52,539 --> 00:03:56,539
auxiliary classifiers는 classification 성능을
올리기 위해서 도입된 것이 아닙니다.

40
00:03:56,539 --> 00:04:03,430
단지 네트워크의 초기 레이어에 gradient를 
직접 흘려보내기 위한  수단이었습니다.

41
00:04:03,430 --> 00:04:10,411
GoogLeNet은 Batch Norm 이전에 이런 테크닉을 사용했습니다.

42
00:04:10,411 --> 00:04:17,321
하지만 Batch Norm이 있다면 굳이 이런 식의
테크닉은 더이상 필요하지 않습니다.

43
00:04:17,321 --> 00:04:24,350
2015년에는 ResNet이라는 아주 멋드러진 모델이 있었습니다.

44
00:04:24,350 --> 00:04:28,310
ResNet은 shortcut connection과 
residual block이 도입된 모델입니다.

45
00:04:28,310 --> 00:04:39,110
ResNet에서는 레이어의 출력은 
입력 +  residual block의 출력 입니다.

46
00:04:39,110 --> 00:04:43,308
아주 흥미로운 구조입니다. 이 구조는
두 가지 아주 좋은 속성을 지니고 있습니다.

47
00:04:43,308 --> 00:04:49,531
하나는, 우선 residual block의 가중치가 0이면 
이 block은 indentity mapping 을 합니다.

48
00:04:49,531 --> 00:04:55,681
이 속성은, 모델이 '필요없는 레이어" 를 사용하지 않도록
학습하는데 아주 유용합니다.

49
00:04:55,681 --> 00:05:02,171
그리고 ResNet의 관점에서 L2 Regularization을
해석해 볼 수도 있습니다.

50
00:05:02,171 --> 00:05:08,321
레이어에 L2 Regularization을 추가시키면
L2는 모든 파라미터가 0이 되도록 노력할 것입니다.

51
00:05:08,321 --> 00:05:12,739
사실 기본적인 CNN 아키텍쳐의 관점에서보면 
모든 파라미터가 0이면 이상합니다.

52
00:05:12,739 --> 00:05:20,510
하지만 ResNet의 관점에서는 파라미터를 0으로 만드려는 속성은
모델이 불필요한 레이어를 사용하지 않도록 해줄 수 있습니다.

53
00:05:20,510 --> 00:05:26,310
파라미터들을 계속 0으로 보내면
residual block이 identity가 되기 떄문이죠.

54
00:05:26,310 --> 00:05:31,371
ResNet의 또 한가지 특성은 backward pass 에서의
gradient flow와 관련있습니다.

55
00:05:31,371 --> 00:05:34,361
"Addition gates (+)" 가 backward pass에
어떤 영향을 미칠지 생각해보면

56
00:05:34,361 --> 00:05:39,881
Upstream gradient 가 "Addition gate" 를 만나면 
두 갈래로 나눠지게 됩니다.

57
00:05:39,881 --> 00:05:46,361
Upstream gradient가 오면 convolution block 
으로도 흘러들어가지만

58
00:05:46,361 --> 00:05:50,811
residual connection 덕분에 현재 레이어를 생략하고 
직접 이전 레이어로 흘러갈 수도 있습니다.

59
00:05:50,811 --> 00:05:59,150
Residual blocks을 수백 레이어 쌓아올린 네트워크가
있다고 해봅시다.

60
00:05:59,150 --> 00:06:05,561
Residual connection은 gradient를 위한 일종의
고속도로 역할을 합니다. gradient를 원활하게 전달하기 위해서죠

61
00:06:05,561 --> 00:06:09,630
이런 특성으로 학습을 더 쉽고 빠르게 할 수 있습니다.

62
00:06:09,630 --> 00:06:15,738
그리고 이런 특성은 모델이 엄청 깊더라도 잘 수렴할 수 있도록 도와줍니다.

63
00:06:15,738 --> 00:06:21,550
모델의 gradient flow를 잘 다루는 것은 
Machine learning 전 분야에 걸쳐서도 아주 중요합니다.

64
00:06:21,550 --> 00:06:28,564
RNN에서도 아주 중요하죠. 따라서 gradient flow는 
오늘 강의의 후반부에 다시한번 다루겠습니다.

65
00:06:31,148 --> 00:06:38,068
지난 시간에 DensNet, FractalNet 과 같은
이색적인 CNN 아키텍쳐도 살펴보았습니다.

66
00:06:38,068 --> 00:06:43,070
이 모델들은 gradient flow의 관점에서 
아주 잘 해석해 볼 수 있습니다.

67
00:06:43,070 --> 00:06:48,619
DenseNet이나 FractalNet 같은 모델들은 모델 내부에 
additional shortcut (identity)를 추가합니다.

68
00:06:48,619 --> 00:07:00,571
이 모델들은 Loss와 각 Layer를 직접 연결하기 때문에 
 backward pass가 아주 수월합니다.

69
00:07:00,571 --> 00:07:09,760
CNN 아키텍쳐에서 Gradient Flow를 잘 다루려는 시도는
최근 몇 년간 아주 활발하게 이루어졌습니다.

70
00:07:09,760 --> 00:07:15,221
앞으로도 신기한 아카텍쳐들이 많이 나올 것 같습니다.

71
00:07:16,257 --> 00:07:24,331
지난 강의에서 보셨던 그래프입니다.
모델 별 연산량, 크기 등을 비교해 보았습니다.

72
00:07:24,331 --> 00:07:27,971
이 그래프를 유심히 살펴보면 아주 재미있는 속성이 있습니다.

73
00:07:27,971 --> 00:07:32,801
AlexNet와 VGGNet는 파라미터가 엄청 많습니다.

74
00:07:32,801 --> 00:07:37,119
사실 파라미터가 많은 이유는 전적으로 
FC-layer 때문입니다.

75
00:07:37,119 --> 00:07:39,959
가령 AlexNet의 경우에는 대략 64M 개의 파라미터가 있습니다.

76
00:07:39,959 --> 00:07:47,771
AlexNet의 FC-Layer를 살펴봅시다. FC-Layers의 입력은 
마지막 Conv output  6 x 6 x 256 입니다.

77
00:07:47,771 --> 00:07:51,190
이 값(256 x 6 x 6) 들이 4096 개의 FC-layer 
노드와 연결됩니다.

78
00:07:51,190 --> 00:07:56,851
이 부분의 가중치 매트릭스는 엄청나게 큽니다.

79
00:07:56,851 --> 00:08:01,921
매트릭스의 크기는 
6x6x256x4096 이 되겠죠

80
00:08:01,921 --> 00:08:06,370
이 레이어 하나에만 38m개의 파라미터가 있습니다.

81
00:08:06,370 --> 00:08:11,859
AlexNet의 전체 파라미터 중 절반 이상이 
FC-Layer에 집중되어 있습니다.

82
00:08:11,859 --> 00:08:24,241
AlexNet의 FC-Layer 의 파라미터 갯수를 다 더해보면 
FC-Layer에서만 62M 가량의 파라미터가 있다는 것을 알 수 있습니다.

83
00:08:24,241 --> 00:08:31,110
GoogLeNet 이나 ResNet 같은 아키텍쳐들은 
FC-Layer를 상당부분 걷어냈습니다.

84
00:08:31,110 --> 00:08:33,698
FC-Layer를 Global Average Pooling (GAP)
으로 대체시킵니다.

85
00:08:33,698 --> 00:08:40,935
GAP를 사용하게 되면서 파라미터 갯수를 상당히 감소시켰습니다.

86
00:08:44,463 --> 00:08:49,604
지금까지는 CNN 아키텍쳐를 복습해보았습니다.

87
00:08:49,604 --> 00:08:56,321
이제부터는 Recurrent Neural Network에 대해서
배워보도록 하겠습니다.

88
00:08:56,321 --> 00:09:03,222
지금까지 배운 아키텍쳐들은(Vanilla Neural Network) 
다음과 같은 모양이하고 할 수 있습니다.

89
00:09:03,222 --> 00:09:08,593
네트워크는 이미지 또는 벡터를 입력으로 받습니다.

90
00:09:08,593 --> 00:09:13,850
입력 하나가 Hidden layer를 거쳐서
하나의 출력을 내보냅니다.

91
00:09:13,850 --> 00:09:18,876
Classification 문제라면 카테고리가 되겠군요

92
00:09:20,071 --> 00:09:25,942
하지만 Machine Learning의 관점에서 생각해보면 모델이 
다양한 입력을 처리할 수 있도록 유연해질 필요가 있습니다.

93
00:09:25,942 --> 00:09:35,313
그런 관점에서 RNN은 네트워크가 다양한 입/출력을 
다룰 수 있는 여지를 제공해줍니다.

94
00:09:35,313 --> 00:09:41,009
RNN 구조를 이용할 때, 가령 "one to many" 
모델을 사용한다고 하면

95
00:09:41,009 --> 00:09:48,721
입력은 이미지와 같은 "단일 입력" 이지만 출력은 
caption 과 같은 가변 길이의 출력일 수 있습니다.

96
00:09:48,721 --> 00:09:54,081
caption에 따라서 단어의 수가 천차만별이겠죠
따라서 출력의 길이는 항상 변합니다.

97
00:09:54,081 --> 00:09:56,491
"many to one" 모델도 있습니다.

98
00:09:56,491 --> 00:10:01,001
이제는 입력의 길이가 변합니다. 
가령 문장이 될 수 있겠군요

99
00:10:01,001 --> 00:10:06,161
그리고 이 문장의 "감정"을 분류하는 것입니다. 
부정적/긍적적인 문장인지를 구별하는 것이죠

100
00:10:06,161 --> 00:10:07,771
또는 컴퓨터 비전 컨텍스트에서,

101
00:10:07,771 --> 00:10:09,401
당신은 입력으로 비디오를 상상할 수 있습니다,

102
00:10:09,401 --> 00:10:12,512
그 비디오는 다양한 프레임 수를 가질 수 있습니다.

103
00:10:12,512 --> 00:10:14,921
이제 우리는이 전체 비디오를 읽고 싶습니다.

104
00:10:14,921 --> 00:10:16,401
잠재적으로 가변 길이의

105
00:10:16,401 --> 00:10:17,439
그리고 결국,

106
00:10:17,439 --> 00:10:18,742
분류 결정을 내리다

107
00:10:18,742 --> 00:10:20,142
어쩌면 어떤 종류의 활동이나 행동

108
00:10:20,142 --> 00:10:22,721
그 비디오에서 계속되고 있습니다.

109
00:10:22,721 --> 00:10:26,702
우리는 또한 문제가있을 수 있습니다.

110
00:10:26,702 --> 00:10:28,091
우리는 양쪽 입력을 원한다.

111
00:10:28,091 --> 00:10:29,931
출력은 길이가 가변적이다.

112
00:10:29,931 --> 00:10:31,491
우리는 이런 것을 볼 수 있습니다.

113
00:10:31,491 --> 00:10:33,433
기계 번역에서,

114
00:10:33,433 --> 00:10:35,870
우리의 의견은 어쩌면 영어 문장 일 수도 있고,

115
00:10:35,870 --> 00:10:37,302
가변 길이를 가질 수있는

116
00:10:37,302 --> 00:10:39,393
우리의 결과물은 아마도 프랑스어로 된 문장 일 것입니다.

117
00:10:39,393 --> 00:10:41,633
가변 길이를 가질 수도 있습니다.

118
00:10:41,633 --> 00:10:44,032
그리고 결정적으로 영어 문장의 길이

119
00:10:44,032 --> 00:10:46,801
프랑스 문장의 길이와 다를 수 있습니다.

120
00:10:46,801 --> 00:10:48,710
따라서 용량이있는 일부 모델이 필요합니다.

121
00:10:48,710 --> 00:10:50,782
두 가지 가변 길이 시퀀스를 받아 들인다.

122
00:10:50,782 --> 00:10:53,931
입력과 출력에서.

123
00:10:53,931 --> 00:10:56,331
마지막으로, 우리는 문제를 고려할 수도 있습니다.

124
00:10:56,331 --> 00:10:58,153
우리의 입력은 가변 길이이며,

125
00:10:58,153 --> 00:10:59,891
비디오 시퀀스와 같은 것

126
00:10:59,891 --> 00:11:01,553
프레임 수는 다양합니다.

127
00:11:01,553 --> 00:11:02,920
이제 우리는 결정을 내리고 싶습니다.

128
00:11:02,920 --> 00:11:04,771
해당 입력 시퀀스의 각 요소에 대해

129
00:11:04,771 --> 00:11:06,721
따라서 비디오의 맥락에서,

130
00:11:06,721 --> 00:11:09,201
어떤 분류 결정을 내릴지도 모르는

131
00:11:09,201 --> 00:11:11,891
동영상의 모든 프레임을 따라

132
00:11:11,891 --> 00:11:13,281
그리고 재귀 신경 네트워크

133
00:11:13,281 --> 00:11:15,171
이런 일반적인 패러다임

134
00:11:15,171 --> 00:11:17,401
가변 크기의 시퀀스 데이터 처리

135
00:11:17,401 --> 00:11:19,302
우리가 꽤 자연스럽게 잡을 수있게 해주는

136
00:11:19,302 --> 00:11:23,469
우리 모델의 모든 다른 유형의 설정.

137
00:11:24,349 --> 00:11:26,662
따라서 반복적 인 신경 회로망은 실제로 중요합니다.

138
00:11:26,662 --> 00:11:30,030
고정 된 크기의 입력이있는 일부 문제의 경우에도

139
00:11:30,030 --> 00:11:31,414
고정 크기 출력

140
00:11:31,414 --> 00:11:33,752
재발 성 신경망은 여전히 유용 할 수 있습니다.

141
00:11:33,752 --> 00:11:34,763
따라서이 예에서,

142
00:11:34,763 --> 00:11:35,982
우리는 예를 들어,

143
00:11:35,982 --> 00:11:38,793
우리 입력의 순차 처리.

144
00:11:38,793 --> 00:11:40,721
여기에서는 고정 크기 입력을 받고 있습니다.

145
00:11:40,721 --> 00:11:41,774
이미지처럼,

146
00:11:41,774 --> 00:11:43,732
우리는 분류 결정을 내리고 싶다.

147
00:11:43,732 --> 00:11:46,227
이 이미지에는 어떤 숫자가 표시됩니까?

148
00:11:46,227 --> 00:11:48,854
하지만 지금은 단일 피드 포워드 패스를 사용하는 것보다

149
00:11:48,854 --> 00:11:50,393
결정을 한꺼번에 만드는 것,

150
00:11:50,393 --> 00:11:52,702
이 네트워크는 실제로 이미지를 둘러보고 있습니다.

151
00:11:52,702 --> 00:11:55,553
이미지의 여러 부분을 다양한 형태로보실 수 있습니다.

152
00:11:55,553 --> 00:11:58,233
그리고 일련의 흘끗 보임을 만든 후에,

153
00:11:58,233 --> 00:11:59,882
최종 결정을 내린다.

154
00:11:59,882 --> 00:12:01,742
어떤 종류의 숫자가 존재하는지에 관해서.

155
00:12:01,742 --> 00:12:03,233
그래서 여기에, 우리는 하나를 가지고 있습니다.

156
00:12:03,233 --> 00:12:05,462
그래서 여기에, 우리의 입력과 출력,

157
00:12:05,462 --> 00:12:06,622
우리의 의견은 이미지였습니다.

158
00:12:06,622 --> 00:12:09,054
우리의 산출물은 분류 결정이었고,

159
00:12:09,054 --> 00:12:10,243
이 문맥조차,

160
00:12:10,243 --> 00:12:11,761
처리 할 수있는이 아이디어

161
00:12:11,761 --> 00:12:14,121
반복적 인 신경 네트워크를 이용한 가변 길이 처리

162
00:12:14,121 --> 00:12:17,473
정말 재미있는 유형의 모델로 이어질 수 있습니다.

163
00:12:17,473 --> 00:12:20,273
내가 좋아하는 정말 멋진 종이가있어.

164
00:12:20,273 --> 00:12:22,140
이 같은 유형의 아이디어를 적용한

165
00:12:22,140 --> 00:12:23,923
새로운 이미지를 생성하는 것.

166
00:12:23,923 --> 00:12:27,083
이제는 모델이 새로운 이미지를 합성하기를 원합니다.

167
00:12:27,083 --> 00:12:29,723
그것은 훈련에서 본 이미지와 비슷합니다.

168
00:12:29,723 --> 00:12:32,489
우리는 재발 성 신경망 구조를 사용할 수있다.

169
00:12:32,489 --> 00:12:34,222
실제로 이러한 출력 이미지를 페인트

170
00:12:34,222 --> 00:12:36,254
출력에서 한 번에 한 조각 씩.

171
00:12:36,254 --> 00:12:37,342
당신은 그것을 볼 수 있습니다,

172
00:12:37,342 --> 00:12:40,260
비록 우리의 출력이 고정 된 크기의 이미지이지만,

173
00:12:40,260 --> 00:12:42,942
우리는 시간이 지남에 따라 작동하는
이러한 모델을 가질 수 있습니다.

174
00:12:42,942 --> 00:12:46,380
한 번에 하나씩 출력의 부분을 계산합니다.

175
00:12:46,380 --> 00:12:48,577
그리고 재발 성 신경망을 사용할 수 있습니다.

176
00:12:48,577 --> 00:12:51,662
그 유형의 설치에 대해서도 마찬가지입니다.

177
00:12:51,662 --> 00:12:54,054
그래서 멋진 피치

178
00:12:54,054 --> 00:12:55,854
RNN이 할 수있는이 모든 멋진 것들에 대해서,

179
00:12:55,854 --> 00:12:58,785
당신은 궁금해 할 것입니다,
정확하게 이런 것들이 무엇입니까?

180
00:12:58,785 --> 00:13:00,353
그래서 일반적으로 재발 성 신경망

181
00:13:00,353 --> 00:13:04,163
이 작은,이 작은 반복 코어 세포가있다.

182
00:13:04,163 --> 00:13:06,272
그리고 그것은 약간의 입력 x를 취할 것이고,

183
00:13:06,272 --> 00:13:08,734
그 입력을 RNN에 공급하고,

184
00:13:08,734 --> 00:13:11,382
그 RNN은 내부 숨겨진 상태를 가지고 있는데,

185
00:13:11,382 --> 00:13:14,198
숨겨진 내부 상태가 업데이트됩니다.

186
00:13:14,198 --> 00:13:17,641
RNN이 새 입력을 읽을 때마다

187
00:13:17,641 --> 00:13:19,654
그리고 그 숨겨진 내부 상태

188
00:13:19,654 --> 00:13:21,243
그런 다음 모델로 다시 공급됩니다.

189
00:13:21,243 --> 00:13:23,980
다음에 입력을 읽습니다.

190
00:13:23,980 --> 00:13:26,334
그리고 자주, 우리는 우리의 RNN을 원할 것입니다.

191
00:13:26,334 --> 00:13:28,822
또한 매 시간 단계마다 약간의 출력을 내고,

192
00:13:28,822 --> 00:13:31,043
그래서 우리는 입력을 읽을 패턴을 가질 것이고,

193
00:13:31,043 --> 00:13:32,219
숨겨진 상태를 갱신하고,

194
00:13:32,219 --> 00:13:34,469
그런 다음 출력을 생성합니다.

195
00:13:35,814 --> 00:13:37,213
그럼 질문은

196
00:13:37,213 --> 00:13:39,862
이 반복 관계의 기능적 형태는 무엇인가?

197
00:13:39,862 --> 00:13:40,961
우리가 계산하는거야?

198
00:13:40,961 --> 00:13:42,923
그래서이 작은 녹색 RNN 블록 내부에,

199
00:13:42,923 --> 00:13:45,062
우리는 몇 가지 반복 관계를 계산하고 있습니다.

200
00:13:45,062 --> 00:13:46,443
함수 f.

201
00:13:46,443 --> 00:13:49,094
따라서이 함수 f는 어떤 가중치 W에 의존 할 것입니다.

202
00:13:49,094 --> 00:13:52,822
이전에 숨겨진 상태, h t-1,

203
00:13:52,822 --> 00:13:55,374
현재 상태에서의 입력, x t,

204
00:13:55,374 --> 00:13:56,683
그러면 출력됩니다.

205
00:13:56,683 --> 00:13:59,782
다음 숨은 상태, 또는 갱신 된 숨은 상태,

206
00:13:59,782 --> 00:14:01,420
우리는 ht라고 부른다.

207
00:14:01,420 --> 00:14:02,253
그리고 지금,

208
00:14:02,253 --> 00:14:04,382
다음 입력을 읽을 때,

209
00:14:04,382 --> 00:14:05,283
이 숨겨진 상태,

210
00:14:05,283 --> 00:14:06,894
이 새로운 숨은 상태, h t,

211
00:14:06,894 --> 00:14:08,774
같은 함수에 전달됩니다.

212
00:14:08,774 --> 00:14:11,552
우리는 다음 입력 인 x t와 1을 읽습니다.

213
00:14:11,552 --> 00:14:13,894
그리고 지금, 만약 우리가 어떤 산출물을 생산하고 싶다면

214
00:14:13,894 --> 00:14:15,273
이 네트워크의 모든 단계에서,

215
00:14:15,273 --> 00:14:20,203
우리는 추가로 완전히 연결된
레이어를 추가 할 수 있습니다.

216
00:14:20,203 --> 00:14:21,797
매 시간마다이 시간을 읽습니다.

217
00:14:21,797 --> 00:14:23,407
결정을 기반으로

218
00:14:23,407 --> 00:14:27,327
모든 시간 단계에서 숨겨진 상태로

219
00:14:27,327 --> 00:14:29,018
주목할 사실 중 하나는

220
00:14:29,018 --> 00:14:31,087
우리는 같은 함수 f w를 사용한다.

221
00:14:31,087 --> 00:14:32,495
그리고 같은 가중치, w,

222
00:14:32,495 --> 00:14:35,662
계산의 매 단계마다.

223
00:14:36,921 --> 00:14:39,706
가장 단순한 함수 형식

224
00:14:39,706 --> 00:14:40,634
당신이 상상할 수있는

225
00:14:40,634 --> 00:14:43,434
우리가 바닐라 재발 신경망이라고 부르는 것입니다.

226
00:14:43,434 --> 00:14:45,826
여기에 우리는이 동일한 기능적 형태를 가지고 있습니다.

227
00:14:45,826 --> 00:14:46,866
이전 슬라이드에서

228
00:14:46,866 --> 00:14:49,191
우리는 이전의 숨겨진 상태를 취하고 있습니다.

229
00:14:49,191 --> 00:14:50,145
우리의 현재 입력

230
00:14:50,145 --> 00:14:52,483
우리는 다음 숨겨진 상태를 만들어야합니다.

231
00:14:52,483 --> 00:14:54,954
그리고 당신이 상상할 수있는 가장 단순한 종류의

232
00:14:54,954 --> 00:14:57,724
우리가 어떤 가중치 행렬을 가지고 있다는 것입니다, w x h,

233
00:14:57,724 --> 00:15:00,124
우리가 입력 xt에 대해 곱하면,

234
00:15:00,124 --> 00:15:03,162
다른 가중치 행렬 w h h,

235
00:15:03,162 --> 00:15:05,615
이전 숨겨진 상태에 대해 우리가 곱하면됩니다.

236
00:15:05,615 --> 00:15:07,226
그래서 우리는이 두 곱셈을합니다.

237
00:15:07,226 --> 00:15:08,494
우리 두 주에 대하여,

238
00:15:08,494 --> 00:15:09,327
그들을 함께 추가,

239
00:15:09,327 --> 00:15:10,924
그 (것)들을 탄 (tanh)

240
00:15:10,924 --> 00:15:13,514
그래서 우리는 시스템에서 일종의 비선형 성을 얻습니다.

241
00:15:13,514 --> 00:15:15,287
왜 우리가 여기서 tanh를
사용하는지 궁금해 할 것입니다.

242
00:15:15,287 --> 00:15:17,312
다른 종류의 비선형 성이 아닌가?

243
00:15:17,312 --> 00:15:18,824
우리가 부정적인 말을 한 후에

244
00:15:18,824 --> 00:15:20,594
이전 강연에서 tanh에 대해,

245
00:15:20,594 --> 00:15:22,424
우리는 조금만 돌아올 것 같아요.

246
00:15:22,424 --> 00:15:23,257
나중에 우리가 이야기 할 때

247
00:15:23,257 --> 00:15:26,507
lstm과 같은 고급 아키텍처.

248
00:15:27,346 --> 00:15:28,695
그럼,이,

249
00:15:28,695 --> 00:15:30,872
그런 다음이 아키텍처에 추가적으로,

250
00:15:30,872 --> 00:15:33,394
우리가 모든 시간 단계에서 약간의 생산을 원한다면,

251
00:15:33,394 --> 00:15:35,284
다른 가중치 행렬 w를 가질 수도 있습니다.

252
00:15:35,284 --> 00:15:38,055
다른 가중치 행렬이있을 수 있습니다.

253
00:15:38,055 --> 00:15:39,055
이 숨겨진 상태를 받아 들인다.

254
00:15:39,055 --> 00:15:40,375
그런 다음이를 y로 변환합니다.

255
00:15:40,375 --> 00:15:42,724
어쩌면 몇 가지 클래스 점수 예측을 생산할 수 있습니다.

256
00:15:42,724 --> 00:15:44,826
매 단계마다.

257
00:15:44,826 --> 00:15:46,723
그리고 재발 성 신경 네트워크에 관해 생각할 때,

258
00:15:46,723 --> 00:15:48,775
나도 생각해. 너도 할 수있어.

259
00:15:48,775 --> 00:15:50,634
당신은 재발 성 신경 네트워크를 생각할 수 있습니다.

260
00:15:50,634 --> 00:15:51,487
두 가지면에서.

261
00:15:51,487 --> 00:15:53,713
하나는 숨겨진 상태를 갖는 개념입니다.

262
00:15:53,713 --> 00:15:57,095
반복적으로 자체적으로 피드를 제공합니다.

263
00:15:57,095 --> 00:15:59,135
그러나 그 그림이 조금 혼란 스럽습니다.

264
00:15:59,135 --> 00:16:01,073
때때로, 나는 그것을 더 명확하게 발견한다.

265
00:16:01,073 --> 00:16:04,546
이 계산 그래프를 풀기를 생각해 보라.

266
00:16:04,546 --> 00:16:05,914
여러 시간 단계.

267
00:16:05,914 --> 00:16:08,382
그리고 이것은 숨겨진 상태의 데이터 흐름을 만든다.

268
00:16:08,382 --> 00:16:09,914
및 입력들 및 출력들 및 가중치들

269
00:16:09,914 --> 00:16:11,786
어쩌면 조금 더 명확 할 것입니다.

270
00:16:11,786 --> 00:16:13,095
첫 번째 단계에서,

271
00:16:13,095 --> 00:16:15,494
우리는 초기 숨겨진 상태를 0으로 만들 것입니다.

272
00:16:15,494 --> 00:16:18,914
대부분의 경우 컨텍스트가 0으로 초기화됩니다.

273
00:16:18,914 --> 00:16:22,415
대부분의 문맥에서, 우리는 어떤
입력, x t를 가질 것이다.

274
00:16:22,415 --> 00:16:25,084
이 초기 숨겨진 상태, h 제로,

275
00:16:25,084 --> 00:16:26,906
그리고 우리의 현재 입력, x t,

276
00:16:26,906 --> 00:16:28,324
우리의 fw 함수로 갈 것입니다.

277
00:16:28,324 --> 00:16:32,604
그러면 다음 숨겨진 상태 인 h가 생성됩니다.

278
00:16:32,604 --> 00:16:34,034
그런 다음이 과정을 반복하겠습니다.

279
00:16:34,034 --> 00:16:36,154
다음 입력을받을 때.

280
00:16:36,154 --> 00:16:38,295
그래서 현재 우리의 현재 하나와 우리의 하나,

281
00:16:38,295 --> 00:16:40,036
그 같은 f w로 들어가고,

282
00:16:40,036 --> 00:16:42,847
우리의 다음 결과물을 만들어내는 겁니다.

283
00:16:42,847 --> 00:16:45,866
그리고이 과정은 반복해서 반복 될 것입니다.

284
00:16:45,866 --> 00:16:48,365
우리가 모든 입력을 소비 할 때, x ts,

285
00:16:48,365 --> 00:16:50,866
우리의 일련의 입력에서.

286
00:16:50,866 --> 00:16:52,116
그리고 지금 주목할 사항 중 하나는

287
00:16:52,116 --> 00:16:54,756
우리는 실제로 이것이 훨씬 더 명백하게 만들 수 있습니다.

288
00:16:54,756 --> 00:16:58,036
우리의 계산 그래프에 w 행렬을 씁니다.

289
00:16:58,036 --> 00:16:59,058
여기에

290
00:16:59,058 --> 00:17:01,434
우리는 같은 w 행렬을 다시 사용하고있다.

291
00:17:01,434 --> 00:17:03,415
계산의 매 단계마다.

292
00:17:03,415 --> 00:17:06,145
그래서 지금 우리가이 작은 f w 블록을 가질 때마다,

293
00:17:06,145 --> 00:17:08,724
고유 한 h 및 고유 x를 수신하고 있으며,

294
00:17:08,724 --> 00:17:11,006
그러나이 모든 블록들은 같은 것을 사용하고 있습니다.

295
00:17:11,007 --> 00:17:12,236
그리고 당신이 기억한다면,

296
00:17:12,236 --> 00:17:16,116
그라디언트가 역 전파에 어떻게
흐르게되는지에 대해 이야기했습니다.

297
00:17:16,116 --> 00:17:17,058
당신이 같은 것을 다시 사용할 때,

298
00:17:17,058 --> 00:17:19,218
같은 노드를 여러 번 다시 사용할 때

299
00:17:19,218 --> 00:17:20,786
전산 그래프에서,

300
00:17:20,786 --> 00:17:22,257
후진하는 동안 기억하고,

301
00:17:22,257 --> 00:17:24,047
당신은 그라디언트를 합산 해 버린다.

302
00:17:24,047 --> 00:17:25,257
w 행렬에 넣는다.

303
00:17:25,257 --> 00:17:28,218
당신이 계산을 할 때.

304
00:17:28,218 --> 00:17:30,557
그럼, 네가 생각해 보면

305
00:17:30,557 --> 00:17:32,526
이 모델의 역 전파,

306
00:17:32,526 --> 00:17:34,555
그러면 w에 대한 별도의 그라디언트가 생깁니다.

307
00:17:34,555 --> 00:17:36,506
각각의 시간 단계에서 흘러 나와,

308
00:17:36,506 --> 00:17:38,276
w에 대한 최종 그래디언트

309
00:17:38,276 --> 00:17:39,586
그 (것)들의 모두의 합계 일 것이다

310
00:17:39,586 --> 00:17:42,503
시간 단계 gradiants 당 개별.

311
00:17:43,615 --> 00:17:46,073
우리는 명시 적으로이 y t에 쓸 수 있습니다.

312
00:17:46,073 --> 00:17:47,727
이 계산 그래프에서.

313
00:17:47,727 --> 00:17:50,668
그런 다음,이 출력 ht는 매 시간 단계마다

314
00:17:50,668 --> 00:17:52,967
다른 작은 신경 네트워크로 피드 수 있습니다.

315
00:17:52,967 --> 00:17:54,858
yt를 생성 할 수있는

316
00:17:54,858 --> 00:17:57,256
그것은 어떤 학급 점수 일 수도
있고, 그런 것일 수도 있습니다.

317
00:17:57,256 --> 00:17:59,087
매 단계마다.

318
00:17:59,087 --> 00:18:00,738
또한 손실을보다 명확하게 나타낼 수 있습니다.

319
00:18:00,738 --> 00:18:03,167
그래서 많은 경우에, 당신은 생산,

320
00:18:03,167 --> 00:18:05,778
네가 어떤 진실의 진실을 알고 있다고 상상해 보라.

321
00:18:05,778 --> 00:18:07,588
시퀀스의 모든 단계에서

322
00:18:07,588 --> 00:18:08,978
그리고 나서 당신은 약간의 손실을 계산할 것입니다,

323
00:18:08,978 --> 00:18:10,487
일부 개인 손실,

324
00:18:10,487 --> 00:18:11,596
매 시간마다

325
00:18:11,596 --> 00:18:14,068
이 출력, y t.

326
00:18:14,068 --> 00:18:14,915
그리고이 손실은,

327
00:18:14,915 --> 00:18:17,556
그것은 종종 소프트 맥스 손실과 같은 것입니다,

328
00:18:17,556 --> 00:18:19,497
네가 가진 경우에, 아마,

329
00:18:19,497 --> 00:18:22,497
시퀀스의 모든 타임 스텝에서 지상 진실 라벨.

330
00:18:22,497 --> 00:18:24,076
그리고 지금 전체에 대한 최종 손실,

331
00:18:24,076 --> 00:18:25,395
이 전체 훈련 중지를 위해,

332
00:18:25,395 --> 00:18:27,887
이러한 개별 손실의 합계가됩니다.

333
00:18:27,887 --> 00:18:30,485
이제 우리는 매 단계마다 스케일러 손실을 겪었습니까?

334
00:18:30,485 --> 00:18:32,818
그리고 우리는 최종 결과를 얻기
위해 그것들을 요약했습니다.

335
00:18:32,818 --> 00:18:34,196
네트워크 상단의 스케일러 손실

336
00:18:34,196 --> 00:18:36,207
그리고 지금, 만약 당신이 생각한다면,

337
00:18:36,207 --> 00:18:37,898
다시,이 물건을 통해 다시 전파,

338
00:18:37,898 --> 00:18:38,896
우리는 모델을 훈련시키기 위해서,

339
00:18:38,896 --> 00:18:40,215
우리는 그라디언트를 계산할 필요가 있습니다.

340
00:18:40,215 --> 00:18:42,098
w에 관한 손실의

341
00:18:42,098 --> 00:18:44,498
그래서, 우리는 마지막 손실로부터 손실이 생길 것입니다.

342
00:18:44,498 --> 00:18:46,178
각각의 시간 간격으로

343
00:18:46,178 --> 00:18:47,708
그리고 나서 각각의 시간 간격

344
00:18:47,708 --> 00:18:49,840
가중치에 대한 로컬 그래디언트를 계산합니다. w,

345
00:18:49,840 --> 00:18:52,010
그러면이 모든 것이 합쳐져 우리에게 최종 결정권을줍니다.

346
00:18:52,010 --> 00:18:54,343
가중치에 대한 그래디언트, w.

347
00:18:55,597 --> 00:18:58,268
이제 우리가 일종의,이 다 대일 상황이 있다면,

348
00:18:58,268 --> 00:19:01,188
어쩌면 우리는 정서 분석과 같은 것을하고 싶을 것입니다.

349
00:19:01,188 --> 00:19:03,348
그런 다음 우리는 일반적으로 그 결정을 내릴 것입니다.

350
00:19:03,348 --> 00:19:05,799
이 네트워크의 최종 숨겨진 상태를 기반으로합니다.

351
00:19:05,799 --> 00:19:07,450
이 최종 숨김 상태

352
00:19:07,450 --> 00:19:08,988
종류는 모든 문맥을 요약한다.

353
00:19:08,988 --> 00:19:11,868
전체 시퀀스에서.

354
00:19:11,868 --> 00:19:14,788
또한, 우리가 일대 다 상황의 일종을 가지고 있다면,

355
00:19:14,788 --> 00:19:17,319
고정 크기 입력을 받고자하는 곳

356
00:19:17,319 --> 00:19:19,319
다양한 크기의 출력을 생성 할 수 있습니다.

357
00:19:19,319 --> 00:19:22,698
그런 다음 고정 크기 입력을 일반적으로 사용합니다.

358
00:19:22,698 --> 00:19:23,988
여하튼,

359
00:19:23,988 --> 00:19:26,050
모델의 초기 숨겨진 상태,

360
00:19:26,050 --> 00:19:27,986
이제는 재귀 네트워크가 틱 할 것입니다.

361
00:19:27,986 --> 00:19:30,079
출력의 각 셀에 대해.

362
00:19:30,079 --> 00:19:32,748
그리고 이제는 가변 크기 출력을 생성 할 때,

363
00:19:32,748 --> 00:19:36,915
출력의 각 요소에 대한 그래프를 언 롤합니다.

364
00:19:38,490 --> 00:19:41,370
따라서 시퀀스 모델에 대한 시퀀스를 말할 때

365
00:19:41,370 --> 00:19:44,308
여기서 기계 번역 같은 것을 할 수 있습니다.

366
00:19:44,308 --> 00:19:46,099
가변 크기 입력

367
00:19:46,099 --> 00:19:47,648
및 가변적 인 크기의 출력을 포함한다.

368
00:19:47,648 --> 00:19:49,300
이것을 조합으로 생각할 수 있습니다.

369
00:19:49,300 --> 00:19:50,719
많은 사람들이

370
00:19:50,719 --> 00:19:52,398
한 대를 더한 것.

371
00:19:52,398 --> 00:19:54,889
그래서 우리는 두 단계로 진행할 것입니다.

372
00:19:54,889 --> 00:19:56,900
우리가 인코더와 디코더라고 부르는 것.

373
00:19:56,900 --> 00:19:58,119
그래서 당신이 인코더라면,

374
00:19:58,119 --> 00:20:00,119
우리는 다양한 크기의 입력을 받게 될 것입니다.

375
00:20:00,119 --> 00:20:02,159
영어로 된 문장일지도 모릅니다.

376
00:20:02,159 --> 00:20:04,311
전체 문장을 요약합니다.

377
00:20:04,311 --> 00:20:08,110
인코더 네트워크의 최종 숨김 상태를 사용합니다.

378
00:20:08,110 --> 00:20:11,894
그리고 이제 우리는이 많은 상황에 처해 있습니다.

379
00:20:11,894 --> 00:20:14,396
이 전체 가변 크기 입력을 요약했습니다.

380
00:20:14,396 --> 00:20:15,769
이 단일 벡터에서,

381
00:20:15,769 --> 00:20:17,449
이제는 두 번째 디코더 네트워크가 있습니다.

382
00:20:17,449 --> 00:20:19,409
이는 일대 다 상황인데,

383
00:20:19,409 --> 00:20:21,740
그 단일 벡터를 입력합니다

384
00:20:21,740 --> 00:20:23,111
입력 문장 요약

385
00:20:23,111 --> 00:20:25,487
이제이 가변 출력을 생성합니다.

386
00:20:25,487 --> 00:20:28,969
다른 언어로 된 문장 일 수도 있습니다.

387
00:20:28,969 --> 00:20:30,980
그리고 지금이 가변적 인 규모의 결과물에서,

388
00:20:30,980 --> 00:20:32,820
우리는 매 단계마다 몇 가지 예언을 할 수 있습니다.

389
00:20:32,820 --> 00:20:34,609
어쩌면 어떤 단어를 사용할지를

390
00:20:34,609 --> 00:20:36,631
그리고 당신은이 모든 것을 훈련하는
것을 상상할 수 있습니다.

391
00:20:36,631 --> 00:20:38,199
이 계산 그래프를 풀어서

392
00:20:38,199 --> 00:20:40,608
출력 시퀀스에서의 손실 합산

393
00:20:40,608 --> 00:20:44,692
평소처럼 전파 전파를 수행합니다.

394
00:20:44,692 --> 00:20:46,391
그래서 약간 구체적인 예로서,

395
00:20:46,391 --> 00:20:47,729
우리가 자주 사용하는 한가지

396
00:20:47,729 --> 00:20:48,961
재발 성 신경 네트워크,

397
00:20:48,961 --> 00:20:50,940
언어 모델링이라고하는이 문제입니다.

398
00:20:50,940 --> 00:20:52,990
그래서 언어 모델링 문제에서,

399
00:20:52,990 --> 00:20:55,060
우리는 몇 가지 시퀀스를 읽고 싶다.

400
00:20:55,060 --> 00:20:58,151
우리는 우리의 네트워크를 가지기를 원합니다.

401
00:20:58,151 --> 00:21:00,908
자연어를 만드는 법.

402
00:21:00,908 --> 00:21:04,071
따라서 캐릭터 레벨에서 일어날 수 있습니다.

403
00:21:04,071 --> 00:21:06,601
여기서 우리 모델은 한 번에 하나씩 문자를 생성합니다.

404
00:21:06,601 --> 00:21:08,389
이것은 단어 수준에서도 발생할 수 있습니다.

405
00:21:08,389 --> 00:21:10,769
여기서 우리 모델은 한 번에 하나씩 단어를 만들어냅니다.

406
00:21:10,769 --> 00:21:12,129
그러나 아주 간단한 예제에서,

407
00:21:12,129 --> 00:21:14,740
당신은이 문자 레벨 언어 모델을 상상할 수 있습니다.

408
00:21:14,740 --> 00:21:15,980
우리가 원하는 곳에,

409
00:21:15,980 --> 00:21:18,081
네트워크가 일련의 문자를 읽는 곳

410
00:21:18,081 --> 00:21:19,388
그리고 나서 그것은 예측할 필요가 있습니다.

411
00:21:19,388 --> 00:21:22,780
다음 문자는이 텍스트 스트림에 무엇이 있습니까?

412
00:21:22,780 --> 00:21:24,700
따라서이 예에서,

413
00:21:24,700 --> 00:21:27,460
우리는이 편지에 네 글자의 아주 작은 어휘가 있습니다.

414
00:21:27,460 --> 00:21:31,213
h, e, l, 및 o를 포함하며,이 예시적인 훈련 시퀀스

415
00:21:31,213 --> 00:21:33,884
안녕하세요, h, e, l, l, o.

416
00:21:33,884 --> 00:21:34,911
그래서 훈련 도중,

417
00:21:34,911 --> 00:21:37,586
우리가이 언어 모델을 훈련 할 때,

418
00:21:37,586 --> 00:21:42,100
우리는이 트레이닝 시퀀스의 캐릭터를 공급할 것입니다.

419
00:21:42,100 --> 00:21:46,119
입력으로, x ts, 우리 입력의 아웃 입력,

420
00:21:46,119 --> 00:21:49,689
우리는 우리 훈련 과정의 인물들에게 먹이를 줄 것이며,

421
00:21:49,689 --> 00:21:52,191
이것들은 우리가 입력으로 먹는 x ts가 될 것입니다.

422
00:21:52,191 --> 00:21:53,980
우리의 재귀 신경 네트워크.

423
00:21:53,980 --> 00:21:56,168
그리고 나서, 각각의 입력,

424
00:21:56,168 --> 00:21:57,678
그것은 편지,

425
00:21:57,678 --> 00:21:58,890
우리는 길을 찾아야 해.

426
00:21:58,890 --> 00:22:01,039
우리 네트워크의 편지를 나타낼 수 있습니다.

427
00:22:01,039 --> 00:22:02,759
그래서 우리가 일반적으로 할 일은 알아내는 것입니다.

428
00:22:02,759 --> 00:22:05,100
우리의 전체 어휘는 무엇입니까?

429
00:22:05,100 --> 00:22:07,460
이 경우 어휘에는 네 가지 요소가 있습니다.

430
00:22:07,460 --> 00:22:09,967
그리고 각 문자는 벡터로 표현됩니다.

431
00:22:09,967 --> 00:22:12,589
모든 슬롯에는 0이 있지만 하나는,

432
00:22:12,589 --> 00:22:15,031
어휘에서 슬롯을위한 것

433
00:22:15,031 --> 00:22:16,628
그 편지에 해당.

434
00:22:16,628 --> 00:22:18,092
이 작은 예에서,

435
00:22:18,092 --> 00:22:20,751
우리의 보카에는 4 개의 글자 h, e, l, o,

436
00:22:20,751 --> 00:22:22,324
그리고 나서 우리의 입력 시퀀스,

437
00:22:22,324 --> 00:22:25,374
h는 4 요소 벡터

438
00:22:25,374 --> 00:22:26,535
첫 번째 슬롯에 하나

439
00:22:26,535 --> 00:22:28,684
다른 세 슬롯에는 0이 표시됩니다.

440
00:22:28,684 --> 00:22:29,876
그리고 우리는 같은 종류의 패턴을 사용합니다.

441
00:22:29,876 --> 00:22:31,306
모든 다른 문자를 나타 내기 위해

442
00:22:31,306 --> 00:22:33,139
입력 순서에서.

443
00:22:34,914 --> 00:22:37,021
지금,이 전진 통과 중에

444
00:22:37,021 --> 00:22:38,575
이 네트워크가 무엇을하고 있는지,

445
00:22:38,575 --> 00:22:39,965
제 1 단계에서,

446
00:22:39,965 --> 00:22:41,874
입력 문자 h가 수신됩니다.

447
00:22:41,874 --> 00:22:45,285
그것은 첫 번째 RNN으로 들어가고,

448
00:22:45,285 --> 00:22:46,714
RNN 셀에,

449
00:22:46,714 --> 00:22:48,594
그리고 나서 우리는이 결과를 생산할 것입니다, y t,

450
00:22:48,594 --> 00:22:50,285
예측을 만드는 네트워크입니다.

451
00:22:50,285 --> 00:22:52,525
어휘의 각 문자에 대해,

452
00:22:52,525 --> 00:22:54,411
어떤 편지가 가장 가능성이 있다고 생각하니?

453
00:22:54,411 --> 00:22:56,024
다음에 올거야.

454
00:22:56,024 --> 00:22:57,245
이 예에서,

455
00:22:57,245 --> 00:22:59,330
올바른 출력 문자는 e

456
00:22:59,330 --> 00:23:01,405
우리 훈련 과정이여 깄기 때문에,

457
00:23:01,405 --> 00:23:04,488
그러나 모델은 실제로 예측하고 있습니다.

458
00:23:05,477 --> 00:23:06,310
나는 그것이 실제로 생각한다.

459
00:23:06,310 --> 00:23:07,850
o를 가장 가능성있는 편지로 예측합니다.

460
00:23:07,850 --> 00:23:09,690
그래서이 경우에,이 예측은 잘못되었습니다.

461
00:23:09,690 --> 00:23:11,210
우리는 softmaxt 손실을 사용할 것입니다.

462
00:23:11,210 --> 00:23:13,889
이러한 예측으로 불행을 계량화하십시오.

463
00:23:13,889 --> 00:23:15,192
다음 번 단계에서는,

464
00:23:15,192 --> 00:23:16,680
우리는 두 번째 편지를 먹을 것입니다.

465
00:23:16,680 --> 00:23:18,031
트레이닝 시퀀스에서, e,

466
00:23:18,031 --> 00:23:19,741
이 과정이 반복됩니다.

467
00:23:19,741 --> 00:23:22,181
e를 벡터로 나타냅니다.

468
00:23:22,181 --> 00:23:24,232
그 입력 벡터를 함께 사용하십시오.

469
00:23:24,232 --> 00:23:25,649
이전 숨겨진 상태로

470
00:23:25,649 --> 00:23:27,271
새로운 숨겨진 상태를 생성한다.

471
00:23:27,271 --> 00:23:28,792
이제 두 번째 숨겨진 상태를 사용하십시오.

472
00:23:28,792 --> 00:23:30,032
다시 예측하다

473
00:23:30,032 --> 00:23:31,912
어휘의 모든 문자 위에

474
00:23:31,912 --> 00:23:34,232
이 경우, 우리의 훈련 과정이여 겼기 때문에,

475
00:23:34,232 --> 00:23:35,712
문자 e 이후에,

476
00:23:35,712 --> 00:23:36,810
우리는 우리 모델이 l을 예측하기를 원한다.

477
00:23:36,810 --> 00:23:37,893
이 경우,

478
00:23:40,183 --> 00:23:41,832
우리 모델은 예측이 매우 낮을 수 있습니다.

479
00:23:41,832 --> 00:23:44,244
편지 l 때문에, 우리는 높은 손실을 입을 것입니다.

480
00:23:44,244 --> 00:23:46,794
그리고 당신은이 과정을 반복해서 반복합니다.

481
00:23:46,794 --> 00:23:50,343
그리고이 모델을 많은 다른 시퀀스로 훈련 시키면,

482
00:23:50,343 --> 00:23:52,023
결국 그것은 배워야한다.

483
00:23:52,023 --> 00:23:54,303
시퀀스의 다음 문자를 예측하는 방법

484
00:23:54,303 --> 00:23:56,763
모든 이전 문자의 컨텍스트를 기반으로

485
00:23:56,763 --> 00:23:58,596
전에 보았던 것.

486
00:23:59,893 --> 00:24:01,655
그리고 지금, 테스트 시간에 무슨
일이 일어나는지 생각한다면,

487
00:24:01,655 --> 00:24:03,388
우리가이 모델을 훈련시킨 후에,

488
00:24:03,388 --> 00:24:05,033
우리가 그걸로 할 수있는 한가지

489
00:24:05,033 --> 00:24:07,594
모델의 샘플입니다.

490
00:24:07,594 --> 00:24:09,673
실제로 훈련 된 신경망 모델을 사용합니다.

491
00:24:09,673 --> 00:24:11,764
새 텍스트 합성

492
00:24:11,764 --> 00:24:13,592
그런 종류의 정신이 비슷해 보인다.

493
00:24:13,592 --> 00:24:15,103
훈련 된 텍스트에

494
00:24:15,103 --> 00:24:16,312
이 방법이 효과가있다.

495
00:24:16,312 --> 00:24:17,833
우리는 전형적으로 모델을 볼 것인가?

496
00:24:17,833 --> 00:24:19,634
텍스트의 일부 접두어가 붙어 있습니다.

497
00:24:19,634 --> 00:24:22,716
이 경우, 접두사는 단지 하나의 문자 h이고,

498
00:24:22,716 --> 00:24:24,327
이제 우리는 그 편지 h를 먹을거야.

499
00:24:24,327 --> 00:24:27,295
우리의 재귀 신경 네트워크의 첫 번째 단계를 통해.

500
00:24:27,295 --> 00:24:30,335
이 점수 분포를 산출합니다.

501
00:24:30,335 --> 00:24:32,916
어휘의 모든 문자에 대해

502
00:24:32,916 --> 00:24:35,592
이제 교육 시간에이 점수를 사용합니다.

503
00:24:35,592 --> 00:24:37,501
그것에서 실제로 견본을 내기 위하여.

504
00:24:37,501 --> 00:24:38,893
그래서 우리는 softmaxt 함수를 사용할 것입니다.

505
00:24:38,893 --> 00:24:41,421
점수를 확률 분포로 변환

506
00:24:41,421 --> 00:24:44,162
그 확률 분포로부터 표본을 추출 할 것입니다.

507
00:24:44,162 --> 00:24:47,362
시퀀스의 두 번째 문자를 실제로 합성합니다.

508
00:24:47,362 --> 00:24:50,491
그리고이 경우 점수가 꽤 나빴지만,

509
00:24:50,491 --> 00:24:53,061
어쩌면 우리는 운이 좋았고 편지 e를 샘플링했을 것입니다.

510
00:24:53,061 --> 00:24:54,771
이 확률 분포로부터.

511
00:24:54,771 --> 00:24:56,962
그리고 이제, 우리는이 편지를 가져갈 것입니다. e

512
00:24:56,962 --> 00:24:59,253
이 분포에서 샘플링 한

513
00:24:59,253 --> 00:25:01,522
네트워크로 입력으로 다시 피드

514
00:25:01,522 --> 00:25:02,492
다음 번 단계에서.

515
00:25:02,492 --> 00:25:05,118
자, 우리는 이것을 가지고 위쪽에서 내려 놓고,

516
00:25:05,118 --> 00:25:07,170
그것을 네트워크로 되 돌린다.

517
00:25:07,170 --> 00:25:10,071
이 중 하나로서, 일종의 뜨거운 벡터 표현,

518
00:25:10,071 --> 00:25:11,530
그런 다음 프로세스를 반복하십시오.

519
00:25:11,530 --> 00:25:15,008
출력에서 두 번째 문자를 합성합니다.

520
00:25:15,008 --> 00:25:17,290
그리고 우리는이 과정을 반복해서 반복 할 수 있습니다.

521
00:25:17,290 --> 00:25:20,722
이 훈련 된 모델을 사용하여 새로운 시퀀스를 합성하고,

522
00:25:20,722 --> 00:25:22,442
여기서 우리는 서열을 합성하고있다.

523
00:25:22,442 --> 00:25:23,770
한 번에 한 문자 씩

524
00:25:23,770 --> 00:25:25,922
이들 예측 된 확률 분포를 이용하여

525
00:25:25,922 --> 00:25:27,712
각 시간 단계마다.

526
00:25:27,712 --> 00:25:28,545
문제?

527
00:25:34,792 --> 00:25:36,181
그래, 큰 질문이다.

528
00:25:36,181 --> 00:25:37,960
그래서 질문은 우리가

529
00:25:37,960 --> 00:25:39,381
그냥 문자를 가져가는 대신

530
00:25:39,381 --> 00:25:41,315
가장 큰 점수?

531
00:25:41,315 --> 00:25:42,398
이 경우,

532
00:25:43,294 --> 00:25:45,624
우리가 가진 확률 분포 때문에,

533
00:25:45,624 --> 00:25:47,451
올바른 성격을 얻는 것이 불가능했습니다.

534
00:25:47,451 --> 00:25:49,642
그래서 우리는 표본을 가지고 있으므로
예제가 효과적 일 수 있습니다.

535
00:25:49,642 --> 00:25:51,512
그것은 의미가 있습니다.

536
00:25:51,512 --> 00:25:54,384
그러나 실제로는 때때로 두 가지를 모두 보게됩니다.

537
00:25:54,384 --> 00:25:56,432
그래서 때로는 argmax 확률을 취할 것입니다.

538
00:25:56,432 --> 00:25:59,482
그리고 그것은 때때로 조금 더 안정 될 것입니다,

539
00:25:59,482 --> 00:26:01,791
그러나 샘플링의 한 가지 이점은 일반적으로,

540
00:26:01,791 --> 00:26:04,264
그것은 당신이 당신의 모델로부터
다양성을 얻을 수 있다는 것입니다.

541
00:26:04,264 --> 00:26:07,242
때로는 동일한 입력을 가질 수도 있지만,

542
00:26:07,242 --> 00:26:08,490
어쩌면 같은 접두사,

543
00:26:08,490 --> 00:26:09,722
또는 이미지 캡션의 경우,

544
00:26:09,722 --> 00:26:11,421
어쩌면 같은 이미지.

545
00:26:11,421 --> 00:26:13,583
하지만 argmax를 취하는 것보다는 샘플을 얻는다면,

546
00:26:13,583 --> 00:26:15,562
그러면 때때로 이러한 훈련 된 모델이

547
00:26:15,562 --> 00:26:18,352
실제로 여러 가지 유형을 생산할 수있다.

548
00:26:18,352 --> 00:26:20,032
합리적인 출력 순서,

549
00:26:20,032 --> 00:26:21,053
종류에 따라,

550
00:26:21,053 --> 00:26:22,573
그들이 취하는 샘플에 따라

551
00:26:22,573 --> 00:26:23,824
처음 단계에서.

552
00:26:23,824 --> 00:26:25,973
실제로 이점이 있습니다.

553
00:26:25,973 --> 00:26:29,213
우리가 지금 우리의 산출물에 더 많은
다양성을 갖게 할 수 있기 때문입니다.

554
00:26:29,213 --> 00:26:30,630
다른 질문?

555
00:26:35,143 --> 00:26:37,153
우리는 softmax 벡터를 먹을 수 있을까요?

556
00:26:37,153 --> 00:26:38,602
하나의 요소 벡터 대신에?

557
00:26:38,602 --> 00:26:40,435
시험 시간에?

558
00:26:46,162 --> 00:26:47,992
그래, 그래, 질문은, 시험 시간에,

559
00:26:47,992 --> 00:26:49,712
우리는이 전체 softmax 벡터를 먹을 수 있을까요?

560
00:26:49,712 --> 00:26:51,373
하나의 뜨거운 벡터보다는 오히려?

561
00:26:51,373 --> 00:26:52,752
그런데 두 가지 문제가 있습니다.

562
00:26:52,752 --> 00:26:54,762
하나는 그것이 매우 다른 것입니다.

563
00:26:54,762 --> 00:26:56,782
교육 시간에 본 데이터로부터

564
00:26:56,782 --> 00:26:58,792
일반적으로 모델에 질문하면

565
00:26:58,792 --> 00:26:59,943
시험 시간에 무언가를하려면,

566
00:26:59,943 --> 00:27:01,223
훈련 시간과는 다른,

567
00:27:01,223 --> 00:27:02,752
그 때 그것은 일반적으로 날아갈 것입니다.

568
00:27:02,752 --> 00:27:03,784
보통 쓰레기를 줄거야.

569
00:27:03,784 --> 00:27:05,413
그리고 너는 보통 슬퍼 할 것이다.

570
00:27:05,413 --> 00:27:07,083
다른 문제는 실제로,

571
00:27:07,083 --> 00:27:09,112
우리의 어휘는 매우 클 수 있습니다.

572
00:27:09,112 --> 00:27:10,480
어쩌면이 간단한 예에서,

573
00:27:10,480 --> 00:27:12,182
우리의 어휘는 단지 네 가지 요소 일뿐입니다.

574
00:27:12,182 --> 00:27:13,202
그래서 큰 문제는 아닙니다.

575
00:27:13,202 --> 00:27:15,611
그러나 한 번에 하나씩 단어를
생성하는 것에 대해 생각하고 있다면,

576
00:27:15,611 --> 00:27:18,773
이제 당신의 어휘는 영어로 된 모든 단어입니다.

577
00:27:18,773 --> 00:27:21,162
그것은 수만 가지 요소와 같을 수 있습니다.

578
00:27:21,162 --> 00:27:23,642
그래서 실제로,이 첫 번째 요소,

579
00:27:23,642 --> 00:27:26,373
이 첫 번째 작업은이 하나의 핫 벡터를 사용합니다.

580
00:27:26,373 --> 00:27:29,173
스파 스 벡터 연산을 사용하여 종종 수행됩니다

581
00:27:29,173 --> 00:27:30,533
밀집한 요인보다는.

582
00:27:30,533 --> 00:27:32,693
일종의, 계산적으로 정말 나쁜 것입니다.

583
00:27:32,693 --> 00:27:35,018
이 하중을 받고 싶다면

584
00:27:35,018 --> 00:27:36,827
10,000 요소 softmax 벡터.

585
00:27:36,827 --> 00:27:38,802
그래서 대개 대신에 뜨거운 것을 사용합니다.

586
00:27:38,802 --> 00:27:40,302
테스트 시간에도.

587
00:27:42,121 --> 00:27:44,482
우리가 시퀀스를 가지고 있다는 생각

588
00:27:44,482 --> 00:27:47,104
시퀀스의 매 시간 단계마다 출력을 생성합니다.

589
00:27:47,104 --> 00:27:48,693
마지막으로 약간의 손실을 계산하고,

590
00:27:48,693 --> 00:27:51,543
이것은 때때로 시간에 따른 역 전파라고 불린다.

591
00:27:51,543 --> 00:27:53,962
당신은 앞으로의 패스에서,

592
00:27:53,962 --> 00:27:55,819
당신은 일종의 시간을 통해 앞으로 나아갈 수 있습니다.

593
00:27:55,819 --> 00:27:57,053
그 후 역방향 패스 동안,

594
00:27:57,053 --> 00:27:59,042
당신은 일종의 거슬러 올라가는 시간입니다.

595
00:27:59,042 --> 00:28:00,762
모든 그라디언트를 계산할 수 있습니다.

596
00:28:00,762 --> 00:28:03,389
실제로 문제가 될 수 있습니다.

597
00:28:03,389 --> 00:28:06,162
매우 길다는 것을 훈련시키고 자한다면.

598
00:28:06,162 --> 00:28:08,333
그래서 우리가 일종의 기차 훈련을하고 있다고 상상한다면

599
00:28:08,333 --> 00:28:09,602
신경망 언어 모델

600
00:28:09,602 --> 00:28:12,107
어쩌면 위키 백과의 전체 텍스트에,

601
00:28:12,107 --> 00:28:13,171
그건 그렇고,

602
00:28:13,171 --> 00:28:15,453
사람들이 꽤 자주하는 일,

603
00:28:15,453 --> 00:28:16,672
이것은 매우 천천히,

604
00:28:16,672 --> 00:28:19,282
그라데이션 단계를 만들 때마다

605
00:28:19,282 --> 00:28:20,670
우리는 앞으로 전달해야 할 것입니다.

606
00:28:20,670 --> 00:28:23,328
모든 위키 피 디아의 전체 텍스트를 통해,

607
00:28:23,328 --> 00:28:25,901
위키 피 디아를 모두 뒤쪽으로 통과 시켜라.

608
00:28:25,901 --> 00:28:27,813
그런 다음 단일 그래디언트 업데이트를 수행하십시오.

609
00:28:27,813 --> 00:28:28,750
그리고 그것은 아주 느릴 것입니다.

610
00:28:28,750 --> 00:28:29,984
귀하의 모델은 절대 수렴하지 않습니다.

611
00:28:29,984 --> 00:28:31,853
그것은 또한 우스운 양의 기억을 취할 것입니다.

612
00:28:31,853 --> 00:28:34,172
그래서 이것은 정말로 나쁠 것입니다.

613
00:28:34,172 --> 00:28:36,384
실제로 사람들이하는 일은 이것, 일종의,

614
00:28:36,384 --> 00:28:39,933
근사는 시간을 통한 잘린
backpropagation이라고합니다.

615
00:28:39,933 --> 00:28:41,522
여기서 아이디어는,

616
00:28:41,522 --> 00:28:43,973
비록 우리의 입력 시퀀스가 매우 길다 할지라도,

617
00:28:43,973 --> 00:28:45,562
심지어 잠재적으로 무한한,

618
00:28:45,562 --> 00:28:47,184
우리가 할 일은,

619
00:28:47,184 --> 00:28:48,521
우리가 모델을 훈련 할 때,

620
00:28:48,521 --> 00:28:50,544
우리는 몇 단계를 거치며 앞으로 나아갈 것입니다.

621
00:28:50,544 --> 00:28:54,402
어쩌면 백 개가 구장 번호 같은 것 같아.

622
00:28:54,402 --> 00:28:56,232
사람들이 자주 사용하는,

623
00:28:56,232 --> 00:28:58,623
우리는 아마 100 걸음 씩 나아갈 것입니다.

624
00:28:58,623 --> 00:29:01,944
데이터의이 서브 시퀀스에 대해서만 손실을 계산하고,

625
00:29:01,944 --> 00:29:04,312
이 서브 시퀀스를 통해 다시 전파하고,

626
00:29:04,312 --> 00:29:06,261
이제 그라데이션 단계를 만듭니다.

627
00:29:06,261 --> 00:29:08,102
그리고 지금, 우리가 반복 할 때, 음,

628
00:29:08,102 --> 00:29:10,072
우리는 여전히 이러한 숨겨진 상태를 가지고 있습니다.

629
00:29:10,072 --> 00:29:12,064
첫 번째 배치에서 계산 한

630
00:29:12,064 --> 00:29:14,664
이제 다음 번 데이터 배치를 계산할 때,

631
00:29:14,664 --> 00:29:17,933
우리는 그 숨겨진 상태를 제 시간에 전진시킬 것이며,

632
00:29:17,933 --> 00:29:20,631
따라서 정방향 패스는 완전히 동일합니다.

633
00:29:20,631 --> 00:29:23,184
그러나 이제 우리가 그라데이션 단계를 계산할 때

634
00:29:23,184 --> 00:29:24,392
이 다음 데이터 묶음의 경우,

635
00:29:24,392 --> 00:29:28,124
이 두 번째 일괄 처리를 통해서만
다시 백 프로게이트 할 것입니다.

636
00:29:28,124 --> 00:29:29,659
이제 우리는 그라데이션 단계를 만들 것입니다.

637
00:29:29,659 --> 00:29:32,760
시간을 통한이 잘린 역 전파를 기반으로합니다.

638
00:29:32,760 --> 00:29:34,392
이 과정은 계속 될 것이며,

639
00:29:34,392 --> 00:29:36,440
이제 우리가 다음 배치를 만들 때,

640
00:29:36,440 --> 00:29:38,250
우리는이 숨겨진 상태들을 다시 복사 할 것이고,

641
00:29:38,250 --> 00:29:40,930
그러나 앞으로 나아가고 나서 뒤로 물러서십시오,

642
00:29:40,930 --> 00:29:43,840
그러나 약간의 시간 단계에 대해서만 가능합니다.

643
00:29:43,840 --> 00:29:44,673
그래서,

644
00:29:44,673 --> 00:29:45,600
너는 이걸 생각 해낼 수있어.

645
00:29:45,600 --> 00:29:48,250
그라데이션 강하에서 캐스트 된 alegist 인 것으로

646
00:29:48,250 --> 00:29:49,872
시퀀스의 경우.

647
00:29:49,872 --> 00:29:52,280
모델 기억에 대해 이야기 할 때 기억하십시오.

648
00:29:52,280 --> 00:29:53,690
큰 데이터 세트,

649
00:29:53,690 --> 00:29:54,672
그런 다음이 데이터 세트,

650
00:29:54,672 --> 00:29:56,880
그라디언트를 계산하는 데 비용이 많이들 것입니다.

651
00:29:56,880 --> 00:29:58,720
데이터 세트의 모든 요소에 대해

652
00:29:58,720 --> 00:30:00,861
그래서 대신 우리는 작은 표본을 취합니다.

653
00:30:00,861 --> 00:30:02,520
작은 미니 배치 대신,

654
00:30:02,520 --> 00:30:05,290
미니 배치 데이터를 사용하여 그래디언트 스톱을 계산합니다.

655
00:30:05,290 --> 00:30:08,053
모든 종류의 이미지 분류 경우.

656
00:30:08,053 --> 00:30:08,886
문제?

657
00:30:12,441 --> 00:30:14,415
이런 종류의 질문입니까?

658
00:30:14,415 --> 00:30:15,989
마크 홉 (Mark Hobb) 가정을
만드는 것은 이런 종류입니까?

659
00:30:15,989 --> 00:30:17,239
아니 정말.

660
00:30:18,100 --> 00:30:20,001
우리가이 숨겨진 상태를 앞으로 옮기고 있기 때문에

661
00:30:20,001 --> 00:30:21,442
영원히.

662
00:30:21,442 --> 00:30:23,512
마르코 비아 가정을하고있다.

663
00:30:23,512 --> 00:30:25,792
숨겨진 상태를 조건으로,

664
00:30:25,792 --> 00:30:27,733
그러나 숨겨진 상태가 우리가 필요로하는 모든 것입니다.

665
00:30:27,733 --> 00:30:28,971
미래를 예언하기

666
00:30:28,971 --> 00:30:31,101
시퀀스의

667
00:30:31,101 --> 00:30:33,019
하지만 그 가정은 일종의 건축입니다.

668
00:30:33,019 --> 00:30:35,012
재발 성 신경 네트워크 공식

669
00:30:35,012 --> 00:30:35,941
출발점에서.

670
00:30:35,941 --> 00:30:37,269
그리고 그것은 정말로 특별하지 않습니다.

671
00:30:37,269 --> 00:30:39,032
시간을 통한 전파를 역전 시키십시오.

672
00:30:39,032 --> 00:30:40,372
역 전파 시간,

673
00:30:40,372 --> 00:30:43,061
또는 죄송합니다.

674
00:30:43,061 --> 00:30:44,352
이 기울기를 근사화하는 방법 일뿐입니다.

675
00:30:44,352 --> 00:30:47,072
거꾸로 통과하지 않고

676
00:30:47,072 --> 00:30:51,239
잠재적으로 매우 큰 데이터 시퀀스를 통해

677
00:30:52,677 --> 00:30:55,301
이 모든 것은 매우 복잡하고 혼란스럽게 들립니다.

678
00:30:55,301 --> 00:30:56,871
그리고 많은 코드를 작성하는 것처럼 들리지만,

679
00:30:56,871 --> 00:30:59,649
그러나 사실 이것은 매우 간결 할 수 있습니다.

680
00:30:59,649 --> 00:31:03,418
Andrea는 min-char-rnn이라고하는이
예를 가지고 있습니다.

681
00:31:03,418 --> 00:31:04,816
이 모든 것들을 수행합니다.

682
00:31:04,816 --> 00:31:07,474
파이썬 112 라인처럼.

683
00:31:07,474 --> 00:31:09,045
어휘 작성을 처리합니다.

684
00:31:09,045 --> 00:31:09,925
그것은 모델을 훈련시킨다.

685
00:31:09,925 --> 00:31:11,725
시간 경과에 따른 잘린 전파가있다.

686
00:31:11,725 --> 00:31:13,936
그런 다음 실제로 해당 모델에서 샘플링 할 수 있습니다.

687
00:31:13,936 --> 00:31:16,584
실제로 너무 많은 코드는 아닙니다.

688
00:31:16,584 --> 00:31:17,925
그럼에도 불구하고

689
00:31:17,925 --> 00:31:18,965
크고 무서운 과정의 종류,

690
00:31:18,965 --> 00:31:20,862
실제로 그렇게 어렵지는 않습니다.

691
00:31:20,862 --> 00:31:22,314
나는 당신을 혼란스럽게한다면,

692
00:31:22,314 --> 00:31:23,456
아마 이것 좀 봐봐.

693
00:31:23,456 --> 00:31:25,125
자신의 시간에 코드를 단계별로 실행하십시오.

694
00:31:25,125 --> 00:31:27,074
이러한 구체적인 단계를 모두 볼 수 있습니다.

695
00:31:27,074 --> 00:31:27,954
코드에서 발생합니다.

696
00:31:27,954 --> 00:31:30,184
그래서 이것은 모두 하나의 파일에 있습니다.

697
00:31:30,184 --> 00:31:31,723
모두 종속성이없는 numpy를 사용합니다.

698
00:31:31,723 --> 00:31:34,473
이것은 상대적으로 읽기가 쉬웠습니다.

699
00:31:35,584 --> 00:31:36,896
그럼 일단 우리가이 생각을하면

700
00:31:36,896 --> 00:31:39,056
재발 성 신경망 언어 모델을 훈련시키는 것,

701
00:31:39,056 --> 00:31:41,593
우리는 실제로 이것으로 많은 즐거움을 누릴 수 있습니다.

702
00:31:41,593 --> 00:31:43,984
그리고 우리는 우리가 원하는
텍스트를 받아 들일 수 있습니다.

703
00:31:43,984 --> 00:31:46,424
당신이 생각할 수있는 무작위 텍스트

704
00:31:46,424 --> 00:31:47,384
인터넷에서,

705
00:31:47,384 --> 00:31:49,656
우리의 반복 신경 네트워크 언어 모델을 훈련시키다.

706
00:31:49,656 --> 00:31:50,616
이 본문에,

707
00:31:50,616 --> 00:31:52,304
새 텍스트를 생성하십시오.

708
00:31:52,304 --> 00:31:55,502
따라서이 예에서이 전체 텍스트를 가져 왔습니다.

709
00:31:55,502 --> 00:31:57,394
셰익스피어의 모든 작품 중,

710
00:31:57,394 --> 00:31:59,314
그런 다음 훈련에 사용했습니다.

711
00:31:59,314 --> 00:32:00,885
재발 성 신경망 언어 모델

712
00:32:00,885 --> 00:32:02,634
모든 셰익스피어에.

713
00:32:02,634 --> 00:32:03,896
그리고 당신은 훈련의 시작,

714
00:32:03,896 --> 00:32:05,773
어쩌면 무작위로 횡설수설하는 쓰레기를 생산하는 종류입니다.

715
00:32:05,773 --> 00:32:08,216
그러나 훈련 과정에서,

716
00:32:08,216 --> 00:32:11,584
그것은 비교적 합리적인 것처럼 보이는 것들을 만들어 낸다.

717
00:32:11,584 --> 00:32:12,784
네가 한 후에,

718
00:32:12,784 --> 00:32:14,272
이 모델이 꽤 잘 훈련 된 후에,

719
00:32:14,272 --> 00:32:16,104
그때 그것은 보이는 텍스트를 생산합니다,

720
00:32:16,104 --> 00:32:18,224
일종의, 셰익스피어 - 나에게 에스 케야.

721
00:32:18,224 --> 00:32:20,264
"그 날에 어찌하여 어찌하여 행 하느냐?"

722
00:32:20,264 --> 00:32:23,184
뭐든, 맞아, 이걸 읽을 수있어.

723
00:32:23,184 --> 00:32:24,754
마치 셰익스피어처럼 보입니다.

724
00:32:24,754 --> 00:32:27,184
그리고이 모델을 실제로 훈련 시키면,

725
00:32:27,184 --> 00:32:28,765
그것을 더 멀리 수렴하게하십시오,

726
00:32:28,765 --> 00:32:31,264
이러한 긴 시퀀스를 샘플링하여,

727
00:32:31,264 --> 00:32:33,904
당신은 모든 종류의 멋진 것들을
배우는 것을 볼 수 있습니다.

728
00:32:33,904 --> 00:32:35,864
그것은 실제로 셰익스피어 희곡처럼 보입니다.

729
00:32:35,864 --> 00:32:38,856
아마도이 표제를 사용한다는 것을 알고 있습니다.

730
00:32:38,856 --> 00:32:40,016
to say 누가 말하고 있는지.

731
00:32:40,016 --> 00:32:42,224
그런 다음이 텍스트 비트를 생성합니다.

732
00:32:42,224 --> 00:32:43,736
미친 대화가있는

733
00:32:43,736 --> 00:32:45,565
그건 셰익스피어처럼 들리 네요.

734
00:32:45,565 --> 00:32:46,714
줄 바꿈을 넣는 것을 알고있다.

735
00:32:46,714 --> 00:32:47,744
이 다른 것들 사이에.

736
00:32:47,744 --> 00:32:49,342
그리고 이것은 모두 정말 멋진 것입니다.

737
00:32:49,342 --> 00:32:52,958
모두 데이터의 구조에서 배웠습니다.

738
00:32:52,958 --> 00:32:55,536
우리는 실제로 이것보다 더 미칠 수 있습니다.

739
00:32:55,536 --> 00:32:58,368
이것은 제가 가장 좋아하는 예입니다.

740
00:32:58,368 --> 00:33:00,443
온라인에서 찾았습니다.

741
00:33:00,443 --> 00:33:02,725
이 방에있는 누군가 수학자입니까?

742
00:33:02,725 --> 00:33:05,914
혹시 대수학 토폴로지 과정을 수강 한 사람이 있습니까?

743
00:33:05,914 --> 00:33:07,325
와우, 부부, 인상적입니다.

744
00:33:07,325 --> 00:33:10,765
그래서 당신은 아마 저보다 더 많은
대수적 토폴로지를 알고 있습니다.

745
00:33:10,765 --> 00:33:12,165
하지만이 오픈 소스를 발견했습니다.

746
00:33:12,165 --> 00:33:15,114
대수학 토폴로지 교과서 온라인.

747
00:33:15,114 --> 00:33:16,514
그것은 기술 파일의 무리 다.

748
00:33:16,514 --> 00:33:19,554
그것은이 초 밀도 수학과 같습니다.

749
00:33:19,554 --> 00:33:22,593
그리고 라텍 (LaTac)은 이런 종류의 일입니다.

750
00:33:22,593 --> 00:33:24,774
방정식과 다이어그램을 작성해 보겠습니다.

751
00:33:24,774 --> 00:33:26,325
모든 것은 일반 텍스트 만 사용합니다.

752
00:33:26,325 --> 00:33:27,455
우리는 실제로

753
00:33:27,455 --> 00:33:28,816
재발 성 신경망 언어 모델

754
00:33:28,816 --> 00:33:31,457
원시 Latac 소스 코드

755
00:33:31,457 --> 00:33:33,146
이 대수학 토폴로지 교과서.

756
00:33:33,146 --> 00:33:37,687
그리고 우리가 그렇게한다면, 모델로부터 샘플을 얻은 후에,

757
00:33:37,687 --> 00:33:39,208
그러면 우리는 비슷한 것처럼 보이는 것을 얻습니다.

758
00:33:39,208 --> 00:33:41,446
대수 토폴로지와 같은 종류입니다.

759
00:33:41,446 --> 00:33:43,797
그래서 방정식을 넣는 것을 좋아합니다.

760
00:33:43,797 --> 00:33:46,679
모든 종류의 미친 것들을 넣습니다.

761
00:33:46,679 --> 00:33:48,717
연구를 증명하는 것과 같습니다.

762
00:33:48,717 --> 00:33:51,574
우리는 F sub U가 x 프라임의 덮개이며,

763
00:33:51,574 --> 00:33:52,773
blah, blah, blah, blah, blah.

764
00:33:52,773 --> 00:33:53,992
그것은 노동 조합을 어디에 둘 것인지를 안다.

765
00:33:53,992 --> 00:33:56,576
증명의 끝 부분에 사각형을 놓는 것을 알고 있습니다.

766
00:33:56,576 --> 00:33:57,528
그것은 보조 정리를 만든다.

767
00:33:57,528 --> 00:34:00,026
그것은 이전의 보조 정리를 참조합니다.

768
00:34:00,026 --> 00:34:01,225
맞아, 우리가 듣는 것처럼.

769
00:34:01,225 --> 00:34:03,781
즉, 이중 보조 정리 문제입니다.

770
00:34:03,781 --> 00:34:05,917
우리는 R이 기하학적으로 어떤 것을 알 수 있습니다.

771
00:34:05,917 --> 00:34:08,417
그래서 그것은 실제로 꽤 미쳤습니다.

772
00:34:09,496 --> 00:34:12,913
또한 때로는 다이어그램을 만들려고합니다.

773
00:34:14,239 --> 00:34:16,132
대수적 토폴로지를 사용하는 사람들에게는,

774
00:34:16,132 --> 00:34:17,322
당신은이 교환 형 다이어그램들이

775
00:34:17,322 --> 00:34:19,313
너와 많이 일하는 그런 종류 야.

776
00:34:19,313 --> 00:34:21,153
그래서 일종의 일반적인 요지가 있습니다.

777
00:34:21,154 --> 00:34:22,353
그 다이어그램을 만드는 방법,

778
00:34:22,353 --> 00:34:26,379
그러나 그들은 실제로 아무 의미가 없습니다.

779
00:34:26,380 --> 00:34:28,130
그리고 사실,

780
00:34:28,130 --> 00:34:29,440
여기 내가 좋아하는 예제 중 하나

781
00:34:29,440 --> 00:34:32,728
때로는 증명을 생략한다는 것입니다.

782
00:34:32,728 --> 00:34:33,788
그래서 때때로 이렇게 말할 것입니다.

783
00:34:33,789 --> 00:34:35,418
때로는 뭔가를 말할 것입니다.

784
00:34:35,418 --> 00:34:39,695
정리, ㅋ, ㅋ, ㅋ, ㅋ, ㅋ, 증명 생략.

785
00:34:39,695 --> 00:34:41,559
이런 종류의 요점은

786
00:34:41,559 --> 00:34:45,392
이 수학 교과서 중 일부가 어떻게 생겼는지에 대해

787
00:34:47,831 --> 00:34:49,122
우리는 이것으로 많은 즐거움을 누릴 수 있습니다.

788
00:34:49,123 --> 00:34:50,792
그래서 우리는 또한이 모델들 중
하나를 훈련 시키려고 노력했습니다.

789
00:34:50,792 --> 00:34:53,498
리눅스 커널의 전체 소스 코드

790
00:34:53,498 --> 00:34:55,791
다시 캐릭터 레벨의 물건으로

791
00:34:55,792 --> 00:34:56,801
우리가 훈련 할 수있는,

792
00:34:56,801 --> 00:34:58,630
그리고 나서 이것을 샘플링 할 때,

793
00:34:58,630 --> 00:35:01,483
그것은 acutally 다시 C 소스 코드처럼 보입니다.

794
00:35:01,483 --> 00:35:03,534
if 문을 작성하는 방법을 알고 있습니다.

795
00:35:03,534 --> 00:35:06,192
그것은 꽤 좋은 코드 포맷 기술입니다.

796
00:35:06,192 --> 00:35:08,110
이 if 문 다음에 들여 쓰기를합니다.

797
00:35:08,110 --> 00:35:09,552
중괄호를 넣는 것을 알고 있습니다.

798
00:35:09,552 --> 00:35:12,289
실제로 어떤 것에 대해서도 의견을 말합니다.

799
00:35:12,289 --> 00:35:15,052
그건 보통 말도 안돼.

800
00:35:15,052 --> 00:35:18,089
이 모델의 한 가지 문제점은

801
00:35:18,089 --> 00:35:19,842
그것은 변수를 선언하는 방법을 알고 있습니다.

802
00:35:19,842 --> 00:35:23,355
그러나 항상 선언 한 변수를 사용하는 것은 아닙니다.

803
00:35:23,355 --> 00:35:24,414
그리고 때때로 그것은 시도합니다.

804
00:35:24,414 --> 00:35:26,134
선언되지 않은 변수를 사용합니다.

805
00:35:26,134 --> 00:35:27,554
이것은 컴파일되지 않습니다.

806
00:35:27,554 --> 00:35:28,814
나는 이것을 보내는 것을 추천하지 않을 것이다.

807
00:35:28,814 --> 00:35:30,555
리눅스로 끌어 오기 요청.

808
00:35:30,555 --> 00:35:34,606
이 것은 또한 GNU를 암송하는 방법을 알아 낸다.

809
00:35:34,606 --> 00:35:37,867
이 GNU 라이센스 문자.

810
00:35:37,867 --> 00:35:40,454
GNU 라이센스를 암송해야한다는 것을 알고 있습니다.

811
00:35:40,454 --> 00:35:42,696
면허가 포함 된 후 일부 포함,

812
00:35:42,696 --> 00:35:44,962
다음 다른 소스 코드를 포함합니다.

813
00:35:44,962 --> 00:35:46,864
이 것은 사실 많은 것을 실제로 배웠습니다.

814
00:35:46,864 --> 00:35:48,836
데이터의 일반적인 구조에 대해

815
00:35:48,836 --> 00:35:50,096
다시, 훈련 중에,

816
00:35:50,096 --> 00:35:51,518
우리가이 모델에 요청한 전부

817
00:35:51,518 --> 00:35:53,398
시퀀스의 다음 문자를 예측하려고했습니다.

818
00:35:53,398 --> 00:35:55,406
우리는이 구조에 대해 말하지 않았습니다.

819
00:35:55,406 --> 00:35:57,534
그러나 어쨌든, 코스를 통해

820
00:35:57,534 --> 00:35:58,856
이 훈련 과정에서,

821
00:35:58,856 --> 00:36:01,438
그것은 잠재 구조에 대해 많은 것을 배웠다.

822
00:36:01,438 --> 00:36:03,355
순차적 데이터에서.

823
00:36:05,808 --> 00:36:07,406
예, 코드를 작성하는 법을 알고 있습니다.

824
00:36:07,406 --> 00:36:10,246
그것은 멋진 것들을 많이 않습니다.

825
00:36:10,246 --> 00:36:13,575
나는 Andre와 함께이 신문을 몇 년 전에 보았습니다.

826
00:36:13,575 --> 00:36:14,966
여기서 우리는이 모델들을 훈련 시켰습니다.

827
00:36:14,966 --> 00:36:17,176
우리는 두뇌를 찌르려고했습니다.

828
00:36:17,176 --> 00:36:18,009
이 모델들

829
00:36:18,009 --> 00:36:19,376
그들이하는 일을 알아 내라.

830
00:36:19,376 --> 00:36:20,856
왜 그들이 일하고 있는지.

831
00:36:20,856 --> 00:36:22,027
그래서 우리는 우리가 보았을 때,

832
00:36:22,027 --> 00:36:23,386
이러한 되풀이 신경 네트워크

833
00:36:23,386 --> 00:36:24,947
이 숨겨진 벡터가 있습니다.

834
00:36:24,947 --> 00:36:28,165
어쩌면 모든 시간 단계에 걸쳐
업데이트되는 벡터 일 수도 있습니다.

835
00:36:28,165 --> 00:36:29,838
그리고 우리가 알아 내려고 시도한 것은

836
00:36:29,838 --> 00:36:32,158
이 벡터의 몇 가지 요소를 찾을 수 있을까요?

837
00:36:32,158 --> 00:36:34,176
시만텍은 해석상의 의미가 있습니다.

838
00:36:34,176 --> 00:36:35,336
그래서 우리가 한 일은

839
00:36:35,336 --> 00:36:37,096
우리는 신경 네트워크 언어 모델을 훈련 시켰고,

840
00:36:37,096 --> 00:36:38,507
이 캐릭터 레벨 모델 중 하나

841
00:36:38,507 --> 00:36:39,917
이러한 데이터 세트 중 하나에서,

842
00:36:39,917 --> 00:36:42,965
그런 다음 숨겨진 벡터의 요소 중 하나를 선택했습니다.

843
00:36:42,965 --> 00:36:45,406
이제 우리는 그 숨겨진 벡터의
가치가 무엇인지 살펴 봅니다.

844
00:36:45,406 --> 00:36:47,923
순차적으로

845
00:36:47,923 --> 00:36:49,926
어쩌면 어떤 감각을 얻으려고

846
00:36:49,926 --> 00:36:52,206
이 다른 숨겨진 상태가 찾고있는 것.

847
00:36:52,206 --> 00:36:54,086
이렇게하면 많은 사람들이 결국은

848
00:36:54,086 --> 00:36:56,406
무작위로 횡설수설 쓰레기 같은 종류.

849
00:36:56,406 --> 00:36:57,736
그래서 여기 다시, 우리가 한 일은,

850
00:36:57,736 --> 00:36:59,806
벡터의 한 요소를 선택 했습니까?

851
00:36:59,806 --> 00:37:01,467
이제 시퀀스를 앞으로 돌립니다.

852
00:37:01,467 --> 00:37:02,686
숙련 된 모델을 통해,

853
00:37:02,686 --> 00:37:04,056
이제 각 문자의 색상

854
00:37:04,056 --> 00:37:07,271
단 하나의 크기에 해당합니다.

855
00:37:07,271 --> 00:37:09,446
매 시간 단계마다 숨겨진 벡터의 스케일러 요소

856
00:37:09,446 --> 00:37:10,876
시퀀스를 읽을 때.

857
00:37:10,876 --> 00:37:12,006
그래서 당신은 많이 볼 수 있습니다.

858
00:37:12,006 --> 00:37:13,958
숨겨진 상태의 벡터들

859
00:37:13,958 --> 00:37:15,883
일종의 해석이 쉽지 않습니다.

860
00:37:15,883 --> 00:37:17,404
그들이 이런 종류의 일을하는 것처럼 보입니다.

861
00:37:17,404 --> 00:37:19,158
저수준 언어 모델링

862
00:37:19,158 --> 00:37:21,396
다음에 어떤 캐릭터가 등장하는지 알아 내야합니다.

863
00:37:21,396 --> 00:37:23,438
그러나 그들 중 일부는 아주 멋지게 끝납니다.

864
00:37:23,438 --> 00:37:26,375
그래서 여기서 우리는 따옴표를 찾는이 벡터를 발견했습니다.

865
00:37:26,375 --> 00:37:28,507
이 숨겨진 요소가 하나 있다는 것을 알 수 있습니다.

866
00:37:28,507 --> 00:37:30,318
벡터의이 한 요소,

867
00:37:30,318 --> 00:37:32,486
꺼짐, 꺼짐, 꺼짐, 꺼짐, 꺼짐 파란색

868
00:37:32,486 --> 00:37:33,827
그런 다음 견적을 받으면

869
00:37:33,827 --> 00:37:37,136
켜지고 그 기간 동안 계속 켜져있다.

870
00:37:37,136 --> 00:37:38,303
이 인용문.

871
00:37:39,187 --> 00:37:41,025
그리고 이제 우리가 두 번째 인용 부호를 쳤을 때,

872
00:37:41,025 --> 00:37:42,598
그 셀은 꺼집니다.

873
00:37:42,598 --> 00:37:44,958
이렇게 여하튼, 비록이 모형이 단지 훈련 되었더라도

874
00:37:44,958 --> 00:37:46,606
시퀀스의 다음 문자를 예측하려면,

875
00:37:46,606 --> 00:37:48,856
그것은 어떻게 든 유용한 것,

876
00:37:48,856 --> 00:37:49,976
이것을하기 위해서,

877
00:37:49,976 --> 00:37:54,222
따옴표를 탐지하려고하는 셀이있을 수 있습니다.

878
00:37:54,222 --> 00:37:55,507
우리는 또한이 다른 세포를 발견했습니다.

879
00:37:55,507 --> 00:37:58,956
즉, 문자 수를 계산하는 것처럼 보입니다.

880
00:37:58,956 --> 00:38:00,104
줄 바꿈 이후로.

881
00:38:00,104 --> 00:38:01,507
그래서 당신은 각 줄의 시작 부분에,

882
00:38:01,507 --> 00:38:04,176
이 요소는 0에서 시작합니다.

883
00:38:04,176 --> 00:38:05,536
줄의 과정을 통하여,

884
00:38:05,536 --> 00:38:07,055
그것은 점차적으로 빨갛다.

885
00:38:07,055 --> 00:38:08,486
값이 증가합니다.

886
00:38:08,486 --> 00:38:10,078
그리고 새 줄 문자 뒤에,

887
00:38:10,078 --> 00:38:11,976
0으로 재설정됩니다.

888
00:38:11,976 --> 00:38:13,356
아마이 세포가

889
00:38:13,356 --> 00:38:15,336
네트워크가 계속 추적하게한다.

890
00:38:15,336 --> 00:38:16,966
쓰기가 필요할 때

891
00:38:16,966 --> 00:38:19,545
이러한 새로운 줄 문자를 생성합니다.

892
00:38:19,545 --> 00:38:20,936
우리는 또한 그것들을 발견했습니다,

893
00:38:20,936 --> 00:38:22,987
우리가 리눅스 소스 코드를 훈련했을 때,

894
00:38:22,987 --> 00:38:25,307
우리는 몇 가지 예를 발견했습니다.

895
00:38:25,307 --> 00:38:27,158
if 문 조건 내에서.

896
00:38:27,158 --> 00:38:29,216
그래서 이것은 아마도 네트워크

897
00:38:29,216 --> 00:38:32,364
if 문 외부에 있는지 여부를 구분합니다.

898
00:38:32,364 --> 00:38:33,838
또는 그 조건 안에서,

899
00:38:33,838 --> 00:38:35,806
이 시퀀스를 더 잘 모델링하는 데 도움이 될 수 있습니다.

900
00:38:35,806 --> 00:38:38,976
우리는 또한 코멘트에서 켜는 것을 발견했다.

901
00:38:38,976 --> 00:38:41,467
또는 계산중인 것처럼 보이는 일부

902
00:38:41,467 --> 00:38:44,765
들여 쓰기 레벨의 수.

903
00:38:44,765 --> 00:38:46,298
이것은 모두 정말 멋진 것들입니다.

904
00:38:46,298 --> 00:38:47,488
그것은

905
00:38:47,488 --> 00:38:49,630
비록 우리가이 모델을 훈련하려고하고 있지만

906
00:38:49,630 --> 00:38:50,758
다음 문자를 예측하기 위해,

907
00:38:50,758 --> 00:38:52,229
그것은 어떻게 든 많이 배우는 것을 끝낸다.

908
00:38:52,229 --> 00:38:55,646
입력 데이터에 대한 유용한 구조

909
00:38:57,161 --> 00:38:59,307
우리가 자주 사용하는 한 종류의 것,

910
00:38:59,307 --> 00:39:01,718
그래서 이것은 지금까지 컴퓨터 비전이 아니 었습니다.

911
00:39:01,718 --> 00:39:03,878
우리는이를 컴퓨터 비전으로 되돌려 놓아야합니다.

912
00:39:03,878 --> 00:39:05,528
이것은 비전 수업이기 때문에.

913
00:39:05,528 --> 00:39:07,107
우리는 이것을 여러 번 암시했다.

914
00:39:07,107 --> 00:39:08,747
이미지 캡션 모델

915
00:39:08,747 --> 00:39:10,528
우리가 입력 할 수있는 모델을 만들고 싶습니다.

916
00:39:10,528 --> 00:39:14,376
이미지를 입력 한 다음 자막을 자연 언어로 출력합니다.

917
00:39:14,376 --> 00:39:17,408
몇 년 전에 여러 장의 논문이있었습니다.

918
00:39:17,408 --> 00:39:19,787
모두 상대적으로 비슷한 접근 방식을 가졌습니다.

919
00:39:19,787 --> 00:39:22,776
하지만 우리 연구실의 논문에서
그 숫자를 보여주고 있습니다.

920
00:39:22,776 --> 00:39:25,026
완전히 편견없는 방식으로

921
00:39:26,876 --> 00:39:29,198
그러나 여기의 생각은 캡션

922
00:39:29,198 --> 00:39:31,648
우리가 할 수있는 가변 길이 순서입니다.

923
00:39:31,648 --> 00:39:34,198
시퀀스에는 다른 숫자가있을 수 있습니다.

924
00:39:34,198 --> 00:39:35,608
다른 자막을위한 단어들.

925
00:39:35,608 --> 00:39:37,059
그래서 이것은 완전히 자연스러운 적합성입니다.

926
00:39:37,059 --> 00:39:39,947
재발 성 신경망 언어 모델을 위해.

927
00:39:39,947 --> 00:39:41,598
그럼이 모델이 어떻게 생겼는지

928
00:39:41,598 --> 00:39:43,878
우리는 컨볼 루션 네트워크를 가지고 있는가?

929
00:39:43,878 --> 00:39:44,958
어떤 입력,

930
00:39:44,958 --> 00:39:46,786
이것은 입력으로서 이미지를 취할 것이고,

931
00:39:46,786 --> 00:39:48,397
우리는 방법에 대해 많이 보았습니다.

932
00:39:48,397 --> 00:39:50,379
컨볼 루션 네트워크는이 시점에서 작동하며,

933
00:39:50,379 --> 00:39:52,753
그리고 그 컨볼 루션 네트워크는

934
00:39:52,753 --> 00:39:54,569
이미지의 요약 벡터

935
00:39:54,569 --> 00:39:56,227
그러면 처음 단계로 들어가게됩니다.

936
00:39:56,227 --> 00:39:58,928
이러한 재귀 신경 네트워크 언어 모델 중 하나의

937
00:39:58,928 --> 00:40:02,888
그러면 한 번에 하나씩 자막의 단어가 생성됩니다.

938
00:40:02,888 --> 00:40:04,945
그래서 테스트 시간에 이런 종류의 작동 방식

939
00:40:04,945 --> 00:40:06,425
모델 훈련 후

940
00:40:06,425 --> 00:40:07,847
거의 똑같이 생겼다.

941
00:40:07,847 --> 00:40:09,665
이러한 문자 레벨 언어 모델

942
00:40:09,665 --> 00:40:11,178
우리가 조금 전에 보았던.

943
00:40:11,178 --> 00:40:12,699
우리는 우리의 입력 이미지,

944
00:40:12,699 --> 00:40:14,499
우리의 컨볼 루션 네트워크를 통해 피드.

945
00:40:14,499 --> 00:40:16,987
하지만 이제 소프트 맥스 점수 대신

946
00:40:16,987 --> 00:40:18,638
이미지 넷 모델로부터,

947
00:40:18,638 --> 00:40:22,997
대신 우리는이 4,096 차원 벡터를 취할 것입니다.

948
00:40:22,997 --> 00:40:24,915
모델의 끝에서,

949
00:40:24,915 --> 00:40:27,998
우리는 그 벡터를 가지고 그것을 사용할 것입니다.

950
00:40:29,038 --> 00:40:31,377
이미지의 전체 내용을 요약합니다.

951
00:40:31,377 --> 00:40:34,208
이제 우리가 RNN 언어 모델에
대해 이야기 할 때 기억하십시오.

952
00:40:34,208 --> 00:40:36,379
우리는 언어 모델을 볼 필요가 있다고 말했다.

953
00:40:36,379 --> 00:40:37,947
첫 번째 초기 입력

954
00:40:37,947 --> 00:40:39,528
텍스트 생성을 시작하도록 알려줍니다.

955
00:40:39,528 --> 00:40:42,118
그래서이 경우에는 특별한 시작 토큰을 주겠다.

956
00:40:42,118 --> 00:40:45,236
그것은 단지 말하고 있습니다, 이봐,이 문장의 시작입니다.

957
00:40:45,236 --> 00:40:46,728
일부 텍스트 생성을 시작하십시오.

958
00:40:46,728 --> 00:40:49,006
이 이미지 정보를 조건으로합니다.

959
00:40:49,006 --> 00:40:52,338
이제 이전에 우리는이 RNN 언어 모델에서,

960
00:40:52,338 --> 00:40:55,328
우리는 이전 매트릭스를 가지고
있던 매트릭스를 가지고있었습니다.

961
00:40:55,328 --> 00:40:57,107
현재 시간 단계에서의 입력

962
00:40:57,107 --> 00:40:58,558
이전 시간 단계의 숨겨진 상태

963
00:40:58,558 --> 00:41:01,240
다음 숨겨진 상태를 얻기 위해 이들을 결합하는 것입니다.

964
00:41:01,240 --> 00:41:04,678
이제는이 이미지 정보를 추가해야합니다.

965
00:41:04,678 --> 00:41:06,667
그래서 편도 사람들은 정확히 놀아요.

966
00:41:06,667 --> 00:41:09,288
이 이미지 정보를 통합하는 다른 방법,

967
00:41:09,288 --> 00:41:10,688
하지만 하나의 간단한 방법은

968
00:41:10,688 --> 00:41:13,105
제 3 가중 행렬은

969
00:41:14,561 --> 00:41:16,779
매 시간 단계마다이 이미지 정보를 추가

970
00:41:16,779 --> 00:41:18,598
다음 숨겨진 상태를 계산합니다.

971
00:41:18,598 --> 00:41:21,145
이제 우리는이 분포를 계산할 것입니다.

972
00:41:21,145 --> 00:41:23,635
우리 어휘의 모든 점수 이상

973
00:41:23,635 --> 00:41:25,056
여기서 우리의 어휘는 무언가입니다.

974
00:41:25,056 --> 00:41:25,907
모든 영어 단어처럼,

975
00:41:25,907 --> 00:41:27,307
그래서 꽤 클 수 있습니다.

976
00:41:27,307 --> 00:41:28,699
우리는 그 분포에서 샘플을 볼 것입니다.

977
00:41:28,699 --> 00:41:32,099
다음 번 단계에서 그 단어를 입력으로 다시 전달합니다.

978
00:41:32,099 --> 00:41:34,488
그러면 그 단어를 먹을 것입니다.

979
00:41:34,488 --> 00:41:37,418
vocab의 모든 단어에 대한 분포를 다시 얻습니다.

980
00:41:37,418 --> 00:41:39,546
다시 다음 단어를 생성하기 위해 샘플링합니다.

981
00:41:39,546 --> 00:41:42,248
그럼, 그 일이 모두 끝난 후에,

982
00:41:42,248 --> 00:41:44,477
우리는 아마 생성 할 것이다,

983
00:41:44,477 --> 00:41:47,538
우리는이 완전한 문장을 생성 할 것입니다.

984
00:41:47,538 --> 00:41:50,956
특수 종료 토큰을 샘플링하면 생성을 중지합니다.

985
00:41:50,956 --> 00:41:52,887
기간에 해당하는 종류

986
00:41:52,887 --> 00:41:54,347
문장의 끝에.

987
00:41:54,347 --> 00:41:56,419
그런 다음 네트워크가이 종료 토큰을 샘플링하면,

988
00:41:56,419 --> 00:41:57,859
우리는 세대를 멈추고 끝난다.

989
00:41:57,859 --> 00:42:00,328
우리는이 이미지에 대한 캡션을 얻었습니다.

990
00:42:00,328 --> 00:42:01,798
그리고 지금, 훈련 도중,

991
00:42:01,798 --> 00:42:03,419
우리는 이것을 생성하도록 훈련 시켰고,

992
00:42:03,419 --> 00:42:04,939
우리가 마지막에 토큰을 넣는 것처럼

993
00:42:04,939 --> 00:42:06,739
훈련 중 모든 캡션의

994
00:42:06,739 --> 00:42:08,408
그래서 훈련 중에 배운 네트워크 종류

995
00:42:08,408 --> 00:42:10,907
끝 토큰은 시퀀스의 마지막에옵니다.

996
00:42:10,907 --> 00:42:12,958
그럼, 테스트 시간 동안,

997
00:42:12,958 --> 00:42:14,739
이 끝 토큰을 샘플링하는 경향이있다.

998
00:42:14,739 --> 00:42:16,848
일단 그것이 생성 완료됩니다.

999
00:42:16,848 --> 00:42:18,457
그래서 우리는이 모델을 훈련 시켰습니다.

1000
00:42:18,457 --> 00:42:20,139
완전히 감독 된 방식으로.

1001
00:42:20,139 --> 00:42:21,547
데이터 세트를 찾을 수 있습니다.

1002
00:42:21,547 --> 00:42:24,763
자연어 캡션과 함께 이미지가 있습니다.

1003
00:42:24,763 --> 00:42:26,805
Microsoft COCO는 아마도 가장 큰 것입니다.

1004
00:42:26,805 --> 00:42:28,666
이 작업에 가장 널리 사용됩니다.

1005
00:42:28,666 --> 00:42:30,191
그러나 당신은이 모델을 훈련시킬 수 있습니다.

1006
00:42:30,191 --> 00:42:31,883
순전히 감독 된 방식으로

1007
00:42:31,883 --> 00:42:34,935
그리고 나서 다시 훈련하여 공동 훈련을하십시오.

1008
00:42:34,935 --> 00:42:37,653
이 재발 성 신경망 언어 모델

1009
00:42:37,653 --> 00:42:38,635
그라데이션을 전달합니다.

1010
00:42:38,635 --> 00:42:40,846
이것의 마지막 레이어로 다시 CNN

1011
00:42:40,846 --> 00:42:42,396
추가적으로 가중치를 업데이트

1012
00:42:42,396 --> 00:42:45,580
모델의 모든 부분을 공동으로 조정하는 CNN

1013
00:42:45,580 --> 00:42:47,837
이 작업을 수행합니다.

1014
00:42:47,837 --> 00:42:49,528
이 모델들을 훈련 시키면,

1015
00:42:49,528 --> 00:42:52,438
그들은 실제로 꽤 합리적인 것들을합니다.

1016
00:42:52,438 --> 00:42:54,997
이것들은 모델의 실제 결과입니다.

1017
00:42:54,997 --> 00:42:56,689
이러한 훈련 된 모델 중 하나에서,

1018
00:42:56,689 --> 00:42:58,226
고양이 앉아있는 것 같아.

1019
00:42:58,226 --> 00:43:00,995
바닥에 가방에,

1020
00:43:00,995 --> 00:43:02,583
그것은 꽤 인상적이다.

1021
00:43:02,583 --> 00:43:04,975
나무 가지에 앉아있는 고양이에 대해 알고 있습니다.

1022
00:43:04,975 --> 00:43:06,499
그것은 또한 꽤 근사하다.

1023
00:43:06,499 --> 00:43:07,796
그것은 걷는 두 사람에 대해 안다.

1024
00:43:07,796 --> 00:43:09,631
서핑 보드와 해변에서.

1025
00:43:09,631 --> 00:43:11,755
그래서이 모델들은 실제로 꽤 강력합니다.

1026
00:43:11,755 --> 00:43:14,642
비교적 복잡한 캡션을 생성 할 수 있습니다.

1027
00:43:14,642 --> 00:43:16,635
이미지를 설명합니다.

1028
00:43:16,635 --> 00:43:17,630
그러나 그것이 말하게되면서,

1029
00:43:17,630 --> 00:43:19,808
이 모델은 실제로 완벽하지 않습니다.

1030
00:43:19,808 --> 00:43:21,227
그들은 마법이 아닙니다.

1031
00:43:21,227 --> 00:43:24,319
모델을 학습하는 모든 기계처럼,

1032
00:43:24,319 --> 00:43:25,629
데이터로 실행하려고하면

1033
00:43:25,629 --> 00:43:27,458
그것은 훈련 데이터와 매우 다릅니다.

1034
00:43:27,458 --> 00:43:28,830
그들은 잘 작동하지 않습니다.

1035
00:43:28,830 --> 00:43:30,364
예를 들어,이 예에서는,

1036
00:43:30,364 --> 00:43:33,317
여자가 고양이를 손에 들고 있다고합니다.

1037
00:43:33,317 --> 00:43:35,421
이미지에 고양이가 분명히 없습니다.

1038
00:43:35,421 --> 00:43:36,863
하지만 그녀는 모피 코트를 입고있다.

1039
00:43:36,863 --> 00:43:38,106
어쩌면 그 코트의 질감

1040
00:43:38,106 --> 00:43:41,359
모델의 고양이처럼 보였습니다.

1041
00:43:41,359 --> 00:43:42,903
여기, 우리는 해변에 서있는 한 여인을 보았습니다.

1042
00:43:42,903 --> 00:43:44,379
서핑 보드를 들고.

1043
00:43:44,379 --> 00:43:46,744
글쎄, 그녀는 확실히 서핑 보드를 안고있다.

1044
00:43:46,744 --> 00:43:47,892
그리고 그녀는 역마차를하고있다,

1045
00:43:47,892 --> 00:43:49,909
어쩌면 그 이미지의 흥미로운 부분 일 것이다.

1046
00:43:49,909 --> 00:43:51,820
그리고 그 모델은 완전히 그것을 놓쳤다.

1047
00:43:51,820 --> 00:43:53,623
또한 여기에서이 예제를 봅니다.

1048
00:43:53,623 --> 00:43:56,137
스파이더 웹 사진이있는 곳

1049
00:43:56,137 --> 00:43:57,168
나뭇 가지에,

1050
00:43:57,168 --> 00:43:58,238
그리고 그것은 완전히,

1051
00:43:58,238 --> 00:43:59,071
그리고 그것은 뭔가를 말합니다.

1052
00:43:59,071 --> 00:44:00,382
나뭇 가지에 앉아 새입니다.

1053
00:44:00,382 --> 00:44:01,802
그래서 그것은 완전히 거미를 놓쳤다.

1054
00:44:01,802 --> 00:44:03,144
그러나 훈련 도중,

1055
00:44:03,144 --> 00:44:05,139
거미의 예를 실제로 본 적이 없습니다.

1056
00:44:05,139 --> 00:44:06,722
새들이 앉아 있다는 것을 알고 있습니다.

1057
00:44:06,722 --> 00:44:08,217
훈련 도중 나뭇 가지에.

1058
00:44:08,217 --> 00:44:10,152
그래서 이런 종류의 실수는 합리적인 실수입니다.

1059
00:44:10,152 --> 00:44:10,985
또는 여기 하단에,

1060
00:44:10,985 --> 00:44:12,155
차이점을 실제로 말할 수는 없다.

1061
00:44:12,155 --> 00:44:14,338
던지고 공을 잡는이 녀석과

1062
00:44:14,338 --> 00:44:15,712
그러나 야구 선수라는 것을 알고 있습니다.

1063
00:44:15,712 --> 00:44:17,709
그리고 볼과 관련된 것들이 있습니다.

1064
00:44:17,709 --> 00:44:19,347
다시 말하면,이 모델들이

1065
00:44:19,347 --> 00:44:20,441
완벽하지 않습니다.

1066
00:44:20,441 --> 00:44:23,313
이미지 캡션을달라고 할 때 꽤 잘 작동합니다.

1067
00:44:23,313 --> 00:44:25,109
훈련 데이터와 유사한

1068
00:44:25,109 --> 00:44:26,711
그러나 그들은 확실히 어려움을 겪는다.

1069
00:44:26,711 --> 00:44:29,188
그 이상으로 일반화.

1070
00:44:29,188 --> 00:44:30,560
그래서 당신이 때때로 볼 수있는 또 다른 것

1071
00:44:30,560 --> 00:44:34,152
Attention이라고하는 약간 더 고급 모델입니다.

1072
00:44:34,152 --> 00:44:36,447
이제 우리가이 캡션의 단어를 생성 할 때,

1073
00:44:36,447 --> 00:44:39,120
우리는 모델이주의를 돌리는 것을 허용 할 수있다.

1074
00:44:39,120 --> 00:44:41,036
이미지의 다른 부분으로

1075
00:44:41,036 --> 00:44:44,670
그리고 나는 이것에 너무 많은 시간을 보내고 싶지 않다.

1076
00:44:44,670 --> 00:44:46,859
그러나 이것이 작동하는 일반적인 방식은

1077
00:44:46,859 --> 00:44:48,925
이제 우리의 길쌈 네트워크,

1078
00:44:48,925 --> 00:44:50,664
단일 벡터를 만드는 것보다

1079
00:44:50,664 --> 00:44:52,227
전체 이미지 요약,

1080
00:44:52,227 --> 00:44:54,274
이제는 벡터 그리드를 생성합니다.

1081
00:44:54,274 --> 00:44:55,683
그 요약,

1082
00:44:55,683 --> 00:44:58,503
각 공간 위치에 대해 하나의 벡터를 제공 할 수 있습니다.

1083
00:44:58,503 --> 00:44:59,954
이미지에서.

1084
00:44:59,954 --> 00:45:00,787
그리고 지금, 우리가,

1085
00:45:00,787 --> 00:45:03,304
이 모델이 앞으로 나아갈 때,

1086
00:45:03,304 --> 00:45:06,618
매 단계마다 어휘를 샘플링하는 것 외에도,

1087
00:45:06,618 --> 00:45:08,523
그것은 또한 분포를 생성한다.

1088
00:45:08,523 --> 00:45:09,964
이미지의 위치에

1089
00:45:09,964 --> 00:45:11,263
보길 원하는 곳.

1090
00:45:11,263 --> 00:45:13,581
이제 이미지 위치에 대한 이러한 분포

1091
00:45:13,581 --> 00:45:15,561
일종의 긴장감으로 보일 수있다.

1092
00:45:15,561 --> 00:45:18,276
모델은 훈련 중에 보일 것입니다.

1093
00:45:18,276 --> 00:45:19,397
이제 첫 번째 숨겨진 상태

1094
00:45:19,397 --> 00:45:23,155
이미지 위치에 대한이 분포를 계산하고,

1095
00:45:23,155 --> 00:45:26,316
그런 다음 벡터 집합으로 돌아갑니다.

1096
00:45:26,316 --> 00:45:27,832
단일 요약 벡터를 제공한다.

1097
00:45:27,832 --> 00:45:31,372
그 이미지의 한 부분에 집중할 수 있습니다.

1098
00:45:31,372 --> 00:45:32,795
그리고 요약 벡터가 공급되면,

1099
00:45:32,795 --> 00:45:34,422
추가 입력으로,

1100
00:45:34,422 --> 00:45:36,508
신경 네트워크의 다음 단계에서.

1101
00:45:36,508 --> 00:45:38,278
그리고 다시 한 번, 두 개의 출력을 생성합니다.

1102
00:45:38,278 --> 00:45:40,414
하나는 어휘에 대한 우리의 분포입니다.

1103
00:45:40,414 --> 00:45:43,714
그리고 다른 하나는 이미지 위치에 대한 분포입니다.

1104
00:45:43,714 --> 00:45:45,402
이 모든 과정은 계속 될 것이며,

1105
00:45:45,402 --> 00:45:47,577
그리고 그것은이 두 가지 다른 일을 수행 할 것입니다.

1106
00:45:47,577 --> 00:45:49,160
매 단계마다.

1107
00:45:50,296 --> 00:45:51,390
그리고 모델을 훈련 한 후에,

1108
00:45:51,390 --> 00:45:53,905
그러면 당신은 그것의 종류를 볼 수 있습니다.

1109
00:45:53,905 --> 00:45:56,104
그것이 이미지 주위에 관심을 이동합니다

1110
00:45:56,104 --> 00:45:58,298
캡션에서 생성되는 모든 단어에 대해

1111
00:45:58,298 --> 00:45:59,827
여기에서 볼 수 있습니다.

1112
00:45:59,827 --> 00:46:02,723
캡션을 제작하고, 새가 날으며,

1113
00:46:02,723 --> 00:46:04,436
나는 그걸 멀리 볼 수 없다.

1114
00:46:04,436 --> 00:46:06,474
그러나 당신은 그것의주의를 볼 수 있습니다.

1115
00:46:06,474 --> 00:46:08,556
이미지의 다른 부분으로 이동하고 있습니다.

1116
00:46:08,556 --> 00:46:12,473
생성하는 캡션의 각 단어에 대해

1117
00:46:14,112 --> 00:46:15,417
이 세심한 관심의 개념이 있습니다.

1118
00:46:15,417 --> 00:46:16,544
대 소프트주의,

1119
00:46:16,544 --> 00:46:19,147
나는 너무 많이 들어가기를 정말로 원하지 않는다.

1120
00:46:19,147 --> 00:46:21,434
그러나 부드러운주의의이 아이디어로,

1121
00:46:21,434 --> 00:46:24,405
우리는 일종의 가중 조합을 취하고 있습니다.

1122
00:46:24,405 --> 00:46:26,614
모든 이미지 위치의 모든 기능들,

1123
00:46:26,614 --> 00:46:28,275
단단한주의 경우에는,

1124
00:46:28,275 --> 00:46:31,004
우리는 모델에 정확히 하나의 위치를 선택하도록 강요합니다.

1125
00:46:31,004 --> 00:46:34,259
각 시간 단계에서 이미지를 볼 수 있습니다.

1126
00:46:34,259 --> 00:46:35,123
그래서 하드주의 사례

1127
00:46:35,123 --> 00:46:36,946
정확히 하나의 이미지 위치를 선택합니다.

1128
00:46:36,946 --> 00:46:38,277
조금 까다 롭다.

1129
00:46:38,277 --> 00:46:41,270
그것이 실제로는 차별화 된 기능이 아니기 때문에,

1130
00:46:41,270 --> 00:46:42,774
그래서 너는 약간 더 좋아하는 것을 할 필요가있다.

1131
00:46:42,774 --> 00:46:44,292
바닐라 역 전파보다

1132
00:46:44,292 --> 00:46:46,247
그 시나리오에서 모델을 훈련시키기 위해서입니다.

1133
00:46:46,247 --> 00:46:48,145
그리고 나중에 조금 이야기 할 것입니다.

1134
00:46:48,145 --> 00:46:51,498
강화 학습 강의.

1135
00:46:51,498 --> 00:46:53,577
자, 기차를보고 난 후에 보자.

1136
00:46:53,577 --> 00:46:54,750
이 관심 모델 중 하나

1137
00:46:54,750 --> 00:46:57,415
그것을 실행하여 캡션을 생성하고,

1138
00:46:57,415 --> 00:46:59,326
당신은 주목하는 경향이 있다는 것을 알 수 있습니다.

1139
00:46:59,326 --> 00:47:01,980
어쩌면 현저하거나 의미 론적으로 의미있는 부분에

1140
00:47:01,980 --> 00:47:04,067
캡션을 생성 할 때 이미지의

1141
00:47:04,067 --> 00:47:06,419
자막이 여자라는 것을 알 수 있습니다.

1142
00:47:06,419 --> 00:47:08,413
공원에 프리즈 비를 던지고있다.

1143
00:47:08,413 --> 00:47:10,257
그리고 당신은이주의 마스크가

1144
00:47:10,257 --> 00:47:11,637
그것이 단어를 생성했을 때,

1145
00:47:11,637 --> 00:47:13,648
그 모델이 프리스비라는 단어를 만들었을 때,

1146
00:47:13,648 --> 00:47:14,516
동시에,

1147
00:47:14,516 --> 00:47:17,403
이 이미지 영역에주의를 집중했습니다.

1148
00:47:17,403 --> 00:47:18,976
실제로 프리스비가 들어 있습니다.

1149
00:47:18,976 --> 00:47:20,644
이것은 실제로 정말 멋지다.

1150
00:47:20,644 --> 00:47:23,410
우리는 모형을 어디에서 봐야하는지 말하지 않았다.

1151
00:47:23,410 --> 00:47:24,542
매 단계마다.

1152
00:47:24,542 --> 00:47:26,552
그것은 일종의 모든 것을 알아 냈습니다.

1153
00:47:26,552 --> 00:47:27,955
훈련 과정에서.

1154
00:47:27,955 --> 00:47:29,716
어쨌든, 그것은

1155
00:47:29,716 --> 00:47:31,540
그 이미지 영역에서해야 할 일은 옳았습니다.

1156
00:47:31,540 --> 00:47:32,721
이 이미지.

1157
00:47:32,721 --> 00:47:34,610
그리고이 모델의 모든 것이 차별화되기 때문에,

1158
00:47:34,610 --> 00:47:35,998
역 전파 할 수 있기 때문에

1159
00:47:35,998 --> 00:47:38,161
이러한 모든주의 집중 단계를 통해,

1160
00:47:38,161 --> 00:47:39,592
이 부드러운주의 집중

1161
00:47:39,592 --> 00:47:42,541
교육 과정을 통해 나옵니다.

1162
00:47:42,541 --> 00:47:45,297
정말 정말 멋집니다.

1163
00:47:45,297 --> 00:47:47,345
그건 그렇고, 반복적 인 신경 네트워크의이 아이디어

1164
00:47:47,345 --> 00:47:49,975
다른 작업에서주의가 실제로 사용됩니다.

1165
00:47:49,975 --> 00:47:51,565
이미지 캡션 이상.

1166
00:47:51,565 --> 00:47:53,169
최근의 한 가지 예가이 아이디어입니다.

1167
00:47:53,169 --> 00:47:54,691
시각적 인 질문 응답.

1168
00:47:54,691 --> 00:47:58,195
그래서 여기에서 우리 모델은
입력으로 두 가지를 취할 것입니다.

1169
00:47:58,195 --> 00:47:59,259
이미지를 찍을 것입니다.

1170
00:47:59,259 --> 00:48:01,608
그리고 자연어 질문도 받아 들일 것입니다.

1171
00:48:01,608 --> 00:48:04,010
그 이미지에 대해 몇 가지 질문을합니다.

1172
00:48:04,010 --> 00:48:06,122
여기, 우리는이 이미지를 왼쪽에 보일지도 모른다.

1173
00:48:06,122 --> 00:48:07,536
우리는 그 질문을 할 수도 있습니다.

1174
00:48:07,536 --> 00:48:10,163
어떤 멸종 위기에 처한 동물이 트럭에 등장 했습니까?

1175
00:48:10,163 --> 00:48:11,915
이제 모델에서 하나를 선택해야합니다.

1176
00:48:11,915 --> 00:48:13,835
이 4 가지 자연어 답변 중

1177
00:48:13,835 --> 00:48:17,327
이 답변들 중 어느 것이 그 질문에 정확하게 대답하는지

1178
00:48:17,327 --> 00:48:19,027
이미지 컨텍스트에서.

1179
00:48:19,027 --> 00:48:21,586
그래서 당신은이 모델을 함께
묶는 것을 상상할 수 있습니다.

1180
00:48:21,586 --> 00:48:24,774
자연스럽게 CNN과 RNN을 사용합니다.

1181
00:48:24,774 --> 00:48:26,274
자, 이제 우리는

1182
00:48:28,125 --> 00:48:29,701
다 대일 시나리오,

1183
00:48:29,701 --> 00:48:31,978
이제 우리 모델은 입력으로 받아 들여야합니다.

1184
00:48:31,978 --> 00:48:33,566
이 자연 언어 시퀀스,

1185
00:48:33,566 --> 00:48:35,552
우리는 재발 성 신경 네트워크를
실행하는 것을 상상할 수 있습니다.

1186
00:48:35,552 --> 00:48:37,485
그 입력 질문의 각 요소에 대해,

1187
00:48:37,485 --> 00:48:40,111
이제 입력 질문을 단일 벡터로 요약합니다.

1188
00:48:40,111 --> 00:48:43,928
그리고 우리는 이미지를 다시 요약하기
위해 CNN을 가질 수 있습니다.

1189
00:48:43,928 --> 00:48:46,012
이제 CNN의 벡터와

1190
00:48:46,012 --> 00:48:49,531
질문 및 코딩 RNN의 벡터

1191
00:48:49,531 --> 00:48:51,645
대답에 대한 분포를 예측합니다.

1192
00:48:51,645 --> 00:48:52,786
우리는 때때로,

1193
00:48:52,786 --> 00:48:54,416
당신은 때때로이 아이디어를 볼 것입니다.

1194
00:48:54,416 --> 00:48:56,275
부드러운 공간주의의 도입

1195
00:48:56,275 --> 00:48:59,175
시각적 인 질문 응답과 같은 것들로

1196
00:48:59,175 --> 00:49:00,394
그래서 당신은 여기에서 볼 수 있습니다,

1197
00:49:00,394 --> 00:49:03,956
이 모형은 또한 공간적인 관심을 가지고있다.

1198
00:49:03,956 --> 00:49:05,342
시도 할 때 이미지 위에

1199
00:49:05,342 --> 00:49:07,062
질문에 대한 답변을 결정합니다.

1200
00:49:07,062 --> 00:49:09,062
그냥, 그래, 질문?

1201
00:49:18,715 --> 00:49:19,548
그래서 질문은

1202
00:49:19,548 --> 00:49:20,878
서로 다른 입력은 어떻게 결합됩니까?

1203
00:49:20,878 --> 00:49:23,116
인코딩 된 질문 벡터와 같은 의미입니까?

1204
00:49:23,116 --> 00:49:25,998
및 인코딩 된 이미지 벡터

1205
00:49:25,998 --> 00:49:27,188
그래, 그 질문은

1206
00:49:27,188 --> 00:49:28,445
인코딩 된 이미지는 어떻습니까?

1207
00:49:28,445 --> 00:49:30,395
인코딩 된 질문 벡터가 결합 되었습니까?

1208
00:49:30,395 --> 00:49:31,756
가장 간단한 일의 종류

1209
00:49:31,756 --> 00:49:32,885
그냥 그들을 연결하는 것입니다

1210
00:49:32,885 --> 00:49:34,847
그들을 완전히 연결된 레이어에 붙이십시오.

1211
00:49:34,847 --> 00:49:35,883
아마 가장 일반적인 것입니다.

1212
00:49:35,883 --> 00:49:37,864
아마 그게 제일 먼저 시도하는 것입니다.

1213
00:49:37,864 --> 00:49:39,396
때로는 사람들이 약간 애호가를하는 경우가 있습니다.

1214
00:49:39,396 --> 00:49:41,673
곱셈 적 상호 작용을 시도 할 수있는 곳

1215
00:49:41,673 --> 00:49:42,885
그 두 벡터 사이

1216
00:49:42,885 --> 00:49:44,389
더 강력한 기능을 허용합니다.

1217
00:49:44,389 --> 00:49:46,467
하지만 일반적으로 연결은 일종의 좋은 것입니다.

1218
00:49:46,467 --> 00:49:48,050
시도 할 것이다 첫번째 것.

1219
00:49:49,426 --> 00:49:51,727
자, 이제 시나리오에 대해 이야기 해 보았습니다.

1220
00:49:51,727 --> 00:49:54,083
여기서 RNN은 여러 종류의 문제에 사용됩니다.

1221
00:49:54,083 --> 00:49:55,084
그리고 나는 그것이 아주 멋지다라고 생각한다

1222
00:49:55,084 --> 00:49:56,758
그것은 당신이

1223
00:49:56,758 --> 00:49:58,855
정말로 복잡한 문제를 다루기 시작하십시오.

1224
00:49:58,855 --> 00:50:02,386
이미지와 컴퓨터 비전 결합

1225
00:50:02,386 --> 00:50:04,072
자연어 처리.

1226
00:50:04,072 --> 00:50:05,794
그리고 당신은 우리가 함께 스틱을 할
수 있다는 것을 알 수 있습니다.

1227
00:50:05,794 --> 00:50:06,776
레고 블록과 같은 모델

1228
00:50:06,776 --> 00:50:08,870
정말 복잡한 일을 공격하고,

1229
00:50:08,870 --> 00:50:11,369
이미지 캡션 또는 시각적 질문 응답

1230
00:50:11,369 --> 00:50:14,069
이 단순한 비교적 단순한 스티칭으로

1231
00:50:14,069 --> 00:50:16,736
신경망 모듈의 유형.

1232
00:50:18,708 --> 00:50:20,861
그러나 나는 또한 언급하고 싶다.

1233
00:50:20,861 --> 00:50:22,359
지금까지이 아이디어에 대해 이야기했습니다.

1234
00:50:22,359 --> 00:50:24,060
단일 반복 네트워크 계층의

1235
00:50:24,060 --> 00:50:26,274
우리는 일종의 숨겨진 상태를 가지고 있습니다.

1236
00:50:26,274 --> 00:50:29,404
그리고 당신이 꽤 보편적으로 볼 수있는 또 다른 것

1237
00:50:29,404 --> 00:50:32,581
다중 층 반복 신경망에 대한이 아이디어입니다.

1238
00:50:32,581 --> 00:50:36,577
여기, 이것은 3 층 반복 신경망이며,

1239
00:50:36,577 --> 00:50:38,535
이제 우리의 의견이 들어갑니다.

1240
00:50:38,535 --> 00:50:42,292
들어가서 들어가서 일련의 숨은 상태를 만든다.

1241
00:50:42,292 --> 00:50:44,554
첫 번째 반복적 인 신경 네트워크 계층에서.

1242
00:50:44,554 --> 00:50:46,309
그리고 지금 우리가 일종의

1243
00:50:46,309 --> 00:50:47,893
하나의 반복적 인 신경 네트워크 계층,

1244
00:50:47,893 --> 00:50:50,468
그러면 숨겨진 상태의 전체 시퀀스가 생깁니다.

1245
00:50:50,468 --> 00:50:52,991
이제 숨겨진 상태의 시퀀스를 사용할 수 있습니다.

1246
00:50:52,991 --> 00:50:54,733
입력 시퀀스로 다른

1247
00:50:54,733 --> 00:50:56,474
재발 성 신경 네트워크 계층.

1248
00:50:56,474 --> 00:50:57,801
그리고 나서 당신은 상상할 수 있습니다.

1249
00:50:57,801 --> 00:50:59,757
그러면 숨겨진 상태의 다른 시퀀스가 생성됩니다.

1250
00:50:59,757 --> 00:51:01,577
제 2 RNN 계층으로부터 수신한다.

1251
00:51:01,577 --> 00:51:02,410
그리고 나서 당신은 상상할 수 있습니다.

1252
00:51:02,410 --> 00:51:04,181
이 물건들을 서로 쌓아 올리면,

1253
00:51:04,181 --> 00:51:06,108
우리는 다른 상황에서 본 적이 있다는 것을 알기 때문에

1254
00:51:06,108 --> 00:51:08,253
더 깊은 모델은 더 잘 수행하는 경향이 있습니다.

1255
00:51:08,253 --> 00:51:09,398
다양한 문제.

1256
00:51:09,398 --> 00:51:11,851
그리고 RNN에서도 같은 종류의 보유가 가능합니다.

1257
00:51:11,851 --> 00:51:14,377
많은 문제들에 대해, 당신은 아마도 두 가지를 보게 될 것입니다.

1258
00:51:14,377 --> 00:51:16,714
또는 3 계층 반복 신경망 모델

1259
00:51:16,714 --> 00:51:18,215
꽤 일반적으로 사용됩니다.

1260
00:51:18,215 --> 00:51:22,086
일반적으로 RNN에는 수퍼 심층 모델이 표시되지 않습니다.

1261
00:51:22,086 --> 00:51:25,449
일반적으로 2, 3, 4 층 RNN

1262
00:51:25,449 --> 00:51:29,327
어쩌면 당신이 전형적으로 갈만큼 깊을 수도 있습니다.

1263
00:51:29,327 --> 00:51:32,231
그렇다면 정말 흥미롭고 중요하다고 생각합니다.

1264
00:51:32,231 --> 00:51:33,500
에 대해 생각하는 것,

1265
00:51:33,500 --> 00:51:34,714
이제 우리는

1266
00:51:34,714 --> 00:51:38,222
이 RNN이 어떤 종류의 문제에 사용될 수 있는지,

1267
00:51:38,222 --> 00:51:40,079
하지만 조금 더 신중하게 생각해야합니다.

1268
00:51:40,079 --> 00:51:41,795
이 모델에 정확히 무슨 일이 일어나는지

1269
00:51:41,795 --> 00:51:43,229
우리가 그들을 훈련 시키려고 할 때.

1270
00:51:43,229 --> 00:51:45,704
그래서 여기,이 작은 바닐라 RNN 세포를 그렸습니다.

1271
00:51:45,704 --> 00:51:47,447
우리가 지금까지 이야기했던 것.

1272
00:51:47,447 --> 00:51:50,639
그래서 여기, 우리는 현재의 입력 xt를 취합니다.

1273
00:51:50,639 --> 00:51:52,971
그리고 우리의 이전의 숨겨진 상태,

1274
00:51:52,971 --> 00:51:55,307
그리고 나서 우리는 두 개의 벡터를 쌓습니다.

1275
00:51:55,307 --> 00:51:57,359
그래서 우리는 그들을 함께 쌓을 수 있습니다.

1276
00:51:57,359 --> 00:51:59,033
그런 다음이 행렬 곱셈을 수행하십시오.

1277
00:51:59,033 --> 00:52:00,431
우리의 체중 매트릭스,

1278
00:52:00,431 --> 00:52:01,911
우리에게,

1279
00:52:01,911 --> 00:52:03,798
그 결과를 탄 (tanh)을 통해 스쿼시 (squash)

1280
00:52:03,798 --> 00:52:05,783
그리고 그것은 우리에게 다음에 숨겨진 상태를 줄 것입니다.

1281
00:52:05,783 --> 00:52:07,687
그리고 그것은 기본적인 기능적 형태의 일종입니다.

1282
00:52:07,687 --> 00:52:10,131
이 바닐라 재발 신경 네트워크의

1283
00:52:10,131 --> 00:52:11,347
그렇다면 우리는

1284
00:52:11,347 --> 00:52:14,080
이 아키텍처에서 일어나는 일

1285
00:52:14,080 --> 00:52:17,738
우리가 그라디언트를 계산하려고 할 때 역방향 패스 중에?

1286
00:52:17,738 --> 00:52:19,435
그래서 우리가 계산하려고 생각하면,

1287
00:52:19,435 --> 00:52:21,426
그래서 그 후 거꾸로 지나가는 동안,

1288
00:52:21,426 --> 00:52:24,759
우리는 우리가 얻은 파생물을 받게 될 것입니다.

1289
00:52:25,850 --> 00:52:27,075
우리는 손실 파생 상품을 받게 될 것입니다.

1290
00:52:27,075 --> 00:52:28,568
ht와 관련하여

1291
00:52:28,568 --> 00:52:30,885
그리고 셀을 통한 역방향 통과 동안,

1292
00:52:30,885 --> 00:52:32,720
우리는 손실 파생 상품을 계산해야합니다.

1293
00:52:32,720 --> 00:52:34,632
ht에서 1을 뺀 값까지.

1294
00:52:34,632 --> 00:52:37,026
그런 다음 우리가이 역 통과를 계산할 때,

1295
00:52:37,026 --> 00:52:38,601
그라디언트가 역방향으로 흐른 것을 볼 수 있습니다.

1296
00:52:38,601 --> 00:52:39,881
이 빨간 길을 통해서.

1297
00:52:39,881 --> 00:52:41,706
먼저 그라디언트가 뒤로 이동합니다.

1298
00:52:41,706 --> 00:52:43,068
이 탄 게이트를 통해,

1299
00:52:43,068 --> 00:52:44,036
그런 다음 뒤쪽으로 흐를 것입니다.

1300
00:52:44,036 --> 00:52:45,958
이 행렬 곱셈 게이트를 통해

1301
00:52:45,958 --> 00:52:48,644
그리고 우리가 숙제에서 보았 듯이

1302
00:52:48,644 --> 00:52:52,054
이들 매트릭스 승산 층을 구현할 때,

1303
00:52:52,054 --> 00:52:53,252
백 프로 페지 할 때

1304
00:52:53,252 --> 00:52:54,711
이 행렬 곱셈 게이트,

1305
00:52:54,711 --> 00:52:56,785
당신은 트랜스 포즈에 의해 mp 거리게됩니다.

1306
00:52:56,785 --> 00:52:58,457
그 무게 매트릭스의.

1307
00:52:58,457 --> 00:53:00,642
그래서 우리가 백 프로게이트 할 때마다

1308
00:53:00,642 --> 00:53:03,857
이들 바닐라 RNN 세포 중 하나를 통해,

1309
00:53:03,857 --> 00:53:08,024
우리는 가중치 행렬의 일부분을 곱하는 것을 끝내게됩니다.

1310
00:53:09,110 --> 00:53:11,124
그래서 지금 우리가 많은 것을 고집하고 있다고 상상한다면

1311
00:53:11,124 --> 00:53:13,834
이러한 재발 성 신경망 세포 중에서,

1312
00:53:13,834 --> 00:53:15,413
다시 이것은 RNN이기 때문입니다.

1313
00:53:15,413 --> 00:53:16,599
모델 시퀀스가 필요합니다.

1314
00:53:16,599 --> 00:53:18,976
이제 그라디언트 흐름이 어떻게되는지 상상해보십시오.

1315
00:53:18,976 --> 00:53:20,921
이들 층들의 시퀀스를 통해,

1316
00:53:20,921 --> 00:53:23,770
어떤 종류의 물고기가 발생하기 시작합니다.

1317
00:53:23,770 --> 00:53:25,207
왜냐하면 지금, 우리가 계산하기를 원할 때

1318
00:53:25,207 --> 00:53:27,863
h 0에 대한 손실의 기울기,

1319
00:53:27,863 --> 00:53:29,440
우리는 모든 것을 백 프로 퍼 게이트해야한다.

1320
00:53:29,440 --> 00:53:30,810
이러한 RNN 세포 중.

1321
00:53:30,810 --> 00:53:32,903
그리고 당신이 하나의 셀을 통해 역 전파 할 때마다,

1322
00:53:32,903 --> 00:53:35,641
당신은 이러한 w transpose
factor 중 하나를 선택할 것입니다.

1323
00:53:35,641 --> 00:53:38,176
즉, 최종 표현식

1324
00:53:38,176 --> 00:53:40,289
그라데이션이 0 인 경우

1325
00:53:40,289 --> 00:53:42,161
많은, 많은 요인들을 포함 할 것이다.

1326
00:53:42,161 --> 00:53:43,550
이 가중치 행렬의

1327
00:53:43,550 --> 00:53:44,967
나쁜 일이 될 수 있습니다.

1328
00:53:44,967 --> 00:53:46,976
어쩌면 무게에 대해 생각하지 않아도됩니다.

1329
00:53:46,976 --> 00:53:48,329
행렬 경우,

1330
00:53:48,329 --> 00:53:50,138
스케일러 케이스를 상상해보십시오.

1331
00:53:50,138 --> 00:53:51,877
우리가 끝내면 우리가 스케일러를 가지고 있다면

1332
00:53:51,877 --> 00:53:53,662
우리는 같은 번호로 반복해서 곱합니다.

1333
00:53:53,662 --> 00:53:54,563
그리고 다시,

1334
00:53:54,563 --> 00:53:56,038
어쩌면 네 가지 예를 들어,

1335
00:53:56,038 --> 00:53:57,133
그러나 100과 같은 것

1336
00:53:57,133 --> 00:54:00,269
또는 수백 개의 시간 간격,

1337
00:54:00,269 --> 00:54:01,832
같은 수를 곱하면된다.

1338
00:54:01,832 --> 00:54:03,835
반복해서 정말 나쁜 것입니다.

1339
00:54:03,835 --> 00:54:05,244
스케일러의 경우,

1340
00:54:05,244 --> 00:54:06,756
폭발 할거야.

1341
00:54:06,756 --> 00:54:08,971
그 수가 1보다 큰 경우

1342
00:54:08,971 --> 00:54:10,504
또는 0으로 사라질 것입니다.

1343
00:54:10,504 --> 00:54:12,912
숫자가 1보다 작은 경우

1344
00:54:12,912 --> 00:54:14,069
절대 값.

1345
00:54:14,069 --> 00:54:16,355
그리고 이것이 일어나지 않을 유일한 길

1346
00:54:16,355 --> 00:54:18,186
그 숫자가 정확히 하나라면,

1347
00:54:18,186 --> 00:54:20,911
실제로 실제로는 거의 발생하지 않습니다.

1348
00:54:20,911 --> 00:54:22,569
그것은 우리를 떠난다.

1349
00:54:22,569 --> 00:54:25,229
동일한 직감이 행렬의 경우까지 확장됩니다.

1350
00:54:25,229 --> 00:54:27,560
그러나 지금, 스케일러 수의 절대 값보다는 오히려,

1351
00:54:27,560 --> 00:54:29,261
대신에 가장 큰 것을보아야합니다.

1352
00:54:29,261 --> 00:54:32,071
이 가중치 행렬의 가장 큰 특이 값.

1353
00:54:32,071 --> 00:54:34,707
이제 그 가장 큰 특이 값이 1보다 큰 경우,

1354
00:54:34,707 --> 00:54:36,502
이 역방향 패스 동안,

1355
00:54:36,502 --> 00:54:38,824
가중치 행렬을 계속해서 곱하면,

1356
00:54:38,824 --> 00:54:42,135
그 그라디언트를 h w, 0 h, 미안,

1357
00:54:42,135 --> 00:54:43,915
매우 커질 것입니다.

1358
00:54:43,915 --> 00:54:46,079
그 행렬이 너무 클 때.

1359
00:54:46,079 --> 00:54:48,947
그리고 그것은 우리가 폭발적인
그라데이션 문제라고 부르는 것입니다.

1360
00:54:48,947 --> 00:54:52,047
이제이 그래디언트가 기하 급수적으로
폭발적으로 폭발 할 것입니다

1361
00:54:52,047 --> 00:54:53,427
시간 간격의 수와 함께

1362
00:54:53,427 --> 00:54:55,061
우리가 통해 역 전파합니다.

1363
00:54:55,061 --> 00:54:57,643
그리고 가장 큰 특이 값이 1보다 작 으면,

1364
00:54:57,643 --> 00:54:59,123
그런 다음 우리는 정반대의 문제를 겪습니다.

1365
00:54:59,123 --> 00:55:00,036
이제 우리의 그라디언트가 줄어들 것입니다.

1366
00:55:00,036 --> 00:55:01,563
기하 급수적으로 줄어들고 축소됩니다.

1367
00:55:01,563 --> 00:55:03,945
우리가 역 전파하고 점점 더 많은 요소들을 선택함에 따라

1368
00:55:03,945 --> 00:55:05,349
이 가중치 행렬의

1369
00:55:05,349 --> 00:55:08,863
이를 사라지는 그라데이션 문제라고합니다.

1370
00:55:08,863 --> 00:55:11,172
사람들이 때로는 해킹하는 비트가 있습니다.

1371
00:55:11,172 --> 00:55:12,836
폭발 그라데이션 문제를 해결하는 방법

1372
00:55:12,836 --> 00:55:14,208
그라디언트 클리핑,

1373
00:55:14,208 --> 00:55:16,264
이것은 단순한 발견 적 방법 일 뿐이다.

1374
00:55:16,264 --> 00:55:18,660
그라디언트를 계산 한 후에,

1375
00:55:18,660 --> 00:55:19,713
그 그라데이션,

1376
00:55:19,713 --> 00:55:22,156
L2 규범이 어떤 임계 값 이상인 경우,

1377
00:55:22,156 --> 00:55:24,112
그런 다음 그것을 단단히 고정시키고 나눕니다.

1378
00:55:24,112 --> 00:55:27,955
이 최대 임계 값을 갖도록 클램프를 잠급니다.

1379
00:55:27,955 --> 00:55:29,271
이것은 일종의 불쾌한 해킹입니다.

1380
00:55:29,271 --> 00:55:30,964
하지만 실제로 실제로 많이 사용됩니다.

1381
00:55:30,964 --> 00:55:32,645
재발 성 신경 네트워크를 훈련 할 때.

1382
00:55:32,645 --> 00:55:34,517
그리고 그것은 비교적 유용한 도구입니다

1383
00:55:34,517 --> 00:55:39,034
폭발하는 그라디언트 문제를 공격했습니다.

1384
00:55:39,034 --> 00:55:40,994
그러나 이제 사라지는 그라디언트 문제에 대해,

1385
00:55:40,994 --> 00:55:42,254
우리가 일반적으로하는 일

1386
00:55:42,254 --> 00:55:43,640
우리가로 이동해야 할 수도 있습니다

1387
00:55:43,640 --> 00:55:46,207
보다 복잡한 RNN 아키텍처.

1388
00:55:46,207 --> 00:55:48,939
그래서 이것은 LSTM에 대한이 아이디어에 동기를 부여합니다.

1389
00:55:48,939 --> 00:55:53,524
Long Short Term
Memory를 나타내는 LSTM

1390
00:55:53,524 --> 00:55:55,700
약간 재밌는 반복 관계인가?

1391
00:55:55,700 --> 00:55:58,316
이러한 재발 성 신경 네트워크에 대해서.

1392
00:55:58,316 --> 00:55:59,984
그것은 정말로 완화시키는 것을 돕기 위해 디자인되었습니다.

1393
00:55:59,984 --> 00:56:03,330
사라지고 폭발하는 그라디언트의 문제.

1394
00:56:03,330 --> 00:56:05,591
그래서 그것의 위에 해킹의 종류보다,

1395
00:56:05,591 --> 00:56:07,794
우리는 단지 아키텍처를 디자인합니다.

1396
00:56:07,794 --> 00:56:10,193
그래디언트 유동 특성이 더 좋다.

1397
00:56:10,193 --> 00:56:13,566
그 멋진 CNN 아키텍처에 대한 유추의 종류

1398
00:56:13,566 --> 00:56:16,556
우리가 강의의 꼭대기에서 보았던 것.

1399
00:56:16,556 --> 00:56:17,389
지적 할 또 다른 점

1400
00:56:17,389 --> 00:56:20,807
LSTM 세포가 실제로 1997 년부터 온다는 것입니다.

1401
00:56:20,807 --> 00:56:22,337
그래서 LSTM에 대한이 아이디어

1402
00:56:22,337 --> 00:56:24,073
꽤 오랫동안 주변에 있었고,

1403
00:56:24,073 --> 00:56:26,108
이 사람들은 이러한 아이디어를 연구하고있었습니다.

1404
00:56:26,108 --> 00:56:27,110
90 년대에

1405
00:56:27,110 --> 00:56:28,852
확실히 커브보다 앞서있었습니다.

1406
00:56:28,852 --> 00:56:31,225
이 모델들은 지금 어디서나 사용되기 때문에

1407
00:56:31,225 --> 00:56:32,475
20 년 후.

1408
00:56:33,864 --> 00:56:37,921
LSTM에는 이런 재미있는 기능적 형태가 있습니다.

1409
00:56:37,921 --> 00:56:39,814
이 바닐라가 언제 있었는지 기억해.

1410
00:56:39,814 --> 00:56:41,198
재발 성 신경 네트워크,

1411
00:56:41,198 --> 00:56:42,538
그것은이 숨겨진 상태였습니다.

1412
00:56:42,538 --> 00:56:44,056
그리고 우리는이 반복 관계를 사용했습니다.

1413
00:56:44,056 --> 00:56:46,397
매 시간 단계마다 숨겨진 상태를 업데이트합니다.

1414
00:56:46,397 --> 00:56:47,570
자, LSTM에서,

1415
00:56:47,570 --> 00:56:48,842
우리는 실제로 2,

1416
00:56:48,842 --> 00:56:51,462
매 시간 단계마다 두 가지 숨겨진 상태를 유지합니다.

1417
00:56:51,462 --> 00:56:52,931
하나는 이것입니다,

1418
00:56:52,931 --> 00:56:54,439
숨겨진 상태라고하는,

1419
00:56:54,439 --> 00:56:57,334
숨겨진 상태에 비유하는 것입니다.

1420
00:56:57,334 --> 00:56:59,305
우리가 바닐라 RNN에서 가지고 있었던

1421
00:56:59,305 --> 00:57:02,524
그러나 LSTM은 두 번째 벡터 인 ct를 유지합니다.

1422
00:57:02,524 --> 00:57:03,604
세포 상태를 불렀다.

1423
00:57:03,604 --> 00:57:06,459
그리고 세포 상태는 일종의 내부 벡터입니다.

1424
00:57:06,459 --> 00:57:08,546
LSTM 안에 보관,

1425
00:57:08,546 --> 00:57:12,240
그리고 그것은 실제로 외부 세계에 노출되지 않습니다.

1426
00:57:12,240 --> 00:57:13,170
그리고 우리는 볼 것입니다,

1427
00:57:13,170 --> 00:57:15,260
이 업데이트 방정식을 통해,

1428
00:57:15,260 --> 00:57:17,371
우리가 언제,

1429
00:57:17,371 --> 00:57:19,235
먼저 우리가 이것을 계산할 때,

1430
00:57:19,235 --> 00:57:20,803
우리는 두 개의 입력을받습니다.

1431
00:57:20,803 --> 00:57:23,966
우리는이 네 개의 게이트를 계산하기 위해 그것들을 사용합니다.

1432
00:57:23,966 --> 00:57:25,485
i, f, o, n, g라고 불리는

1433
00:57:25,485 --> 00:57:28,634
이 게이트를 사용하여 셀 상태를 업데이트합니다.

1434
00:57:28,634 --> 00:57:30,302
그리고 우리는 우리의 세포 상태의 일부를 드러낸다.

1435
00:57:30,302 --> 00:57:33,802
다음 시간 단계에서 숨겨진 상태로

1436
00:57:36,704 --> 00:57:38,282
이것은 일종의 재미있는 기능적 형태입니다,

1437
00:57:38,282 --> 00:57:40,227
몇 장의 슬라이드를보고 싶다.

1438
00:57:40,227 --> 00:57:42,407
정확히 왜 우리는이 아키텍처를 사용합니까?

1439
00:57:42,407 --> 00:57:44,014
왜 그것이 합리적인지,

1440
00:57:44,014 --> 00:57:45,418
특히 맥락에서

1441
00:57:45,418 --> 00:57:47,731
사라지는 또는 폭발하는 그라데이션.

1442
00:57:47,731 --> 00:57:51,044
우리가 LSTM에서하는 첫 번째 일

1443
00:57:51,044 --> 00:57:54,534
이 이전에 숨겨진 상태 인 ht가 주어진다는 것입니다.

1444
00:57:54,534 --> 00:57:57,213
우리는 현재 입력 벡터 인 x t를받습니다.

1445
00:57:57,213 --> 00:57:58,611
바닐라 RNN처럼.

1446
00:57:58,611 --> 00:58:00,251
바닐라 RNN에서는

1447
00:58:00,251 --> 00:58:02,617
우리는 두 개의 입력 벡터를 사용했습니다.

1448
00:58:02,617 --> 00:58:03,908
우리는 그들을 연결했다.

1449
00:58:03,908 --> 00:58:05,098
그런 다음 행렬 곱셈을했습니다.

1450
00:58:05,098 --> 00:58:08,663
RNN에서 다음 숨김 상태를 직접 계산합니다.

1451
00:58:08,663 --> 00:58:10,751
이제 LSTM은 조금 다른 것을합니다.

1452
00:58:10,751 --> 00:58:13,055
우리는 이전의 숨겨진 상태를 취하려고합니다.

1453
00:58:13,055 --> 00:58:14,429
우리의 현재 입력,

1454
00:58:14,429 --> 00:58:15,315
그들을 쌓아 라,

1455
00:58:15,315 --> 00:58:18,748
이제는 매우 큰 가중치 행렬 w를 곱합니다.

1456
00:58:18,748 --> 00:58:21,926
네 개의 다른 게이트를 계산하려면,

1457
00:58:21,926 --> 00:58:24,391
모두 숨겨진 상태와 같은 크기입니다.

1458
00:58:24,391 --> 00:58:26,193
때로는 다른 방식으로 작성된 것을 볼 수 있습니다.

1459
00:58:26,193 --> 00:58:29,549
일부 저자는 다른 가중치 행렬을 쓸 것입니다.

1460
00:58:29,549 --> 00:58:30,808
각 게이트마다.

1461
00:58:30,808 --> 00:58:31,793
일부 저자는 모두 그들을 결합합니다

1462
00:58:31,793 --> 00:58:33,160
하나의 큰 무게 매트릭스로.

1463
00:58:33,160 --> 00:58:34,677
그러나 그것은 모두 정말로 같은 것입니다.

1464
00:58:34,677 --> 00:58:36,334
아이디어는 우리가 숨겨진 상태를 취하는 것입니다.

1465
00:58:36,334 --> 00:58:37,282
우리의 현재 입력,

1466
00:58:37,282 --> 00:58:40,288
그런 다음 이들을 사용하여이 네 가지 게이트를 계산합니다.

1467
00:58:40,288 --> 00:58:42,163
이 4 개의 문은,

1468
00:58:42,163 --> 00:58:45,725
당신은 종종 이것을 i, f, o, g, ifog,

1469
00:58:45,725 --> 00:58:47,768
그들이 무엇인지 기억하기가 쉽습니다.

1470
00:58:47,768 --> 00:58:49,435
내가 입력 게이트입니다.

1471
00:58:50,324 --> 00:58:53,655
그것은 우리 세포에 얼마나 많은
양을 입력하고 싶은지를 말합니다.

1472
00:58:53,655 --> 00:58:55,431
F는 잊어 버리는 게이트입니다.

1473
00:58:55,431 --> 00:58:57,860
우리가 얼마나 세포 기억을 잊고 싶니?

1474
00:58:57,860 --> 00:59:00,194
이전 시간 간격에서 이전 시간 간격으로.

1475
00:59:00,194 --> 00:59:01,333
O는 출력 게이트이고,

1476
00:59:01,333 --> 00:59:03,327
우리 자신을 얼마나 드러내고 싶은가?

1477
00:59:03,327 --> 00:59:04,653
바깥 세상에.

1478
00:59:04,653 --> 00:59:07,092
그리고 G는 정말 좋은 이름이 아닙니다.

1479
00:59:07,092 --> 00:59:10,130
그래서 저는 보통 게이트 게이트라고 부릅니다.

1480
00:59:10,130 --> 00:59:13,109
G, 우리가 얼마나 쓰고 싶어하는지 알려주지.

1481
00:59:13,109 --> 00:59:14,626
입력 셀에 입력합니다.

1482
00:59:14,626 --> 00:59:16,665
그리고이 4 개의 문 각각이

1483
00:59:16,665 --> 00:59:19,665
다른 비선형 성을 사용하고 있습니다.

1484
00:59:21,724 --> 00:59:23,809
입력, 잊어 버림 및 출력 게이트

1485
00:59:23,809 --> 00:59:25,047
모두 시그 모이 드를 사용하고 있습니다.

1486
00:59:25,047 --> 00:59:28,571
이는 값이 0과 1 사이에 있음을 의미합니다.

1487
00:59:28,571 --> 00:59:31,109
게이트 게이트는 tanh를 사용하는 반면,

1488
00:59:31,109 --> 00:59:34,316
이것은 출력이 -1과 -1 사이임을 의미합니다.

1489
00:59:34,316 --> 00:59:36,810
그래서, 이것들은 이상합니다.

1490
00:59:36,810 --> 00:59:38,551
하지만 조금 더 의미가 있습니다.

1491
00:59:38,551 --> 00:59:41,725
그들 모두를 이진 값으로 상상해보십시오.

1492
00:59:41,725 --> 00:59:43,077
맞아, 극단에서 일어나는 일처럼

1493
00:59:43,077 --> 00:59:44,744
이 두 값 중?

1494
00:59:46,033 --> 00:59:46,866
그것은 일종의 일 이죠.

1495
00:59:46,866 --> 00:59:48,490
우리가이 게이트들을 계산하면

1496
00:59:48,490 --> 00:59:50,200
이 다음 방정식을 보면,

1497
00:59:50,200 --> 00:59:51,423
당신은 우리의 세포 상태를 볼 수 있습니다.

1498
00:59:51,423 --> 00:59:53,926
잊어 버린 게이트에 의해 요소가
현명하게 곱해지고 있습니다.

1499
00:59:53,926 --> 00:59:56,269
죄송합니다. 이전 시간 단계의 셀 상태

1500
00:59:56,269 --> 00:59:59,305
이 잊어 버린 게이트로 요소를 현명하게 곱하고 있습니다.

1501
00:59:59,305 --> 01:00:00,350
그리고 지금이 문을 잊어 버리면,

1502
01:00:00,350 --> 01:00:03,861
여러분은 그것을 0과 1의 벡터로 생각할 수 있습니다.

1503
01:00:03,861 --> 01:00:06,162
그것은 세포 상태의 각 요소에 대해 알려주고 있습니다.

1504
01:00:06,162 --> 01:00:08,416
우리가 세포의 그 요소를 잊고 싶습니까?

1505
01:00:08,416 --> 01:00:10,644
잊어 버린 게이트가 0 인 경우에?

1506
01:00:10,644 --> 01:00:12,819
아니면 세포의 그 요소를 기억하고 싶습니까?

1507
01:00:12,819 --> 01:00:14,935
잊어 버린 게이트가 하나 인 경우.

1508
01:00:14,935 --> 01:00:17,258
이제 우리가 잊어 버린 문을 사용하면

1509
01:00:17,258 --> 01:00:19,767
셀 상태의 부분을 게이트 오프하기 위해,

1510
01:00:19,767 --> 01:00:21,088
우리는 두 번째 용어를 가지고 있습니다.

1511
01:00:21,088 --> 01:00:24,814
이것은 i와 g의 요소 현명한 산출물이다.

1512
01:00:24,814 --> 01:00:27,431
이제 저는이 0과 1의 벡터입니다.

1513
01:00:27,431 --> 01:00:28,824
Sigmoid를 통해오고 있기 때문에,

1514
01:00:28,824 --> 01:00:31,480
세포 상태의 각 요소에 대해 알려주고,

1515
01:00:31,480 --> 01:00:33,909
우리는 셀 상태의 해당 요소에 쓰고 싶습니까?

1516
01:00:33,909 --> 01:00:35,728
내가 하나 인 경우,

1517
01:00:35,728 --> 01:00:37,771
또는 우리는 셀 상태의 해당 요소에 쓰고 싶지 않습니다.

1518
01:00:37,771 --> 01:00:38,687
이 단계에서

1519
01:00:38,687 --> 01:00:41,719
i가 0 인 경우

1520
01:00:41,719 --> 01:00:42,585
그리고 이제 게이트 게이트,

1521
01:00:42,585 --> 01:00:44,254
그것은 tanh를 통해오고 있기 때문에,

1522
01:00:44,254 --> 01:00:46,031
1 또는 1이 될 것입니다.

1523
01:00:46,031 --> 01:00:47,495
이것이 우리가 원하는 가치입니다.

1524
01:00:47,495 --> 01:00:50,500
우리가 글쓰기를 고려할 수있는 후보 값

1525
01:00:50,500 --> 01:00:54,022
이 시간 단계에서 셀 상태의 각 요소에 전달합니다.

1526
01:00:54,022 --> 01:00:56,098
그런 다음 셀 상태 방정식을 보면,

1527
01:00:56,098 --> 01:00:58,157
당신은 매 단계마다,

1528
01:00:58,157 --> 01:00:59,963
세포 상태는 이런 종류의

1529
01:00:59,963 --> 01:01:02,499
이들 서로 다른 독립적 인 스케일러 값들은,

1530
01:01:02,499 --> 01:01:05,230
그들은 모두 하나씩 증가하거나 감소합니다.

1531
01:01:05,230 --> 01:01:07,466
그래서 종류가 비슷합니다.

1532
01:01:07,466 --> 01:01:09,560
세포 상태 안에서, 우리는 기억할 수있다.

1533
01:01:09,560 --> 01:01:11,028
우리 이전의 상태를 잊어 버리거나,

1534
01:01:11,028 --> 01:01:13,480
그런 다음 증가 또는 감소시킬 수 있습니다.

1535
01:01:13,480 --> 01:01:14,765
그 셀 상태의 각 요소

1536
01:01:14,765 --> 01:01:16,535
매 시간 간격마다 하나씩

1537
01:01:16,535 --> 01:01:19,430
그래서 당신은 세포 상태의 이러한
요소들을 생각할 수 있습니다.

1538
01:01:19,430 --> 01:01:22,189
작은 스케일러 정수 카운터 들로서

1539
01:01:22,189 --> 01:01:24,061
증감 할 수있는

1540
01:01:24,061 --> 01:01:25,708
각 시간 단계마다.

1541
01:01:25,708 --> 01:01:28,489
그리고 이제, 우리가 세포 상태를 계산 한 후에,

1542
01:01:28,489 --> 01:01:31,624
그런 다음 우리는 지금 업데이트 된 셀 상태를 사용합니다

1543
01:01:31,624 --> 01:01:32,874
숨겨진 상태를 계산하기 위해,

1544
01:01:32,874 --> 01:01:36,513
우리는 바깥 세상에 드러 낼 것입니다.

1545
01:01:36,513 --> 01:01:38,832
그래서이 세포 상태가이 해석을 가지고 있기 때문에

1546
01:01:38,832 --> 01:01:39,884
카운터가되는 것,

1547
01:01:39,884 --> 01:01:41,280
하나씩 세는 종류

1548
01:01:41,280 --> 01:01:43,353
또는 각 시간 단계에서 마이너스 1,

1549
01:01:43,353 --> 01:01:45,518
우리는 그 카운터 값을 스쿼시하려고합니다.

1550
01:01:45,518 --> 01:01:48,895
tanh를 사용하여 멋진 0에서 1 범위로

1551
01:01:48,895 --> 01:01:50,483
그리고 지금, 우리는 요소 현명하고,

1552
01:01:50,483 --> 01:01:51,826
이 출력 게이트에 의해.

1553
01:01:51,826 --> 01:01:54,441
그리고 출력 게이트는 다시 S 자형을 통과하게됩니다.

1554
01:01:54,441 --> 01:01:57,597
그래서 당신은 거의 0과 1 인
것으로 생각할 수 있습니다.

1555
01:01:57,597 --> 01:01:58,947
출력 게이트가 우리에게 알려줍니다.

1556
01:01:58,947 --> 01:02:00,826
우리의 세포 상태의 각 요소에 대해,

1557
01:02:00,826 --> 01:02:02,773
우리가 계시를 밝히지 않겠습니까?

1558
01:02:02,773 --> 01:02:04,614
우리 세포 상태의 그 요소

1559
01:02:04,614 --> 01:02:06,994
우리가 외부의 숨겨진 상태를 계산할 때

1560
01:02:06,994 --> 01:02:08,577
이 시간 단계.

1561
01:02:09,736 --> 01:02:11,609
그리고 저는 전통의 종류가 있다고 생각합니다.

1562
01:02:11,609 --> 01:02:13,479
LSTM을 설명하려고하는 사람들에게,

1563
01:02:13,479 --> 01:02:14,524
모두가 올 필요가있다.

1564
01:02:14,524 --> 01:02:17,132
잠재적으로 혼란스러운 LSTM 다이어그램을 사용합니다.

1565
01:02:17,132 --> 01:02:18,882
그래서 여기에 내 시도가있다.

1566
01:02:20,380 --> 01:02:23,866
여기서 우리는이 LSTM 셀 내부에서 어떤
일이 벌어지고 있는지를 볼 수 있습니다.

1567
01:02:23,866 --> 01:02:24,865
우리가 우리의,

1568
01:02:24,865 --> 01:02:27,566
우리는 우리의 이전 셀 상태를 왼쪽에 입력으로 취하고있다.

1569
01:02:27,566 --> 01:02:28,937
이전 숨겨진 상태,

1570
01:02:28,937 --> 01:02:31,266
뿐만 아니라 우리의 현재 입력, x t.

1571
01:02:31,266 --> 01:02:32,537
이제 우리는 현재의,

1572
01:02:32,537 --> 01:02:34,985
우리의 이전의 숨겨진 상태,

1573
01:02:34,985 --> 01:02:36,453
뿐만 아니라 우리의 현재 입력,

1574
01:02:36,453 --> 01:02:37,346
그들을 쌓아 라,

1575
01:02:37,346 --> 01:02:39,526
이 가중치 행렬 w를 곱하면,

1576
01:02:39,526 --> 01:02:41,166
우리의 네 가지 문을 만들 수 있습니다.

1577
01:02:41,166 --> 01:02:42,627
그리고 여기서는 비선형 성을 배제했습니다.

1578
01:02:42,627 --> 01:02:44,836
이전 슬라이드에서이 슬라이드를 보았 기 때문입니다.

1579
01:02:44,836 --> 01:02:47,294
이제 잊어 버린 게이트는 요소를 현명하게 곱합니다.

1580
01:02:47,294 --> 01:02:48,143
세포 상태.

1581
01:02:48,143 --> 01:02:51,174
입력 및 게이트 게이트에 엘리먼트를 현명하게 곱합니다.

1582
01:02:51,174 --> 01:02:52,689
셀 상태에 추가됩니다.

1583
01:02:52,689 --> 01:02:54,524
그리고 그것은 우리에게 다음 세포를줍니다.

1584
01:02:54,524 --> 01:02:56,533
다음 셀은 tanh를 통해 부숴지며,

1585
01:02:56,533 --> 01:02:58,616
이 출력 게이트와 현저하게 곱해진 소자

1586
01:02:58,616 --> 01:03:01,366
우리의 다음 숨겨진 상태를 생산합니다.

1587
01:03:02,417 --> 01:03:03,250
문제?

1588
01:03:13,116 --> 01:03:14,587
아니, 그래서 그들은 이것을 통해오고있다.

1589
01:03:14,587 --> 01:03:17,878
그들은이 무게 매트릭스의 다른 부분에서 왔습니다.

1590
01:03:17,878 --> 01:03:19,147
그래서 우리의 숨겨진,

1591
01:03:19,147 --> 01:03:23,343
x와 h가 모두이 차원 h 인 경우,

1592
01:03:23,343 --> 01:03:24,300
우리가 그들을 쌓은 후에,

1593
01:03:24,300 --> 01:03:26,415
그들은 벡터 크기가 2 시간 일 겁니다.

1594
01:03:26,415 --> 01:03:28,718
이제 우리의 가중치 행렬이이 행렬이됩니다.

1595
01:03:28,718 --> 01:03:30,393
크기가 4 시간 h 인 경우 2 시간.

1596
01:03:30,393 --> 01:03:31,873
그래서 당신은 그것을 일종의 것으로 생각할 수 있습니다.

1597
01:03:31,873 --> 01:03:34,076
이 무게 매트릭스의 네 청크.

1598
01:03:34,076 --> 01:03:37,344
그리고 각각의 무게 매트릭스의 4 개의 덩어리

1599
01:03:37,344 --> 01:03:41,511
이 게이트 중 다른 하나를 계산하려고합니다.

1600
01:03:42,404 --> 01:03:44,673
당신은 종종 이것을 명확히하기 위해 쓰여진 것을 보게 될 것이며,

1601
01:03:44,673 --> 01:03:46,449
네 가지 모두를 결합하는 종류의

1602
01:03:46,449 --> 01:03:49,161
가중치 행렬을 하나의 큰 행렬 w로,

1603
01:03:49,161 --> 01:03:51,109
표기법의 편의를 위해서입니다.

1604
01:03:51,109 --> 01:03:52,484
그러나 그들은 모두 계산됩니다.

1605
01:03:52,484 --> 01:03:56,067
가중치 행렬의 다른 부분을 사용합니다.

1606
01:03:57,080 --> 01:03:58,645
그러나 당신은 모두 계산된다는 점에서 정확합니다.

1607
01:03:58,645 --> 01:04:00,458
동일한 기능적 형태를 사용하여

1608
01:04:00,458 --> 01:04:01,658
두 가지를 쌓아 두는 것의

1609
01:04:01,658 --> 01:04:04,574
행렬 곱셈을 취한다.

1610
01:04:04,574 --> 01:04:06,393
이제 우리는이 그림을 가지고 있습니다.

1611
01:04:06,393 --> 01:04:09,519
우리는 LSTM 세포가 어떻게 될지 생각할 수 있습니다.

1612
01:04:09,519 --> 01:04:11,196
거꾸로 통과하는 동안?

1613
01:04:11,196 --> 01:04:12,795
우리는 바닐라의 맥락에서 보았다.

1614
01:04:12,795 --> 01:04:13,738
재발 성 신경 네트워크,

1615
01:04:13,738 --> 01:04:15,803
거꾸로 지나가는 동안 나쁜 일이 일어 났고,

1616
01:04:15,803 --> 01:04:16,958
우리가 계속적으로

1617
01:04:16,958 --> 01:04:18,999
그 무게 매트릭스에 의해, w.

1618
01:04:18,999 --> 01:04:20,555
그러나 지금 상황은 많이 보입니다.

1619
01:04:20,555 --> 01:04:23,238
LSTM에서는 꽤 다른 점이 있습니다.

1620
01:04:23,238 --> 01:04:26,468
이 길을 거꾸로 상상하면

1621
01:04:26,468 --> 01:04:28,323
셀 상태의 그래디언트를 계산하고,

1622
01:04:28,323 --> 01:04:29,952
우리는 아주 멋진 그림을 얻습니다.

1623
01:04:29,952 --> 01:04:32,266
이제 상류의 그라디언트가있을 때

1624
01:04:32,266 --> 01:04:33,320
들어오는 세포에서

1625
01:04:33,320 --> 01:04:36,281
일단 우리가 뒤로 역 분개하면

1626
01:04:36,281 --> 01:04:37,737
이 가산 연산에 의해,

1627
01:04:37,737 --> 01:04:40,572
이 추가가 단지 사본임을 기억하십시오.

1628
01:04:40,572 --> 01:04:43,663
두 지점으로의 상류 구배,

1629
01:04:43,663 --> 01:04:45,641
우리의 업스트림 그래디언트가 직접 복사됩니다.

1630
01:04:45,641 --> 01:04:47,954
역 전파로 직접 전달

1631
01:04:47,954 --> 01:04:50,435
이 요소를 현명하게 곱하십시오.

1632
01:04:50,435 --> 01:04:52,192
그러면 우리의 업스트림 그래디언트가 끝납니다.

1633
01:04:52,192 --> 01:04:56,455
잊어 버린 게이트에 의해 현명하게 곱해졌다.

1634
01:04:56,455 --> 01:05:00,364
이 셀 상태를 거꾸로 백 프로 퍼 게이트 할 때,

1635
01:05:00,364 --> 01:05:01,397
일어나는 유일한 일

1636
01:05:01,397 --> 01:05:03,818
우리의 업스트림 셀 상태 그라데이션으로

1637
01:05:03,818 --> 01:05:05,935
그것이 현명하게 곱한 요소를 얻는 것을 끝내는 것입니다.

1638
01:05:05,935 --> 01:05:07,171
잊어 버린 문으로.

1639
01:05:07,171 --> 01:05:09,939
이것은 정말 더 좋네요.

1640
01:05:09,939 --> 01:05:12,640
두 가지 이유로 바닐라 RNN보다

1641
01:05:12,640 --> 01:05:14,318
하나는 게이트를 잊어 버리는 것입니다.

1642
01:05:14,318 --> 01:05:16,488
이제 요소 현명한 곱셈입니다.

1643
01:05:16,488 --> 01:05:18,498
전체 행렬 곱셈보다

1644
01:05:18,498 --> 01:05:19,923
원소 현명한 곱셈

1645
01:05:19,923 --> 01:05:23,205
조금 더 좋을거야.

1646
01:05:23,205 --> 01:05:24,964
전체 행렬 곱셈보다

1647
01:05:24,964 --> 01:05:27,208
두 번째는 요소 현명한 곱셈입니다.

1648
01:05:27,208 --> 01:05:29,710
잠재적으로 다른

1649
01:05:29,710 --> 01:05:31,354
매 단계마다 게이트를 잊어 버려라.

1650
01:05:31,354 --> 01:05:33,087
그래서 바닐라 RNN에서,

1651
01:05:33,087 --> 01:05:35,638
우리는 계속해서 같은 무게 매트릭스를 곱하고있었습니다.

1652
01:05:35,638 --> 01:05:36,660
다시 반복하여,

1653
01:05:36,660 --> 01:05:38,305
매우 명백하게 이끌어 낸

1654
01:05:38,305 --> 01:05:40,563
이러한 폭발적이거나 사라지는 그라디언트.

1655
01:05:40,563 --> 01:05:41,943
그러나 현재 LSTM의 경우,

1656
01:05:41,943 --> 01:05:45,161
이 잊지 문은 각 시간 단계마다 다를 수 있습니다.

1657
01:05:45,161 --> 01:05:47,463
이제는 모델이 훨씬 더 쉽습니다.

1658
01:05:47,463 --> 01:05:49,560
이러한 문제를 피하기 위해

1659
01:05:49,560 --> 01:05:51,670
폭발 및 사라지는 그라디언트의.

1660
01:05:51,670 --> 01:05:53,377
마지막으로,이 게이트를 잊어 버리기 때문에

1661
01:05:53,377 --> 01:05:54,902
S 자 결장에서 나오는거야.

1662
01:05:54,902 --> 01:05:56,178
이 요소 현명한 곱하기

1663
01:05:56,178 --> 01:05:58,438
0과 1 사이에있는 것이 보증됩니다.

1664
01:05:58,438 --> 01:06:00,868
다시 한 번 더 좋은 수치 적 속성으로 연결됩니다.

1665
01:06:00,868 --> 01:06:02,457
이런 것들로 번식하는 것을 상상한다면

1666
01:06:02,457 --> 01:06:04,278
다시 반복하여.

1667
01:06:04,278 --> 01:06:07,063
주의해야 할 또 다른 사항은 컨텍스트에서

1668
01:06:07,063 --> 01:06:08,909
바닐라 재발 성 신경 네트워크의

1669
01:06:08,909 --> 01:06:10,239
우리는 역 통과 동안 그것을 보았습니다,

1670
01:06:10,239 --> 01:06:13,146
우리의 그라디언트도 tanh를 통해 흐르고있었습니다.

1671
01:06:13,146 --> 01:06:14,273
매 단계마다.

1672
01:06:14,273 --> 01:06:15,940
그러나 현재 LSTM에서,

1673
01:06:17,459 --> 01:06:18,792
우리의 산출물은,

1674
01:06:20,790 --> 01:06:22,452
LSTM에서는 숨겨진 상태가 사용됩니다.

1675
01:06:22,452 --> 01:06:24,110
그 출력을 계산하기 위해 y t,

1676
01:06:24,110 --> 01:06:26,141
그래서 지금, 각각의 숨겨진 상태,

1677
01:06:26,141 --> 01:06:29,229
최종 숨겨진 상태에서 백 프로
퍼 게이트하는 것을 상상한다면

1678
01:06:29,229 --> 01:06:31,290
다시 제 1 셀 상태로,

1679
01:06:31,290 --> 01:06:33,024
그 후진 경로를 통해,

1680
01:06:33,024 --> 01:06:37,396
우리는 단일 tanh non linearity를
통해 backpropagate 만

1681
01:06:37,396 --> 01:06:42,044
매 단계마다 별도의 탄을 사용하는 것이 아닙니다.

1682
01:06:42,044 --> 01:06:44,059
당신이이 모든 것을 하나로 모을 때의 종류는,

1683
01:06:44,059 --> 01:06:45,927
이 거꾸로 패스를 볼 수 있습니다.

1684
01:06:45,927 --> 01:06:48,646
셀 상태를 통해 역 전파

1685
01:06:48,646 --> 01:06:50,483
그라디언트 슈퍼 고속도로의 일종이다

1686
01:06:50,483 --> 01:06:53,205
그래디언트가 상대적으로 방해가되지 않도록합니다.

1687
01:06:53,205 --> 01:06:55,047
모델의 맨 끝에서의 손실로부터

1688
01:06:55,047 --> 01:06:56,765
다시 초기 셀 상태로 돌아 간다.

1689
01:06:56,765 --> 01:06:59,172
모델의 시작 부분에서.

1690
01:06:59,172 --> 01:07:00,922
질문 있니?

1691
01:07:02,901 --> 01:07:04,693
그래, w와 관련된 그라데이션은 어때?

1692
01:07:04,693 --> 01:07:06,792
그것이 궁극적으로 우리가 염려하는 것입니다.

1693
01:07:06,792 --> 01:07:08,752
그래서, w에 관한 그라데이션

1694
01:07:08,752 --> 01:07:10,252
올 것이다.

1695
01:07:11,737 --> 01:07:12,570
매 단계마다,

1696
01:07:12,570 --> 01:07:13,771
우리의 현재 셀 상태를 취할 것입니다.

1697
01:07:13,771 --> 01:07:15,059
우리의 현재 숨겨진 상태

1698
01:07:15,059 --> 01:07:16,272
그리고 그것은 우리에게 요소를 줄 것이다.

1699
01:07:16,272 --> 01:07:18,480
그것은 우리에게 우리의 로컬 그라디언트 w를 줄 것이다.

1700
01:07:18,480 --> 01:07:19,848
그 시간 단계.

1701
01:07:19,848 --> 01:07:21,340
그래서 우리의 세포 상태,

1702
01:07:21,340 --> 01:07:23,791
바닐라 RNN 사건에서

1703
01:07:23,791 --> 01:07:27,307
우리는 그 첫 스텝 그라디언트를 추가하게 될 것입니다.

1704
01:07:27,307 --> 01:07:29,587
w에 대한 최종 그라데이션을 계산합니다.

1705
01:07:29,587 --> 01:07:33,139
하지만 지금 상황을 상상해 보면

1706
01:07:33,139 --> 01:07:34,961
우리는 매우 긴 서열을 가지고 있습니다.

1707
01:07:34,961 --> 01:07:36,405
그리고 우리는 끝까지 기울기만을 얻고 있습니다.

1708
01:07:36,405 --> 01:07:37,280
시퀀스의

1709
01:07:37,280 --> 01:07:38,945
자, 당신이 백 프로게이트 할 때,

1710
01:07:38,945 --> 01:07:40,978
우리는 w에 로컬 그라데이션을 갖습니다.

1711
01:07:40,978 --> 01:07:43,219
각 시간 단계마다,

1712
01:07:43,219 --> 01:07:44,484
그 지역 그라디언트 w

1713
01:07:44,484 --> 01:07:48,506
이 기울기를 통해 c와 h에 올 것입니다.

1714
01:07:48,506 --> 01:07:50,994
그래서 우리는 c에 그라데이션을 유지하기 때문에

1715
01:07:50,994 --> 01:07:52,751
LSTM의 경우 훨씬 더 훌륭하게,

1716
01:07:52,751 --> 01:07:54,988
각 시간 단계에서의 w에 대한 그 지역 구배

1717
01:07:54,988 --> 01:07:57,221
또한 앞뒤로 이월됩니다.

1718
01:07:57,221 --> 01:07:59,804
시간을 훨씬 더 깔끔하게

1719
01:08:01,627 --> 01:08:03,044
다른 질문?

1720
01:08:17,428 --> 01:08:18,645
그래, 그 질문은

1721
01:08:18,645 --> 01:08:19,886
비선형 성 때문에,

1722
01:08:19,886 --> 01:08:22,088
이것은 여전히 사라지는 그라데이션에 취약 할 수 있습니까?

1723
01:08:22,089 --> 01:08:24,077
그리고 그럴 수도 있습니다.

1724
01:08:24,077 --> 01:08:26,176
사실, 당신이 상상할 수있는 한 가지 문제가 있습니다.

1725
01:08:26,176 --> 01:08:27,560
이게 문을 잊어 버린 것일 수도 있습니다.

1726
01:08:27,560 --> 01:08:29,322
항상 0보다 작다.

1727
01:08:29,323 --> 01:08:30,252
또는 항상 하나 미만,

1728
01:08:30,252 --> 01:08:31,411
사라지는 그라디언트가 나타날 수 있습니다.

1729
01:08:31,411 --> 01:08:34,103
당신이 끊임없이 잊어 버린이 문을 통과 할 때.

1730
01:08:34,103 --> 01:08:35,960
사람들이 실제로하는 일종의 트릭입니다.

1731
01:08:35,960 --> 01:08:38,513
때로는,

1732
01:08:38,513 --> 01:08:40,689
망각문의 편향을 초기화하다.

1733
01:08:40,689 --> 01:08:42,746
다소 긍정적이다.

1734
01:08:42,746 --> 01:08:44,004
그래서 훈련 초반에,

1735
01:08:44,004 --> 01:08:46,305
그 잊지 문은 항상 하나에 가깝습니다.

1736
01:08:46,305 --> 01:08:48,118
적어도 훈련 시작 부분에,

1737
01:08:48,118 --> 01:08:52,285
다음 우리는 그렇게하지 않았습니다,
상대적으로 깨끗한 그라디언트 흐름

1738
01:08:53,265 --> 01:08:54,381
이 잊혀진 문을 통해,

1739
01:08:54,381 --> 01:08:56,631
그들은 모두 하나에 가까워 지도록
초기화 되었기 때문입니다.

1740
01:08:56,631 --> 01:08:58,934
그리고 교육 기간 내내,

1741
01:08:58,935 --> 01:09:00,308
그 모델은 그 편향을 배울 수있다.

1742
01:09:00,308 --> 01:09:02,742
그리고 그것이 필요한 곳을 잊는 법을 배웁니다.

1743
01:09:02,742 --> 01:09:04,886
당신은 여전히 잠재력이있을 수 있습니다.

1744
01:09:04,886 --> 01:09:06,246
여기에 사라지는 그라데이션이 있습니다.

1745
01:09:06,246 --> 01:09:07,569
그러나 그것은 훨씬 덜 극단적이다.

1746
01:09:07,569 --> 01:09:09,182
바닐라 RNN의 경우보다

1747
01:09:09,182 --> 01:09:12,126
둘 다 fs가 각 시간 단계마다 다를 수 있기 때문에,

1748
01:09:12,126 --> 01:09:14,590
우리가하고 있기 때문에 또한

1749
01:09:14,590 --> 01:09:15,620
이 엘리먼트 현명한 곱셈

1750
01:09:15,620 --> 01:09:19,126
전체 행렬 곱셈보다

1751
01:09:19,126 --> 01:09:20,801
그래서 당신은이 LSTM

1752
01:09:20,801 --> 01:09:23,048
실제로는 ResNet과 매우 유사합니다.

1753
01:09:23,048 --> 01:09:24,510
이 잔여 네트워크에서,

1754
01:09:24,510 --> 01:09:26,732
이 신원 연결 경로가있었습니다.

1755
01:09:26,732 --> 01:09:28,069
네트워크를 통해 뒤로 이동

1756
01:09:28,069 --> 01:09:30,362
그리고 그것은 그라데이션 슈퍼 고속도로의 일종을주었습니다.

1757
01:09:30,363 --> 01:09:32,303
그라디언트가 ResNet에서 역방향으로 흐르도록합니다.

1758
01:09:32,303 --> 01:09:34,924
그리고 이제는 LSTM에서 똑같은 직감입니다.

1759
01:09:34,924 --> 01:09:37,944
이 첨가물과 원소가 현명한 곳에

1760
01:09:37,944 --> 01:09:40,226
셀 상태의 곱셈 적 상호 작용

1761
01:09:40,227 --> 01:09:42,305
비슷한 그라디언트 슈퍼 하이웨이를 줄 수있다.

1762
01:09:42,305 --> 01:09:44,328
그라디언트가 셀 상태를 거쳐 후방으로 흐를 때

1763
01:09:44,328 --> 01:09:45,245
LSTM에서.

1764
01:09:46,343 --> 01:09:48,593
그런데 다른 종류의 멋진 종이가 있습니다.

1765
01:09:48,593 --> 01:09:49,682
소위 고속도로 네트워크,

1766
01:09:49,682 --> 01:09:51,808
이 아이디어 사이에 일종의

1767
01:09:51,808 --> 01:09:53,225
이 LSTM 셀

1768
01:09:54,138 --> 01:09:56,471
및 이러한 잔여 네트워크.

1769
01:09:57,796 --> 01:09:58,697
그래서이 고속도로 네트워크

1770
01:09:58,697 --> 01:10:01,165
실제로 잔여 네트워크 전에왔다.

1771
01:10:01,165 --> 01:10:03,008
그리고 그들은이 생각을 어디서 가지고 있었습니까

1772
01:10:03,008 --> 01:10:04,541
고속도로 네트워크의 모든 계층에서,

1773
01:10:04,541 --> 01:10:05,984
우리는

1774
01:10:05,984 --> 01:10:07,373
후보 활성화의 일종,

1775
01:10:07,373 --> 01:10:08,804
게이팅 기능

1776
01:10:08,804 --> 01:10:10,792
그것은 우리에게 interprelates

1777
01:10:10,792 --> 01:10:13,045
그 레이어에서 우리의 이전 입력 사이에,

1778
01:10:13,045 --> 01:10:14,676
그 후보 활성화

1779
01:10:14,676 --> 01:10:17,017
우리의 회심을 통해 온 것이거나 그렇지 않은 것.

1780
01:10:17,017 --> 01:10:19,455
실제로 많은 건축 학적 유사점이 있습니다.

1781
01:10:19,455 --> 01:10:20,472
이 사이에,

1782
01:10:20,472 --> 01:10:21,821
사람들은 많은 영감을 얻습니다.

1783
01:10:21,821 --> 01:10:24,363
매우 깊은 CNN 훈련에서

1784
01:10:24,363 --> 01:10:25,196
매우 깊은 RNN

1785
01:10:25,196 --> 01:10:27,688
여기에는 많은 크로스 오버가 있습니다.

1786
01:10:27,688 --> 01:10:31,682
매우 간단히 말해서, 당신은 많은 다른
유형의 분산을 보게 될 것입니다.

1787
01:10:31,682 --> 01:10:33,777
거기에 재발 성 신경 네트워크 아키텍처의

1788
01:10:33,777 --> 01:10:34,675
야생에서.

1789
01:10:34,675 --> 01:10:36,760
아마도 LSTM을 제외하고는 가장 일반적 일 것입니다.

1790
01:10:36,760 --> 01:10:40,853
gated recurrent
unit이라고하는이 GRU입니다.

1791
01:10:40,853 --> 01:10:42,665
그리고 당신은 그 업데이트 방정식을
여기에서 볼 수 있습니다,

1792
01:10:42,665 --> 01:10:45,701
그것은 LSTM과 비슷한 맛을 지니고 있습니다.

1793
01:10:45,701 --> 01:10:49,318
이 곱셈 요소 현명한 문을 사용하는 곳

1794
01:10:49,318 --> 01:10:51,417
이들 첨가물 상호 작용과 함께

1795
01:10:51,417 --> 01:10:53,828
이 사라지는 그라디언트 문제를 피하십시오.

1796
01:10:53,828 --> 01:10:55,584
이 멋진 종이가 있습니다.

1797
01:10:55,584 --> 01:10:57,679
LSTM : 검색 기반 oddysey,

1798
01:10:57,679 --> 01:10:59,734
매우 창의적인 제목,

1799
01:10:59,734 --> 01:11:02,853
그들은 LSTM 방정식을 가지고 놀려고했습니다.

1800
01:11:02,853 --> 01:11:04,980
한 점에서 비선형 성을 교환하고,

1801
01:11:04,980 --> 01:11:06,329
우리가 정말로 그 tanh을 필요로하는 것처럼

1802
01:11:06,329 --> 01:11:07,343
출력 게이트를 노출시키기 위해,

1803
01:11:07,343 --> 01:11:09,756
그리고 그들은이 많은 다른 질문들에 대답하려고 애썼다.

1804
01:11:09,756 --> 01:11:11,367
각각의 비선형 성들에 관하여,

1805
01:11:11,367 --> 01:11:14,017
각각의 LSTM 업데이트 방정식의 조각.

1806
01:11:14,017 --> 01:11:15,374
모델을 변경하면 어떻게됩니까?

1807
01:11:15,374 --> 01:11:18,068
LSTM 방정식을 약간 조정하십시오.

1808
01:11:18,068 --> 01:11:19,038
그리고 결론의 종류는

1809
01:11:19,038 --> 01:11:20,260
그들 모두가 똑같이 일한다는 것

1810
01:11:20,260 --> 01:11:22,414
그들 중 일부는 다른 것들보다 조금 더 잘 작동합니다.

1811
01:11:22,414 --> 01:11:24,521
한 가지 문제 또는 다른 문제

1812
01:11:24,521 --> 01:11:25,735
그러나 일반적으로, 어떤 것도,

1813
01:11:25,735 --> 01:11:28,036
그들이 시도한 LSTM의 비틀기

1814
01:11:28,036 --> 01:11:30,911
원래의 LSTM

1815
01:11:30,911 --> 01:11:32,148
모든 문제에.

1816
01:11:32,148 --> 01:11:33,647
그래서 당신에게 조금 더 믿음을줍니다.

1817
01:11:33,647 --> 01:11:36,505
LSTM 업데이트 방정식은 마술처럼 보일 것입니다.

1818
01:11:36,505 --> 01:11:38,889
그러나 그들은 어쨌든 유용합니다.

1819
01:11:38,889 --> 01:11:41,084
당신은 아마 당신의 문제를 고려해야합니다.

1820
01:11:41,084 --> 01:11:43,692
몇 년 전에 Google의 멋진 종이가 있습니다.

1821
01:11:43,692 --> 01:11:44,575
그들이 사용하려고 시도했던 곳에,

1822
01:11:44,575 --> 01:11:46,520
그들은 진화론 적 탐구를 한 곳에서

1823
01:11:46,520 --> 01:11:48,117
많은 사람들을 수색했는데,

1824
01:11:48,117 --> 01:11:52,605
매우 많은 수의 무작위 RNN 아키텍처에서,

1825
01:11:52,605 --> 01:11:55,694
그들은 일종의 무작위로 이러한
업데이트 방정식을 전제로합니다.

1826
01:11:55,694 --> 01:11:57,936
덧셈과 곱셈을 시도해보십시오.

1827
01:11:57,936 --> 01:11:59,332
및 게이트 및 비선형 성

1828
01:11:59,332 --> 01:12:00,860
다른 종류의 조합.

1829
01:12:00,860 --> 01:12:03,288
그들은 거대한 Google 클러스터를
통해이를 폭발 시켰습니다.

1830
01:12:03,288 --> 01:12:04,351
그리고 방금 총알을 시험해 보았습니다.

1831
01:12:04,351 --> 01:12:08,570
다양한 풍미의 다양한 계량 업데이트가 제공됩니다.

1832
01:12:08,570 --> 01:12:09,742
그리고 다시, 그것은 같은 이야기였습니다.

1833
01:12:09,742 --> 01:12:11,299
그들은 아무것도 찾지 못했다.

1834
01:12:11,299 --> 01:12:12,607
그게 훨씬 낫다.

1835
01:12:12,607 --> 01:12:15,446
이러한 기존의 GRU 또는 LSTM 스타일보다

1836
01:12:15,446 --> 01:12:16,978
작동하는 유사 콘텐츠가 있지만

1837
01:12:16,978 --> 01:12:19,518
특정 문제에 대해서는 약간 더
좋거나 나 빠를 수 있습니다.

1838
01:12:19,518 --> 01:12:21,304
그러나 멀리 떨어져 나가는 종류는

1839
01:12:21,304 --> 01:12:24,252
아마도 LSTM 또는 GRU를 사용하여

1840
01:12:24,252 --> 01:12:27,080
그 방정식에 그다지 마술이 아닙니다.

1841
01:12:27,080 --> 01:12:29,292
그래디언트 흐름을 적절하게 관리하는이 아이디어는

1842
01:12:29,292 --> 01:12:30,633
이러한 첨가제 연결을 통해

1843
01:12:30,633 --> 01:12:32,023
및 이들 승산 게이트들

1844
01:12:32,023 --> 01:12:33,356
매우 유용합니다.

1845
01:12:34,888 --> 01:12:37,286
그래, 요약하면 RNN은 매우 멋지다.

1846
01:12:37,286 --> 01:12:40,103
새로운 종류의 문제를 공격 할 수 있습니다.

1847
01:12:40,103 --> 01:12:42,024
때로는 사라질 수 있습니다.

1848
01:12:42,024 --> 01:12:43,431
또는 폭발 그라디언트.

1849
01:12:43,431 --> 01:12:44,740
하지만 우리는 체중 감량으로 해결할 수 있습니다.

1850
01:12:44,740 --> 01:12:47,412
그리고 더 매끈한 아키텍처.

1851
01:12:47,412 --> 01:12:48,899
그리고 멋진 오버랩이 많이 있습니다.

1852
01:12:48,899 --> 01:12:52,043
CNN 아키텍쳐와 RNN 아키텍쳐 사이.

1853
01:12:52,043 --> 01:12:53,856
그래서 다음에, 당신은 중간 고사를 할 것입니다.

1854
01:12:53,856 --> 01:12:57,856
하지만 그 후에, 우리는 미안하다, 질문을 할까?

1855
01:12:59,525 --> 01:13:00,801
중간 고사는이 강연 후입니다.

1856
01:13:00,801 --> 01:13:04,142
그래서이 시점까지는 공정한 게임입니다.

1857
01:13:04,142 --> 00:00:00,000
그리고 여러분, 화요일 중간 고사에 행운을 빈다.

