1
00:00:07,961 --> 00:00:12,153
잘 들리시나요? 좋습니다.
잠시 지연이 있었습니다.

2
00:00:12,153 --> 00:00:24,081
잠시 기술적인 문제가 있었습니다.

3
00:00:25,353 --> 00:00:30,420
오늘은 10강입니다.
Recurrent Neural Networks(RNN)에 대해 배워보겠습니다.

4
00:00:30,420 --> 00:00:33,003
그 전에 공지사항을 전달하겠습니다.

5
00:00:33,003 --> 00:00:37,353
현재 과제1을 채점 중에 있습니다.

6
00:00:37,353 --> 00:00:46,251
최종 성적이 아마도 오늘 발표될 예정입니다.
과제2 deadline 전에는 과제1 성적이 나왔으면 좋겠네요

7
00:00:46,251 --> 00:00:50,361
과제2의 제출기한은 금일 11:59 p.m. 까지입니다.

8
00:00:50,361 --> 00:00:56,633
과제 2를 다 마치신 분 있으신가요?
절반정도 되는군요

9
00:00:56,633 --> 00:01:03,811
제가 예전에 과제2는 상당히 오래걸릴 수 있으니
미리 시작하는 것이 좋다고 말씀드렸습니다.

10
00:01:03,811 --> 00:01:06,561
late dates가 남아있길 바랍니다.

11
00:01:06,561 --> 00:01:10,531
화요일 수업시간에는 중간고사가 있습니다.

12
00:01:10,531 --> 00:01:15,881
아마 강의실이 비좁아서 모두 시험을 치룰 수 없을 것입니다.

13
00:01:15,881 --> 00:01:20,062
따라서 다른 강의실들도 함께 빌려서 진행할 예정입니다.

14
00:01:20,062 --> 00:01:26,099
향후 세부사항은 다시한번 공지하겠습니다.

15
00:01:26,099 --> 00:01:28,179
또 한가지 공지사항이 있습니다.

16
00:01:28,179 --> 00:01:34,950
현재 Train Game 이라는 사이트를 개설하고있습니다.
Train Game은 웹 브라우저 기반이며

17
00:01:34,950 --> 00:01:39,927
여러분 누구나 접속해서 딥러닝 모델을 학습시키고
학습 도중 하이퍼파라미터를 조정하실 수 있습니다.

18
00:01:39,927 --> 00:01:47,646
앞서 강의들에서 배웠던 다양한 하이퍼파라미터들을
손으로 직접 만져볼 수 있는 아주 좋은 툴입니다.

19
00:01:47,646 --> 00:01:53,190
Train Game를 무조건 하실 필요는 없습니다.
하지만, Train Game는 실제로 다양한 데이터를 다뤄보며

20
00:01:53,190 --> 00:01:57,481
데이터 타입에 따라 하이퍼파라미터가 어떤 영향을 미치는지에
직관을 얻을 수 있는 아주 좋은 기회가 될것입니다.

21
00:01:57,481 --> 00:02:05,790
현재 몇 가지 버그를 수정하고 있습니다. 조만간 여러분에게
사용법과 함께 공개하도록 하겠습니다.

22
00:02:05,790 --> 00:02:11,008
다시한번 말씀드리지만, 필수는 아닙니다. 하지만 해보시면
도움이 많이 될 것입니다.

23
00:02:11,008 --> 00:02:17,204
Train Game을 써보신 분들에게는
Extra credit을 지급할 예정입니다.

24
00:02:18,208 --> 00:02:23,458
아무튼 버그만 다 잡으면
다시 한번 공지드리겠습니다.

25
00:02:24,720 --> 00:02:28,139
지난 강의 복습을 해봅시다.
CNN 아키텍쳐들을 배웠습니다.

26
00:02:28,139 --> 00:02:35,006
ImageNet Classification Challenge를
중심으로 연대순 우승자들을 알아봤습니다.

27
00:02:35,006 --> 00:02:41,331
2012년에는 AlexNet이 있었습니다.
9-layer CNN인데, 아주 잘 동작했습니다.

28
00:02:41,331 --> 00:02:48,081
AlexNet이 Computer Vision의 진화를 촉발시켰으며
딥러닝 시대의 서막을 열었습니다.

29
00:02:48,081 --> 00:02:56,699
2014년으로 넘어가봅시다. 이전의 모델들보다 훨씬 더
깊어진 두 모델이 있었습니다. VGG와 GoogLeNet 입니다.

30
00:02:56,699 --> 00:03:02,930
VGGNet은 16/19 레이어를, GoogLeNet은 22 레이어를
가진 모델이었습니다.

31
00:03:02,930 --> 00:03:11,230
2014년 모델들에서 흥미로운 점은 2014년에는
Batch normalization 이 발명되기 이전이라는 점입니다.

32
00:03:11,230 --> 00:03:18,761
Batch Norm이 없던 시절이라서 레이어가 깊을 모델을
학습시키는 일은 상당히 어려웠습습니다.

33
00:03:18,761 --> 00:03:24,869
이 두 모델은 깊은 모델을 수렴시키기 위해서
각종 테크닉(hackery)을 써야했습니다.

34
00:03:24,869 --> 00:03:28,579
가령 VGGNet는 16/19 레이어가 있습니다.

35
00:03:28,579 --> 00:03:34,107
하지만 처음에는 11레이어 모델을 학습시켰습니다.
11 레이어가 모델이 잘 수렴하는 한계였습니다.

36
00:03:34,107 --> 00:03:40,059
그리고 나서 11레이어 중간에 레이어를 무작위로 추가해서
VGG-16, VGG-19 를 학습시킨 것이죠

37
00:03:40,059 --> 00:03:46,539
이렇게, Bath norm이 없던 2014년에는
학습 자체가 상당히 어려운 문제였습니다.

38
00:03:46,539 --> 00:03:52,539
유사한 사례로 GoogLeNet의 경우에는
auxiliary classifiers를 도입했습니다.

39
00:03:52,539 --> 00:03:56,539
auxiliary classifiers는 classification 성능을
올리기 위해서 도입된 것이 아닙니다.

40
00:03:56,539 --> 00:04:03,430
단지 네트워크의 초기 레이어에 gradient를
직접 흘려보내기 위한  수단이었습니다.

41
00:04:03,430 --> 00:04:10,411
GoogLeNet은 Batch Norm 이전에 이런 테크닉을 사용했습니다.

42
00:04:10,411 --> 00:04:17,321
하지만 Batch Norm이 있다면 굳이 이런 식의
테크닉은 더이상 필요하지 않습니다.

43
00:04:17,321 --> 00:04:24,350
2015년에는 ResNet이라는 아주 멋드러진 모델이 있었습니다.

44
00:04:24,350 --> 00:04:28,310
ResNet은 shortcut connection과
residual block이 도입된 모델입니다.

45
00:04:28,310 --> 00:04:39,110
ResNet에서는 레이어의 출력은
입력 +  residual block의 출력 입니다.

46
00:04:39,110 --> 00:04:43,308
아주 흥미로운 구조입니다. 이 구조는
두 가지 아주 좋은 속성을 지니고 있습니다.

47
00:04:43,308 --> 00:04:49,531
하나는, 우선 residual block의 가중치가 0이면
이 block은 indentity mapping 을 합니다.

48
00:04:49,531 --> 00:04:55,681
이 속성은, 모델이 '필요없는 레이어" 를 사용하지 않도록
학습하는데 아주 유용합니다.

49
00:04:55,681 --> 00:05:02,171
그리고 ResNet의 관점에서 L2 Regularization을
해석해 볼 수도 있습니다.

50
00:05:02,171 --> 00:05:08,321
레이어에 L2 Regularization을 추가시키면
L2는 모든 파라미터가 0이 되도록 노력할 것입니다.

51
00:05:08,321 --> 00:05:12,739
사실 기본적인 CNN 아키텍쳐의 관점에서보면
모든 파라미터가 0이면 이상합니다.

52
00:05:12,739 --> 00:05:20,510
하지만 ResNet의 관점에서는 파라미터를 0으로 만드려는 속성은
모델이 불필요한 레이어를 사용하지 않도록 해줄 수 있습니다.

53
00:05:20,510 --> 00:05:26,310
파라미터들을 계속 0으로 보내면
residual block이 identity가 되기 떄문이죠.

54
00:05:26,310 --> 00:05:31,371
ResNet의 또 한가지 특성은 backward pass 에서의
gradient flow와 관련있습니다.

55
00:05:31,371 --> 00:05:34,361
"Addition gates (+)" 가 backward pass에
어떤 영향을 미칠지 생각해보면

56
00:05:34,361 --> 00:05:39,881
Upstream gradient 가 "Addition gate" 를 만나면
두 갈래로 나눠지게 됩니다.

57
00:05:39,881 --> 00:05:46,361
Upstream gradient가 오면 convolution block
으로도 흘러들어가지만

58
00:05:46,361 --> 00:05:50,811
residual connection 덕분에 현재 레이어를 생략하고
직접 이전 레이어로 흘러갈 수도 있습니다.

59
00:05:50,811 --> 00:05:59,150
Residual blocks을 수백 레이어 쌓아올린 네트워크가
있다고 해봅시다.

60
00:05:59,150 --> 00:06:05,561
Residual connection은 gradient를 위한 일종의
고속도로 역할을 합니다. gradient를 원활하게 전달하기 위해서죠

61
00:06:05,561 --> 00:06:09,630
이런 특성으로 학습을 더 쉽고 빠르게 할 수 있습니다.

62
00:06:09,630 --> 00:06:15,738
그리고 이런 특성은 모델이 엄청 깊더라도 잘 수렴할 수 있도록 도와줍니다.

63
00:06:15,738 --> 00:06:21,550
모델의 gradient flow를 잘 다루는 것은
Machine learning 전 분야에 걸쳐서도 아주 중요합니다.

64
00:06:21,550 --> 00:06:28,564
RNN에서도 아주 중요하죠. 따라서 gradient flow는
오늘 강의의 후반부에 다시한번 다루겠습니다.

65
00:06:31,148 --> 00:06:38,068
지난 시간에 DensNet, FractalNet 과 같은
이색적인 CNN 아키텍쳐도 살펴보았습니다.

66
00:06:38,068 --> 00:06:43,070
이 모델들은 gradient flow의 관점에서
아주 잘 해석해 볼 수 있습니다.

67
00:06:43,070 --> 00:06:48,619
DenseNet이나 FractalNet 같은 모델들은 모델 내부에
additional shortcut (identity)를 추가합니다.

68
00:06:48,619 --> 00:07:00,571
이 모델들은 Loss와 각 Layer를 직접 연결하기 때문에
 backward pass가 아주 수월합니다.

69
00:07:00,571 --> 00:07:09,760
CNN 아키텍쳐에서 Gradient Flow를 잘 다루려는 시도는
최근 몇 년간 아주 활발하게 이루어졌습니다.

70
00:07:09,760 --> 00:07:15,221
앞으로도 신기한 아카텍쳐들이 많이 나올 것 같습니다.

71
00:07:16,257 --> 00:07:24,331
지난 강의에서 보셨던 그래프입니다.
모델 별 연산량, 크기 등을 비교해 보았습니다.

72
00:07:24,331 --> 00:07:27,971
이 그래프를 유심히 살펴보면 아주 재미있는 속성이 있습니다.

73
00:07:27,971 --> 00:07:32,801
AlexNet와 VGGNet는 파라미터가 엄청 많습니다.

74
00:07:32,801 --> 00:07:37,119
사실 파라미터가 많은 이유는 전적으로
FC-layer 때문입니다.

75
00:07:37,119 --> 00:07:39,959
가령 AlexNet의 경우에는 대략 64M 개의 파라미터가 있습니다.

76
00:07:39,959 --> 00:07:47,771
AlexNet의 FC-Layer를 살펴봅시다. FC-Layers의 입력은
마지막 Conv output  6 x 6 x 256 입니다.

77
00:07:47,771 --> 00:07:51,190
이 값(256 x 6 x 6) 들이 4096 개의 FC-layer
노드와 연결됩니다.

78
00:07:51,190 --> 00:07:56,851
이 부분의 가중치 매트릭스는 엄청나게 큽니다.

79
00:07:56,851 --> 00:08:01,921
매트릭스의 크기는
6x6x256x4096 이 되겠죠

80
00:08:01,921 --> 00:08:06,370
이 레이어 하나에만 38m개의 파라미터가 있습니다.

81
00:08:06,370 --> 00:08:11,859
AlexNet의 전체 파라미터 중 절반 이상이
FC-Layer에 집중되어 있습니다.

82
00:08:11,859 --> 00:08:24,241
AlexNet의 FC-Layer 의 파라미터 갯수를 다 더해보면
FC-Layer에서만 62M 가량의 파라미터가 있다는 것을 알 수 있습니다.

83
00:08:24,241 --> 00:08:31,110
GoogLeNet 이나 ResNet 같은 아키텍쳐들은
FC-Layer를 상당부분 걷어냈습니다.

84
00:08:31,110 --> 00:08:33,698
FC-Layer를 Global Average Pooling (GAP)
으로 대체시킵니다.

85
00:08:33,698 --> 00:08:40,935
GAP를 사용하게 되면서 파라미터 갯수를 상당히 감소시켰습니다.

86
00:08:44,463 --> 00:08:49,604
지금까지는 CNN 아키텍쳐를 복습해보았습니다.

87
00:08:49,604 --> 00:08:56,321
이제부터는 Recurrent Neural Network에 대해서
배워보도록 하겠습니다.

88
00:08:56,321 --> 00:09:03,222
지금까지 배운 아키텍쳐들은(Vanilla Neural Network)
다음과 같은 모양이하고 할 수 있습니다.

89
00:09:03,222 --> 00:09:08,593
네트워크는 이미지 또는 벡터를 입력으로 받습니다.

90
00:09:08,593 --> 00:09:13,850
입력 하나가 Hidden layer를 거쳐서
하나의 출력을 내보냅니다.

91
00:09:13,850 --> 00:09:18,876
Classification 문제라면 카테고리가 되겠군요

92
00:09:20,071 --> 00:09:25,942
하지만 Machine Learning의 관점에서 생각해보면 모델이
다양한 입력을 처리할 수 있도록 유연해질 필요가 있습니다.

93
00:09:25,942 --> 00:09:35,313
그런 관점에서 RNN은 네트워크가 다양한 입/출력을
다룰 수 있는 여지를 제공해줍니다.

94
00:09:35,313 --> 00:09:41,009
RNN 구조를 이용할 때, 가령 "one to many" 모델은

95
00:09:41,009 --> 00:09:48,721
입력은 이미지와 같은 "단일 입력" 이지만 출력은
caption 과 같은 "가변 출력" 입니다.

96
00:09:48,721 --> 00:09:54,081
caption에 따라서 단어의 수가 천차 만별이겠죠
따라서 출력은 가변 출력입니다.

97
00:09:54,081 --> 00:09:56,491
"many to one" 모델도 있습니다.

98
00:09:56,491 --> 00:10:01,001
이제는 입력이 "가변 입력" 입니다.
가령 문장이 될 수 있겠군요

99
00:10:01,001 --> 00:10:06,161
그리고 이 문장의 "감정"을 분류하는 것입니다.
부정적/긍적적인 문장인지를 구별하는 것이죠

100
00:10:06,161 --> 00:10:12,512
그리고 Computer Vision Task의 경우에는, 가령 입력이
비디오라고 해봅시다. 비디오에 따라 전체 프레임 수가 다양합니다.

101
00:10:12,512 --> 00:10:16,401
전체 비디오를 읽으려면 가변 길이의 입력을 받아야 합니다.

102
00:10:16,401 --> 00:10:22,721
비디오를 입력으로 받아서, 비디오에 나타나는
activity 또는 action을 분류하는 문제를 생각해봅시다.

103
00:10:22,721 --> 00:10:29,931
이 경우에는 입/출력 모두 가변이어야 합니다.

104
00:10:29,931 --> 00:10:37,302
또는 Machine translation의 예를 볼 수도 있습니다.
입력은 English sentence 입니다. 가변 입력이죠

105
00:10:37,302 --> 00:10:41,633
그리고 출력은 번역 결과인 "Fench sentence" 입니다.
출력 또한 가변 출력인 것입니다.

106
00:10:41,633 --> 00:10:46,801
그리고 English Sentence의 길이는
변역된 French Sentence의 길이와 다를 것입니다.

107
00:10:46,801 --> 00:10:53,931
이 경우에는 가변 입/출력이 가능한 모델이 필요합니다.

108
00:10:53,931 --> 00:11:04,771
마지막으로, 입력은 비디오와 같은 가변 입력이고 각 프레임마다
출력 값이 나와야 하는 상황을 생각해 볼 수 있습니다.

109
00:11:04,771 --> 00:11:11,891
가령 비디오가 입력이고 매 프레임마다
classification을 해야 하는 상황일 수 있습니다.

110
00:11:11,891 --> 00:11:17,401
Recurrent Neural Networks는 가변 길이의 데이터를 다루기
위해 필요한 일반적인 방법(paradigm) 입니다.

111
00:11:17,401 --> 00:11:23,469
RNN은 앞서 말씀드린 다양한 상황들을
모델이 잘 처리할 수 있도록 줍니다.

112
00:11:24,349 --> 00:11:33,752
그리고 고정 길이의 입/출력이 필요한 상황에서도
RNN은 상당히 중요합니다. 여전히 유용하죠.

113
00:11:33,752 --> 00:11:38,793
입/출력은 고정이지만 sequential processing가
요구되는 경우입니다.

114
00:11:38,793 --> 00:11:46,227
가령 여기 이미지가 있습니다. 고정 입력이죠
그리고 이미지의 숫자가 몇 인지 분류하는 문제입니다.

115
00:11:46,227 --> 00:11:50,393
입력 이미지의 정답을 feed forward pass
한 번만 가지고 결정하는 것이 아니라

116
00:11:50,393 --> 00:11:55,553
이 네트워크는 이미지의 여러 부분은 조금씩 살펴봅니다.

117
00:11:55,553 --> 00:12:01,742
이미지를 살펴 본 후에 숫자가 몇 인지를 최종적으로 판단합니다.

118
00:12:01,742 --> 00:12:17,473
이 예제 처럼 입/출력이 고정된 길이라고 해도
"가변 과정(processing)" 인 경우에  RNN은 상당이 유용합니다.

119
00:12:17,473 --> 00:12:23,923
제가 상당히 좋아하는 논문입니다. 동일한 방법으로
"이미지를 생성" 을 제안하기도 합니다.

120
00:12:23,923 --> 00:12:29,723
자 이제는 모델은 Train time에서 본 이미지들을
바탕으로 새로운 이미지를 생성해 냅니다.

121
00:12:29,723 --> 00:12:36,254
이를 위해 RNN을 이용할 수 있습니다. 순차적으로 전체 출력의
일부분씩 생성해 내는 것입니다.

122
00:12:36,254 --> 00:12:46,380
이 경우에도 전체 출력은 고정된 길이지만, RNN을
이용해서 일부분씩 순차적으로 처리할 수 있습니다.

123
00:12:46,380 --> 00:12:51,662
RNN을 이런 경우에도 사용될 수 있는 것이죠.

124
00:12:51,662 --> 00:12:58,785
RNN을 활용할 수 있는 다양한 예시들을 살펴보았습니다.
그렇다면 도대체 어떻게 동작하는 것일까요?

125
00:12:58,785 --> 00:13:04,163
일반적으로 RNN은 작은
"Recurrent Core Cell" 을 가지고 있습니다.

126
00:13:04,163 --> 00:13:11,382
입력 x가 RNN으로 들어갑니다.
RNN에는 내부에 "hidden state" 를 가지고있습니다.

127
00:13:11,382 --> 00:13:17,641
"hidden state"는 RNN이 새로운 입력을
불러들일 때마다 매번 업데이트됩니다.

128
00:13:17,641 --> 00:13:23,980
"hidden state"는 모델에 feed back되고
이후에 또 다시 새로운 입력 x가 들어옵니다.

129
00:13:23,980 --> 00:13:28,822
RNN이 매 단계마다 값을 출력하는 경우를 생각해 봅시다.

130
00:13:28,822 --> 00:13:31,043
그러면 이런 식으로 진행이 됩니다.
1. RNN이 입력을 받습니다.

131
00:13:31,043 --> 00:13:34,469
2. "hidden state"를 업데이트합니다.
3. 출력 값을 내보냅니다.

132
00:13:35,814 --> 00:13:40,961
그렇다면 RNN 구조를 수식적으로 표현하면 어떨까요?

133
00:13:40,961 --> 00:13:46,443
오른쪽 그림의 초록색 RNN block은 "재귀적인
관계"를 연산할 수 있도록 설계됩니다. "함수 f" 로 말이죠

134
00:13:46,443 --> 00:13:49,094
파라미터 W를 가진 함수 f가 있습니다.

135
00:13:49,094 --> 00:13:55,374
함수 f는 "이전 상태의 hidden state인 h_t-1" 과
"현재 상태의 입력인 x_t" 를 입력으로 받습니다.

136
00:13:55,374 --> 00:14:01,420
그리고 h_t를 출력합니다. h_t는 "다음 상태의 hidden state"
입니다.(updated hidden state)

137
00:14:01,420 --> 00:14:11,552
그리고 다음 단계(state)에서는
h_t와 x_t+1가 입력이 됩니다.

138
00:14:11,552 --> 00:14:21,797
RNN에서 출력 값(y)을 가지려면 h_t 를 입력으로 하는
FC-Layer을 추가해야 합니다.

139
00:14:21,797 --> 00:14:27,327
FC-Layer는 매번 업데이트되는 Hidden state(h_t)를
기반으로 출력 값을 결정합니다.

140
00:14:27,327 --> 00:14:35,662
중요한 사실은 함수 f와 파라미터 W는
매 스텝 동일하다는 것입니다.

141
00:14:36,921 --> 00:14:43,434
자, RNN을 가장 간단하게 수식적으로 나타내려면
단순한(vanilla)  RNN부터 시작하는게 좋겠군요

142
00:14:43,434 --> 00:14:46,866
지금 보이는 수식은 앞서 있던 수식과 동일합니다.

143
00:14:46,866 --> 00:14:52,483
"이전 hidden state"와 "현재 입력"을 받아서
"다음 hidden state"를 출력합니다.

144
00:14:52,483 --> 00:15:00,124
이를 수식적으로 가장 간단하게 표현해보면,
가중치 행렬 W_xh 와 입력 x_t의 곱으로 나타낼 수 있습니다.

145
00:15:00,124 --> 00:15:05,615
또한 가중치 행렬 W_hh도 있습니다.
"이전 hidden state"와 곱해지는 값입니다.

146
00:15:05,615 --> 00:15:09,327
이렇게 두 입력(h,x) 에 대한 행렬 곱 연산이 있고
두 결과 값을 더해줍니다.

147
00:15:09,327 --> 00:15:13,514
그리고 System에 non-linearity를 구현하기 위해
tanh를 적용합니다.

148
00:15:13,514 --> 00:15:17,312
여러분 중에 왜 다른 non-linearity가 아닌
tanh를 쓰는지 궁금하신 분들도 계실 것입니다.

149
00:15:17,312 --> 00:15:20,594
지난 강의 동안에는 tanh가 안좋다고만 배웠는데 말이죠

150
00:15:20,594 --> 00:15:26,507
tanh와 관련된 문제는 lstm 배울 때
다시한번 다뤄드리도록 하겠습니다.

151
00:15:27,346 --> 00:15:33,394
자 이제 우리는 매 스텝마다 출력 y를 얻고 싶습니다.

152
00:15:33,394 --> 00:15:40,375
이를 위해서는 "hidden state"인 h_t 를
새로운 가중치 행렬 W_hy 와 곱해줍니다.

153
00:15:40,375 --> 00:15:44,826
가령 매 스텝에 출력 y는 class score가
될 수 있을 것입니다.

154
00:15:44,826 --> 00:15:51,487
Recurrent Neural Network는 크게 두 가지
방법으로 해석해 볼 수 있습니다.

155
00:15:51,487 --> 00:15:57,095
하나는 RNN이 hidden state를 가지며
이를 "재귀적으로" feed back 한다는 점입니다.

156
00:15:57,095 --> 00:16:05,914
RNN을 사실 이런 식으로 표현하면 많이 헷갈립니다.
Multiple time steps을 unrolling 해서 보면 좋습니다.

157
00:16:05,914 --> 00:16:11,786
Unrolling을 하면 hidden states, 입/출력, 가중치 행렬들
간의 관계를 조금 더 명확히 이해할 수 있습니다.

158
00:16:11,786 --> 00:16:15,494
첫 step에서는 initial hidden state인
 h_0 가 있습니다.

159
00:16:15,494 --> 00:16:22,415
대부분의 경우 h_0 는 0으로 초기화시킵니다.
그리고 입력 x_t가 있습니다.

160
00:16:22,415 --> 00:16:28,324
h_0와 x_1 이 함수 f_w의 입력으로 들어갑니다.

161
00:16:28,324 --> 00:16:36,154
f_w(h_0, x_1) 의 출력은 h_1 입니다. 이 과정이 반복됩니다.
이번에는 x_2 이 다음 입력으로 들어옵니다.

162
00:16:36,154 --> 00:16:42,847
f_w(h_1, x_2) 의 출력은 h_2가 됩니다.

163
00:16:42,847 --> 00:16:50,866
이 과정을 반복하면서 가변 입력 x_t 를 받습니다.

164
00:16:50,866 --> 00:16:58,036
이제는 조금 더 구체적인 이해를 위해서
행렬 W를 그려넣어 봅시다.

165
00:16:58,036 --> 00:17:03,415
여기에서 주목할 점은 "동일한 가중치 행렬 W"가
매번 사용된다는 점입니다.

166
00:17:03,415 --> 00:17:11,006
매번 h와 x는 달라지지만 W는 매번 동일합니다.

167
00:17:11,007 --> 00:17:20,786
앞서 Computational graph에서 동일한 node를 여러번
사용할 때 back prop gradient flow이 어떨지 배웠습니다.

168
00:17:20,786 --> 00:17:28,218
backward pass 시 dLoss/dW 를 계산하려면
행렬 W의 gradient를 전부 더해줬습니다.

169
00:17:28,218 --> 00:17:32,526
따라서 이 RNN 모델의 backprop을 위한
행렬 W의 그레디언트를 구하려면

170
00:17:32,526 --> 00:17:42,503
각 스텝에서의 W에 대한 그레디언트를 전부 계산한 뒤에
이 값들을 모두 더해주면 됩니다.

171
00:17:43,615 --> 00:17:47,727
Computational graph에 y_t도 넣어볼 수 있습니다.

172
00:17:47,727 --> 00:17:54,858
RNN의 출력 값 h_t가 또 다른 네트워크의 입력으로 들어가서
 y_t를 만들어 냅니다.

173
00:17:54,858 --> 00:17:59,087
가량 y_t는 매 스텝의 class score가 될 수 있겠죠

174
00:17:59,087 --> 00:18:00,738
RNN의 Loss도 한번 살펴봅시다.

175
00:18:00,738 --> 00:18:14,068
각 시퀀스마다 Ground truth label이 있다고 해봅시다. 그러면
각 스텝마다 개별적으로 y_t에 대한 Loss를 계산할 수 있습니다.

176
00:18:14,068 --> 00:18:22,497
여기에서 Loss는 가령 softmax loss가 될 수 있겠죠

177
00:18:22,497 --> 00:18:27,887
RNN의 최종 Loss는 각 개별 loss들의 합입니다.

178
00:18:27,887 --> 00:18:34,196
각 단계에서 Loss가 발생하면 전부 더하여
최종 네트워크 Loss를 계산합니다.

179
00:18:34,196 --> 00:18:42,098
이 네트워크의 Backprop을 생각해보면
모델을 학습시키려면 dLoss/dW를 구해야 합니다.

180
00:18:42,098 --> 00:18:46,178
Loss flowing은 각 스텝에서 이루어집니다.

181
00:18:46,178 --> 00:18:49,840
이 경우에는 각 스텝마다 가중치 W에 대한
local gradient를 계산할 수 있습니다.

182
00:18:49,840 --> 00:18:54,343
이렇게 개별로 계산된 local gradient를
최종 gradient에 더합니다.

183
00:18:55,597 --> 00:19:01,188
그렇다면 "many to one" 이라면 어떨까요?
가령 감정분석(sentiment analysis) 처럼 말입니다.

184
00:19:01,188 --> 00:19:05,799
이 경우에는 네트워크의 최종 hidden state에서만
결과 값이 나올 것입니다.

185
00:19:05,799 --> 00:19:11,868
최종 hidden state가 전체 시퀀스의 내용에 대한
일종의 요약으로 볼 수 있기 떄문입니다.

186
00:19:11,868 --> 00:19:14,788
그렇다면 "one to many" 의 경우에는 어떨까요?

187
00:19:14,788 --> 00:19:19,319
"고정 입력" 을 받지만  "가변 출력"인 네트워크입니다.

188
00:19:19,319 --> 00:19:26,050
이 경우에는 대게 고정 입력은 모델의 initial hidden state를
초기화시키는 용도로 사용합니다.

189
00:19:26,050 --> 00:19:30,079
그리고 RNN은 모든 스텝에서
출력 값을 가집니다.

190
00:19:30,079 --> 00:19:36,915
이렇게 가변 출력을 가지는 경우에도 이렇게 그래프를
Unroll 해 볼 수 있습니다.

191
00:19:38,490 --> 00:19:44,308
"sequence to sequent" 모델에 대해서 알아보겠습니다.
가령 machine tranlation 에 사용할 수 있는 모델입니다.

192
00:19:44,308 --> 00:19:47,648
"가변 입력" 과 "가변 출력" 을 가지는 모델입니다.

193
00:19:47,648 --> 00:19:52,398
우리는 이 모델을 "many to one" 모델과
"one to many" 모델의 결합으로 볼 수 있습니다.

194
00:19:52,398 --> 00:19:56,900
두 개의 스테이지로 구성되는 것이죠.
encoder와 decoder 구조입니다.

195
00:19:56,900 --> 00:20:02,159
encoder는 가변 입력을 받습니다.
가령 English sentence가 될 수 있겠죠

196
00:20:02,159 --> 00:20:08,110
그리고 encoder의 final hidden state 를 통해
전체 sentence를 요약합니다.

197
00:20:08,110 --> 00:20:15,769
Encoder 에서는 "many to one" 을 수행합니다.
"가변 입력"을 하나의 벡터로 요약합니다.

198
00:20:15,769 --> 00:20:23,111
반면 Decoder는 "one to many"를 수행합니다.
입력은 앞서 요약한 "하나의 벡터" 입니다.

199
00:20:23,111 --> 00:20:28,969
그리고 decoder는 "가변 출력"을 내뱉습니다.
가령 다른 언어로 변역된 문장이 될 수 있겠죠

200
00:20:28,969 --> 00:20:34,609
"가변 출력" 은 매 스텝 적절한 단어를 내뱉습니다.

201
00:20:34,609 --> 00:20:38,199
그림처럼 전체 Computational graph를 풀어서
전체 학습과정을 해석해보면

202
00:20:38,199 --> 00:20:44,692
Output sentence의 각 losses를 합해서
Backprob 을 진행합니다.

203
00:20:44,692 --> 00:20:50,940
조금 더 구체적인 예를 살펴보겠습니다. 대게는 RNN 은
Language modeling에서 자주 사용합니다.

204
00:20:50,940 --> 00:21:00,908
Language modeling 문제에서 하고싶은 것은 바로
"어떻게 자연어(natural lang)을 만들어낼지 입니다.

205
00:21:00,908 --> 00:21:06,601
가령 문자(characters) 를 내뱉는 모델이라면 매 스텝
어떻게 문자를 생성해 낼지를 풀어야 합니다.

206
00:21:06,601 --> 00:21:10,769
단어(word) 를 내뱉는 모델이라면 매 스텝 어떻게 단어를
생성해 낼 지를 풀어야겠죠

207
00:21:10,769 --> 00:21:14,740
우선은 간단한 예제를 위해서 character level
language model 을 살펴보겠습니다.

208
00:21:14,740 --> 00:21:22,780
네트워크는 문자열 시퀀스를 읽어드리고, 현재 문맥에서
다음 문자를 예측해야 합니다.

209
00:21:22,780 --> 00:21:33,884
이번 예제에서는 간단하게 단어가 [h,e,l,o] 만 있습니다.
그리고 학습시킬 문장은 "h, e, l, l, o" 입니다.

210
00:21:33,884 --> 00:21:49,689
Train time에서는 training sequence("hello") 의
각 단어들을 입력으로 넣어줘야 합니다.

211
00:21:49,689 --> 00:21:53,980
"hello" 가 RNN의 x_t입니다.

212
00:21:53,980 --> 00:22:01,039
우선 입력은 한 글자(letter)씩 입니다. 네트워크도
적절한 글자들을 출력해야 합니다.

213
00:22:01,039 --> 00:22:07,460
일반적으로 입력을 넣어주는 방법이 있습니다.
우선 vocabulary는 총 네 가지입니다.[h, e, l, o]

214
00:22:07,460 --> 00:22:12,589
각 글자는 하나의 벡터로 표현할 수 있습니다.
이 벡터는 1이 하나 있고 나머지는 0인 벡터입니다.

215
00:22:12,589 --> 00:22:16,628
벡터에서 해당 글자 위치만 1로 표시합니다.

216
00:22:16,628 --> 00:22:22,324
이 단순한 예제에서는 글자가  h, e, l, o 뿐입니다.
따라서 4-d 벡터로 표현할 수 있습니다.

217
00:22:22,324 --> 00:22:28,684
가령 h를 벡터로 표현하는 경우([1,0,0,0])
h에 해당하는 자리만 1이고 나머지는 0입니다.

218
00:22:28,684 --> 00:22:33,139
이런 식으로 다은 문자들도 벡터로 표현할 수 있습니다.

219
00:22:34,914 --> 00:22:41,874
Forward pass에서 네트워크가 어떻게 동작하는지 살펴봅시다.
우선 첫 스텝에서는, 입력 문자 'h' 가 들어옵니다.

220
00:22:41,874 --> 00:22:48,594
첫 번째 RNN cell로는 'h' 가 들어갑니다.
그러면 네트워크는 y_t를 출력합니다.

221
00:22:48,594 --> 00:22:56,024
y_t는 어떤 문자가 'h' 다음에 나올 것
같은지를 예측한 값입니다.

222
00:22:56,024 --> 00:23:01,405
이 예제에서는 'h'  다음에는 'e' 를 예측해야 정답입니다.

223
00:23:01,405 --> 00:23:06,861
하지만 현재 이 모델은 다음에 나올 글자가
'o' 라고 예측하고 있습니다.

224
00:23:07,850 --> 00:23:13,889
잘못 예측한 경우입니다. softmax loss가 이 예측이
얼마나 형편없는지 알려줄 것입니다.

225
00:23:13,889 --> 00:23:19,741
다음 스텝에서는 두 번째 단어 'e' 가 입력으로 들어갑니다.
이런 과정이 반복됩니다.

226
00:23:19,741 --> 00:23:27,271
우선 'e' 벡터입니다. 그리고 "이전 hidden state"와 함께
"새로운  hidden state" 를 만들어내죠

227
00:23:27,271 --> 00:23:31,912
이제는 두번째 hidden state를 이용해서
적절한 값을 예측해야 합니다.

228
00:23:31,912 --> 00:23:36,810
'hello' 를 예측해야 하므로 'e' 가
입력이라면 'l' 을 예측해야합니다.

229
00:23:36,810 --> 00:23:41,954
이 경우 'l' 의 예측 치가 상당히 낮기 때문에
Loss가 높습니다.

230
00:23:44,244 --> 00:23:50,343
이 과정을 반복합니다.
모델을 다양한 문장으로 학습시킨다면

231
00:23:50,343 --> 00:23:58,596
결국 모델은 이전 문장의 문맥을 참고해서 다음 문자가
무엇일지를 학습해야 할 것입니다.

232
00:23:59,893 --> 00:24:01,655
그렇다면 이 모델의 Test time은 어떨까요?

233
00:24:01,655 --> 00:24:07,594
이렇게 학습시킨 모델을 활용할 수 있는 방법들 중 하나는
Model로부터 Sampling 하는 것입니다.

234
00:24:07,594 --> 00:24:15,103
다시말해, Train time에 모델이 봤을 법한 문장을
모델 스스로 생성해 내는 것입니다.

235
00:24:15,103 --> 00:24:22,716
우선 모델에게 문장의 첫 글자만 줍니다.
이 경우에는 'h' 가 되겠군요

236
00:24:22,716 --> 00:24:27,295
RNN 의 첫 스텝의 입력은 'h'가 될 것입니다.

237
00:24:27,295 --> 00:24:32,916
'h'가 주어지면 모든 문자(h,e,l,o) 에 대한
스코어를 얻을 수 있습니다(output layer)

238
00:24:32,916 --> 00:24:37,501
Test time에서는 이 스코어를
"sampling"(다음 글자 선택)에 이용 합니다.

239
00:24:37,501 --> 00:24:41,421
스코어를 확률분포로 표현하기 위해서
sofrmax 함수를 사용할 수 있습니다.

240
00:24:41,421 --> 00:24:47,362
문장의 두 번째 글자를 선택하기 위해서
이 확률분포를 이용합니다.

241
00:24:47,362 --> 00:24:54,771
이 경우에 'e'가 뽑힐 확률(13%)이 엄청 낮았음에도
아주 운좋게 'e'가 샘플링되었습니다.

242
00:24:54,771 --> 00:25:02,492
이제는 확률 분포에서 운좋게 뽑힌 'e' 를
다음 스텝의 네트워크 입력으로 넣어 줄 것입니다.

243
00:25:02,492 --> 00:25:15,008
'e' 를 다시 벡터[0,1,0,0]로 만들어주고 다음 입력으로
넣어주면 네트워크는 두번째 출력을 만들어 냅니다.

244
00:25:15,008 --> 00:25:20,722
이 학습된 모델만 가지고 새로운 문장을
만들어내기 위해 이 과정을 반복합니다.

245
00:25:20,722 --> 00:25:27,712
전체 문장을 만들어내기 위해 타임 스텝마다
확률 분포에서 문자를 하나씩 뽑아냅니다.

246
00:25:27,712 --> 00:25:28,545
질문 있나요?

247
00:25:34,792 --> 00:25:41,315
질문은 "가장 높은 스코어를 선택하면 그만인데
왜 굳이 확률분포에서 샘플링하는지" 입니다.

248
00:25:41,315 --> 00:25:46,555
이 예제의 경우에는 가장 스코어가 높은 값만 사용하면
올바른 결과를 낼 수 없었습니다. (e 스코어가 낮았음)

249
00:25:47,451 --> 00:25:51,512
따라서 확률분포에서 샘플링을 했기 때문에
hello를 잘 만들 수 있었습니다.

250
00:25:51,512 --> 00:25:54,384
실제로는 두 경우 모두 사용할 수 있습니다.
(확률 분포에서 샘플링 or 그냥 가장 높은 값)

251
00:25:54,384 --> 00:25:59,482
어떤 경우에는 argmax probability만 사용할 수 있습니다.
(가장 높은 값을 취함) 더 안정적인 방법일 수 있습니다.

252
00:25:59,482 --> 00:26:04,264
하지만 확률 분포에서 샘플링하는 방법을 사용하면
일반적으로는 모델에서의 다양성을 얻을 수 있습니다.

253
00:26:04,264 --> 00:26:11,421
만약 항상 같은 접두사(예제에서 h)라던지, 이미지라면
같은 사진이라던지를 모델 입력으로 주면

254
00:26:11,421 --> 00:26:20,032
argmax을 하지 않고 샘플링을 하게되면 그럴듯한
다양한 문장을 출력할 수 있게 됩니다.

255
00:26:20,032 --> 00:26:23,824
첫 스텝에서 어떤 값이 샘플링되는지에
따라서 출력이 다양해 질 수 있는 것입니다.

256
00:26:23,824 --> 00:26:29,213
샘플링 하는 방법은 다양한 출력을 얻을 수 있다는
관점에서 좋은 방법이 될 수 있습니다.

257
00:26:29,213 --> 00:26:30,630
다른 질문 있나요?

258
00:26:35,143 --> 00:26:40,435
[학생이 질문]

259
00:26:46,162 --> 00:26:51,373
질문은 "Test time에 sofrmax vector를
one hot vector 대신에 넣어줄 수 있는지" 입니다.

260
00:26:51,373 --> 00:26:56,782
두 가지 문제가 발생할 수 있습니다. 첫 째는 그렇게 되면
입력이 Train time에서 본 입력과 달라집니다.

261
00:26:56,782 --> 00:27:05,413
모델에게 Train time에서 보지 못한 입력값을 주게되면
대게는 모델이 아무것도 하지 못합니다.

262
00:27:05,413 --> 00:27:09,112
두 번째 문제는 실제로는
vocabularies가 아주 크다는 것입니다.

263
00:27:09,112 --> 00:27:13,202
예제에서는 voca가 4개 뿐이 없어서
크게 문제될 것이 없겠지만

264
00:27:13,202 --> 00:27:18,773
한 스텝마다 단어(word)를 생성하는 모델이라고 보면
voca 안에는 이세상의 모든 영어단어가 들어가야합니다.

265
00:27:18,773 --> 00:27:21,162
voca는 수만개 이상의 요소를 가지게 되겠죠

266
00:27:21,162 --> 00:27:30,533
실제론 one hot vector를 sparse vector operation
으로 처리합니다. (dense vector가 아닌)

267
00:27:30,533 --> 00:27:36,827
10,000dim softmax vector를 연산해야 한다면
연산량이 어마어마 할 것입니다.

268
00:27:36,827 --> 00:27:40,302
Test time에서도 one hot을 사용하는 이유입니다.

269
00:27:42,121 --> 00:27:47,104
이런 모델의 경우 시퀀스 스텝마다 출력값이 존재합니다.

270
00:27:47,104 --> 00:27:51,543
이 출력값들의 Loss를 계산해 final loss를 얻는데
이를 "backpropagation through time" 라고 합니다.

271
00:27:51,543 --> 00:27:57,053
이 경우, forward pass의 경우에는 전체 시퀀스가
끝날 때 까지 출력값이 생성됩니다.

272
00:27:57,053 --> 00:28:00,762
반대로 backward pass에서도 전체 시퀀스를
가지고 Loss를 계산해야 합니다.

273
00:28:00,762 --> 00:28:06,162
하지만 이 경우 시퀀스가 아주 긴 경우에는
문제가 될 소지가 있습니다.

274
00:28:06,162 --> 00:28:15,453
가령 Wikipedia 전체 문서로 모델을 학습시킨다고 해봅시다.

275
00:28:15,453 --> 00:28:23,328
이 경우 학습이 정말 느릴 것입니다. gradient를 계산하려면
Wikipedia 전체 문서를 다 거쳐야 할 것입니다.

276
00:28:23,328 --> 00:28:27,813
Wikipedia 문서 전체에 대한 gradient를 계산하고 나면
gradient update가 1회 수행됩니다.

277
00:28:27,813 --> 00:28:34,172
이 과정은 아주 느릴 것입니다. 모델이 수렴될리 없겠죠.
메모리 사용량도 어마어마할 것입니다.

278
00:28:34,172 --> 00:28:39,933
실제로는 "truncated backpropagation" 를 통해서
backprob 을 근사시키는 기법을 사용합니다.

279
00:28:39,933 --> 00:28:45,562
이 방법의 아이디어는 비록 입력 스퀀스가 엄청나게
길어서 무한대라고 할지라도

280
00:28:45,562 --> 00:28:56,232
Train time에 한 스텝을 일정 단위로 자릅니다.
대략 100 정도로 말이죠

281
00:28:56,232 --> 00:29:06,261
100 스텝만 forward pass를 하고 이 서브스퀀스의
Loss를 계산합니다. 그리고 gradient step을 진행하는 것이죠.

282
00:29:06,261 --> 00:29:12,064
이 과정을 반복합니다. 다만 이전 batch에서 계산한
hidden states는 계속 유지합니다.

283
00:29:12,064 --> 00:29:20,631
다음 Batch의 forward pass를 계산할 때는
이전 hidden state를 이용합니다.

284
00:29:20,631 --> 00:29:28,124
그리고 gradient step은 현재 Batch에서만 진행합니다.

285
00:29:28,124 --> 00:29:32,760
이 방법을 "truncated backpropagation
through time" 이라고 합니다.

286
00:29:32,760 --> 00:29:38,250
이 과정을 반복합니다. 이전 batch에서 hidden states를
가져와서 foward pass를 수행하고

287
00:29:38,250 --> 00:29:43,840
backprob은 현재 배치만큼만 진행합니다.

288
00:29:43,840 --> 00:29:49,872
이 방법은 Stocastic gradient descent의
시퀀스 데이터 버전이라고 볼 수 있습니다.

289
00:29:49,872 --> 00:29:53,690
지난 시간이 Data Sets이 엄청 큰 경우 어떻게
학습시켜야 하는지 배웠습니다.

290
00:29:53,690 --> 00:29:58,720
Large datasets에서는 전체 데이터로 gradients를
계산하기에는 계산량이 너무 컸습니다.

291
00:29:58,720 --> 00:30:02,520
그래서 대신에 mini batch를 사용해서
small samples만 가지고 계산했습니다.

292
00:30:02,520 --> 00:30:08,053
그리고 mini batch만을 이용해서 gradient를 계산했습니다.
가령 Image classification에서 그랬었죠

293
00:30:08,053 --> 00:30:08,886
질문있나요?

294
00:30:12,441 --> 00:30:15,989
질문은 "RNN이 Markov Assumption 을 따르는지" 입니다.

295
00:30:15,989 --> 00:30:20,581
그렇지 않습니다. 왜냐하면 RNN은 이전 hidden state를
계속해서 앞으로 가져가기 때문입니다.(영원히)

296
00:30:21,442 --> 00:30:25,792
마르코 비아 가정을하고있다.
숨겨진 상태를 조건으로,

297
00:30:25,792 --> 00:30:31,101
그러나 숨겨진 상태가 우리가 필요로하는 모든 것입니다.
미래를 예언하기 시퀀스의

298
00:30:31,101 --> 00:30:35,941
하지만 그 가정은 일종의 건축입니다.
재발 성 신경 네트워크 공식 출발점에서.

299
00:30:35,941 --> 00:30:39,032
그리고 그것은 정말로 특별하지 않습니다.
시간을 통한 전파를 역전 시키십시오.

300
00:30:39,032 --> 00:30:51,239
Truncated backprop은 very large sequence
data의 gradient를 근사시키는 방법입니다.

301
00:30:52,677 --> 00:30:59,649
RNN을 구현하려면 코드도 엄청 복잡하고 어려울 것 같아 보이지만
사실은 그렇지 않습니다. 엄청 간결합니다.

302
00:30:59,649 --> 00:31:07,474
Andrej가 min-char-rnn 이라는 코드를 만들어놨습니다.
112 줄의 코드로 RNN 전체 과정을 구현하였습니다.

303
00:31:07,474 --> 00:31:11,725
vocabulary를 만들고, truncated back propagation
을 수행하는 모델을 구현한 코드입니다.

304
00:31:11,725 --> 00:31:16,584
그리고 실제 Model을 학습시키고 샘플링도 할 수 있습니다.

305
00:31:16,584 --> 00:31:20,862
RNN을 구성하는 것이 엄청나게 어려워 보일
수 있지만 사실은 그렇지 않습니다.

306
00:31:20,862 --> 00:31:27,954
슬라이드만 보면 잘 모르실 수 있습니다. 여러분이 직접
파일을 열어서 코드를 한줄 씩 살펴보시기 바랍니다.

307
00:31:27,954 --> 00:31:34,473
Numpy로만 작성된 dependencies가 없는
코드입니다. 코드를 읽기에도 비교적 쉽습니다.

308
00:31:35,584 --> 00:31:41,593
RNN Language model 로 아주 재미있는 것들을
학습시킬 수 있습니다.

309
00:31:41,593 --> 00:31:52,304
RNN으로 어떤 문장이건 학습시킬 수 있습니다. 인터넷에서
아무 문장이나 긁어와서 학습시킬 수 있습니다.

310
00:31:52,304 --> 00:32:02,634
가령 셰익스피어의 작품들을 RNN으로
학습시켜 볼 수 있습니다.

311
00:32:02,634 --> 00:32:11,584
학습 초기에는 모델이 의미없는 문장들만 뱉어냅니다.
하지만 학습을 시키면 시킬수록 의미있는 문장을 만들어냅니다.

312
00:32:11,584 --> 00:32:18,224
학습이 잘 끝나면 상당히 셰익스피어 풍의
문장들을 만들어냅니다.

313
00:32:18,224 --> 00:32:24,754
"Why do what that day... replied"  어쩄든
상당히 셰익스피어의 문장 처럼 보입니다.

314
00:32:24,754 --> 00:32:31,264
더 학습을 많이 시키면 훨씬 더 긴 문장들도
만들어낼 수 있습니다.

315
00:32:31,264 --> 00:32:35,864
RNN을 이용하면 이런 셰익스피어 풍의 문장과
같은 여러가지 신기한 것들을 학습시킬 수 있습니다.

316
00:32:35,864 --> 00:32:40,016
모델이 HEAD에 발화자를 넣어야한다는 것을 압니다.
(예 : "VIOLA:")

317
00:32:40,016 --> 00:32:45,565
그리고 바로 밑에 셰익스피어풍의 문장들을
만들어냅니다.

318
00:32:45,565 --> 00:32:47,744
다른 문장을 시작할 때 한 줄을 비워야 하는 것도 압니다.

319
00:32:47,744 --> 00:32:52,958
이건 엄청난 것입니다.
문장의 구조를 아주 완벽하게 학습했습니다.

320
00:32:52,958 --> 00:33:02,725
훨씬 더 엄청난 것도 있습니다. 인터넷에서 찾았는데요,
혹시 여기 수학 전공자가 있나요?

321
00:33:02,725 --> 00:33:07,325
"algebraic topology" 수업을 들어본 분이 계신가요?
몇 분정도 계시는군요

322
00:33:07,325 --> 00:33:15,114
여러분이 저보다 "algebraic topology"를 더 잘 하시겠네요.
제가 얼마전에 온라인"algebraic topology" 교과서를 찾았습니다.

323
00:33:15,114 --> 00:33:19,554
이 교과서 파일은 수학적인 내용으로 가득 차 있습니다.

324
00:33:19,554 --> 00:33:26,325
그리고 수식이 LaTex 문법으로 작성되어 있습니다.
LaTex을 쓰면 수식을 코드로 작성할 수 있습니다.

325
00:33:26,325 --> 00:33:33,146
그러면 algebraic topology 교과서의 LaTex 코드를
우리가 만든 RNN Language Model로 학습시킬 수 있습니다.

326
00:33:33,146 --> 00:33:41,446
자 이걸 학습시키게 되면 (웃음) 이런 식으로
algebraic topology스러운 문장들을 만들어낼 수 있습니다.

327
00:33:41,446 --> 00:33:46,679
모델이 수식도 만들어 냅니다.

328
00:33:46,679 --> 00:33:52,773
"To prrov study that F_sub U is a covering
of x prime.. blah blah" (왼쪽 4번째 문단)

329
00:33:52,773 --> 00:33:56,576
unions도 만들어내고, proofs의 끝에
사각형도 만들어냅니다.

330
00:33:56,576 --> 00:34:00,026
lemmas도 만들어내고
이전 lemmas를 참조하기도 합니다.

331
00:34:00,026 --> 00:34:08,417
 bi-lemma question 라던지 R 같은 기하학적인
내용도 모두 만들어냅니다. 엄청 대단하죠

332
00:34:09,496 --> 00:34:12,913
diagrams을 만들어내려고도 합니다.

333
00:34:14,239 --> 00:34:19,313
commutative diagram은 algebraic topology에서
아주 많이 사용됩니다.

334
00:34:19,313 --> 00:34:26,379
모델이 이 다이어그램을 열심히 만들어 내려 했지만
물론 이 다이어그램은 아무 의미가 없습니다.

335
00:34:26,380 --> 00:34:32,728
그리고 또 재미있는 점 중 하나는
모델이 "증명 생략" 도 학습합니다.

336
00:34:32,728 --> 00:34:39,694
가끔 이런 경우가 있죠.
"theorem : blah blah blah, proof : 생략"

337
00:34:39,695 --> 00:34:45,392
교과서에서 증명이 생각되는 부분도 모델이
정확하게 학습한 것입니다.

338
00:34:47,831 --> 00:34:53,497
또 다른 재미있는 것들도 할 수 있습니다. 이번에는
Linux Kernel의 전체 소스코드를 학습시켰습니다.

339
00:34:53,498 --> 00:34:56,801
코드를 character level로 학습시켰습니다.

340
00:34:56,801 --> 00:35:01,483
학습된 모델에서 샘플링을 해보면
C 소스코드 스러운 것들이 나옵니다.

341
00:35:01,483 --> 00:35:06,192
모델이 if 문 작성법을 알고 있습니다.
코드 짜는 스킬이 어마어마 합니다.

342
00:35:06,192 --> 00:35:09,552
들여쓰기와 중괄호를 쓰는법도 잘 알고있습니다.

343
00:35:09,552 --> 00:35:15,052
주석다는 법도 알고있습니다.
물론 뜻은 아무 의미도 없습니다.

344
00:35:15,052 --> 00:35:23,355
이 모델의 문제점 중 하나는. 변수를 선언해 놓고
쓰지를 않다는 것입니다.

345
00:35:23,355 --> 00:35:27,554
반대로 선언하지도 않은 변수를 쓰기도 합니다.
이 코드는 컴파일되지 않습니다.

346
00:35:27,554 --> 00:35:30,555
이 코드를 Linux에 pull request 하고싶진 않네요

347
00:35:30,555 --> 00:35:37,867
이 모델은 GNU license를 추가하는 법도 압니다.

348
00:35:37,867 --> 00:35:44,962
GNU license 다음에 include 가 나오고 include 가
몇 번 나오고 소스코드가 나옵니다.

349
00:35:44,962 --> 00:35:48,836
이 모델은 데이터의 일반적인 구조를
아주 잘 학습했습니다.

350
00:35:48,836 --> 00:35:53,398
다시 한번 말씀드립니다만, 우리가 모델에게 요청한 것은
그저 시퀀스의 다음 문자를 예측하라는 것이였습니다.

351
00:35:53,398 --> 00:35:55,406
구조에 대해서는 전혀 말해주지 않았죠

352
00:35:55,406 --> 00:36:03,355
하지만 모델은 학습과정 동안에 시퀀스 데이터의
숨겨진 구조(latent structure)를 알아서 학습합니다.

353
00:36:05,808 --> 00:36:10,246
결국 알아서 코드를 작성하게 된 것입니다.
아주 대단합니다.

354
00:36:10,246 --> 00:36:20,856
저와 Andrej가 몇 년전에 낸 논문이 있습니다. 학습한 모델이
도대체 어떤 방식으로 동작하는지 알아내고 싶었습니다.

355
00:36:20,856 --> 00:36:28,165
RNN에는 hidden vector가 있고 이 vector가
계속 업데이트됩니다.

356
00:36:28,165 --> 00:36:34,176
우리는 그 Vector를 추출해보면 해석 가능한 의미있는 것들이
나올수도 있지 않을까 추측해 보았습니다.

357
00:36:34,176 --> 00:36:39,917
그래서 우리는 RNN language model을
특정 dataset으로 학습시키고,

358
00:36:39,917 --> 00:36:47,923
그리고 hidden vector를 하나 뽑아서 어떤 값들이
들어있는지 살펴보기로 했습니다.

359
00:36:47,923 --> 00:36:52,206
이 서로 다른 hidden states가 도대체 어떤 것들을
예의주시하는지 알아보기 위해서였죠

360
00:36:52,206 --> 00:36:56,406
이 실험을 해보면, 대부분의 hidden state는
아무 의미 없는 패턴으로 예의 주시하고 있습니다.

361
00:36:56,406 --> 00:37:02,686
여기서 우리가 한 일은 vector 하나를 뽑은 다음에 이 시퀀스를
다시 한번 forward 시켜 보는 것입니다.

362
00:37:02,686 --> 00:37:10,876
여기에서 각 색깔은, 시퀀스를 읽는 동안에 앞서 뽑은
hidden vector의 값을 의미합니다.

363
00:37:10,876 --> 00:37:15,883
RNN의 대부분의 Cell은  해석하기 어렵습니다.

364
00:37:15,883 --> 00:37:21,396
대부분은 다음에 어떤 문자가 와야할 지 알아내기 위한
low level language modeling 정도로밖에 보이지 않습니다.

365
00:37:21,396 --> 00:37:23,438
하지만 일부는 해석해볼만 합니다.

366
00:37:23,438 --> 00:37:26,375
그러다가 따옴표(quotes)를 찾는 벡터를 알아냈습니다.

367
00:37:26,375 --> 00:37:32,486
이 vector가 하는 일을 살펴보면 처음에는 계속
off off off off ... 입니다. 파란색이죠. (파란색이 off)

368
00:37:32,486 --> 00:37:38,303
그러다가 따옴표를 만나면 값이 켜집니다(빨간색)
그리고 따옴표가 닫히기 전까지 유지됩니다.

369
00:37:39,187 --> 00:37:42,598
그러다가 두 번째 따옴표를 만나면 다시
값이 꺼집니다.

370
00:37:42,598 --> 00:37:48,856
이런 식으로, 그저 우리는 모델이 다음 문자를 예측하도록
학습시켰을 뿐이지만 모델은 더 유용한 것들을 학습하는 것입니다.

371
00:37:48,856 --> 00:37:54,222
그런 유용한 것들을 학습하기 위해서 그 Cell이
따옴표를 감지할 수 있게 된 것입니다.

372
00:37:54,222 --> 00:38:00,104
우리는 또, 줄 바꿈을 위해서 현재 줄의 단어 갯수를
세는 듯해 보이는 Cell도 발견했습니다.

373
00:38:00,104 --> 00:38:07,055
처음에는 0으로 시작했다가. 줄이 점점 길어지면
점점 빨간색으로 변합니다.

374
00:38:07,055 --> 00:38:11,976
점점 증가하다가 줄바꿈이 이루어지면
다시 0으로 리셋됩니다.

375
00:38:11,976 --> 00:38:19,545
이 Cell은 언제 모델이 new line characters을 필요로 할 지
지속적으로 추적하는 역할을 한다고 볼 수 있습니다.

376
00:38:19,545 --> 00:38:22,987
그리고 우리는 linux 코드를 학습시킬 때 발견한 것도 있습니다.

377
00:38:22,987 --> 00:38:33,838
우리는 if 문의 조건부에서 값이 켜지는 Cell을 발견했습니다.
이 Cell은 조건부의 내부와 외부를 구별할 수 있습니다.

378
00:38:33,838 --> 00:38:35,806
이를 통해 모델이 시퀀스를 더 잘
학습할 수 있도록 도와줍니다.

379
00:38:35,806 --> 00:38:44,765
그 밖에도 Comments를 찾아내거나 혹은
들여쓰기 레벨을 세는 듯한 Cell도 발견 했습니다.

380
00:38:44,765 --> 00:38:50,758
정말 놀라운 일입니다. 우리는 그저 다음 문자를
예측하는 모델을 학습시켰을 뿐인데

381
00:38:50,758 --> 00:38:55,646
결국 모델은 입력 데이터의 구조를
학습하게 되었기 때문입니다.

382
00:38:57,161 --> 00:39:05,528
우리는 Computer Vision 수업이니 다시 Computer Vision
에 관련된 예시로 돌아가 보겠습니다.

383
00:39:05,528 --> 00:39:08,747
앞서 Image Captioning Model에 대해서
몇 번 언급했었습니다.

384
00:39:08,747 --> 00:39:14,376
이 모델은 입력은 이미지이고
출력은 자연어로 된 Caption 입니다.

385
00:39:14,376 --> 00:39:19,787
이 문제를 다루는 몇몇 논문들이 있습니다.
다들 접근 방법은 비슷합니다.

386
00:39:19,787 --> 00:39:25,026
이 그림은 그 논문들 중 저희 연구실에서 나온 논문입니다.
아주 "공정한 기준으로?" 예시를 이 논문으로 선택헀습니다.

387
00:39:26,876 --> 00:39:35,608
우선 Caption은 가변길이 입니다. Caption 마다
다양한 시퀀스 길이를 가지고 있습니다.

388
00:39:35,608 --> 00:39:39,947
여기에 RNN Language Model가 아주 잘 어울립니다.

389
00:39:39,947 --> 00:39:50,379
모델에는 입력 이미지를 받기 위핸 CNN이 있습니다.
CNN은 지금까지 아주 많이 살펴봤었죠

390
00:39:50,379 --> 00:39:58,928
CNN은 요약된 이미지정보가 들어있는 Vector를 출력합니다.
이 Vector는 RNN의 초기 Step의 입력으로 들어갑니다.

391
00:39:58,928 --> 00:40:02,888
그러면 RNN은 Caption에 사용할
문자들을 하나씩 만들어냅니다.

392
00:40:02,888 --> 00:40:06,425
그렇다면 이 모델을 학습시킨 후에 
Test time에 어떻게 동작하는지 알아봅시다.

393
00:40:06,425 --> 00:40:11,178
지금까지 살펴봤던 character level languate
model과 아주 유사합니다.

394
00:40:11,178 --> 00:40:18,638
우선 입력 이미지를 받아서 CNN의 입력으로 넣습니다. 
다만 softmax scores를 사용하지 않고

395
00:40:18,638 --> 00:40:24,915
직전의 4,096-dim vector를 출력으로 합니다.

396
00:40:24,915 --> 00:40:31,377
이 vector를 이용해서 전체 이미지 정보를 
요약하는데 사용할 것입니다.

397
00:40:31,377 --> 00:40:39,528
앞서 RNN language models에서 배웠듯이 모델이 문장을
생성해 내기에 앞서 초기 값을 넣어줘야 합니다.

398
00:40:39,528 --> 00:40:49,006
이 경우에는 입력이 특별합니다. 모델에게 "여기 이미지 정보가
있으니 이 조건에 맞는 문장을 만들어줘!" 라고 시작해야합니다.

399
00:40:49,006 --> 00:40:57,107
이전 까지의 모델에서는 RNN 모델이 두 개의 가중치 행렬을
입력으로 받았습니다. 하나는 현재 스텝의 입력이고

400
00:40:57,107 --> 00:41:01,240
다른 하나는 이전 스텝의 Hidden state 이었죠. 그리고
이 둘을 조합해서 다음 hidden state를 얻었습니다.

401
00:41:01,240 --> 00:41:04,678
하지만 이제는 이미지 정보도 더해줘야 합니다.

402
00:41:04,678 --> 00:41:13,105
사람마다 모델에 이미지정보를 추가하는 방법이 다르겠지만
가장 쉬운 방법은 세 번째 가중치 행렬을 추가하는 것입니다.

403
00:41:14,561 --> 00:41:18,598
다음 hidden state를 계산할 때마다 모든 스텝에 
이 이미지 정보를 추가합니다.

404
00:41:18,598 --> 00:41:23,635
자 이제는 vocabulary의 모든 스코어들에 대한
분포를 계산할 차례입니다.

405
00:41:23,635 --> 00:41:27,307
이 문제에서 vocabulary는 "모든 영어 단어들" 과 
같이 엄청나게 큽니다.

406
00:41:27,307 --> 00:41:32,099
그 분포에서 샘플링을 하고 그 단어를
다음 스텝의 입력으로 다시 넣어줄 것입니다.

407
00:41:32,099 --> 00:41:39,546
샘플링된 단어(y0)가 들어가면 다시 vacab에 대한 분포를
추정하고 다음 단어를 만들어냅니다.

408
00:41:39,546 --> 00:41:47,538
모든 스텝이 종료되면 결국 한 문장이 만들어집니다.

409
00:41:47,538 --> 00:41:54,347
이 때, <END> 라는 특별한 토큰이 있는데 이는
문장의 끝을 알려줍니다.

410
00:41:54,347 --> 00:42:00,328
<END> 가 샘플링되면 모델은 더이상 단어를 생성하지
않으며 이미지에 대한 caption이 완성됩니다.

411
00:42:00,328 --> 00:42:06,739
Train time에는 모든 caption의 종료지점에 
<END> 토큰을 삽입합니다.

412
00:42:06,739 --> 00:42:10,907
네트워크가 학습하는 동안에 시퀀스의 끝에
<END> 토큰을 넣어야 한다는 것을 알려줘야 하기 때문입니다.

413
00:42:10,907 --> 00:42:16,848
학습이 끝나고 Test time에는 모델이 문장 생성을
끝마치면 <END> 토큰을 샘플링합니다.

414
00:42:16,848 --> 00:42:20,139
이 모델은 완전히 supervised learning으로 학습시킵니다.

415
00:42:20,139 --> 00:42:24,763
따라서 이 모델을 학습시키기 위해서는 natural
language caption이 있는 이미지를 가지고 있어야 합니다.

416
00:42:24,763 --> 00:42:28,666
이 Task를 위한 가장 큰 데이터셋은 아마
Microsoft COCO 데이터셋이 아닐까 싶습니다.

417
00:42:28,666 --> 00:42:47,837
이 모델을 학습시키기 위해서는 natural language model과 
CNN을 동시에 backp 시킬 수 있습니다.

418
00:42:47,837 --> 00:42:52,438
이 모델을 학습시키고 나면
상당히 그럴듯한 결과를 보입니다.

419
00:42:52,438 --> 00:42:56,689
지금 보이는 것들이 실제 모델의 결과입니다.

420
00:42:56,689 --> 00:43:00,995
고양이가 여행용 가방에 앉아있습니다. 
"A cat sitting on a suitcase on the floor"

421
00:43:00,995 --> 00:43:02,583
상당히 인상적인 결과입니다.

422
00:43:02,583 --> 00:43:04,975
고양이가 나뭇가지에 앉아있습니다. 
"A cat is sitting on a tree branch"

423
00:43:04,975 --> 00:43:06,499
이 또한 상당히 놀랍습니다.

424
00:43:06,499 --> 00:43:09,631
두 사람이 서핑보드를 들고 해변을 걷고있습니다. 
"Two people walking on the beach with surfbards"

425
00:43:09,631 --> 00:43:16,635
이 모델은 상당히 파워풀하고 이미지를 묘사하기 위해
비교적 복잡한 captions도 만들어낼 수 있습니다.

426
00:43:16,635 --> 00:43:19,808
하지만 그렇긴해도 
모델이 아주 완벽하진 않습니다.

427
00:43:19,808 --> 00:43:24,319
전부다 마법같은 일만 일어나는 것은 아닙니다. 
이 모델도 다른 Machine learning 모델들 처럼

428
00:43:24,319 --> 00:43:28,830
Train time에 보지 못한 데이터에 대해서는
엄청 잘 동작하지는 않습니다.

429
00:43:28,830 --> 00:43:33,317
가령, 여자가 손에 고양이를 들고 있습니다.
"A woman is holding a cat in her hand"

430
00:43:33,317 --> 00:43:35,421
하지만 이미지에 고양이는 없습니다.

431
00:43:35,421 --> 00:43:41,359
하지만 여자가 모피 코트를 입고있습니다. 아마도 모델이
모피 코트의 텍스쳐를 고양이로 보고 있는 것 같습니다.

432
00:43:41,359 --> 00:43:44,379
여자가 해변에서 서핑보드를 들고있습니다. 
"A Woman standing on a beach holding a surfboard"

433
00:43:44,379 --> 00:43:47,892
하지만 분명 이 사람은 서핑보드를 들고있지 않습니다. 
물구나무서기를 하고 있죠

434
00:43:47,892 --> 00:43:51,820
모델이 물구나무서기를 하고있다는 사실을 놓쳤습니다.

435
00:43:51,820 --> 00:43:58,238
 그리고 여기 나뭇가지에 쳐있는 거미줄 사진이 있습니다.

436
00:43:58,238 --> 00:44:00,382
하지만 모델은 나뭇가지에 새가 앉아있다고 생각했습니다. 
"A bird is perched on a tree branch"

437
00:44:00,382 --> 00:44:05,139
모델은 거미에 관한 것들을 완전히 놓쳤습니다. 하지만
이 모델은 Train time에 거미를 보지 못했습니다.

438
00:44:05,139 --> 00:44:08,217
Train time에는 새가 나뭇가지에 않는다는 사실만
배웠을 뿐입니다.

439
00:44:08,217 --> 00:44:10,152
따라서 납득이 가는 실수하고 볼 수 있습니다.

440
00:44:10,152 --> 00:44:14,338
그리고 여기 밑에 보시면 모델은 이 사람이 공을
던지는 중인지 받는 중인지 구분하지 못합니다.

441
00:44:14,338 --> 00:44:17,709
하지만 어찌됐든 야구선수와 야구공이 어떤
연관이 있음은 모델이 알고 있습니다.

442
00:44:17,709 --> 00:44:20,441
다시한번 말씀드리지만 이 모델은
완벽하지 않습니다.

443
00:44:20,441 --> 00:44:25,109
Train data와 유사한 이미지를 던져주면 
꾀나 잘 동작합니다.

444
00:44:25,109 --> 00:44:29,188
하지만 그 이상 일반적인 문제를 풀기는 조금 어렵습니다.

445
00:44:29,188 --> 00:44:34,152
조금 더 진보된 모델이 있습니다. 
Attention 이라는 모델입니다.

446
00:44:34,152 --> 00:44:41,036
이 모델은 Caption을 생성할 때 이미지의 다양한 부분을
집중해서(attention) 볼 수 있습니다.

447
00:44:41,036 --> 00:44:44,670
이번 시간에는 Attention 모델을 깊게 다루지는 않겠습니다.

448
00:44:44,670 --> 00:44:48,925
어떻게 동작하는지 간단하게 말씀드리자면 
우선 CNN이 있습니다.

449
00:44:48,925 --> 00:44:59,954
CNN로 벡터 하나를 만드는게 아니라 각 벡터가 공간정보를
가지고 있는 grid of vector를 만들어냅니다.(LxD)

450
00:44:59,954 --> 00:45:06,618
Forward pass시에 매 스텝 vocabulary에서
샘플링을 할 때

451
00:45:06,618 --> 00:45:11,263
모델이 이미지에서 보고싶은 위치에 대한 분포 또한 만들어냅니다.

452
00:45:11,263 --> 00:45:18,276
이미지의 각 위치에 대한 분포는 Train time에 
모델이 어느 위치를 봐야하는지에 대한 attention이라 할 수 있습니다.

453
00:45:18,276 --> 00:45:23,155
첫 번째 hidden state(h0)는 이미지의 위치에 대한
분포를 계산합니다.

454
00:45:23,155 --> 00:45:31,372
이 분포(a1)를 다시 벡터 집합(LxD Feature)과 연산하여 
이미지 attention(z1)을 생성합니다.

455
00:45:31,372 --> 00:45:36,508
이 요약된 벡터(z1)는 Neural network의 
다음 스텝의 입력으로 들어갑니다.

456
00:45:36,508 --> 00:45:38,278
그리고 두 개의 출력이 생성됩니다. (a2, d1)

457
00:45:38,278 --> 00:45:43,714
하나는 vocabulary의 각 단어들의 분포입니다(d1).
그리고 다른 하나(a2)는 이미지 위치에 대한 분포입니다.

458
00:45:43,714 --> 00:45:49,160
이 과정을 반복하면 매 스텝마다
값 두개(a, d)가 계속 만들어집니다.

459
00:45:50,296 --> 00:45:58,298
Train을 끝마치면, 모델이 caption을 생성하기 위해서 
이미지의 attention을 이동시키는 모습을 볼 수 있습니다.

460
00:45:58,298 --> 00:46:04,436
가령 여기 만들어진 caption은
"A bird flying over a body of water." 입니다.

461
00:46:04,436 --> 00:46:12,473
caption을 만들 때 마다 이미지 내에 다양한 곳들에
attention을 주는 것을 볼 수 있습니다.

462
00:46:14,112 --> 00:46:19,147
사실 여기에는 hard/soft attention의 개념이
있습니다만, 너무 깊게 들어가진 않겠습니다.

463
00:46:19,147 --> 00:46:26,614
soft attention의 경우에는 "모든 특징"과 "모든 이미지 위치"간의
weighted combination을 취하는 경우입니다.

464
00:46:26,614 --> 00:46:34,259
반면 hard attention의 경우에는 모델이 각 타임 스텝마다
단 한곳만 보도록 강제한 경우입니다.

465
00:46:34,259 --> 00:46:38,277
hard attention의 경우에 이미지 위치를 딱 하나만
정해야 하는데, 사실상 하나만 정하기 까다롭습니다.

466
00:46:38,277 --> 00:46:46,247
따라서 Hard attention을 학습시키려면 기본 backprob 보다는
조금 더 fanier한 방법을 써야만 합니다.

467
00:46:46,247 --> 00:46:51,498
이에 관련된 것들은 다음에 
reinforcement learning 시간에 더 다루겠습니다.

468
00:46:51,498 --> 00:46:57,415
Attention Model을 학습시키고 나서 
Caption을 생성해보면

469
00:46:57,415 --> 00:47:04,067
실제로 모델이 Caption을 생성할 때 의미있는 부분에
attention을 집중한다는 것을 알 수 있습니다.

470
00:47:04,067 --> 00:47:08,413
여자가 공원에서 원반을 던지고 있습니다. 에서 
"A woman is throwing a frisbee in a park"

471
00:47:08,413 --> 00:47:14,516
Attention mask를 살펴보면. 
모델이 "원반(frisbee)" 라는 단어를 생성할 때

472
00:47:14,516 --> 00:47:18,976
이미지에서도 실제 원반이 위치하는 곳을
정확하게 attention하는 것을 알 수 있습니다.

473
00:47:18,976 --> 00:47:24,542
엄청 대단합니다. 우리는 모델에게 매 스텝 어디를
보라고 말해준 적이 없습니다.

474
00:47:24,542 --> 00:47:27,955
모델 스스로가 Train time에 알아냈습니다.

475
00:47:27,955 --> 00:47:32,721
모델이 그쪽(원반) 영역에 집중하는 것이
올바른 일이라는 것을 스스로 알아낸 것입니다.

476
00:47:32,721 --> 00:47:34,610
모델 전체가 미분가능하기 때문에

477
00:47:34,610 --> 00:47:42,541
soft attention 또한 backprob이 가능합니다. 따라서 
이 모든 것들은 train time에서 나온 것들입니다.

478
00:47:42,541 --> 00:47:45,297
아주 대단합니다.

479
00:47:45,297 --> 00:47:51,565
RNN + Attention 조합은 Image captioning 뿐만 아니라
더 다양한 것들을 할 수 있습니다.

480
00:47:51,565 --> 00:47:54,691
가령 Visual Question Answering(VQA) 같이 말이죠.

481
00:47:54,691 --> 00:48:04,010
이 문제에서는 입력이 두 가지 입니다. 하나는 이미지이고
다른 하나는 이미지에 관련된 질문입니다.

482
00:48:04,010 --> 00:48:10,163
왼쪽과 같은 이미지에 다음과 같은 질문을 할 수 있을 것입니다. 
Q: 트럭에 그려져 있는 멸종위기 동물은 무엇입니까?

483
00:48:10,163 --> 00:48:19,027
모델은 네개의 보기 중에서 정답을 맞춰야 합니다.

484
00:48:19,027 --> 00:48:24,774
이 모델 또한 RNN과 CNN으로 만들 수 있습니다.

485
00:48:24,774 --> 00:48:29,701
이 경우에는 "many to one" 의 경우입니다.

486
00:48:29,701 --> 00:48:33,566
모델은 자연어 문장(질문)을 입력으로 받아야 합니다.

487
00:48:33,566 --> 00:48:40,111
이는 RNN을 통해 구현할 수 있습니다.
RNN이 질문을 vector로 요약합니다.

488
00:48:40,111 --> 00:48:43,928
그리고 이미지 요약을 위해서는 CNN이 필요합니다.

489
00:48:43,928 --> 00:48:51,645
CNN/RNN에서 나온 벡터를 조합하면 질문에 대한
분포를 예측할 수 있을 것입니다.

490
00:48:51,645 --> 00:48:59,175
그리고 간혹 VQA 문제를 풀기 위해서 soft special attenion
알고리즘을 적용하는 경우도 있습니다.

491
00:48:59,175 --> 00:49:07,062
이 예시를 보시면 모델이 정답을 결정하기 위해서
이미지에 대한 attention을 만들어내는 것을 볼 수 있습니다.

492
00:49:07,062 --> 00:49:09,062
질문있나요?

493
00:49:18,715 --> 00:49:20,878
질문은 "어떻게 서로 다른 입력을 조합하는지" 입니다. 
(CNN output과 RNN output을 어떻게 합치는지)

494
00:49:20,878 --> 00:49:25,998
어떻게 요약된(encoded) 질문 벡터와, 요약된 이미지 벡터를
조합하는지를 여쭤보시는 것인가요?

495
00:49:25,998 --> 00:49:30,395
질문은 "encoded image와 encoded question를
어떻게 조합할 수 있는지" 입니다.

496
00:49:30,395 --> 00:49:34,847
가장 쉬운 방법중 하나는 concat으로 붙혀서
FC-Layer의 입력으로 만드는 방법입니다.

497
00:49:34,847 --> 00:49:37,864
이 방법이 가장 흔한 방법이고. 
가장 먼저 시도해볼 수 있는 방법입니다.

498
00:49:37,864 --> 00:49:44,389
하지만 간혹 사람들이 더 강력한 함수를 만들기 위해서 
두 벤터간의 더 복잡한 조합을 만들어내는 경우도 있습니다.

499
00:49:44,389 --> 00:49:48,050
어찌됐든 Concat하는 방법도 맨 처음 시도해보기에는
나쁘지 않은 방법입니다.

500
00:49:49,426 --> 00:49:55,084
지금까지 RNN을 활용한 다양한 예제들을 살펴보았습니다. 
이 방법들 모두 대단한 방법들입니다.

501
00:49:55,084 --> 00:50:04,072
왜냐하면 RNN을 이용하면 CV와 NLP 문제를 조합시키는 
복잡한 문제들을 풀 수 있기 때문입니다.

502
00:50:04,072 --> 00:50:05,794
그리고 당신은 우리가 함께 스틱을 할
수 있다는 것을 알 수 있습니다.

503
00:50:05,794 --> 00:50:06,776
레고 블록과 같은 모델

504
00:50:06,776 --> 00:50:08,870
정말 복잡한 일을 공격하고,

505
00:50:08,870 --> 00:50:11,369
이미지 캡션 또는 시각적 질문 응답

506
00:50:11,369 --> 00:50:14,069
이 단순한 비교적 단순한 스티칭으로

507
00:50:14,069 --> 00:50:16,736
신경망 모듈의 유형.

508
00:50:18,708 --> 00:50:20,861
그러나 나는 또한 언급하고 싶다.

509
00:50:20,861 --> 00:50:22,359
지금까지이 아이디어에 대해 이야기했습니다.

510
00:50:22,359 --> 00:50:24,060
단일 반복 네트워크 계층의

511
00:50:24,060 --> 00:50:26,274
우리는 일종의 숨겨진 상태를 가지고 있습니다.

512
00:50:26,274 --> 00:50:29,404
그리고 당신이 꽤 보편적으로 볼 수있는 또 다른 것

513
00:50:29,404 --> 00:50:32,581
다중 층 반복 신경망에 대한이 아이디어입니다.

514
00:50:32,581 --> 00:50:36,577
여기, 이것은 3 층 반복 신경망이며,

515
00:50:36,577 --> 00:50:38,535
이제 우리의 의견이 들어갑니다.

516
00:50:38,535 --> 00:50:42,292
들어가서 들어가서 일련의 숨은 상태를 만든다.

517
00:50:42,292 --> 00:50:44,554
첫 번째 반복적 인 신경 네트워크 계층에서.

518
00:50:44,554 --> 00:50:46,309
그리고 지금 우리가 일종의

519
00:50:46,309 --> 00:50:47,893
하나의 반복적 인 신경 네트워크 계층,

520
00:50:47,893 --> 00:50:50,468
그러면 숨겨진 상태의 전체 시퀀스가 생깁니다.

521
00:50:50,468 --> 00:50:52,991
이제 숨겨진 상태의 시퀀스를 사용할 수 있습니다.

522
00:50:52,991 --> 00:50:54,733
입력 시퀀스로 다른

523
00:50:54,733 --> 00:50:56,474
재발 성 신경 네트워크 계층.

524
00:50:56,474 --> 00:50:57,801
그리고 나서 당신은 상상할 수 있습니다.

525
00:50:57,801 --> 00:50:59,757
그러면 숨겨진 상태의 다른 시퀀스가 생성됩니다.

526
00:50:59,757 --> 00:51:01,577
제 2 RNN 계층으로부터 수신한다.

527
00:51:01,577 --> 00:51:02,410
그리고 나서 당신은 상상할 수 있습니다.

528
00:51:02,410 --> 00:51:04,181
이 물건들을 서로 쌓아 올리면,

529
00:51:04,181 --> 00:51:06,108
우리는 다른 상황에서 본 적이 있다는 것을 알기 때문에

530
00:51:06,108 --> 00:51:08,253
더 깊은 모델은 더 잘 수행하는 경향이 있습니다.

531
00:51:08,253 --> 00:51:09,398
다양한 문제.

532
00:51:09,398 --> 00:51:11,851
그리고 RNN에서도 같은 종류의 보유가 가능합니다.

533
00:51:11,851 --> 00:51:14,377
많은 문제들에 대해, 당신은 아마도 두 가지를 보게 될 것입니다.

534
00:51:14,377 --> 00:51:16,714
또는 3 계층 반복 신경망 모델

535
00:51:16,714 --> 00:51:18,215
꽤 일반적으로 사용됩니다.

536
00:51:18,215 --> 00:51:22,086
일반적으로 RNN에는 수퍼 심층 모델이 표시되지 않습니다.

537
00:51:22,086 --> 00:51:25,449
일반적으로 2, 3, 4 층 RNN

538
00:51:25,449 --> 00:51:29,327
어쩌면 당신이 전형적으로 갈만큼 깊을 수도 있습니다.

539
00:51:29,327 --> 00:51:32,231
그렇다면 정말 흥미롭고 중요하다고 생각합니다.

540
00:51:32,231 --> 00:51:33,500
에 대해 생각하는 것,

541
00:51:33,500 --> 00:51:34,714
이제 우리는

542
00:51:34,714 --> 00:51:38,222
이 RNN이 어떤 종류의 문제에 사용될 수 있는지,

543
00:51:38,222 --> 00:51:40,079
하지만 조금 더 신중하게 생각해야합니다.

544
00:51:40,079 --> 00:51:41,795
이 모델에 정확히 무슨 일이 일어나는지

545
00:51:41,795 --> 00:51:43,229
우리가 그들을 훈련 시키려고 할 때.

546
00:51:43,229 --> 00:51:45,704
그래서 여기,이 작은 바닐라 RNN 세포를 그렸습니다.

547
00:51:45,704 --> 00:51:47,447
우리가 지금까지 이야기했던 것.

548
00:51:47,447 --> 00:51:50,639
그래서 여기, 우리는 현재의 입력 xt를 취합니다.

549
00:51:50,639 --> 00:51:52,971
그리고 우리의 이전의 숨겨진 상태,

550
00:51:52,971 --> 00:51:55,307
그리고 나서 우리는 두 개의 벡터를 쌓습니다.

551
00:51:55,307 --> 00:51:57,359
그래서 우리는 그들을 함께 쌓을 수 있습니다.

552
00:51:57,359 --> 00:51:59,033
그런 다음이 행렬 곱셈을 수행하십시오.

553
00:51:59,033 --> 00:52:00,431
우리의 체중 매트릭스,

554
00:52:00,431 --> 00:52:01,911
우리에게,

555
00:52:01,911 --> 00:52:03,798
그 결과를 탄 (tanh)을 통해 스쿼시 (squash)

556
00:52:03,798 --> 00:52:05,783
그리고 그것은 우리에게 다음에 숨겨진 상태를 줄 것입니다.

557
00:52:05,783 --> 00:52:07,687
그리고 그것은 기본적인 기능적 형태의 일종입니다.

558
00:52:07,687 --> 00:52:10,131
이 바닐라 재발 신경 네트워크의

559
00:52:10,131 --> 00:52:11,347
그렇다면 우리는

560
00:52:11,347 --> 00:52:14,080
이 아키텍처에서 일어나는 일

561
00:52:14,080 --> 00:52:17,738
우리가 그라디언트를 계산하려고 할 때 역방향 패스 중에?

562
00:52:17,738 --> 00:52:19,435
그래서 우리가 계산하려고 생각하면,

563
00:52:19,435 --> 00:52:21,426
그래서 그 후 거꾸로 지나가는 동안,

564
00:52:21,426 --> 00:52:24,759
우리는 우리가 얻은 파생물을 받게 될 것입니다.

565
00:52:25,850 --> 00:52:27,075
우리는 손실 파생 상품을 받게 될 것입니다.

566
00:52:27,075 --> 00:52:28,568
ht와 관련하여

567
00:52:28,568 --> 00:52:30,885
그리고 셀을 통한 역방향 통과 동안,

568
00:52:30,885 --> 00:52:32,720
우리는 손실 파생 상품을 계산해야합니다.

569
00:52:32,720 --> 00:52:34,632
ht에서 1을 뺀 값까지.

570
00:52:34,632 --> 00:52:37,026
그런 다음 우리가이 역 통과를 계산할 때,

571
00:52:37,026 --> 00:52:38,601
그라디언트가 역방향으로 흐른 것을 볼 수 있습니다.

572
00:52:38,601 --> 00:52:39,881
이 빨간 길을 통해서.

573
00:52:39,881 --> 00:52:41,706
먼저 그라디언트가 뒤로 이동합니다.

574
00:52:41,706 --> 00:52:43,068
이 탄 게이트를 통해,

575
00:52:43,068 --> 00:52:44,036
그런 다음 뒤쪽으로 흐를 것입니다.

576
00:52:44,036 --> 00:52:45,958
이 행렬 곱셈 게이트를 통해

577
00:52:45,958 --> 00:52:48,644
그리고 우리가 숙제에서 보았 듯이

578
00:52:48,644 --> 00:52:52,054
이들 매트릭스 승산 층을 구현할 때,

579
00:52:52,054 --> 00:52:53,252
백 프로 페지 할 때

580
00:52:53,252 --> 00:52:54,711
이 행렬 곱셈 게이트,

581
00:52:54,711 --> 00:52:56,785
당신은 트랜스 포즈에 의해 mp 거리게됩니다.

582
00:52:56,785 --> 00:52:58,457
그 무게 매트릭스의.

583
00:52:58,457 --> 00:53:00,642
그래서 우리가 백 프로게이트 할 때마다

584
00:53:00,642 --> 00:53:03,857
이들 바닐라 RNN 세포 중 하나를 통해,

585
00:53:03,857 --> 00:53:08,024
우리는 가중치 행렬의 일부분을 곱하는 것을 끝내게됩니다.

586
00:53:09,110 --> 00:53:11,124
그래서 지금 우리가 많은 것을 고집하고 있다고 상상한다면

587
00:53:11,124 --> 00:53:13,834
이러한 재발 성 신경망 세포 중에서,

588
00:53:13,834 --> 00:53:15,413
다시 이것은 RNN이기 때문입니다.

589
00:53:15,413 --> 00:53:16,599
모델 시퀀스가 필요합니다.

590
00:53:16,599 --> 00:53:18,976
이제 그라디언트 흐름이 어떻게되는지 상상해보십시오.

591
00:53:18,976 --> 00:53:20,921
이들 층들의 시퀀스를 통해,

592
00:53:20,921 --> 00:53:23,770
어떤 종류의 물고기가 발생하기 시작합니다.

593
00:53:23,770 --> 00:53:25,207
왜냐하면 지금, 우리가 계산하기를 원할 때

594
00:53:25,207 --> 00:53:27,863
h 0에 대한 손실의 기울기,

595
00:53:27,863 --> 00:53:29,440
우리는 모든 것을 백 프로 퍼 게이트해야한다.

596
00:53:29,440 --> 00:53:30,810
이러한 RNN 세포 중.

597
00:53:30,810 --> 00:53:32,903
그리고 당신이 하나의 셀을 통해 역 전파 할 때마다,

598
00:53:32,903 --> 00:53:35,641
당신은 이러한 w transpose
factor 중 하나를 선택할 것입니다.

599
00:53:35,641 --> 00:53:38,176
즉, 최종 표현식

600
00:53:38,176 --> 00:53:40,289
그라데이션이 0 인 경우

601
00:53:40,289 --> 00:53:42,161
많은, 많은 요인들을 포함 할 것이다.

602
00:53:42,161 --> 00:53:43,550
이 가중치 행렬의

603
00:53:43,550 --> 00:53:44,967
나쁜 일이 될 수 있습니다.

604
00:53:44,967 --> 00:53:46,976
어쩌면 무게에 대해 생각하지 않아도됩니다.

605
00:53:46,976 --> 00:53:48,329
행렬 경우,

606
00:53:48,329 --> 00:53:50,138
스케일러 케이스를 상상해보십시오.

607
00:53:50,138 --> 00:53:51,877
우리가 끝내면 우리가 스케일러를 가지고 있다면

608
00:53:51,877 --> 00:53:53,662
우리는 같은 번호로 반복해서 곱합니다.

609
00:53:53,662 --> 00:53:54,563
그리고 다시,

610
00:53:54,563 --> 00:53:56,038
어쩌면 네 가지 예를 들어,

611
00:53:56,038 --> 00:53:57,133
그러나 100과 같은 것

612
00:53:57,133 --> 00:54:00,269
또는 수백 개의 시간 간격,

613
00:54:00,269 --> 00:54:01,832
같은 수를 곱하면된다.

614
00:54:01,832 --> 00:54:03,835
반복해서 정말 나쁜 것입니다.

615
00:54:03,835 --> 00:54:05,244
스케일러의 경우,

616
00:54:05,244 --> 00:54:06,756
폭발 할거야.

617
00:54:06,756 --> 00:54:08,971
그 수가 1보다 큰 경우

618
00:54:08,971 --> 00:54:10,504
또는 0으로 사라질 것입니다.

619
00:54:10,504 --> 00:54:12,912
숫자가 1보다 작은 경우

620
00:54:12,912 --> 00:54:14,069
절대 값.

621
00:54:14,069 --> 00:54:16,355
그리고 이것이 일어나지 않을 유일한 길

622
00:54:16,355 --> 00:54:18,186
그 숫자가 정확히 하나라면,

623
00:54:18,186 --> 00:54:20,911
실제로 실제로는 거의 발생하지 않습니다.

624
00:54:20,911 --> 00:54:22,569
그것은 우리를 떠난다.

625
00:54:22,569 --> 00:54:25,229
동일한 직감이 행렬의 경우까지 확장됩니다.

626
00:54:25,229 --> 00:54:27,560
그러나 지금, 스케일러 수의 절대 값보다는 오히려,

627
00:54:27,560 --> 00:54:29,261
대신에 가장 큰 것을보아야합니다.

628
00:54:29,261 --> 00:54:32,071
이 가중치 행렬의 가장 큰 특이 값.

629
00:54:32,071 --> 00:54:34,707
이제 그 가장 큰 특이 값이 1보다 큰 경우,

630
00:54:34,707 --> 00:54:36,502
이 역방향 패스 동안,

631
00:54:36,502 --> 00:54:38,824
가중치 행렬을 계속해서 곱하면,

632
00:54:38,824 --> 00:54:42,135
그 그라디언트를 h w, 0 h, 미안,

633
00:54:42,135 --> 00:54:43,915
매우 커질 것입니다.

634
00:54:43,915 --> 00:54:46,079
그 행렬이 너무 클 때.

635
00:54:46,079 --> 00:54:48,947
그리고 그것은 우리가 폭발적인
그라데이션 문제라고 부르는 것입니다.

636
00:54:48,947 --> 00:54:52,047
이제이 그래디언트가 기하 급수적으로
폭발적으로 폭발 할 것입니다

637
00:54:52,047 --> 00:54:53,427
시간 간격의 수와 함께

638
00:54:53,427 --> 00:54:55,061
우리가 통해 역 전파합니다.

639
00:54:55,061 --> 00:54:57,643
그리고 가장 큰 특이 값이 1보다 작 으면,

640
00:54:57,643 --> 00:54:59,123
그런 다음 우리는 정반대의 문제를 겪습니다.

641
00:54:59,123 --> 00:55:00,036
이제 우리의 그라디언트가 줄어들 것입니다.

642
00:55:00,036 --> 00:55:01,563
기하 급수적으로 줄어들고 축소됩니다.

643
00:55:01,563 --> 00:55:03,945
우리가 역 전파하고 점점 더 많은 요소들을 선택함에 따라

644
00:55:03,945 --> 00:55:05,349
이 가중치 행렬의

645
00:55:05,349 --> 00:55:08,863
이를 사라지는 그라데이션 문제라고합니다.

646
00:55:08,863 --> 00:55:11,172
사람들이 때로는 해킹하는 비트가 있습니다.

647
00:55:11,172 --> 00:55:12,836
폭발 그라데이션 문제를 해결하는 방법

648
00:55:12,836 --> 00:55:14,208
그라디언트 클리핑,

649
00:55:14,208 --> 00:55:16,264
이것은 단순한 발견 적 방법 일 뿐이다.

650
00:55:16,264 --> 00:55:18,660
그라디언트를 계산 한 후에,

651
00:55:18,660 --> 00:55:19,713
그 그라데이션,

652
00:55:19,713 --> 00:55:22,156
L2 규범이 어떤 임계 값 이상인 경우,

653
00:55:22,156 --> 00:55:24,112
그런 다음 그것을 단단히 고정시키고 나눕니다.

654
00:55:24,112 --> 00:55:27,955
이 최대 임계 값을 갖도록 클램프를 잠급니다.

655
00:55:27,955 --> 00:55:29,271
이것은 일종의 불쾌한 해킹입니다.

656
00:55:29,271 --> 00:55:30,964
하지만 실제로 실제로 많이 사용됩니다.

657
00:55:30,964 --> 00:55:32,645
재발 성 신경 네트워크를 훈련 할 때.

658
00:55:32,645 --> 00:55:34,517
그리고 그것은 비교적 유용한 도구입니다

659
00:55:34,517 --> 00:55:39,034
폭발하는 그라디언트 문제를 공격했습니다.

660
00:55:39,034 --> 00:55:40,994
그러나 이제 사라지는 그라디언트 문제에 대해,

661
00:55:40,994 --> 00:55:42,254
우리가 일반적으로하는 일

662
00:55:42,254 --> 00:55:43,640
우리가로 이동해야 할 수도 있습니다

663
00:55:43,640 --> 00:55:46,207
보다 복잡한 RNN 아키텍처.

664
00:55:46,207 --> 00:55:48,939
그래서 이것은 LSTM에 대한이 아이디어에 동기를 부여합니다.

665
00:55:48,939 --> 00:55:53,524
Long Short Term
Memory를 나타내는 LSTM

666
00:55:53,524 --> 00:55:55,700
약간 재밌는 반복 관계인가?

667
00:55:55,700 --> 00:55:58,316
이러한 재발 성 신경 네트워크에 대해서.

668
00:55:58,316 --> 00:55:59,984
그것은 정말로 완화시키는 것을 돕기 위해 디자인되었습니다.

669
00:55:59,984 --> 00:56:03,330
사라지고 폭발하는 그라디언트의 문제.

670
00:56:03,330 --> 00:56:05,591
그래서 그것의 위에 해킹의 종류보다,

671
00:56:05,591 --> 00:56:07,794
우리는 단지 아키텍처를 디자인합니다.

672
00:56:07,794 --> 00:56:10,193
그래디언트 유동 특성이 더 좋다.

673
00:56:10,193 --> 00:56:13,566
그 멋진 CNN 아키텍처에 대한 유추의 종류

674
00:56:13,566 --> 00:56:16,556
우리가 강의의 꼭대기에서 보았던 것.

675
00:56:16,556 --> 00:56:17,389
지적 할 또 다른 점

676
00:56:17,389 --> 00:56:20,807
LSTM 세포가 실제로 1997 년부터 온다는 것입니다.

677
00:56:20,807 --> 00:56:22,337
그래서 LSTM에 대한이 아이디어

678
00:56:22,337 --> 00:56:24,073
꽤 오랫동안 주변에 있었고,

679
00:56:24,073 --> 00:56:26,108
이 사람들은 이러한 아이디어를 연구하고있었습니다.

680
00:56:26,108 --> 00:56:27,110
90 년대에

681
00:56:27,110 --> 00:56:28,852
확실히 커브보다 앞서있었습니다.

682
00:56:28,852 --> 00:56:31,225
이 모델들은 지금 어디서나 사용되기 때문에

683
00:56:31,225 --> 00:56:32,475
20 년 후.

684
00:56:33,864 --> 00:56:37,921
LSTM에는 이런 재미있는 기능적 형태가 있습니다.

685
00:56:37,921 --> 00:56:39,814
이 바닐라가 언제 있었는지 기억해.

686
00:56:39,814 --> 00:56:41,198
재발 성 신경 네트워크,

687
00:56:41,198 --> 00:56:42,538
그것은이 숨겨진 상태였습니다.

688
00:56:42,538 --> 00:56:44,056
그리고 우리는이 반복 관계를 사용했습니다.

689
00:56:44,056 --> 00:56:46,397
매 시간 단계마다 숨겨진 상태를 업데이트합니다.

690
00:56:46,397 --> 00:56:47,570
자, LSTM에서,

691
00:56:47,570 --> 00:56:48,842
우리는 실제로 2,

692
00:56:48,842 --> 00:56:51,462
매 시간 단계마다 두 가지 숨겨진 상태를 유지합니다.

693
00:56:51,462 --> 00:56:52,931
하나는 이것입니다,

694
00:56:52,931 --> 00:56:54,439
숨겨진 상태라고하는,

695
00:56:54,439 --> 00:56:57,334
숨겨진 상태에 비유하는 것입니다.

696
00:56:57,334 --> 00:56:59,305
우리가 바닐라 RNN에서 가지고 있었던

697
00:56:59,305 --> 00:57:02,524
그러나 LSTM은 두 번째 벡터 인 ct를 유지합니다.

698
00:57:02,524 --> 00:57:03,604
세포 상태를 불렀다.

699
00:57:03,604 --> 00:57:06,459
그리고 세포 상태는 일종의 내부 벡터입니다.

700
00:57:06,459 --> 00:57:08,546
LSTM 안에 보관,

701
00:57:08,546 --> 00:57:12,240
그리고 그것은 실제로 외부 세계에 노출되지 않습니다.

702
00:57:12,240 --> 00:57:13,170
그리고 우리는 볼 것입니다,

703
00:57:13,170 --> 00:57:15,260
이 업데이트 방정식을 통해,

704
00:57:15,260 --> 00:57:17,371
우리가 언제,

705
00:57:17,371 --> 00:57:19,235
먼저 우리가 이것을 계산할 때,

706
00:57:19,235 --> 00:57:20,803
우리는 두 개의 입력을받습니다.

707
00:57:20,803 --> 00:57:23,966
우리는이 네 개의 게이트를 계산하기 위해 그것들을 사용합니다.

708
00:57:23,966 --> 00:57:25,485
i, f, o, n, g라고 불리는

709
00:57:25,485 --> 00:57:28,634
이 게이트를 사용하여 셀 상태를 업데이트합니다.

710
00:57:28,634 --> 00:57:30,302
그리고 우리는 우리의 세포 상태의 일부를 드러낸다.

711
00:57:30,302 --> 00:57:33,802
다음 시간 단계에서 숨겨진 상태로

712
00:57:36,704 --> 00:57:38,282
이것은 일종의 재미있는 기능적 형태입니다,

713
00:57:38,282 --> 00:57:40,227
몇 장의 슬라이드를보고 싶다.

714
00:57:40,227 --> 00:57:42,407
정확히 왜 우리는이 아키텍처를 사용합니까?

715
00:57:42,407 --> 00:57:44,014
왜 그것이 합리적인지,

716
00:57:44,014 --> 00:57:45,418
특히 맥락에서

717
00:57:45,418 --> 00:57:47,731
사라지는 또는 폭발하는 그라데이션.

718
00:57:47,731 --> 00:57:51,044
우리가 LSTM에서하는 첫 번째 일

719
00:57:51,044 --> 00:57:54,534
이 이전에 숨겨진 상태 인 ht가 주어진다는 것입니다.

720
00:57:54,534 --> 00:57:57,213
우리는 현재 입력 벡터 인 x t를받습니다.

721
00:57:57,213 --> 00:57:58,611
바닐라 RNN처럼.

722
00:57:58,611 --> 00:58:00,251
바닐라 RNN에서는

723
00:58:00,251 --> 00:58:02,617
우리는 두 개의 입력 벡터를 사용했습니다.

724
00:58:02,617 --> 00:58:03,908
우리는 그들을 연결했다.

725
00:58:03,908 --> 00:58:05,098
그런 다음 행렬 곱셈을했습니다.

726
00:58:05,098 --> 00:58:08,663
RNN에서 다음 숨김 상태를 직접 계산합니다.

727
00:58:08,663 --> 00:58:10,751
이제 LSTM은 조금 다른 것을합니다.

728
00:58:10,751 --> 00:58:13,055
우리는 이전의 숨겨진 상태를 취하려고합니다.

729
00:58:13,055 --> 00:58:14,429
우리의 현재 입력,

730
00:58:14,429 --> 00:58:15,315
그들을 쌓아 라,

731
00:58:15,315 --> 00:58:18,748
이제는 매우 큰 가중치 행렬 w를 곱합니다.

732
00:58:18,748 --> 00:58:21,926
네 개의 다른 게이트를 계산하려면,

733
00:58:21,926 --> 00:58:24,391
모두 숨겨진 상태와 같은 크기입니다.

734
00:58:24,391 --> 00:58:26,193
때로는 다른 방식으로 작성된 것을 볼 수 있습니다.

735
00:58:26,193 --> 00:58:29,549
일부 저자는 다른 가중치 행렬을 쓸 것입니다.

736
00:58:29,549 --> 00:58:30,808
각 게이트마다.

737
00:58:30,808 --> 00:58:31,793
일부 저자는 모두 그들을 결합합니다

738
00:58:31,793 --> 00:58:33,160
하나의 큰 무게 매트릭스로.

739
00:58:33,160 --> 00:58:34,677
그러나 그것은 모두 정말로 같은 것입니다.

740
00:58:34,677 --> 00:58:36,334
아이디어는 우리가 숨겨진 상태를 취하는 것입니다.

741
00:58:36,334 --> 00:58:37,282
우리의 현재 입력,

742
00:58:37,282 --> 00:58:40,288
그런 다음 이들을 사용하여이 네 가지 게이트를 계산합니다.

743
00:58:40,288 --> 00:58:42,163
이 4 개의 문은,

744
00:58:42,163 --> 00:58:45,725
당신은 종종 이것을 i, f, o, g, ifog,

745
00:58:45,725 --> 00:58:47,768
그들이 무엇인지 기억하기가 쉽습니다.

746
00:58:47,768 --> 00:58:49,435
내가 입력 게이트입니다.

747
00:58:50,324 --> 00:58:53,655
그것은 우리 세포에 얼마나 많은
양을 입력하고 싶은지를 말합니다.

748
00:58:53,655 --> 00:58:55,431
F는 잊어 버리는 게이트입니다.

749
00:58:55,431 --> 00:58:57,860
우리가 얼마나 세포 기억을 잊고 싶니?

750
00:58:57,860 --> 00:59:00,194
이전 시간 간격에서 이전 시간 간격으로.

751
00:59:00,194 --> 00:59:01,333
O는 출력 게이트이고,

752
00:59:01,333 --> 00:59:03,327
우리 자신을 얼마나 드러내고 싶은가?

753
00:59:03,327 --> 00:59:04,653
바깥 세상에.

754
00:59:04,653 --> 00:59:07,092
그리고 G는 정말 좋은 이름이 아닙니다.

755
00:59:07,092 --> 00:59:10,130
그래서 저는 보통 게이트 게이트라고 부릅니다.

756
00:59:10,130 --> 00:59:13,109
G, 우리가 얼마나 쓰고 싶어하는지 알려주지.

757
00:59:13,109 --> 00:59:14,626
입력 셀에 입력합니다.

758
00:59:14,626 --> 00:59:16,665
그리고이 4 개의 문 각각이

759
00:59:16,665 --> 00:59:19,665
다른 비선형 성을 사용하고 있습니다.

760
00:59:21,724 --> 00:59:23,809
입력, 잊어 버림 및 출력 게이트

761
00:59:23,809 --> 00:59:25,047
모두 시그 모이 드를 사용하고 있습니다.

762
00:59:25,047 --> 00:59:28,571
이는 값이 0과 1 사이에 있음을 의미합니다.

763
00:59:28,571 --> 00:59:31,109
게이트 게이트는 tanh를 사용하는 반면,

764
00:59:31,109 --> 00:59:34,316
이것은 출력이 -1과 -1 사이임을 의미합니다.

765
00:59:34,316 --> 00:59:36,810
그래서, 이것들은 이상합니다.

766
00:59:36,810 --> 00:59:38,551
하지만 조금 더 의미가 있습니다.

767
00:59:38,551 --> 00:59:41,725
그들 모두를 이진 값으로 상상해보십시오.

768
00:59:41,725 --> 00:59:43,077
맞아, 극단에서 일어나는 일처럼

769
00:59:43,077 --> 00:59:44,744
이 두 값 중?

770
00:59:46,033 --> 00:59:46,866
그것은 일종의 일 이죠.

771
00:59:46,866 --> 00:59:48,490
우리가이 게이트들을 계산하면

772
00:59:48,490 --> 00:59:50,200
이 다음 방정식을 보면,

773
00:59:50,200 --> 00:59:51,423
당신은 우리의 세포 상태를 볼 수 있습니다.

774
00:59:51,423 --> 00:59:53,926
잊어 버린 게이트에 의해 요소가
현명하게 곱해지고 있습니다.

775
00:59:53,926 --> 00:59:56,269
죄송합니다. 이전 시간 단계의 셀 상태

776
00:59:56,269 --> 00:59:59,305
이 잊어 버린 게이트로 요소를 현명하게 곱하고 있습니다.

777
00:59:59,305 --> 01:00:00,350
그리고 지금이 문을 잊어 버리면,

778
01:00:00,350 --> 01:00:03,861
여러분은 그것을 0과 1의 벡터로 생각할 수 있습니다.

779
01:00:03,861 --> 01:00:06,162
그것은 세포 상태의 각 요소에 대해 알려주고 있습니다.

780
01:00:06,162 --> 01:00:08,416
우리가 세포의 그 요소를 잊고 싶습니까?

781
01:00:08,416 --> 01:00:10,644
잊어 버린 게이트가 0 인 경우에?

782
01:00:10,644 --> 01:00:12,819
아니면 세포의 그 요소를 기억하고 싶습니까?

783
01:00:12,819 --> 01:00:14,935
잊어 버린 게이트가 하나 인 경우.

784
01:00:14,935 --> 01:00:17,258
이제 우리가 잊어 버린 문을 사용하면

785
01:00:17,258 --> 01:00:19,767
셀 상태의 부분을 게이트 오프하기 위해,

786
01:00:19,767 --> 01:00:21,088
우리는 두 번째 용어를 가지고 있습니다.

787
01:00:21,088 --> 01:00:24,814
이것은 i와 g의 요소 현명한 산출물이다.

788
01:00:24,814 --> 01:00:27,431
이제 저는이 0과 1의 벡터입니다.

789
01:00:27,431 --> 01:00:28,824
Sigmoid를 통해오고 있기 때문에,

790
01:00:28,824 --> 01:00:31,480
세포 상태의 각 요소에 대해 알려주고,

791
01:00:31,480 --> 01:00:33,909
우리는 셀 상태의 해당 요소에 쓰고 싶습니까?

792
01:00:33,909 --> 01:00:35,728
내가 하나 인 경우,

793
01:00:35,728 --> 01:00:37,771
또는 우리는 셀 상태의 해당 요소에 쓰고 싶지 않습니다.

794
01:00:37,771 --> 01:00:38,687
이 단계에서

795
01:00:38,687 --> 01:00:41,719
i가 0 인 경우

796
01:00:41,719 --> 01:00:42,585
그리고 이제 게이트 게이트,

797
01:00:42,585 --> 01:00:44,254
그것은 tanh를 통해오고 있기 때문에,

798
01:00:44,254 --> 01:00:46,031
1 또는 1이 될 것입니다.

799
01:00:46,031 --> 01:00:47,495
이것이 우리가 원하는 가치입니다.

800
01:00:47,495 --> 01:00:50,500
우리가 글쓰기를 고려할 수있는 후보 값

801
01:00:50,500 --> 01:00:54,022
이 시간 단계에서 셀 상태의 각 요소에 전달합니다.

802
01:00:54,022 --> 01:00:56,098
그런 다음 셀 상태 방정식을 보면,

803
01:00:56,098 --> 01:00:58,157
당신은 매 단계마다,

804
01:00:58,157 --> 01:00:59,963
세포 상태는 이런 종류의

805
01:00:59,963 --> 01:01:02,499
이들 서로 다른 독립적 인 스케일러 값들은,

806
01:01:02,499 --> 01:01:05,230
그들은 모두 하나씩 증가하거나 감소합니다.

807
01:01:05,230 --> 01:01:07,466
그래서 종류가 비슷합니다.

808
01:01:07,466 --> 01:01:09,560
세포 상태 안에서, 우리는 기억할 수있다.

809
01:01:09,560 --> 01:01:11,028
우리 이전의 상태를 잊어 버리거나,

810
01:01:11,028 --> 01:01:13,480
그런 다음 증가 또는 감소시킬 수 있습니다.

811
01:01:13,480 --> 01:01:14,765
그 셀 상태의 각 요소

812
01:01:14,765 --> 01:01:16,535
매 시간 간격마다 하나씩

813
01:01:16,535 --> 01:01:19,430
그래서 당신은 세포 상태의 이러한
요소들을 생각할 수 있습니다.

814
01:01:19,430 --> 01:01:22,189
작은 스케일러 정수 카운터 들로서

815
01:01:22,189 --> 01:01:24,061
증감 할 수있는

816
01:01:24,061 --> 01:01:25,708
각 시간 단계마다.

817
01:01:25,708 --> 01:01:28,489
그리고 이제, 우리가 세포 상태를 계산 한 후에,

818
01:01:28,489 --> 01:01:31,624
그런 다음 우리는 지금 업데이트 된 셀 상태를 사용합니다

819
01:01:31,624 --> 01:01:32,874
숨겨진 상태를 계산하기 위해,

820
01:01:32,874 --> 01:01:36,513
우리는 바깥 세상에 드러 낼 것입니다.

821
01:01:36,513 --> 01:01:38,832
그래서이 세포 상태가이 해석을 가지고 있기 때문에

822
01:01:38,832 --> 01:01:39,884
카운터가되는 것,

823
01:01:39,884 --> 01:01:41,280
하나씩 세는 종류

824
01:01:41,280 --> 01:01:43,353
또는 각 시간 단계에서 마이너스 1,

825
01:01:43,353 --> 01:01:45,518
우리는 그 카운터 값을 스쿼시하려고합니다.

826
01:01:45,518 --> 01:01:48,895
tanh를 사용하여 멋진 0에서 1 범위로

827
01:01:48,895 --> 01:01:50,483
그리고 지금, 우리는 요소 현명하고,

828
01:01:50,483 --> 01:01:51,826
이 출력 게이트에 의해.

829
01:01:51,826 --> 01:01:54,441
그리고 출력 게이트는 다시 S 자형을 통과하게됩니다.

830
01:01:54,441 --> 01:01:57,597
그래서 당신은 거의 0과 1 인
것으로 생각할 수 있습니다.

831
01:01:57,597 --> 01:01:58,947
출력 게이트가 우리에게 알려줍니다.

832
01:01:58,947 --> 01:02:00,826
우리의 세포 상태의 각 요소에 대해,

833
01:02:00,826 --> 01:02:02,773
우리가 계시를 밝히지 않겠습니까?

834
01:02:02,773 --> 01:02:04,614
우리 세포 상태의 그 요소

835
01:02:04,614 --> 01:02:06,994
우리가 외부의 숨겨진 상태를 계산할 때

836
01:02:06,994 --> 01:02:08,577
이 시간 단계.

837
01:02:09,736 --> 01:02:11,609
그리고 저는 전통의 종류가 있다고 생각합니다.

838
01:02:11,609 --> 01:02:13,479
LSTM을 설명하려고하는 사람들에게,

839
01:02:13,479 --> 01:02:14,524
모두가 올 필요가있다.

840
01:02:14,524 --> 01:02:17,132
잠재적으로 혼란스러운 LSTM 다이어그램을 사용합니다.

841
01:02:17,132 --> 01:02:18,882
그래서 여기에 내 시도가있다.

842
01:02:20,380 --> 01:02:23,866
여기서 우리는이 LSTM 셀 내부에서 어떤
일이 벌어지고 있는지를 볼 수 있습니다.

843
01:02:23,866 --> 01:02:24,865
우리가 우리의,

844
01:02:24,865 --> 01:02:27,566
우리는 우리의 이전 셀 상태를 왼쪽에 입력으로 취하고있다.

845
01:02:27,566 --> 01:02:28,937
이전 숨겨진 상태,

846
01:02:28,937 --> 01:02:31,266
뿐만 아니라 우리의 현재 입력, x t.

847
01:02:31,266 --> 01:02:32,537
이제 우리는 현재의,

848
01:02:32,537 --> 01:02:34,985
우리의 이전의 숨겨진 상태,

849
01:02:34,985 --> 01:02:36,453
뿐만 아니라 우리의 현재 입력,

850
01:02:36,453 --> 01:02:37,346
그들을 쌓아 라,

851
01:02:37,346 --> 01:02:39,526
이 가중치 행렬 w를 곱하면,

852
01:02:39,526 --> 01:02:41,166
우리의 네 가지 문을 만들 수 있습니다.

853
01:02:41,166 --> 01:02:42,627
그리고 여기서는 비선형 성을 배제했습니다.

854
01:02:42,627 --> 01:02:44,836
이전 슬라이드에서이 슬라이드를 보았 기 때문입니다.

855
01:02:44,836 --> 01:02:47,294
이제 잊어 버린 게이트는 요소를 현명하게 곱합니다.

856
01:02:47,294 --> 01:02:48,143
세포 상태.

857
01:02:48,143 --> 01:02:51,174
입력 및 게이트 게이트에 엘리먼트를 현명하게 곱합니다.

858
01:02:51,174 --> 01:02:52,689
셀 상태에 추가됩니다.

859
01:02:52,689 --> 01:02:54,524
그리고 그것은 우리에게 다음 세포를줍니다.

860
01:02:54,524 --> 01:02:56,533
다음 셀은 tanh를 통해 부숴지며,

861
01:02:56,533 --> 01:02:58,616
이 출력 게이트와 현저하게 곱해진 소자

862
01:02:58,616 --> 01:03:01,366
우리의 다음 숨겨진 상태를 생산합니다.

863
01:03:02,417 --> 01:03:03,250
문제?

864
01:03:13,116 --> 01:03:14,587
아니, 그래서 그들은 이것을 통해오고있다.

865
01:03:14,587 --> 01:03:17,878
그들은이 무게 매트릭스의 다른 부분에서 왔습니다.

866
01:03:17,878 --> 01:03:19,147
그래서 우리의 숨겨진,

867
01:03:19,147 --> 01:03:23,343
x와 h가 모두이 차원 h 인 경우,

868
01:03:23,343 --> 01:03:24,300
우리가 그들을 쌓은 후에,

869
01:03:24,300 --> 01:03:26,415
그들은 벡터 크기가 2 시간 일 겁니다.

870
01:03:26,415 --> 01:03:28,718
이제 우리의 가중치 행렬이이 행렬이됩니다.

871
01:03:28,718 --> 01:03:30,393
크기가 4 시간 h 인 경우 2 시간.

872
01:03:30,393 --> 01:03:31,873
그래서 당신은 그것을 일종의 것으로 생각할 수 있습니다.

873
01:03:31,873 --> 01:03:34,076
이 무게 매트릭스의 네 청크.

874
01:03:34,076 --> 01:03:37,344
그리고 각각의 무게 매트릭스의 4 개의 덩어리

875
01:03:37,344 --> 01:03:41,511
이 게이트 중 다른 하나를 계산하려고합니다.

876
01:03:42,404 --> 01:03:44,673
당신은 종종 이것을 명확히하기 위해 쓰여진 것을 보게 될 것이며,

877
01:03:44,673 --> 01:03:46,449
네 가지 모두를 결합하는 종류의

878
01:03:46,449 --> 01:03:49,161
가중치 행렬을 하나의 큰 행렬 w로,

879
01:03:49,161 --> 01:03:51,109
표기법의 편의를 위해서입니다.

880
01:03:51,109 --> 01:03:52,484
그러나 그들은 모두 계산됩니다.

881
01:03:52,484 --> 01:03:56,067
가중치 행렬의 다른 부분을 사용합니다.

882
01:03:57,080 --> 01:03:58,645
그러나 당신은 모두 계산된다는 점에서 정확합니다.

883
01:03:58,645 --> 01:04:00,458
동일한 기능적 형태를 사용하여

884
01:04:00,458 --> 01:04:01,658
두 가지를 쌓아 두는 것의

885
01:04:01,658 --> 01:04:04,574
행렬 곱셈을 취한다.

886
01:04:04,574 --> 01:04:06,393
이제 우리는이 그림을 가지고 있습니다.

887
01:04:06,393 --> 01:04:09,519
우리는 LSTM 세포가 어떻게 될지 생각할 수 있습니다.

888
01:04:09,519 --> 01:04:11,196
거꾸로 통과하는 동안?

889
01:04:11,196 --> 01:04:12,795
우리는 바닐라의 맥락에서 보았다.

890
01:04:12,795 --> 01:04:13,738
재발 성 신경 네트워크,

891
01:04:13,738 --> 01:04:15,803
거꾸로 지나가는 동안 나쁜 일이 일어 났고,

892
01:04:15,803 --> 01:04:16,958
우리가 계속적으로

893
01:04:16,958 --> 01:04:18,999
그 무게 매트릭스에 의해, w.

894
01:04:18,999 --> 01:04:20,555
그러나 지금 상황은 많이 보입니다.

895
01:04:20,555 --> 01:04:23,238
LSTM에서는 꽤 다른 점이 있습니다.

896
01:04:23,238 --> 01:04:26,468
이 길을 거꾸로 상상하면

897
01:04:26,468 --> 01:04:28,323
셀 상태의 그래디언트를 계산하고,

898
01:04:28,323 --> 01:04:29,952
우리는 아주 멋진 그림을 얻습니다.

899
01:04:29,952 --> 01:04:32,266
이제 상류의 그라디언트가있을 때

900
01:04:32,266 --> 01:04:33,320
들어오는 세포에서

901
01:04:33,320 --> 01:04:36,281
일단 우리가 뒤로 역 분개하면

902
01:04:36,281 --> 01:04:37,737
이 가산 연산에 의해,

903
01:04:37,737 --> 01:04:40,572
이 추가가 단지 사본임을 기억하십시오.

904
01:04:40,572 --> 01:04:43,663
두 지점으로의 상류 구배,

905
01:04:43,663 --> 01:04:45,641
우리의 업스트림 그래디언트가 직접 복사됩니다.

906
01:04:45,641 --> 01:04:47,954
역 전파로 직접 전달

907
01:04:47,954 --> 01:04:50,435
이 요소를 현명하게 곱하십시오.

908
01:04:50,435 --> 01:04:52,192
그러면 우리의 업스트림 그래디언트가 끝납니다.

909
01:04:52,192 --> 01:04:56,455
잊어 버린 게이트에 의해 현명하게 곱해졌다.

910
01:04:56,455 --> 01:05:00,364
이 셀 상태를 거꾸로 백 프로 퍼 게이트 할 때,

911
01:05:00,364 --> 01:05:01,397
일어나는 유일한 일

912
01:05:01,397 --> 01:05:03,818
우리의 업스트림 셀 상태 그라데이션으로

913
01:05:03,818 --> 01:05:05,935
그것이 현명하게 곱한 요소를 얻는 것을 끝내는 것입니다.

914
01:05:05,935 --> 01:05:07,171
잊어 버린 문으로.

915
01:05:07,171 --> 01:05:09,939
이것은 정말 더 좋네요.

916
01:05:09,939 --> 01:05:12,640
두 가지 이유로 바닐라 RNN보다

917
01:05:12,640 --> 01:05:14,318
하나는 게이트를 잊어 버리는 것입니다.

918
01:05:14,318 --> 01:05:16,488
이제 요소 현명한 곱셈입니다.

919
01:05:16,488 --> 01:05:18,498
전체 행렬 곱셈보다

920
01:05:18,498 --> 01:05:19,923
원소 현명한 곱셈

921
01:05:19,923 --> 01:05:23,205
조금 더 좋을거야.

922
01:05:23,205 --> 01:05:24,964
전체 행렬 곱셈보다

923
01:05:24,964 --> 01:05:27,208
두 번째는 요소 현명한 곱셈입니다.

924
01:05:27,208 --> 01:05:29,710
잠재적으로 다른

925
01:05:29,710 --> 01:05:31,354
매 단계마다 게이트를 잊어 버려라.

926
01:05:31,354 --> 01:05:33,087
그래서 바닐라 RNN에서,

927
01:05:33,087 --> 01:05:35,638
우리는 계속해서 같은 무게 매트릭스를 곱하고있었습니다.

928
01:05:35,638 --> 01:05:36,660
다시 반복하여,

929
01:05:36,660 --> 01:05:38,305
매우 명백하게 이끌어 낸

930
01:05:38,305 --> 01:05:40,563
이러한 폭발적이거나 사라지는 그라디언트.

931
01:05:40,563 --> 01:05:41,943
그러나 현재 LSTM의 경우,

932
01:05:41,943 --> 01:05:45,161
이 잊지 문은 각 시간 단계마다 다를 수 있습니다.

933
01:05:45,161 --> 01:05:47,463
이제는 모델이 훨씬 더 쉽습니다.

934
01:05:47,463 --> 01:05:49,560
이러한 문제를 피하기 위해

935
01:05:49,560 --> 01:05:51,670
폭발 및 사라지는 그라디언트의.

936
01:05:51,670 --> 01:05:53,377
마지막으로,이 게이트를 잊어 버리기 때문에

937
01:05:53,377 --> 01:05:54,902
S 자 결장에서 나오는거야.

938
01:05:54,902 --> 01:05:56,178
이 요소 현명한 곱하기

939
01:05:56,178 --> 01:05:58,438
0과 1 사이에있는 것이 보증됩니다.

940
01:05:58,438 --> 01:06:00,868
다시 한 번 더 좋은 수치 적 속성으로 연결됩니다.

941
01:06:00,868 --> 01:06:02,457
이런 것들로 번식하는 것을 상상한다면

942
01:06:02,457 --> 01:06:04,278
다시 반복하여.

943
01:06:04,278 --> 01:06:07,063
주의해야 할 또 다른 사항은 컨텍스트에서

944
01:06:07,063 --> 01:06:08,909
바닐라 재발 성 신경 네트워크의

945
01:06:08,909 --> 01:06:10,239
우리는 역 통과 동안 그것을 보았습니다,

946
01:06:10,239 --> 01:06:13,146
우리의 그라디언트도 tanh를 통해 흐르고있었습니다.

947
01:06:13,146 --> 01:06:14,273
매 단계마다.

948
01:06:14,273 --> 01:06:15,940
그러나 현재 LSTM에서,

949
01:06:17,459 --> 01:06:18,792
우리의 산출물은,

950
01:06:20,790 --> 01:06:22,452
LSTM에서는 숨겨진 상태가 사용됩니다.

951
01:06:22,452 --> 01:06:24,110
그 출력을 계산하기 위해 y t,

952
01:06:24,110 --> 01:06:26,141
그래서 지금, 각각의 숨겨진 상태,

953
01:06:26,141 --> 01:06:29,229
최종 숨겨진 상태에서 백 프로
퍼 게이트하는 것을 상상한다면

954
01:06:29,229 --> 01:06:31,290
다시 제 1 셀 상태로,

955
01:06:31,290 --> 01:06:33,024
그 후진 경로를 통해,

956
01:06:33,024 --> 01:06:37,396
우리는 단일 tanh non linearity를
통해 backpropagate 만

957
01:06:37,396 --> 01:06:42,044
매 단계마다 별도의 탄을 사용하는 것이 아닙니다.

958
01:06:42,044 --> 01:06:44,059
당신이이 모든 것을 하나로 모을 때의 종류는,

959
01:06:44,059 --> 01:06:45,927
이 거꾸로 패스를 볼 수 있습니다.

960
01:06:45,927 --> 01:06:48,646
셀 상태를 통해 역 전파

961
01:06:48,646 --> 01:06:50,483
그라디언트 슈퍼 고속도로의 일종이다

962
01:06:50,483 --> 01:06:53,205
그래디언트가 상대적으로 방해가되지 않도록합니다.

963
01:06:53,205 --> 01:06:55,047
모델의 맨 끝에서의 손실로부터

964
01:06:55,047 --> 01:06:56,765
다시 초기 셀 상태로 돌아 간다.

965
01:06:56,765 --> 01:06:59,172
모델의 시작 부분에서.

966
01:06:59,172 --> 01:07:00,922
질문 있니?

967
01:07:02,901 --> 01:07:04,693
그래, w와 관련된 그라데이션은 어때?

968
01:07:04,693 --> 01:07:06,792
그것이 궁극적으로 우리가 염려하는 것입니다.

969
01:07:06,792 --> 01:07:08,752
그래서, w에 관한 그라데이션

970
01:07:08,752 --> 01:07:10,252
올 것이다.

971
01:07:11,737 --> 01:07:12,570
매 단계마다,

972
01:07:12,570 --> 01:07:13,771
우리의 현재 셀 상태를 취할 것입니다.

973
01:07:13,771 --> 01:07:15,059
우리의 현재 숨겨진 상태

974
01:07:15,059 --> 01:07:16,272
그리고 그것은 우리에게 요소를 줄 것이다.

975
01:07:16,272 --> 01:07:18,480
그것은 우리에게 우리의 로컬 그라디언트 w를 줄 것이다.

976
01:07:18,480 --> 01:07:19,848
그 시간 단계.

977
01:07:19,848 --> 01:07:21,340
그래서 우리의 세포 상태,

978
01:07:21,340 --> 01:07:23,791
바닐라 RNN 사건에서

979
01:07:23,791 --> 01:07:27,307
우리는 그 첫 스텝 그라디언트를 추가하게 될 것입니다.

980
01:07:27,307 --> 01:07:29,587
w에 대한 최종 그라데이션을 계산합니다.

981
01:07:29,587 --> 01:07:33,139
하지만 지금 상황을 상상해 보면

982
01:07:33,139 --> 01:07:34,961
우리는 매우 긴 서열을 가지고 있습니다.

983
01:07:34,961 --> 01:07:36,405
그리고 우리는 끝까지 기울기만을 얻고 있습니다.

984
01:07:36,405 --> 01:07:37,280
시퀀스의

985
01:07:37,280 --> 01:07:38,945
자, 당신이 백 프로게이트 할 때,

986
01:07:38,945 --> 01:07:40,978
우리는 w에 로컬 그라데이션을 갖습니다.

987
01:07:40,978 --> 01:07:43,219
각 시간 단계마다,

988
01:07:43,219 --> 01:07:44,484
그 지역 그라디언트 w

989
01:07:44,484 --> 01:07:48,506
이 기울기를 통해 c와 h에 올 것입니다.

990
01:07:48,506 --> 01:07:50,994
그래서 우리는 c에 그라데이션을 유지하기 때문에

991
01:07:50,994 --> 01:07:52,751
LSTM의 경우 훨씬 더 훌륭하게,

992
01:07:52,751 --> 01:07:54,988
각 시간 단계에서의 w에 대한 그 지역 구배

993
01:07:54,988 --> 01:07:57,221
또한 앞뒤로 이월됩니다.

994
01:07:57,221 --> 01:07:59,804
시간을 훨씬 더 깔끔하게

995
01:08:01,627 --> 01:08:03,044
다른 질문?

996
01:08:17,428 --> 01:08:18,645
그래, 그 질문은

997
01:08:18,645 --> 01:08:19,886
비선형 성 때문에,

998
01:08:19,886 --> 01:08:22,088
이것은 여전히 사라지는 그라데이션에 취약 할 수 있습니까?

999
01:08:22,089 --> 01:08:24,077
그리고 그럴 수도 있습니다.

1000
01:08:24,077 --> 01:08:26,176
사실, 당신이 상상할 수있는 한 가지 문제가 있습니다.

1001
01:08:26,176 --> 01:08:27,560
이게 문을 잊어 버린 것일 수도 있습니다.

1002
01:08:27,560 --> 01:08:29,322
항상 0보다 작다.

1003
01:08:29,323 --> 01:08:30,252
또는 항상 하나 미만,

1004
01:08:30,252 --> 01:08:31,411
사라지는 그라디언트가 나타날 수 있습니다.

1005
01:08:31,411 --> 01:08:34,103
당신이 끊임없이 잊어 버린이 문을 통과 할 때.

1006
01:08:34,103 --> 01:08:35,960
사람들이 실제로하는 일종의 트릭입니다.

1007
01:08:35,960 --> 01:08:38,513
때로는,

1008
01:08:38,513 --> 01:08:40,689
망각문의 편향을 초기화하다.

1009
01:08:40,689 --> 01:08:42,746
다소 긍정적이다.

1010
01:08:42,746 --> 01:08:44,004
그래서 훈련 초반에,

1011
01:08:44,004 --> 01:08:46,305
그 잊지 문은 항상 하나에 가깝습니다.

1012
01:08:46,305 --> 01:08:48,118
적어도 훈련 시작 부분에,

1013
01:08:48,118 --> 01:08:52,285
다음 우리는 그렇게하지 않았습니다,
상대적으로 깨끗한 그라디언트 흐름

1014
01:08:53,265 --> 01:08:54,381
이 잊혀진 문을 통해,

1015
01:08:54,381 --> 01:08:56,631
그들은 모두 하나에 가까워 지도록
초기화 되었기 때문입니다.

1016
01:08:56,631 --> 01:08:58,934
그리고 교육 기간 내내,

1017
01:08:58,935 --> 01:09:00,308
그 모델은 그 편향을 배울 수있다.

1018
01:09:00,308 --> 01:09:02,742
그리고 그것이 필요한 곳을 잊는 법을 배웁니다.

1019
01:09:02,742 --> 01:09:04,886
당신은 여전히 잠재력이있을 수 있습니다.

1020
01:09:04,886 --> 01:09:06,246
여기에 사라지는 그라데이션이 있습니다.

1021
01:09:06,246 --> 01:09:07,569
그러나 그것은 훨씬 덜 극단적이다.

1022
01:09:07,569 --> 01:09:09,182
바닐라 RNN의 경우보다

1023
01:09:09,182 --> 01:09:12,126
둘 다 fs가 각 시간 단계마다 다를 수 있기 때문에,

1024
01:09:12,126 --> 01:09:14,590
우리가하고 있기 때문에 또한

1025
01:09:14,590 --> 01:09:15,620
이 엘리먼트 현명한 곱셈

1026
01:09:15,620 --> 01:09:19,126
전체 행렬 곱셈보다

1027
01:09:19,126 --> 01:09:20,801
그래서 당신은이 LSTM

1028
01:09:20,801 --> 01:09:23,048
실제로는 ResNet과 매우 유사합니다.

1029
01:09:23,048 --> 01:09:24,510
이 잔여 네트워크에서,

1030
01:09:24,510 --> 01:09:26,732
이 신원 연결 경로가있었습니다.

1031
01:09:26,732 --> 01:09:28,069
네트워크를 통해 뒤로 이동

1032
01:09:28,069 --> 01:09:30,362
그리고 그것은 그라데이션 슈퍼 고속도로의 일종을주었습니다.

1033
01:09:30,363 --> 01:09:32,303
그라디언트가 ResNet에서 역방향으로 흐르도록합니다.

1034
01:09:32,303 --> 01:09:34,924
그리고 이제는 LSTM에서 똑같은 직감입니다.

1035
01:09:34,924 --> 01:09:37,944
이 첨가물과 원소가 현명한 곳에

1036
01:09:37,944 --> 01:09:40,226
셀 상태의 곱셈 적 상호 작용

1037
01:09:40,227 --> 01:09:42,305
비슷한 그라디언트 슈퍼 하이웨이를 줄 수있다.

1038
01:09:42,305 --> 01:09:44,328
그라디언트가 셀 상태를 거쳐 후방으로 흐를 때

1039
01:09:44,328 --> 01:09:45,245
LSTM에서.

1040
01:09:46,343 --> 01:09:48,593
그런데 다른 종류의 멋진 종이가 있습니다.

1041
01:09:48,593 --> 01:09:49,682
소위 고속도로 네트워크,

1042
01:09:49,682 --> 01:09:51,808
이 아이디어 사이에 일종의

1043
01:09:51,808 --> 01:09:53,225
이 LSTM 셀

1044
01:09:54,138 --> 01:09:56,471
및 이러한 잔여 네트워크.

1045
01:09:57,796 --> 01:09:58,697
그래서이 고속도로 네트워크

1046
01:09:58,697 --> 01:10:01,165
실제로 잔여 네트워크 전에왔다.

1047
01:10:01,165 --> 01:10:03,008
그리고 그들은이 생각을 어디서 가지고 있었습니까

1048
01:10:03,008 --> 01:10:04,541
고속도로 네트워크의 모든 계층에서,

1049
01:10:04,541 --> 01:10:05,984
우리는

1050
01:10:05,984 --> 01:10:07,373
후보 활성화의 일종,

1051
01:10:07,373 --> 01:10:08,804
게이팅 기능

1052
01:10:08,804 --> 01:10:10,792
그것은 우리에게 interprelates

1053
01:10:10,792 --> 01:10:13,045
그 레이어에서 우리의 이전 입력 사이에,

1054
01:10:13,045 --> 01:10:14,676
그 후보 활성화

1055
01:10:14,676 --> 01:10:17,017
우리의 회심을 통해 온 것이거나 그렇지 않은 것.

1056
01:10:17,017 --> 01:10:19,455
실제로 많은 건축 학적 유사점이 있습니다.

1057
01:10:19,455 --> 01:10:20,472
이 사이에,

1058
01:10:20,472 --> 01:10:21,821
사람들은 많은 영감을 얻습니다.

1059
01:10:21,821 --> 01:10:24,363
매우 깊은 CNN 훈련에서

1060
01:10:24,363 --> 01:10:25,196
매우 깊은 RNN

1061
01:10:25,196 --> 01:10:27,688
여기에는 많은 크로스 오버가 있습니다.

1062
01:10:27,688 --> 01:10:31,682
매우 간단히 말해서, 당신은 많은 다른
유형의 분산을 보게 될 것입니다.

1063
01:10:31,682 --> 01:10:33,777
거기에 재발 성 신경 네트워크 아키텍처의

1064
01:10:33,777 --> 01:10:34,675
야생에서.

1065
01:10:34,675 --> 01:10:36,760
아마도 LSTM을 제외하고는 가장 일반적 일 것입니다.

1066
01:10:36,760 --> 01:10:40,853
gated recurrent
unit이라고하는이 GRU입니다.

1067
01:10:40,853 --> 01:10:42,665
그리고 당신은 그 업데이트 방정식을
여기에서 볼 수 있습니다,

1068
01:10:42,665 --> 01:10:45,701
그것은 LSTM과 비슷한 맛을 지니고 있습니다.

1069
01:10:45,701 --> 01:10:49,318
이 곱셈 요소 현명한 문을 사용하는 곳

1070
01:10:49,318 --> 01:10:51,417
이들 첨가물 상호 작용과 함께

1071
01:10:51,417 --> 01:10:53,828
이 사라지는 그라디언트 문제를 피하십시오.

1072
01:10:53,828 --> 01:10:55,584
이 멋진 종이가 있습니다.

1073
01:10:55,584 --> 01:10:57,679
LSTM : 검색 기반 oddysey,

1074
01:10:57,679 --> 01:10:59,734
매우 창의적인 제목,

1075
01:10:59,734 --> 01:11:02,853
그들은 LSTM 방정식을 가지고 놀려고했습니다.

1076
01:11:02,853 --> 01:11:04,980
한 점에서 비선형 성을 교환하고,

1077
01:11:04,980 --> 01:11:06,329
우리가 정말로 그 tanh을 필요로하는 것처럼

1078
01:11:06,329 --> 01:11:07,343
출력 게이트를 노출시키기 위해,

1079
01:11:07,343 --> 01:11:09,756
그리고 그들은이 많은 다른 질문들에 대답하려고 애썼다.

1080
01:11:09,756 --> 01:11:11,367
각각의 비선형 성들에 관하여,

1081
01:11:11,367 --> 01:11:14,017
각각의 LSTM 업데이트 방정식의 조각.

1082
01:11:14,017 --> 01:11:15,374
모델을 변경하면 어떻게됩니까?

1083
01:11:15,374 --> 01:11:18,068
LSTM 방정식을 약간 조정하십시오.

1084
01:11:18,068 --> 01:11:19,038
그리고 결론의 종류는

1085
01:11:19,038 --> 01:11:20,260
그들 모두가 똑같이 일한다는 것

1086
01:11:20,260 --> 01:11:22,414
그들 중 일부는 다른 것들보다 조금 더 잘 작동합니다.

1087
01:11:22,414 --> 01:11:24,521
한 가지 문제 또는 다른 문제

1088
01:11:24,521 --> 01:11:25,735
그러나 일반적으로, 어떤 것도,

1089
01:11:25,735 --> 01:11:28,036
그들이 시도한 LSTM의 비틀기

1090
01:11:28,036 --> 01:11:30,911
원래의 LSTM

1091
01:11:30,911 --> 01:11:32,148
모든 문제에.

1092
01:11:32,148 --> 01:11:33,647
그래서 당신에게 조금 더 믿음을줍니다.

1093
01:11:33,647 --> 01:11:36,505
LSTM 업데이트 방정식은 마술처럼 보일 것입니다.

1094
01:11:36,505 --> 01:11:38,889
그러나 그들은 어쨌든 유용합니다.

1095
01:11:38,889 --> 01:11:41,084
당신은 아마 당신의 문제를 고려해야합니다.

1096
01:11:41,084 --> 01:11:43,692
몇 년 전에 Google의 멋진 종이가 있습니다.

1097
01:11:43,692 --> 01:11:44,575
그들이 사용하려고 시도했던 곳에,

1098
01:11:44,575 --> 01:11:46,520
그들은 진화론 적 탐구를 한 곳에서

1099
01:11:46,520 --> 01:11:48,117
많은 사람들을 수색했는데,

1100
01:11:48,117 --> 01:11:52,605
매우 많은 수의 무작위 RNN 아키텍처에서,

1101
01:11:52,605 --> 01:11:55,694
그들은 일종의 무작위로 이러한
업데이트 방정식을 전제로합니다.

1102
01:11:55,694 --> 01:11:57,936
덧셈과 곱셈을 시도해보십시오.

1103
01:11:57,936 --> 01:11:59,332
및 게이트 및 비선형 성

1104
01:11:59,332 --> 01:12:00,860
다른 종류의 조합.

1105
01:12:00,860 --> 01:12:03,288
그들은 거대한 Google 클러스터를
통해이를 폭발 시켰습니다.

1106
01:12:03,288 --> 01:12:04,351
그리고 방금 총알을 시험해 보았습니다.

1107
01:12:04,351 --> 01:12:08,570
다양한 풍미의 다양한 계량 업데이트가 제공됩니다.

1108
01:12:08,570 --> 01:12:09,742
그리고 다시, 그것은 같은 이야기였습니다.

1109
01:12:09,742 --> 01:12:11,299
그들은 아무것도 찾지 못했다.

1110
01:12:11,299 --> 01:12:12,607
그게 훨씬 낫다.

1111
01:12:12,607 --> 01:12:15,446
이러한 기존의 GRU 또는 LSTM 스타일보다

1112
01:12:15,446 --> 01:12:16,978
작동하는 유사 콘텐츠가 있지만

1113
01:12:16,978 --> 01:12:19,518
특정 문제에 대해서는 약간 더
좋거나 나 빠를 수 있습니다.

1114
01:12:19,518 --> 01:12:21,304
그러나 멀리 떨어져 나가는 종류는

1115
01:12:21,304 --> 01:12:24,252
아마도 LSTM 또는 GRU를 사용하여

1116
01:12:24,252 --> 01:12:27,080
그 방정식에 그다지 마술이 아닙니다.

1117
01:12:27,080 --> 01:12:29,292
그래디언트 흐름을 적절하게 관리하는이 아이디어는

1118
01:12:29,292 --> 01:12:30,633
이러한 첨가제 연결을 통해

1119
01:12:30,633 --> 01:12:32,023
및 이들 승산 게이트들

1120
01:12:32,023 --> 01:12:33,356
매우 유용합니다.

1121
01:12:34,888 --> 01:12:37,286
그래, 요약하면 RNN은 매우 멋지다.

1122
01:12:37,286 --> 01:12:40,103
새로운 종류의 문제를 공격 할 수 있습니다.

1123
01:12:40,103 --> 01:12:42,024
때로는 사라질 수 있습니다.

1124
01:12:42,024 --> 01:12:43,431
또는 폭발 그라디언트.

1125
01:12:43,431 --> 01:12:44,740
하지만 우리는 체중 감량으로 해결할 수 있습니다.

1126
01:12:44,740 --> 01:12:47,412
그리고 더 매끈한 아키텍처.

1127
01:12:47,412 --> 01:12:48,899
그리고 멋진 오버랩이 많이 있습니다.

1128
01:12:48,899 --> 01:12:52,043
CNN 아키텍쳐와 RNN 아키텍쳐 사이.

1129
01:12:52,043 --> 01:12:53,856
그래서 다음에, 당신은 중간 고사를 할 것입니다.

1130
01:12:53,856 --> 01:12:57,856
하지만 그 후에, 우리는 미안하다, 질문을 할까?

1131
01:12:59,525 --> 01:13:00,801
중간 고사는이 강연 후입니다.

1132
01:13:00,801 --> 01:13:04,142
그래서이 시점까지는 공정한 게임입니다.

1133
01:13:04,142 --> 00:00:00,000
그리고 여러분, 화요일 중간 고사에 행운을 빈다.

