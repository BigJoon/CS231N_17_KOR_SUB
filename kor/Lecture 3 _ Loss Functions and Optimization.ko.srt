1
00:00:07,755 --> 00:00:11,682
CS231N 3강입니다.

2
00:00:08,555 --> 00:00:12,482


3
00:00:11,682 --> 00:00:14,594
오늘은 손실함수(Loss functions)와
최적화(Optimization)을 배울겁니다.

4
00:00:12,482 --> 00:00:15,394


5
00:00:14,594 --> 00:00:20,129
진도 나가기에 앞서 공지사항을 전달합니다.

6
00:00:15,394 --> 00:00:17,520


7
00:00:17,520 --> 00:00:19,762


8
00:00:19,762 --> 00:00:20,929


9
00:00:21,094 --> 00:00:24,547
먼저, 첫 번째 과제가 생겼습니다.

10
00:00:21,894 --> 00:00:25,347


11
00:00:24,547 --> 00:00:26,889
웹사이트에서 확인하실 수 있습니다.

12
00:00:25,347 --> 00:00:27,689


13
00:00:26,889 --> 00:00:30,086
과제를 좀 늦게 내드렸기 때문에

14
00:00:27,689 --> 00:00:29,176


15
00:00:30,086 --> 00:00:35,264
4월 20일 목요일 오후 11시 59분으로 기한을 변경합니다.

16
00:00:30,886 --> 00:00:33,981


17
00:00:33,981 --> 00:00:36,064


18
00:00:36,374 --> 00:00:42,702
앞으로 2주정도 라고 보시면 됩니다.

19
00:00:37,174 --> 00:00:40,081


20
00:00:40,081 --> 00:00:43,502


21
00:00:42,702 --> 00:00:49,087
그리고 변경된 기한으로 강의계획서를 업데이트하겠습니다.

22
00:00:43,502 --> 00:00:47,299


23
00:00:47,299 --> 00:00:49,887


24
00:00:49,087 --> 00:00:54,617
과제를 완료하면 zip파일로 Canvas에 업로드해야 하며

25
00:00:49,887 --> 00:00:51,825


26
00:00:51,825 --> 00:00:55,417


27
00:00:54,617 --> 00:00:57,779
가능한 빨리 채점을 하도록 하겠습니다.

28
00:00:55,417 --> 00:00:57,579


29
00:00:57,579 --> 00:00:58,579


30
00:00:58,799 --> 00:01:04,879
다음 사항은 여러분이 공지사항을 확인하려면
항상 Piazza를 체크해야 한다는 것입니다.

31
00:00:59,599 --> 00:01:04,233


32
00:01:04,879 --> 00:01:11,432
이번주에 Piazza에 몇가지 예제 프로젝트를
공고해 놓을 것입니다.

33
00:01:05,679 --> 00:01:08,588


34
00:01:08,588 --> 00:01:12,232


35
00:01:11,432 --> 00:01:17,220
그래서 Stanford 커뮤니티와, 혹은 Stanford와 연계된
분들로부터 프로젝트 아이디와 관련된 예시를 부탁드렸는데,

36
00:01:12,232 --> 00:01:15,730


37
00:01:15,730 --> 00:01:18,020


38
00:01:17,220 --> 00:01:24,583
그리고 그 분들이 이 수업을 듣는 학생들과 함께 프로젝트를
진행하고 싶다는 제안을 해주셨습니다.

39
00:01:18,020 --> 00:01:20,951


40
00:01:20,951 --> 00:01:23,713


41
00:01:24,583 --> 00:01:26,986
그러니 Pizza에 공고를 확인해 보시고

42
00:01:25,383 --> 00:01:27,786


43
00:01:26,986 --> 00:01:34,231
하고싶은 프로젝트가 있으면 자유롭게
프로젝트 멘토와 직접 얘기하면 되겠습니다.

44
00:01:27,786 --> 00:01:31,384


45
00:01:31,384 --> 00:01:35,031


46
00:01:34,231 --> 00:01:37,090
추가적으로 강의 홈피에 근무시간을
게시하였습니다.

47
00:01:35,031 --> 00:01:37,890


48
00:01:37,090 --> 00:01:45,077
구글 캔린더로 게시하였고 학생들이 근무시간에 대해
많이 물어봐서 이번에 게시하였습니다.

49
00:01:37,890 --> 00:01:41,556


50
00:01:41,556 --> 00:01:45,877


51
00:01:45,077 --> 00:01:48,307
마지막 공지는 구글 클라우드 관련된 것입니다.

52
00:01:45,877 --> 00:01:49,107


53
00:01:48,307 --> 00:01:54,331
구글 클라우드에서 우리 강의를 지원하므로

54
00:01:49,107 --> 00:01:52,545


55
00:01:52,545 --> 00:01:55,131


56
00:01:54,331 --> 00:02:00,687
여러분의 과제와 프로젝트를 수행하기 위해
구글 클라우드 크래딧 100$를 지급하였으며

57
00:01:55,131 --> 00:01:57,833


58
00:01:57,833 --> 00:02:01,487


59
00:02:00,687 --> 00:02:05,246
사용하는 방법은 추후 Piazza에 공지하겠습니다.

60
00:02:01,487 --> 00:02:06,046


61
00:02:05,246 --> 00:02:11,977
공지사항에 대한 질문이 없으면 진도를 나가겠습니다.

62
00:02:06,046 --> 00:02:08,610


63
00:02:08,610 --> 00:02:12,777


64
00:02:13,440 --> 00:02:14,273
좋습니다.

65
00:02:14,240 --> 00:02:15,073


66
00:02:15,559 --> 00:02:17,997
저번 2강을 복습해보면

67
00:02:16,359 --> 00:02:18,797


68
00:02:17,997 --> 00:02:20,412
인식에서의 어려운 점을 살펴보았고

69
00:02:18,797 --> 00:02:21,212


70
00:02:20,412 --> 00:02:24,476
데이터 중심의 방식에 초점을 마춰보았습니다.

71
00:02:21,212 --> 00:02:23,002


72
00:02:23,002 --> 00:02:25,276


73
00:02:24,476 --> 00:02:29,160
그리고 이미지 분류에 대해 배웠고
왜 이미지 분류가 어려운지, 그리고

74
00:02:25,276 --> 00:02:27,721


75
00:02:27,721 --> 00:02:29,960


76
00:02:29,160 --> 00:02:35,812
컴퓨터가 보는것과 사람이 보는 것의 차이가 있다는 것도 배웠습니다.

77
00:02:29,960 --> 00:02:34,002


78
00:02:34,002 --> 00:02:36,612


79
00:02:35,812 --> 00:02:42,124
그리고 분류를 어렵게하는 조명의 변화, 변형에 대해 다뤘고
왜 이게 그토록 어려운지에 대해 배웠습니다.

80
00:02:36,612 --> 00:02:38,445


81
00:02:38,445 --> 00:02:40,757


82
00:02:40,757 --> 00:02:42,924


83
00:02:42,124 --> 00:02:47,912
인간의 시각체계는 이 일을 엄청 쉽게 하는데도 말이죠

84
00:02:42,924 --> 00:02:44,986


85
00:02:44,986 --> 00:02:48,712


86
00:02:47,912 --> 00:02:50,421
그리고 KNN 분류기도 배웠습니다.

87
00:02:48,712 --> 00:02:51,221


88
00:02:50,421 --> 00:02:55,309
데이터 중심 접근방법 중에 가장 단순한 방법이었죠.

89
00:02:51,221 --> 00:02:54,289


90
00:02:54,289 --> 00:02:56,109


91
00:02:55,309 --> 00:03:00,824
그리고 CIFAR-10도 배웠습니다. 저기 좌상단에 보이는
이미지들이죠.

92
00:02:56,109 --> 00:02:58,792


93
00:02:58,792 --> 00:03:01,624


94
00:03:00,824 --> 00:03:05,787
CIFAR-10은 비행기, 자동차 등 10개의 카테고리가 있습니다.

95
00:03:01,624 --> 00:03:04,488


96
00:03:04,488 --> 00:03:06,587


97
00:03:05,787 --> 00:03:11,202
그리고 어떻게 KNN을 이용해서 학습 데이터를 가지고
각 카테고리 클래스를 분류하는

98
00:03:06,587 --> 00:03:09,427


99
00:03:09,427 --> 00:03:12,002


100
00:03:11,202 --> 00:03:15,746
결정 경계를 학습시킬 수 있는지도 배웠습니다.

101
00:03:12,002 --> 00:03:14,404


102
00:03:14,404 --> 00:03:16,546


103
00:03:15,746 --> 00:03:25,190
또한 크로스 벨리데이션데 대해서도 배웠고 트레인, 벨리데이션,
테스트셋으로 나눠서 하이퍼파라미터를 찾는 법도 배웠습니다.

104
00:03:16,546 --> 00:03:19,399


105
00:03:19,399 --> 00:03:21,755


106
00:03:21,755 --> 00:03:25,990


107
00:03:25,190 --> 00:03:32,410
그리고 2강 마지막에는 Linear classification에 대해 배웠
습니다. Linear classifier는 뉴럴넷의 기본요소입니다.

108
00:03:25,990 --> 00:03:28,008


109
00:03:28,008 --> 00:03:30,857


110
00:03:30,857 --> 00:03:33,210


111
00:03:32,410 --> 00:03:38,538
Linear classifier는 parametric classifier의
일종입니다. parametric classifier라는 것은

112
00:03:33,210 --> 00:03:35,526


113
00:03:35,526 --> 00:03:39,338


114
00:03:38,538 --> 00:03:45,444
트레이닝 데이터의 정보가 파라미터인  "행렬 W"로 축약된
다는 것을 뜻합니다. 그리고 W가 학습되는 것입니다.

115
00:03:39,338 --> 00:03:41,328


116
00:03:41,328 --> 00:03:44,146


117
00:03:44,146 --> 00:03:46,244


118
00:03:45,444 --> 00:03:48,448
그리고 이 Linear classifier는 엄청 간단합니다.

119
00:03:46,244 --> 00:03:49,248


120
00:03:48,448 --> 00:03:51,810
이미지를 입력받으면 하나의 긴 벡터로 폅니다.

121
00:03:49,248 --> 00:03:51,115


122
00:03:51,810 --> 00:03:54,974
여기 이미지 x가 있고,

123
00:03:52,610 --> 00:03:55,774


124
00:03:54,974 --> 00:04:02,918
이 이미지는 32x32x3 픽셀이고,
이를 긴 열벡터로 펼치게 됩니다.

125
00:03:55,774 --> 00:03:59,095


126
00:03:59,095 --> 00:04:02,051


127
00:04:02,051 --> 00:04:03,718


128
00:04:04,344 --> 00:04:09,722
32x32는 이미지의 높이와 너비이며 3은
이미지의 채널 red, green, blue 입니다.

129
00:04:05,144 --> 00:04:07,203


130
00:04:07,203 --> 00:04:09,023


131
00:04:09,722 --> 00:04:13,561
그리고 파라미터 "행렬 W"가 있는데

132
00:04:10,522 --> 00:04:14,361


133
00:04:13,561 --> 00:04:18,517
이 행렬은 아까 만들었던 이미지 픽셀과 연산하여

134
00:04:14,361 --> 00:04:16,481


135
00:04:16,481 --> 00:04:19,317


136
00:04:18,517 --> 00:04:24,387
CIFAR-10의 각 10개의 클래스에 해당하는
클래스 스코어를 만들어줍니다.

137
00:04:19,317 --> 00:04:21,642


138
00:04:21,642 --> 00:04:25,187


139
00:04:24,387 --> 00:04:29,617
그리고 해석해보면 스코어가 더 큰 값은

140
00:04:25,187 --> 00:04:26,916


141
00:04:26,916 --> 00:04:30,417


142
00:04:29,617 --> 00:04:34,881
가령 고양이 클래스의 스코어가 더 크다는 것은
분류기가 이 이미지가 고양이일것 같다고 생각하는 것입니다.

143
00:04:30,417 --> 00:04:33,147


144
00:04:33,147 --> 00:04:35,681


145
00:04:34,881 --> 00:04:42,443
반대로 개나 자동차 클래스의 스코어가 더 낮다는 것은
이 이미지가 개나 자동차일 확률이 낮다는 것을 의미하는 것입니다.

146
00:04:35,681 --> 00:04:38,350


147
00:04:38,350 --> 00:04:41,353


148
00:04:41,353 --> 00:04:43,243


149
00:04:42,443 --> 00:04:51,625
또한 저번 수업에서 설명이 좀 부족했던 부분이 있었습니다.
Linear clssification을 각 클래스 템플릿으로 보는 것인데

150
00:04:43,243 --> 00:04:46,564


151
00:04:46,564 --> 00:04:50,209


152
00:04:50,209 --> 00:04:52,425


153
00:04:51,625 --> 00:05:02,444
좌하단의 다이어그램을 보시면, 행렬 W에는 각 10개의 클래스와
이미지의 모든 픽셀에 대응하되는 하나씩의 요소가 있습니다.

154
00:04:52,425 --> 00:04:55,128


155
00:04:55,128 --> 00:04:58,299


156
00:04:58,299 --> 00:05:00,411


157
00:05:00,411 --> 00:05:03,244


158
00:05:02,444 --> 00:05:06,554
그리고 이 것은 "이 픽셀이 클래스를 결정하는데
얼마나 중요한 역할을 하는지"를 의미합니다.

159
00:05:03,244 --> 00:05:07,354


160
00:05:06,554 --> 00:05:12,412
행렬 W의 각 행들이 해당하는 그 클래스의
템플릿이 되는 것입니다.

161
00:05:07,354 --> 00:05:10,416


162
00:05:10,416 --> 00:05:13,212


163
00:05:12,412 --> 00:05:22,551
행렬 W의 각 행은 "이미지의 픽셀 값"과 "해당 클래스" 사이의
가중치가 되기 때문에

164
00:05:13,212 --> 00:05:15,479


165
00:05:15,479 --> 00:05:17,724


166
00:05:17,724 --> 00:05:20,540


167
00:05:20,540 --> 00:05:23,351


168
00:05:22,551 --> 00:05:29,900
그렇기 때문에 우리가 각 행을 풀어서 다시 한 이미지로 재구성하면
각 클래스에 대응하는 학습된 템플릿을 볼 수 있었습니다.

169
00:05:23,351 --> 00:05:26,246


170
00:05:26,246 --> 00:05:28,787


171
00:05:28,787 --> 00:05:30,700


172
00:05:29,900 --> 00:05:35,399
그리고 또한 Linear clssification이 고차원 공간에서의
일종의 "결정 경계"를 학습한다는 측면으로 볼 수 있는데,

173
00:05:30,700 --> 00:05:33,324


174
00:05:33,324 --> 00:05:36,199


175
00:05:35,399 --> 00:05:43,774
그 공간의 차원은 이미지의 픽셀 값에 해당하는 것입니다.

176
00:05:36,199 --> 00:05:38,588


177
00:05:38,588 --> 00:05:41,611


178
00:05:41,611 --> 00:05:44,574


179
00:05:43,774 --> 00:05:47,571
지난 시간에 여기까지 진행했습니다.

180
00:05:44,574 --> 00:05:48,371


181
00:05:47,571 --> 00:05:54,141
지난 시간에는 Linear classifier에 대한 간략한 아이디어만
배우고 끝났습니다.

182
00:05:48,371 --> 00:05:51,615


183
00:05:51,615 --> 00:05:54,941


184
00:05:54,141 --> 00:05:57,554
그때는 실제로 행렬W를 어떻게 만드는지는
다루지 않았습니다.

185
00:05:54,941 --> 00:05:58,354


186
00:05:57,554 --> 00:06:02,628
가장 좋은 행렬W를 구하는데 어떻게 트레이닝 데이터를
활용해야 하는지는 언급하지 않았었습니다.

187
00:05:58,354 --> 00:06:00,189


188
00:06:00,189 --> 00:06:03,428


189
00:06:02,628 --> 00:06:08,292
지난 시간에는 임의의 행렬 W를 사용했었고,

190
00:06:03,428 --> 00:06:05,256


191
00:06:05,256 --> 00:06:09,092


192
00:06:08,292 --> 00:06:12,068
그 행렬 W를 가지고 각 이미지에 해당하는
10개의 클래스 스코어를 계산하였습니다.

193
00:06:09,092 --> 00:06:12,868


194
00:06:12,068 --> 00:06:15,597
이 때의 클래스 스코어는 좋을수도, 나쁠수도 있었습니다.
(임의로 정한 것이니까)

195
00:06:12,868 --> 00:06:16,397


196
00:06:15,597 --> 00:06:20,833
여기 간단한 예시가 있는데 세개의 트레이닝 데이터에 대한

197
00:06:16,397 --> 00:06:17,964


198
00:06:17,964 --> 00:06:21,633


199
00:06:20,833 --> 00:06:26,046
임의의 행렬W를 가지고 예측한 10개의 클래스 스코어입니다.

200
00:06:21,633 --> 00:06:25,384


201
00:06:26,046 --> 00:06:29,506
굵게 표시한 각 스코어를 보면 더 좋은것도 있고
나쁜 것도 있다는 것을 볼 수 있습니다.

202
00:06:26,846 --> 00:06:28,647


203
00:06:29,506 --> 00:06:35,924
예컨데 왼쪽의 이미지를 보면 고양이이죠
우리는 인간이니까 아주 쉽게 알 수 있죠.

204
00:06:30,306 --> 00:06:33,144


205
00:06:33,144 --> 00:06:35,042


206
00:06:35,924 --> 00:06:41,068
하지만 고양이에 부여된 확률을 보면,
(확률이 아니라 스코어죠)

207
00:06:36,724 --> 00:06:39,752


208
00:06:39,752 --> 00:06:41,868


209
00:06:41,068 --> 00:06:48,082
이 분류기가 cat에는 2.9점을 줬습니다.

210
00:06:41,868 --> 00:06:44,236


211
00:06:44,236 --> 00:06:48,882


212
00:06:48,082 --> 00:06:51,018
반면 flog에는 3.78점을 줬군요

213
00:06:48,882 --> 00:06:51,818


214
00:06:51,018 --> 00:06:57,920
이 분류기는 별로 좋아보이지 않습니다. 우리는 정답 클래스(고양이)
가 가장 높은 점수가 되는 분류기를 원합니다.

215
00:06:51,818 --> 00:06:53,909


216
00:06:53,909 --> 00:06:56,236


217
00:06:56,236 --> 00:06:58,720


218
00:06:57,920 --> 00:07:02,729
반면 다른 예를 살펴봅시다. 차동차를 보면

219
00:06:58,720 --> 00:07:00,909


220
00:07:00,909 --> 00:07:03,529


221
00:07:02,729 --> 00:07:06,819
자동차 이미지에서는 자동차의 스코어가 제일 높습니다.
좋은 거죠

222
00:07:03,529 --> 00:07:05,193


223
00:07:05,193 --> 00:07:07,619


224
00:07:06,819 --> 00:07:14,357
개구리의 예를 보면 스코어가 -4입니다. 오히려
다른 스코어보다도 훨씬 낮습니다. 엄청 않좋은 것입니다.

225
00:07:07,619 --> 00:07:11,433


226
00:07:11,433 --> 00:07:13,637


227
00:07:14,357 --> 00:07:16,531
이렇게 분석하는 방법은 좋지 않습니다.

228
00:07:15,157 --> 00:07:17,331


229
00:07:16,531 --> 00:07:20,654
스코어를 눈으로 훑으면서 어느게 좋고
어느게 나쁜지 살펴보기만 하는것은 좋은 생각이 아닙니다.

230
00:07:17,331 --> 00:07:19,140


231
00:07:19,140 --> 00:07:21,454


232
00:07:20,654 --> 00:07:25,264
이런 알고리즘을 만들고, 어떤 W가 가장 좋은지를
결정하기 위해서는

233
00:07:21,454 --> 00:07:23,610


234
00:07:23,610 --> 00:07:26,064


235
00:07:25,264 --> 00:07:31,032
지금 만든 W가 좋은지 나쁜지를 정량화 할 방법이 필요합니다.

236
00:07:26,064 --> 00:07:29,660


237
00:07:29,660 --> 00:07:31,832


238
00:07:31,032 --> 00:07:41,987
W를 입력으로 받아서 각 스코어를 확인하고 이 W가 지금 얼마나
거지같은지를 정량적으로 말해주는 것이 바로 손실함수입니다.

239
00:07:31,832 --> 00:07:35,826


240
00:07:35,826 --> 00:07:39,283


241
00:07:39,283 --> 00:07:42,787


242
00:07:41,987 --> 00:07:49,782
이번 강의에서는 Image classification에서 쓸만한
몇가지 손실함수를 소개해 드리도록 하겠습니다.

243
00:07:42,787 --> 00:07:45,467


244
00:07:45,467 --> 00:07:48,093


245
00:07:48,093 --> 00:07:50,582


246
00:07:49,782 --> 00:07:52,683
손실함수가 어떤 일을 해야하는지 한번 생각해 보세요.

247
00:07:50,582 --> 00:07:53,483


248
00:07:52,683 --> 00:07:58,498
임의의 값 W가 얼마나 좋은지 나쁜지를 정량화해 줘야 합니다.

249
00:07:53,483 --> 00:07:57,532


250
00:07:58,498 --> 00:08:04,770
우리가 실제로 원하는 것은, 행렬 W가 될 수 있는
모든 경우의 수에 대해서

251
00:07:59,298 --> 00:08:00,834


252
00:08:00,834 --> 00:08:02,730


253
00:08:02,730 --> 00:08:05,570


254
00:08:04,770 --> 00:08:10,688
"가장 덜 구린"  W가 무엇인지를 찾고싶은 것입니다.

255
00:08:05,570 --> 00:08:08,934


256
00:08:08,934 --> 00:08:11,488


257
00:08:10,688 --> 00:08:12,860
이 과정이 바로 "최적화 과정" 이며

258
00:08:11,488 --> 00:08:13,660


259
00:08:12,860 --> 00:08:16,276
최적화에 대해서는 좀 있다가 다시 다루겠습니다.

260
00:08:13,660 --> 00:08:17,076


261
00:08:16,276 --> 00:08:21,003
일단 문제를 좀 줄여보겠습니다. 클래스 10개는
다루기가 불편하니깐요

262
00:08:17,076 --> 00:08:19,091


263
00:08:19,091 --> 00:08:21,803


264
00:08:21,003 --> 00:08:28,886
이번 강의에서는 클래스를 3개로 하는
toy 데이터셋을 사용하겠습니다.

265
00:08:21,803 --> 00:08:24,731


266
00:08:24,731 --> 00:08:27,551


267
00:08:27,551 --> 00:08:29,686


268
00:08:28,886 --> 00:08:44,425
이 예제에서 "고양이" 클래스는 잘 분류되지 못했고 "자동차" 는 잘됐고
"개구리"는 최악입니다. 개구리 점수는 다른 것보다 더 낮습니다.

269
00:08:29,686 --> 00:08:33,639


270
00:08:33,639 --> 00:08:38,407


271
00:08:38,407 --> 00:08:41,320


272
00:08:41,320 --> 00:08:45,225


273
00:08:44,425 --> 00:08:48,817
이를 좀 더 공식화해 봅시다.
손실 함수에 대해 생각해보면

274
00:08:45,225 --> 00:08:47,764


275
00:08:47,764 --> 00:08:49,617


276
00:08:48,817 --> 00:08:52,870
트레이닝 데이터 X와 Y가 있고

277
00:08:49,617 --> 00:08:53,670


278
00:08:52,870 --> 00:08:56,196
보통 X는 알고리즘의 입력에 해당하고

279
00:08:53,670 --> 00:08:56,996


280
00:08:56,196 --> 00:09:05,407
Image classification 알고리즘이라면 X는 이미지가 될 것이고
 Y는 예측하고자 하는 것이 될 것입니다.

281
00:08:56,996 --> 00:09:00,004


282
00:09:00,004 --> 00:09:03,862


283
00:09:03,862 --> 00:09:06,207


284
00:09:05,407 --> 00:09:08,930
보통은 Y는 레이블이나 타겟이라고 합니다.

285
00:09:06,207 --> 00:09:09,730


286
00:09:08,930 --> 00:09:16,797
Image classification의 경우라면 각 이미지를 CIFAR-10
의 10개의 카테고리 중 하나로 분류하는 것이고,

287
00:09:09,730 --> 00:09:11,782


288
00:09:11,782 --> 00:09:14,540


289
00:09:14,540 --> 00:09:17,597


290
00:09:16,797 --> 00:09:24,414
여기에서 레이블 y는 1에서 10 사이의 정수 값이 됩니다.
프로그래밍 언어에 따라 0에서 9 일수도 있습니다.

291
00:09:17,597 --> 00:09:19,801


292
00:09:19,801 --> 00:09:22,948


293
00:09:22,948 --> 00:09:25,214


294
00:09:24,414 --> 00:09:30,270
어쨌든 이 y라는 정수값은 각 이미지 x의
정답 카테고리를 의미합니다.

295
00:09:25,214 --> 00:09:27,045


296
00:09:27,045 --> 00:09:31,070


297
00:09:30,270 --> 00:09:42,838
앞서 예측 함수를 정의했었죠 입력 이미지 x와 행렬 W를 입력으로 받아서
y를 예측하는 것입니다.

298
00:09:31,070 --> 00:09:35,284


299
00:09:35,284 --> 00:09:37,693


300
00:09:37,693 --> 00:09:41,769


301
00:09:41,769 --> 00:09:43,638


302
00:09:42,838 --> 00:09:46,446
Image classification 문제라면 y는 10개가 되겠습니다.
(CIFAR-10의 경우)

303
00:09:43,638 --> 00:09:45,235


304
00:09:45,235 --> 00:09:47,246


305
00:09:46,446 --> 00:09:49,938
그 다음 손실함수 L_i를 정의합니다.

306
00:09:47,246 --> 00:09:50,738


307
00:09:49,938 --> 00:09:56,804
앞서 말씀드린 예측함수 f와 정답 값 Y를 입력으로 받아서는

308
00:09:50,738 --> 00:09:53,400


309
00:09:53,400 --> 00:09:54,983


310
00:09:54,983 --> 00:09:57,604


311
00:09:56,804 --> 00:10:02,510
이 트레이닝 샘플을 얼마나 구리게 예측하는지를
정량화 시켜 줍니다.

312
00:09:57,604 --> 00:10:00,112


313
00:10:00,112 --> 00:10:03,310


314
00:10:02,510 --> 00:10:10,478
그리고 최종 Loss인 "L"은 우리 데이터 셋에서
각 N개의 샘플들의 Loss의 평균이 됩니다.

315
00:10:03,310 --> 00:10:06,927


316
00:10:06,927 --> 00:10:09,776


317
00:10:10,478 --> 00:10:13,432
이 함수는 아주 일반적인 공식입니다.

318
00:10:11,278 --> 00:10:14,232


319
00:10:13,432 --> 00:10:16,421
그리고 Image classification 외에도 다양하게
확장할 수 있습니다.

320
00:10:14,232 --> 00:10:17,221


321
00:10:16,421 --> 00:10:24,426
좀 더 나아가서 다른 딥러닝 알고리즘을 살펴보자면
어떤 알고리즘이던 가장 일반적으로 진행되는 일은,

322
00:10:17,221 --> 00:10:19,818


323
00:10:19,818 --> 00:10:22,123


324
00:10:22,123 --> 00:10:25,226


325
00:10:24,426 --> 00:10:33,535
일단 어떤 X와 Y가 존재하고, 여러분이 만들 파라미터 W에
W가 얼마나 좋은지를 정량화하는 손실 함수를 만드는 것입니다.

326
00:10:25,226 --> 00:10:27,639


327
00:10:27,639 --> 00:10:30,849


328
00:10:30,849 --> 00:10:34,335


329
00:10:33,535 --> 00:10:39,982
결국은 W의 공간을 탐색하면서 의트레이닝 데이터의
 Loss를 최소화하는 어떤 W를 찾게 될 것입니다.

330
00:10:34,335 --> 00:10:36,615


331
00:10:36,615 --> 00:10:40,782


332
00:10:41,081 --> 00:10:49,322
구체적으로 한 손실함수의 예를 들어보겠습니다.
이 손실함수는 Image classification에 아주 적합하기도 합니다.

333
00:10:41,881 --> 00:10:46,330


334
00:10:46,330 --> 00:10:50,122


335
00:10:49,322 --> 00:10:52,755
multi-class SVM loss를 대해 알아보겠습니다.

336
00:10:50,122 --> 00:10:53,555


337
00:10:52,755 --> 00:10:59,602
아마 CS229에서 이진 SVM, sopport vector machine을
본 적이 있을 것입니다.

338
00:10:53,555 --> 00:10:57,069


339
00:10:57,069 --> 00:11:00,402


340
00:11:00,662 --> 00:11:05,263
multi-class SVM은 여러 클래스를 다루기 위한
이진 SVM의 일반화된 형태입니다.

341
00:11:01,462 --> 00:11:06,063


342
00:11:05,263 --> 00:11:11,797
CS229에서 다뤘던 이진 SVM은 두개의 클래스만 다룹니다.

343
00:11:06,063 --> 00:11:10,247


344
00:11:10,247 --> 00:11:12,597


345
00:11:11,797 --> 00:11:15,159
각 데이터는 Positive 또는 Negative로 분류될 뿐입니다.

346
00:11:12,597 --> 00:11:14,550


347
00:11:15,159 --> 00:11:20,762
하지만 여긴 10개의 카테고리가 있기 때문에
여러개의 클래스를 다루려면 이 개념을 좀더 일반화 시켜야 합니다.

348
00:11:15,959 --> 00:11:18,415


349
00:11:18,415 --> 00:11:21,562


350
00:11:20,762 --> 00:11:24,854
그렇기 때문에 multi-class SVM의 손실함수는
조금 재밌게 생겼습니다.

351
00:11:21,562 --> 00:11:25,654


352
00:11:24,854 --> 00:11:29,801
지금부터는 이에 대해 좀 더 자세히 알아보도록 하겠습니다.

353
00:11:25,654 --> 00:11:27,006


354
00:11:27,006 --> 00:11:30,601


355
00:11:29,801 --> 00:11:35,584
각각의 트레이닝 데이터에서 Loss L_i
를 구하는 방법 부터 살펴보겠습니다.

356
00:11:30,601 --> 00:11:33,894


357
00:11:33,894 --> 00:11:36,384


358
00:11:35,584 --> 00:11:43,373
L_i를 구하기 위해 우선  "True인 카테고리" 를 제외한
"나머지 카테고리 Y"의 합을 구합니다.

359
00:11:36,384 --> 00:11:41,098


360
00:11:41,098 --> 00:11:44,173


361
00:11:43,373 --> 00:11:46,204
다시말해 맞지 않는 카테고리를 전부 합치는 것입니다.

362
00:11:44,173 --> 00:11:47,004


363
00:11:46,204 --> 00:11:52,509
그리고 올바른 카테고리의 스코어와 올바르지 않은 카테고리의
스코어를 비교해 봅니다.

364
00:11:47,004 --> 00:11:49,242


365
00:11:49,242 --> 00:11:51,046


366
00:11:51,046 --> 00:11:53,309


367
00:11:52,509 --> 00:11:59,589
만약 올바른 카테고리의 점수가 올바르지 않은 카테고리의
점수보다 더 높으면

368
00:11:53,309 --> 00:11:55,879


369
00:11:55,879 --> 00:12:00,389


370
00:11:59,589 --> 00:12:03,814
그리고 그 격차가 일정 마진(safeth margin) 이상이라면,
이 예시에서는 그 마진을 1로 두었죠

371
00:12:00,389 --> 00:12:04,614


372
00:12:03,814 --> 00:12:14,661
이 경우는 True인 스코어가 다른 false 카테고리보다 훨씬 더
크다는 것을 의미합니다.

373
00:12:04,614 --> 00:12:07,949


374
00:12:07,949 --> 00:12:12,140


375
00:12:12,140 --> 00:12:15,461


376
00:12:14,661 --> 00:12:17,663
그렇게 되면 Loss는 0이 됩니다.

377
00:12:15,461 --> 00:12:18,463


378
00:12:17,663 --> 00:12:21,703
이미지 내 정답이 아닌 카테고리의 모든 값들을 합치면

379
00:12:18,463 --> 00:12:22,503


380
00:12:21,703 --> 00:12:27,155
그 값이 바로 한 이미지의 최종 Loss 가 되는 것입니다.

381
00:12:22,503 --> 00:12:25,254


382
00:12:25,254 --> 00:12:27,955


383
00:12:27,155 --> 00:12:31,994
그리고 전체 트레이닝 데이터 셋에서 그 Loss들의 평균을 구합니다.

384
00:12:27,955 --> 00:12:30,511


385
00:12:30,511 --> 00:12:32,794


386
00:12:31,994 --> 00:12:41,263
이걸 수식화 시키면 if-then 으로 표현 할 수 있습니다.
(if:)정답 클래스의 스코어의 점수가 제일 높으면

387
00:12:32,794 --> 00:12:37,737


388
00:12:37,737 --> 00:12:42,063


389
00:12:41,263 --> 00:12:49,327
그럼 "then" 은 다음과 같이 나타낼 수 있습니다.
then, max(0,s_j - s_yj + 1).

390
00:12:42,063 --> 00:12:45,960


391
00:12:45,960 --> 00:12:50,127


392
00:12:50,496 --> 00:12:54,294
그리고 수식을 보다가 헷갈리 때가 있는데

393
00:12:51,296 --> 00:12:53,849


394
00:12:54,294 --> 00:13:00,920
그러면 직접 손으로 써보면서 이해하면 도움이 됩니다.

395
00:12:55,094 --> 00:12:57,478


396
00:12:57,478 --> 00:12:59,436


397
00:12:59,436 --> 00:13:01,720


398
00:13:00,920 --> 00:13:06,805
그리고 0과 다른 값의 최댓값, Max(0, value)
과 같은 식으로 손실 함수를 만드는데,

399
00:13:01,720 --> 00:13:04,589


400
00:13:04,589 --> 00:13:07,605


401
00:13:06,805 --> 00:13:14,627
이런 류의 손실 함수를 "hinge loss"(경첩)라고 부르기도 합니다.
그래프의 모양때문에 붙혀진 이름입니다.

402
00:13:07,605 --> 00:13:10,927


403
00:13:10,927 --> 00:13:14,002


404
00:13:14,627 --> 00:13:22,275
여기에서 x축은 S_Yi입니다.
실제 정답 클래스의 스코어입니다.

405
00:13:15,427 --> 00:13:18,927


406
00:13:19,903 --> 00:13:23,075


407
00:13:22,275 --> 00:13:25,353
그리고 y축은 Loss입니다.

408
00:13:23,075 --> 00:13:26,153


409
00:13:25,353 --> 00:13:34,174
정답 카테고리의 점수가 올라갈수록 Loss가 선형적으로
줄어드는 것을 알 수 있습니다.

410
00:13:26,153 --> 00:13:30,320


411
00:13:31,323 --> 00:13:33,138


412
00:13:33,138 --> 00:13:34,974


413
00:13:34,174 --> 00:13:40,235
이 로스는 0이 된 이후에도 Safety margin 을
넘어설 때 까지 더 줄어듭니다.

414
00:13:34,974 --> 00:13:38,605


415
00:13:38,605 --> 00:13:41,035


416
00:13:40,235 --> 00:13:44,402
Loss가 0이 됐다는 건 클래스를 잘 분류했다는 뜻입니다.

417
00:13:41,035 --> 00:13:45,202


418
00:13:45,550 --> 00:13:47,836
질문 있나요?

419
00:13:46,350 --> 00:13:48,636


420
00:13:47,836 --> 00:13:52,410
[죄송합니다만, S_Yi가 뭔가요?]

421
00:13:48,636 --> 00:13:51,264


422
00:13:51,264 --> 00:13:53,210


423
00:13:52,410 --> 00:13:55,170
[정답 스코어인가요? ]

424
00:13:53,210 --> 00:13:55,970


425
00:13:55,170 --> 00:13:56,321
네 질문은

426
00:13:56,321 --> 00:14:00,097
S가 어떤 것이고 S_Yi가 어떤 것인지 입니다.

427
00:13:57,121 --> 00:14:00,897


428
00:14:00,097 --> 00:14:07,246
S는 분류기의 출력으로 나온 예측된 스코어 입니다.

429
00:14:00,897 --> 00:14:04,706


430
00:14:04,706 --> 00:14:08,046


431
00:14:07,246 --> 00:14:13,942
가령 1이 고양이고 2가 개면
S_1은 고양이 스코어 S_2는 개 스코어 인 것이죠

432
00:14:08,046 --> 00:14:11,734


433
00:14:11,734 --> 00:14:14,742


434
00:14:13,942 --> 00:14:21,056
Y_i는 이미지의 실제 정답 카테고리 입니다. 정수 값이죠

435
00:14:14,742 --> 00:14:18,234


436
00:14:18,234 --> 00:14:21,856


437
00:14:21,056 --> 00:14:32,683
그러니 S_Y_i는 트레이닝 셋의 i번째 이미지의
정답 클래스의 스코어인 것입니다.

438
00:14:21,856 --> 00:14:26,023


439
00:14:26,857 --> 00:14:29,766


440
00:14:29,766 --> 00:14:33,483


441
00:14:32,683 --> 00:14:33,870
질문 있으십니까?

442
00:14:33,870 --> 00:14:35,921
[질문 중]

443
00:14:34,670 --> 00:14:36,721


444
00:14:35,921 --> 00:14:38,677
질문은, 그럼 정확히 무엇을 계산하는 것인지 입니다.

445
00:14:36,721 --> 00:14:39,477


446
00:14:38,677 --> 00:14:44,590
무엇을 계산하는지는 흥미로운 주제입니다.
앞으로 예제를 살펴보면 답을 찾을 수 있을 것입니다.

447
00:14:39,477 --> 00:14:42,425


448
00:14:42,425 --> 00:14:45,390


449
00:14:44,590 --> 00:14:51,984
이 Loss가 말하고자 하는 것은 정답 스코어가
다른 스코어들보다 높으면 좋다는 것입니다.

450
00:14:45,390 --> 00:14:49,475


451
00:14:49,475 --> 00:14:52,784


452
00:14:51,984 --> 00:14:58,364
정답 스코어는 safty margin(여기에서 1) 을 두고
다른 스코어들 보다 훨씬 더 높아야 합니다.

453
00:14:52,784 --> 00:14:55,393


454
00:14:55,393 --> 00:14:59,164


455
00:14:58,364 --> 00:15:06,534
충분히 높지 않으면 Loss가 높아지게 됩니다.

456
00:14:59,164 --> 00:15:02,692


457
00:15:02,692 --> 00:15:07,334


458
00:15:06,534 --> 00:15:13,223
3개의 트레이닝 데이터셋을 가지고 예를 들어보면
좀 더 감을 익힐 수 있을 것입니다.

459
00:15:07,334 --> 00:15:09,457


460
00:15:09,457 --> 00:15:11,400


461
00:15:11,400 --> 00:15:14,023


462
00:15:13,223 --> 00:15:19,513
여기서는 case space notation을 제거하고
zero one notation을 사용했습니다.

463
00:15:14,023 --> 00:15:16,896


464
00:15:16,896 --> 00:15:20,313


465
00:15:19,513 --> 00:15:26,186
맨 왼쪽의 예시를 보면서 multi-class SVM loss가
계산 되는 과정을 살펴보자면

466
00:15:20,313 --> 00:15:21,910


467
00:15:21,910 --> 00:15:25,344


468
00:15:26,186 --> 00:15:31,913
우선 정답이 아닌 클래스를 순회합니다.

469
00:15:26,986 --> 00:15:29,558


470
00:15:29,558 --> 00:15:32,713


471
00:15:31,913 --> 00:15:39,089
Cat은 정답 클래스이니, Car와  Flog 클래스를 순회합니다.
이제 다시 Car의 경우를 살펴보면

472
00:15:32,713 --> 00:15:35,722


473
00:15:35,722 --> 00:15:39,889


474
00:15:41,088 --> 00:15:48,782
이제 (Car 스코어) 5.1 - (Cat 스코어) 3.2 + 1(magin)
을 구합니다. Cat과 Car를 비교할때 짐작할 수 있는 것은

475
00:15:41,888 --> 00:15:45,415


476
00:15:45,415 --> 00:15:49,582


477
00:15:49,969 --> 00:15:55,134
Car가 Cat보다 더 높으니까 Loss가 발생할 것이라는 것입니다.

478
00:15:50,769 --> 00:15:53,927


479
00:15:53,927 --> 00:15:55,934


480
00:15:55,134 --> 00:15:59,153
이 고양이 이미지의 예시를 보면

481
00:15:55,934 --> 00:15:59,953


482
00:15:59,153 --> 00:16:03,387
2.9의 손실이 발생합니다.
Cat-Car 와 Car-Flog를 비교해보면

483
00:15:59,953 --> 00:16:02,133


484
00:16:02,133 --> 00:16:04,187


485
00:16:03,387 --> 00:16:08,480
Car는 3.2, Flog는 -1.7의 에러가 발생합니다.

486
00:16:04,187 --> 00:16:07,312


487
00:16:07,312 --> 00:16:09,280


488
00:16:08,480 --> 00:16:16,409
Cat 스코어는 Frog 스코어보다 훨씬 그므로
Loss는 0이라고 할 수 있습니다.

489
00:16:09,280 --> 00:16:12,361


490
00:16:12,361 --> 00:16:15,528


491
00:16:16,409 --> 00:16:23,094
고양이 이미지의 Multiclass-SVM Loss는
이런 클래스쌍의 Loss의 합이 되며

492
00:16:17,209 --> 00:16:21,548


493
00:16:21,548 --> 00:16:23,894


494
00:16:23,094 --> 00:16:27,112
즉 2.9 + 0 = 2.9 가 되는 것입니다.

495
00:16:23,894 --> 00:16:27,912


496
00:16:27,112 --> 00:16:34,567
여기에서 2.9라는 숫자가 "얼마나 분류기가 이 이미지를
구리게 분류하는지" 에 대한 척도가 되겠습니다.

497
00:16:27,912 --> 00:16:31,280


498
00:16:31,280 --> 00:16:32,950


499
00:16:32,950 --> 00:16:35,367


500
00:16:35,795 --> 00:16:41,574
이런 작업은 여러번 반복합니다.
 Car 클래스에 다시한번 해보면

501
00:16:36,595 --> 00:16:37,897


502
00:16:37,897 --> 00:16:42,374


503
00:16:41,574 --> 00:16:52,072
Car와 Cat을 비교해 볼 것입니다. Car 스코어가 Cat 스코어보다 높기
때문에 여기에서 Loss는 0이 될 것입니다.

504
00:16:42,374 --> 00:16:44,851


505
00:16:44,851 --> 00:16:48,005


506
00:16:48,005 --> 00:16:50,894


507
00:16:50,894 --> 00:16:52,872


508
00:16:52,072 --> 00:16:57,557
Car 와 Frog도 비교해보면 Car의 스코어가 Frog보다 훨씬
더 크다는 것을 알 수 있습니다.

509
00:16:52,872 --> 00:16:55,019


510
00:16:55,019 --> 00:16:58,357


511
00:16:57,557 --> 00:17:02,993
그러므로 Car 의 경우는 전체 Loss도 0이 되겠습니다.

512
00:16:58,357 --> 00:17:00,784


513
00:17:00,784 --> 00:17:03,793


514
00:17:02,993 --> 00:17:09,051
이미 다들 감을 잡으셨겠지만 개구리, frog의 경우도 한번 봅시다.

515
00:17:03,793 --> 00:17:06,565


516
00:17:06,565 --> 00:17:09,851


517
00:17:09,051 --> 00:17:14,895
Frog와 Cat을 보면 엄청 큰 Loss가 발생함을 알 수 있습니다.
Flog스코어가 엄청 낮기 때문이죠. Car와의 경우도 같습니다.

518
00:17:09,851 --> 00:17:12,566


519
00:17:12,566 --> 00:17:15,695


520
00:17:14,895 --> 00:17:19,697
또한 Frog 스코어 자체가 엄청 낮기 때문이죠.
결국 전체 Loss는 12.9가 됩니다.

521
00:17:15,695 --> 00:17:19,163


522
00:17:19,164 --> 00:17:20,497


523
00:17:21,150 --> 00:17:26,578
그리고 전체 트레이닝 셋의 최종 Loss는
각 트레이닝 이미지의 Loss들의 평균이 되겠습니다.

524
00:17:21,950 --> 00:17:24,657


525
00:17:24,657 --> 00:17:25,963


526
00:17:26,578 --> 00:17:29,277
대략 5.3정도 나오는 걸 볼 수 있습니다.

527
00:17:27,378 --> 00:17:30,077


528
00:17:29,277 --> 00:17:34,929
이것이 의미하는 바는, 우리의 분류기가 5.3점 만큼 이 트레이닝 셋을
구리게 분류하고 있다는 "정량적 지표" 가 되는 것입니다.

529
00:17:30,077 --> 00:17:32,330


530
00:17:32,330 --> 00:17:35,729


531
00:17:34,929 --> 00:17:36,732
질문 있으십니까?

532
00:17:35,729 --> 00:17:37,532


533
00:17:36,732 --> 00:17:39,152
[질문 중]

534
00:17:37,532 --> 00:17:39,952


535
00:17:39,152 --> 00:17:41,920
질문은 바로, "1을 더하는 것"은 어떻게 결정하는 것인지 입니다.
(safety margin 이 1이었음)

536
00:17:39,952 --> 00:17:42,720


537
00:17:41,920 --> 00:17:43,574
아무 좋은 질문입니다.

538
00:17:43,574 --> 00:17:46,662
이게 아무 숫자나 막 끼워 넣은 것 같아 보이고

539
00:17:44,374 --> 00:17:47,462


540
00:17:46,662 --> 00:17:52,532
이게 또 손실 함수에서 상수 텀으로 자리잡고 있고
그래서 보기에 불쾌할 수도 있는데요

541
00:17:47,462 --> 00:17:49,578


542
00:17:49,578 --> 00:17:51,750


543
00:17:52,532 --> 00:18:00,305
이게 임의로 선택한 숫자같아 보이긴 하지만, 우리는 사실 손실함수의
"스코어가 정확이 몇인지"는 신경쓰지 않습니다.

544
00:17:53,332 --> 00:17:54,650


545
00:17:54,650 --> 00:17:58,669


546
00:17:58,669 --> 00:18:01,105


547
00:18:00,305 --> 00:18:05,389
우리가 궁금한건 여러 스코어 간의 상대적인 차이인 것입니다.

548
00:18:01,105 --> 00:18:03,595


549
00:18:03,595 --> 00:18:06,189


550
00:18:05,389 --> 00:18:08,944
우리가 관심있어 하는건 오로지 정답 스코어가 다른 스코어에 비해
얼마나 더 큰 스코어를 가지고 있는지 입니다.

551
00:18:06,189 --> 00:18:07,518


552
00:18:07,518 --> 00:18:09,744


553
00:18:08,944 --> 00:18:14,742
행렬 W를 전체적으로 스케일링한다고 생각해 보면
결과 스코어도 이에 따라 스케일이 바뀔 것입니다.

554
00:18:09,744 --> 00:18:12,340


555
00:18:12,340 --> 00:18:15,542


556
00:18:14,742 --> 00:18:20,808
이와 관련된 내용이 온라인 코스 노트에 자세히 나와 있습니다.

557
00:18:15,542 --> 00:18:18,339


558
00:18:18,339 --> 00:18:21,608


559
00:18:20,808 --> 00:18:24,362
그걸 보면 1 이라는게 별 상관 없다는 것을 알게 될 것입니다.

560
00:18:21,608 --> 00:18:25,162


561
00:18:24,362 --> 00:18:32,625
1이라는 파라미터는 없어지고, W의 스케일에 의해 상쇄됩니다.

562
00:18:25,162 --> 00:18:28,457


563
00:18:28,457 --> 00:18:30,112


564
00:18:30,112 --> 00:18:33,425


565
00:18:32,625 --> 00:18:35,489
자세한 내용은 온라인 코스노트를 확인해 주시면 되겠습니다.

566
00:18:33,425 --> 00:18:35,456


567
00:18:35,456 --> 00:18:36,289


568
00:18:37,953 --> 00:18:45,374
그 Loss가 실제무 무슨 일을 하는지를 직관적으로 이해하는
것이 중요하므로 다른 질문들로 다시 접근해 보겠습니다.

569
00:18:38,753 --> 00:18:41,174


570
00:18:41,174 --> 00:18:43,319


571
00:18:43,319 --> 00:18:46,174


572
00:18:45,374 --> 00:18:53,349
만약 Car 스코어가 조금 변하면 Loss에는 무슨 일이 일어날까요?

573
00:18:46,174 --> 00:18:49,515


574
00:18:49,515 --> 00:18:54,149


575
00:18:53,349 --> 00:18:54,182
아이디어 있으십니까?

576
00:18:54,149 --> 00:18:54,982


577
00:18:56,479 --> 00:18:59,856
다들 대답하기 무서워 하는것 같네요

578
00:18:57,279 --> 00:19:00,656


579
00:18:59,856 --> 00:19:00,689
정답?

580
00:19:00,689 --> 00:19:04,272
[대답 하는 학생]

581
00:19:01,489 --> 00:19:05,072


582
00:19:06,983 --> 00:19:13,333
대답은 우리가 Car의 스코어를 조금 바꾸더라고
Loss가 바뀌지 않을 것이라고 대답했습니다.

583
00:19:07,783 --> 00:19:10,813


584
00:19:10,813 --> 00:19:14,133


585
00:19:13,333 --> 00:19:21,918
다시 SVM loss를 상기해보면 이 loss는 오직 정답 스코어와
그 외의 스코어와의 차이만 고려했습니다. 따라서 이 경우에는

586
00:19:14,133 --> 00:19:16,426


587
00:19:16,426 --> 00:19:19,873


588
00:19:19,873 --> 00:19:22,718


589
00:19:21,918 --> 00:19:25,506
Car 스코어가 이미 다른 스코어들보다 엄청 높기 때문에

590
00:19:22,718 --> 00:19:26,306


591
00:19:25,506 --> 00:19:35,437
여기 스코어를 조금 바꾼다고 해도, 서로 간의 간격(Margin)은 여전히
유지될 것이고, 결국 Loss는 변하지 않습니다. 계속 0일 것입니다.

592
00:19:26,306 --> 00:19:28,848


593
00:19:28,848 --> 00:19:31,465


594
00:19:31,465 --> 00:19:34,070


595
00:19:34,070 --> 00:19:36,237


596
00:19:36,870 --> 00:19:40,150
다음 질문은 SVM Loss가 가질 수 있는
최댓/최솟값이 어떻게 될까요?

597
00:19:37,670 --> 00:19:40,117


598
00:19:40,117 --> 00:19:40,950


599
00:19:43,265 --> 00:19:44,445
[대답하는 학생]

600
00:19:44,445 --> 00:19:45,764
잘 못들었습니다?

601
00:19:45,764 --> 00:19:52,018
최솟값은 0이 되겠죠. 왜냐하면 모든 클래스에 걸쳐서

602
00:19:46,564 --> 00:19:50,174


603
00:19:50,174 --> 00:19:52,818


604
00:19:52,018 --> 00:19:57,357
정답 클래스의 스코어가 제일 크면
모든 트레이닝 데이터에서 loss가 0이 될테니깐요

605
00:19:52,818 --> 00:19:56,333


606
00:19:56,333 --> 00:19:58,157


607
00:19:57,357 --> 00:20:03,495
그리고 다시 이 손실 함수가 hinge loss 모양이라는 점을 고려해 보면,

608
00:19:58,157 --> 00:20:01,452


609
00:20:01,452 --> 00:20:04,295


610
00:20:03,495 --> 00:20:07,981
만약 정답 클래스 스코어가 엄청 낮은 음수 값을 가지고
있다고 생각해보면, 아마 Loss가 무한대 일 것입니다.

611
00:20:04,295 --> 00:20:06,992


612
00:20:07,981 --> 00:20:11,538
그러니 최솟값은 0이고 최댓값을 무한대 일 것입니다.

613
00:20:08,781 --> 00:20:12,338


614
00:20:11,538 --> 00:20:16,120
또 한가지는, 파라미터를 초기화하고
처음부터 학습시킬때

615
00:20:16,120 --> 00:20:21,242
보통은 행렬W를 임의의 작은 수로 초기화시키는데, 그렇게되면

616
00:20:18,815 --> 00:20:22,042


617
00:20:21,242 --> 00:20:25,535
처음 학습 시에는 결과 스코어가 임의의 일정한 값을 갖게 됩니다.

618
00:20:25,535 --> 00:20:31,449
그렇다면 만약 모든 스코어 S가 거의 "0에 가깝고" 그리고 
"값이 서로 거의 비슷하다면"

619
00:20:31,449 --> 00:20:35,740
Multiclass SVM에서 Loss가 어떻게 될 것 같습니까?

620
00:20:35,741 --> 00:20:37,502
- [학생이 답변]

621
00:20:37,502 --> 00:20:43,248
네, 답변은 "클래스의 수 - 1" 이었습니다.

622
00:20:42,448 --> 00:20:45,871
우리가 반복한다면

623
00:20:43,248 --> 00:20:46,671


624
00:20:45,871 --> 00:20:48,225
모든 잘못된 클래스들, 그래서 우리는 반복하고있다.

625
00:20:46,671 --> 00:20:49,025


626
00:20:48,225 --> 00:20:51,489
C 클래스에서 하나의 클래스를 뺀 클래스

627
00:20:49,025 --> 00:20:52,289


628
00:20:51,489 --> 00:20:53,738
두 개의 S는 거의 같을 것이다.

629
00:20:52,289 --> 00:20:54,538


630
00:20:53,738 --> 00:20:55,096
그래서 우리는 1의 손실을 얻을 것이다.

631
00:20:55,096 --> 00:20:57,536
마진 때문에 우리는 C 빼기를 얻습니다.

632
00:20:55,896 --> 00:20:58,336


633
00:20:57,536 --> 00:21:00,359
그래서 이것은 실제로 유용합니다. 왜냐하면 여러분,

634
00:20:58,336 --> 00:21:01,159


635
00:21:00,359 --> 00:21:01,964
이것은 유용한 디버깅 전략이다.

636
00:21:01,964 --> 00:21:03,243
당신이 이런 것들을 사용할 때,

637
00:21:03,243 --> 00:21:04,734
훈련을 시작할 때,

638
00:21:04,734 --> 00:21:08,082
당신은 당신의 상실을 기대하는 것에 대해 생각해야합니다.

639
00:21:05,534 --> 00:21:08,882


640
00:21:08,082 --> 00:21:10,931
그리고 훈련이 시작될 때 실제로 본 손실이

641
00:21:08,882 --> 00:21:11,731


642
00:21:10,931 --> 00:21:13,784
첫 번째 반복에서 C에서 1을 뺀 것과 같습니다.

643
00:21:11,731 --> 00:21:14,584


644
00:21:13,784 --> 00:21:14,999
이 경우,

645
00:21:14,999 --> 00:21:16,353
그건 아마 버그가 있다는 것을 의미하고 체크를해야합니다.

646
00:21:16,353 --> 00:21:18,655
귀하의 코드, 그래서 이것은 실제로 유용한 일종의

647
00:21:17,153 --> 00:21:19,455


648
00:21:18,655 --> 00:21:20,905
실제로 확인하는 것.

649
00:21:19,455 --> 00:21:21,705


650
00:21:21,886 --> 00:21:25,252
또 다른 질문은, 만약에 일어나면, 그래서 나는 우리가

651
00:21:22,686 --> 00:21:26,052


652
00:21:25,252 --> 00:21:30,010
잘못된 클래스에 대한 SVM,

653
00:21:26,052 --> 00:21:30,810


654
00:21:30,010 --> 00:21:31,487
올바른 클래스를 넘었다.

655
00:21:31,487 --> 00:21:34,323
우리가 모든 것을 다 통과한다면?

656
00:21:32,287 --> 00:21:35,123


657
00:21:34,323 --> 00:21:36,034
- [학생] 손실이 1 증가합니다.

658
00:21:36,034 --> 00:21:39,326
그래, 대답은 손실이 하나씩 증가한다는 것이다.

659
00:21:36,834 --> 00:21:40,126


660
00:21:39,326 --> 00:21:41,929
우리가 실제로이 일을하는 이유는

661
00:21:40,126 --> 00:21:42,729


662
00:21:41,929 --> 00:21:45,052
일반적으로 제로의 손실은 일종의, 좋은이있다

663
00:21:42,729 --> 00:21:45,852


664
00:21:45,052 --> 00:21:47,356
당신이 전혀 잃지 않는다는 해석,

665
00:21:45,852 --> 00:21:48,156


666
00:21:47,356 --> 00:21:51,363
그래서 좋네요, 그래서 나는 당신의 대답을 생각합니다.

667
00:21:48,156 --> 00:21:52,163


668
00:21:51,363 --> 00:21:52,755
정말 변하지 않을거야.

669
00:21:52,755 --> 00:21:54,512
당신은 결국 같은 분류자를 찾는 것입니다.

670
00:21:54,512 --> 00:21:56,484
실제로 모든 범주를 반복하면

671
00:21:55,312 --> 00:21:57,284


672
00:21:56,484 --> 00:21:59,979
하지만 규칙에 따라 올바른 클래스를 생략하면

673
00:21:57,284 --> 00:22:00,779


674
00:21:59,979 --> 00:22:02,729
우리의 최소 손실은 0이됩니다.

675
00:22:00,779 --> 00:22:03,529


676
00:22:04,931 --> 00:22:06,341
그래서 또 다른 질문은, 만약 우리가 평균

677
00:22:06,341 --> 00:22:08,008
여기에 합계 대신에?

678
00:22:07,141 --> 00:22:08,808


679
00:22:09,943 --> 00:22:11,233
- [학생] 변경되지 않습니다.

680
00:22:11,233 --> 00:22:13,076
- 네, 대답은 변하지 않습니다.

681
00:22:12,033 --> 00:22:13,876


682
00:22:13,076 --> 00:22:15,788
따라서 클래스 수는 미리 고정 될 것입니다.

683
00:22:13,876 --> 00:22:16,588


684
00:22:15,788 --> 00:22:18,075
우리가 우리의 데이터 세트를 선택할
때, 그것은 단지 재 스케일링입니다.

685
00:22:16,588 --> 00:22:18,875


686
00:22:18,075 --> 00:22:20,500
상수에 의한 전체 손실 함수,

687
00:22:18,875 --> 00:22:21,300


688
00:22:20,500 --> 00:22:22,334
그래서 그것은별로 중요하지 않습니다.

689
00:22:21,300 --> 00:22:23,134


690
00:22:22,334 --> 00:22:23,991
다른 모든 규모의 것들과

691
00:22:23,991 --> 00:22:25,754
왜냐하면 우리는 실제로 진정한 가치에
관심을 갖지 않기 때문입니다.

692
00:22:25,754 --> 00:22:28,331
또는 손실의 진정한 가치

693
00:22:26,554 --> 00:22:29,131


694
00:22:28,331 --> 00:22:29,664
그 문제에 대한.

695
00:22:29,131 --> 00:22:30,464


696
00:22:30,541 --> 00:22:32,710
이제 다른 예가 있습니다.

697
00:22:31,341 --> 00:22:33,510


698
00:22:32,710 --> 00:22:35,935
이 손실 공식 및 우리는 실제로 사각 항을 더했습니다.

699
00:22:33,510 --> 00:22:36,735


700
00:22:35,935 --> 00:22:37,934
이 최대의 상단에?

701
00:22:36,735 --> 00:22:38,734


702
00:22:37,934 --> 00:22:40,066
이것이 결국 같은 문제가 될까?

703
00:22:38,734 --> 00:22:40,866


704
00:22:40,066 --> 00:22:42,968
아니면 다른 분류 알고리즘이 될까요?

705
00:22:40,866 --> 00:22:43,768


706
00:22:42,968 --> 00:22:44,178
- [학생] 다르다.

707
00:22:44,178 --> 00:22:45,213
- 그래, 이건 다를거야.

708
00:22:45,213 --> 00:22:47,296
그래서 여기서 아이디어는 우리가
일종의 변화라고 생각합니다.

709
00:22:46,013 --> 00:22:48,096


710
00:22:47,296 --> 00:22:49,145
선과 악의 절충

711
00:22:48,096 --> 00:22:49,945


712
00:22:49,145 --> 00:22:50,903
일종의 비선형 방식으로,

713
00:22:50,903 --> 00:22:52,433
그래서 이것은 결국 실제로 컴퓨팅을 끝낼 것입니다.

714
00:22:52,433 --> 00:22:54,264
다른 손실 함수.

715
00:22:53,233 --> 00:22:55,064


716
00:22:54,264 --> 00:22:57,296
제곱 된 경첩 손실이라는이 아이디어는 실제로 사용됩니다.

717
00:22:55,064 --> 00:22:58,096


718
00:22:57,296 --> 00:22:59,915
때로는 실제로, 그래서 또 다른 속임수입니다

719
00:22:58,096 --> 00:23:00,715


720
00:22:59,915 --> 00:23:01,597
너가 위로 만들 때 너의 부대 안에있을 것이다

721
00:23:01,597 --> 00:23:05,066
자신의 문제에 대한 자신의 손실 함수.

722
00:23:02,397 --> 00:23:05,866


723
00:23:05,066 --> 00:23:07,831
이제 끝낼 것입니다, 오, 질문 있니?

724
00:23:05,866 --> 00:23:08,631


725
00:23:07,831 --> 00:23:09,126
- [학생] 왜 제곱 된 손실을 사용합니까?

726
00:23:09,126 --> 00:23:11,596
제곱이 아닌 손실 대신에?

727
00:23:09,926 --> 00:23:12,396


728
00:23:11,596 --> 00:23:14,018
- 네, 그럼 질문은 왜 당신이

729
00:23:12,396 --> 00:23:14,818


730
00:23:14,018 --> 00:23:16,760
비 제곱 손실 대신 제곱 손실을 사용합니까?

731
00:23:14,818 --> 00:23:17,560


732
00:23:16,760 --> 00:23:18,519
그리고 손실 함수의 전체 요점

733
00:23:18,519 --> 00:23:22,281
실수가 얼마나 다른지를 정량화하는 것입니다.

734
00:23:19,319 --> 00:23:23,081


735
00:23:22,281 --> 00:23:25,143
그리고 분류자가 여러 종류의 실수를 저지르고 있다면,

736
00:23:23,081 --> 00:23:25,943


737
00:23:25,143 --> 00:23:26,995
우리는 어떻게 서로 다른 장단점을 달랠 수 있을까요?

738
00:23:25,943 --> 00:23:27,795


739
00:23:26,995 --> 00:23:28,940
서로 다른 유형의 실수 사이에서 분류 자

740
00:23:27,795 --> 00:23:29,740


741
00:23:28,940 --> 00:23:30,141
만들겠습니까?

742
00:23:30,141 --> 00:23:32,265
그래서 제곱 된 손실을 사용한다면,

743
00:23:30,941 --> 00:23:33,065


744
00:23:32,265 --> 00:23:36,371
그 종류의 말은 매우 나쁜 것들이

745
00:23:33,065 --> 00:23:37,171


746
00:23:36,371 --> 00:23:37,834
이제 제곱이 나빠질거야.

747
00:23:37,834 --> 00:23:39,192
그래서 그것은 정말로, 정말로 나쁘고,

748
00:23:39,192 --> 00:23:40,711
우리는 아무것도 원하지 않는 것처럼

749
00:23:40,711 --> 00:23:43,623
그것은 완전히 파국적으로 잘못 분류 된 것입니다.

750
00:23:41,511 --> 00:23:44,423


751
00:23:43,623 --> 00:23:47,321
반면에이 경첩 손실을 사용하는 경우,

752
00:23:44,423 --> 00:23:48,121


753
00:23:47,321 --> 00:23:49,556
우리는 조금 틀린 것 사이에서 실제로 신경 쓰지 않는다.

754
00:23:48,121 --> 00:23:50,356


755
00:23:49,556 --> 00:23:52,553
그리고 많은 잘못되고, 틀린 종류의,

756
00:23:50,356 --> 00:23:53,353


757
00:23:52,553 --> 00:23:55,340
예를 많이 잘못 생각하면 그것을 증가시킵니다.

758
00:23:53,353 --> 00:23:56,140


759
00:23:55,340 --> 00:23:57,051
조금만 잘못하면

760
00:23:57,051 --> 00:23:59,608
그것은 예제와 같은 장점입니다.

761
00:23:57,851 --> 00:24:00,408


762
00:23:59,608 --> 00:24:02,603
그것은 조금 잘못되어 다시 증가시키는 것입니다.

763
00:24:00,408 --> 00:24:03,403


764
00:24:02,603 --> 00:24:04,375
조금 더 옳다.

765
00:24:04,375 --> 00:24:06,268
그래서 약간 물결 모양의 물결 모양입니다.

766
00:24:05,175 --> 00:24:07,068


767
00:24:06,268 --> 00:24:08,785
그러나 선형 대 정사각형을 사용하는이 아이디어

768
00:24:07,068 --> 00:24:09,585


769
00:24:08,785 --> 00:24:11,198
우리가 얼마나 신경을 쓰는지를 수치화하는 방법입니다.

770
00:24:09,585 --> 00:24:11,998


771
00:24:11,198 --> 00:24:13,597
서로 다른 범주의 오류에 대해

772
00:24:11,998 --> 00:24:14,397


773
00:24:13,597 --> 00:24:15,375
그리고 이것은 확실히 당신이 생각해야 할 것입니다.

774
00:24:15,375 --> 00:24:17,938
실제로 이러한 것들을 실제로 적용 할 때,

775
00:24:16,175 --> 00:24:18,738


776
00:24:17,938 --> 00:24:20,245
손실 함수가 방법이기 때문에

777
00:24:18,738 --> 00:24:21,045


778
00:24:20,245 --> 00:24:22,770
알고리즘에 어떤 유형의 오류

779
00:24:21,045 --> 00:24:23,570


780
00:24:22,770 --> 00:24:24,104
당신이 신경을 쓰고 어떤 종류의 오류

781
00:24:24,104 --> 00:24:26,267
그것은 반대해야합니다.

782
00:24:24,904 --> 00:24:27,067


783
00:24:26,267 --> 00:24:27,907
실제로 실제로는 매우 중요합니다.

784
00:24:27,907 --> 00:24:30,407
귀하의 응용 프로그램에 따라.

785
00:24:28,707 --> 00:24:31,207


786
00:24:32,315 --> 00:24:35,017
여기 벡터화 된 코드 종류의 작은 조각이 있습니다.

787
00:24:33,115 --> 00:24:35,817


788
00:24:35,017 --> 00:24:37,566
numpy로 끝내면 결국 구현됩니다.

789
00:24:35,817 --> 00:24:38,366


790
00:24:37,566 --> 00:24:40,962
첫 번째 임무를 위해 이와 비슷한 것,

791
00:24:38,366 --> 00:24:41,762


792
00:24:40,962 --> 00:24:43,593
그러나 이런 종류의 감각은 당신에게이 합계

793
00:24:41,762 --> 00:24:44,393


794
00:24:43,593 --> 00:24:44,952
실제로는 꽤 쉽다.

795
00:24:44,952 --> 00:24:46,798
numpy로 구현하려면 몇 줄 밖에 걸리지 않습니다.

796
00:24:45,752 --> 00:24:47,598


797
00:24:46,798 --> 00:24:48,768
벡터화 된 코드

798
00:24:47,598 --> 00:24:49,568


799
00:24:48,768 --> 00:24:50,926
그리고 실제로 하나의 멋진 트릭을 볼 수 있습니다.

800
00:24:49,568 --> 00:24:51,726


801
00:24:50,926 --> 00:24:55,904
실제로 여기에 들어가서 여백을
제로화 할 수 있다는 것입니다.

802
00:24:51,726 --> 00:24:56,704


803
00:24:55,904 --> 00:24:57,856
올바른 클래스에 해당하는

804
00:24:56,704 --> 00:24:58,656


805
00:24:57,856 --> 00:25:00,750
그리고 그것은 바로 그때에 쉽게,

806
00:24:58,656 --> 00:25:01,550


807
00:25:00,750 --> 00:25:04,345
그것은 건너 뛰는 멋진 벡터화 된 트릭의 일종이며,

808
00:25:01,550 --> 00:25:05,145


809
00:25:04,345 --> 00:25:06,069
하나의 클래스를 제외한 모든 클래스를 반복합니다.

810
00:25:06,069 --> 00:25:07,891
당신은 건너 뛰고 싶은 사람을 제로로 해줍니다.

811
00:25:06,869 --> 00:25:08,691


812
00:25:07,891 --> 00:25:09,854
그리고 어쨌든 합계를 계산하면 좋은 트릭입니다.

813
00:25:08,691 --> 00:25:10,654


814
00:25:09,854 --> 00:25:13,315
당신은 과제에 대한 사용을 고려할 수 있습니다.

815
00:25:10,654 --> 00:25:14,115


816
00:25:13,315 --> 00:25:17,106
자, 이제이 손실 함수에 대한 또 다른 질문입니다.

817
00:25:14,115 --> 00:25:17,906


818
00:25:17,106 --> 00:25:19,439
당신이 W를 찾을만큼 충분히 운이 좋다면

819
00:25:17,906 --> 00:25:20,239


820
00:25:19,439 --> 00:25:21,629
그 손실은 0입니다. 당신은 전혀 손실되지 않습니다.

821
00:25:20,239 --> 00:25:22,429


822
00:25:21,629 --> 00:25:22,745
너는 완전히 이기고있다.

823
00:25:22,745 --> 00:25:24,139
이 손실 함수는 그것을 분쇄하고 있습니다.

824
00:25:24,139 --> 00:25:27,819
하지만 질문이 있습니다.이 독특한 W입니다.

825
00:25:24,939 --> 00:25:28,619


826
00:25:27,819 --> 00:25:29,057
또는 다른 W가 있었나요

827
00:25:29,057 --> 00:25:32,390
그것은 또한 손실을 0으로 만들 수 있었습니까?

828
00:25:29,857 --> 00:25:33,190


829
00:25:33,251 --> 00:25:34,271
- [학생] 다른 W가 있습니다.

830
00:25:34,271 --> 00:25:37,444
- 답변, 예, 그래서 다른 W가 있습니다.

831
00:25:35,071 --> 00:25:38,244


832
00:25:37,444 --> 00:25:39,998
그리고 특히, 우리가 조금 이야기했기 때문에

833
00:25:38,244 --> 00:25:40,798


834
00:25:39,998 --> 00:25:43,976
전체 문제를 위 또는 아래로 조정하는 것에 관한 것

835
00:25:40,798 --> 00:25:44,776


836
00:25:43,976 --> 00:25:46,709
W에 따라 실제로 W를 취할 수 있습니다.

837
00:25:44,776 --> 00:25:47,509


838
00:25:46,709 --> 00:25:50,876
두 배로 곱해졌고 W가 두 배로
증가했습니다 (지금 쿼드 U입니까?

839
00:25:47,509 --> 00:25:51,676


840
00:25:51,984 --> 00:25:52,978
나도 몰라.)

841
00:25:52,978 --> 00:25:54,110
[웃음]

842
00:25:54,110 --> 00:25:56,601
이것은 또한 손실을 0으로 만들 것입니다.

843
00:25:54,910 --> 00:25:57,401


844
00:25:56,601 --> 00:25:58,568
그래서 이것의 구체적인 예로서,

845
00:25:57,401 --> 00:25:59,368


846
00:25:58,568 --> 00:26:00,066
좋아하는 예제로 돌아갈 수 있습니다.

847
00:26:00,066 --> 00:26:01,184
어쩌면 숫자를 통해 일할 수도 있습니다.

848
00:26:01,184 --> 00:26:02,351
나중에 조금,

849
00:26:02,351 --> 00:26:05,309
그러나 당신이 W를 가져 가고 우리가 W를 두 배로한다면,

850
00:26:03,151 --> 00:26:06,109


851
00:26:05,309 --> 00:26:09,129
올바른 점수와 잘못된 점수 사이의 마진

852
00:26:06,109 --> 00:26:09,929


853
00:26:09,129 --> 00:26:10,583
또한 두 배가됩니다.

854
00:26:10,583 --> 00:26:12,190
그래서이 모든 마진이

855
00:26:12,190 --> 00:26:14,521
이미 하나보다 컸고 우리는 그들을 두배로 늘 렸습니다.

856
00:26:12,990 --> 00:26:15,321


857
00:26:14,521 --> 00:26:16,274
그들은 여전히 하나보다 커질 것입니다.

858
00:26:16,274 --> 00:26:18,857
그래서 당신은 여전히 손실이 없을 것입니다.

859
00:26:17,074 --> 00:26:19,657


860
00:26:20,180 --> 00:26:22,182
그리고 이것은 재미있는 일종의,

861
00:26:20,980 --> 00:26:22,982


862
00:26:22,182 --> 00:26:24,572
왜냐하면 우리의 손실 함수

863
00:26:22,982 --> 00:26:25,372


864
00:26:24,572 --> 00:26:27,428
우리가 우리가 원하는 W를 우리
분류 자에게 말하는 방식입니다.

865
00:26:25,372 --> 00:26:28,228


866
00:26:27,428 --> 00:26:29,045
그리고 우리가 신경 쓰는 W,

867
00:26:29,045 --> 00:26:30,333
이것은 조금 이상하다.

868
00:26:30,333 --> 00:26:31,795
지금이 불일치가 있습니다.

869
00:26:31,795 --> 00:26:35,197
분류 자의 선택 방법

870
00:26:32,595 --> 00:26:35,997


871
00:26:35,197 --> 00:26:36,862
이들 서로 다른 버전의 W 사이

872
00:26:36,862 --> 00:26:39,112
모두 제로 손실을 달성합니까?

873
00:26:37,662 --> 00:26:39,912


874
00:26:40,099 --> 00:26:42,210
그리고 그것은 우리가 여기서 한 일이

875
00:26:40,899 --> 00:26:43,010


876
00:26:42,210 --> 00:26:45,303
데이터 측면에서 손실 만 기록됩니다.

877
00:26:43,010 --> 00:26:46,103


878
00:26:45,303 --> 00:26:48,136
우리는 분류 자에게만 말했습니다.

879
00:26:46,103 --> 00:26:48,936


880
00:26:48,982 --> 00:26:50,448
W를 찾아야한다고

881
00:26:50,448 --> 00:26:52,239
교육 자료에 맞는

882
00:26:52,239 --> 00:26:54,233
하지만 실제로 실제로, 우리는 실제로 신경 쓰지 않습니다.

883
00:26:53,039 --> 00:26:55,033


884
00:26:54,233 --> 00:26:56,465
훈련 자료를 맞추는 것에 관한 것,

885
00:26:55,033 --> 00:26:57,265


886
00:26:56,465 --> 00:26:57,760
기계 학습의 요점

887
00:26:57,760 --> 00:27:01,553
우리가 훈련 데이터를 사용하여
일부 분류자를 찾는 것입니다.

888
00:26:58,560 --> 00:27:02,353


889
00:27:01,553 --> 00:27:04,222
그런 다음 테스트 데이터에 적용 할 것입니다.

890
00:27:02,353 --> 00:27:05,022


891
00:27:04,222 --> 00:27:06,858
따라서 우리는 교육 데이터 성능에
대해 실제로 신경 쓰지 않습니다.

892
00:27:05,022 --> 00:27:07,658


893
00:27:06,858 --> 00:27:08,503
우리는 정말 성능에 신경을 썼다.

894
00:27:08,503 --> 00:27:10,928
테스트 데이터에 대한이 분류 기준.

895
00:27:09,303 --> 00:27:11,728


896
00:27:10,928 --> 00:27:12,785
결과적으로 유일한 경우

897
00:27:11,728 --> 00:27:13,585


898
00:27:12,785 --> 00:27:14,671
우리는 분류 자에게 할 일을 알려줍니다.

899
00:27:13,585 --> 00:27:15,471


900
00:27:14,671 --> 00:27:16,176
훈련 자료에 적합하다.

901
00:27:16,176 --> 00:27:18,053
우리는 스스로를 인도 할 수있다.

902
00:27:16,976 --> 00:27:18,853


903
00:27:18,053 --> 00:27:20,307
때로는이 이상한 상황에 빠지기도합니다.

904
00:27:18,853 --> 00:27:21,107


905
00:27:20,307 --> 00:27:24,079
분류기는 직관적이지 않은 동작을 할 수 있습니다.

906
00:27:21,107 --> 00:27:24,879


907
00:27:24,079 --> 00:27:27,786
그래서 이런 종류의 구체적이고 표준적인 예는,

908
00:27:24,879 --> 00:27:28,586


909
00:27:27,786 --> 00:27:29,985
그건 그렇고, 이것은 더 이상 선형 분류가 아니며,

910
00:27:28,586 --> 00:27:30,785


911
00:27:29,985 --> 00:27:31,426
이것은 좀 더 일반적인 것입니다.

912
00:27:31,426 --> 00:27:32,918
기계 학습 개념,

913
00:27:32,918 --> 00:27:35,815
우리가이 파란 점의 데이터 세트를 가지고 있다고 가정하면,

914
00:27:33,718 --> 00:27:36,615


915
00:27:35,815 --> 00:27:38,668
우리는 훈련 데이터에 약간의 곡선을 맞출 것입니다.

916
00:27:36,615 --> 00:27:39,468


917
00:27:38,668 --> 00:27:39,827
파란 점들,

918
00:27:39,827 --> 00:27:42,783
그런 다음 우리가 분류 자에게 말했던 유일한 것

919
00:27:40,627 --> 00:27:43,583


920
00:27:42,783 --> 00:27:44,532
훈련 자료를 시험하고 맞추는 것,

921
00:27:44,532 --> 00:27:46,451
들어가서 아주 휘어지는 커브를 가질 수 있습니다.

922
00:27:45,332 --> 00:27:47,251


923
00:27:46,451 --> 00:27:47,812
완벽하게 분류하려고 시도하다.

924
00:27:47,812 --> 00:27:49,647
모든 교육 데이터 포인트.

925
00:27:48,612 --> 00:27:50,447


926
00:27:49,647 --> 00:27:52,235
그러나 우리가 실제로 걱정하지 않기 때문에 이것은 나쁘다.

927
00:27:50,447 --> 00:27:53,035


928
00:27:52,235 --> 00:27:53,519
이 성능에 대해,

929
00:27:53,519 --> 00:27:56,329
우리는 테스트 데이터의 성능에 신경을 썼다.

930
00:27:54,319 --> 00:27:57,129


931
00:27:56,329 --> 00:27:58,379
이제 새로운 데이터가 있으면

932
00:27:57,129 --> 00:27:59,179


933
00:27:58,379 --> 00:28:00,710
그 종류의 추세는 같은 경향을 따르고 있습니다.

934
00:27:59,179 --> 00:28:01,510


935
00:28:00,710 --> 00:28:02,267
이 아주 휘황 찬란 선

936
00:28:02,267 --> 00:28:03,872
완전히 틀리게 될 것입니다.

937
00:28:03,872 --> 00:28:05,601
그리고 사실 우리가 선호했을만한 것은

938
00:28:05,601 --> 00:28:07,806
할 분류 기준은 아마도 예측할 수 있습니다.

939
00:28:06,401 --> 00:28:08,606


940
00:28:07,806 --> 00:28:09,334
이 똑바로 녹색 선,

941
00:28:09,334 --> 00:28:11,683
이 복잡한 위그 라인보다

942
00:28:10,134 --> 00:28:12,483


943
00:28:11,683 --> 00:28:15,171
모든 교육 데이터에 완벽하게 맞춰야합니다.

944
00:28:12,483 --> 00:28:15,971


945
00:28:15,171 --> 00:28:17,821
그리고 이것은 근본적인 근본적인 문제입니다.

946
00:28:15,971 --> 00:28:18,621


947
00:28:17,821 --> 00:28:19,232
기계 학습에서,

948
00:28:19,232 --> 00:28:20,662
우리가 일반적으로 해결하는 방법,

949
00:28:20,662 --> 00:28:22,816
이 정례화의 개념입니다.

950
00:28:21,462 --> 00:28:23,616


951
00:28:22,816 --> 00:28:25,159
여기에 추가 용어를 추가 할 것입니다.

952
00:28:23,616 --> 00:28:25,959


953
00:28:25,159 --> 00:28:26,443
손실 함수.

954
00:28:26,443 --> 00:28:27,832
데이터 손실 외에도,

955
00:28:27,832 --> 00:28:30,255
그것은 우리 분류 자에게 적합하다고 말해 줄 것입니다.

956
00:28:28,632 --> 00:28:31,055


957
00:28:30,255 --> 00:28:32,448
교육 데이터는 일반적으로 추가 할 것입니다.

958
00:28:31,055 --> 00:28:33,248


959
00:28:32,448 --> 00:28:34,309
손실 함수에 대한 또 다른 용어

960
00:28:33,248 --> 00:28:35,109


961
00:28:34,309 --> 00:28:36,057
정규화 용어 (regularization term)

962
00:28:36,057 --> 00:28:40,691
모델이 어떻게 든 더 단순한 W를 선택하도록 장려한다.

963
00:28:36,857 --> 00:28:41,491


964
00:28:40,691 --> 00:28:42,492
단순한 개념

965
00:28:41,491 --> 00:28:43,292


966
00:28:42,492 --> 00:28:45,992
종류는 작업과 모델에 따라 다릅니다.

967
00:28:43,292 --> 00:28:46,792


968
00:28:47,925 --> 00:28:49,639
Occam의 면도날에 대한 아이디어가 있습니다.

969
00:28:49,639 --> 00:28:52,868
이것은 과학적 발견에서의이 근본적인 생각이다.

970
00:28:50,439 --> 00:28:53,668


971
00:28:52,868 --> 00:28:55,574
더 넓게, 그것은 당신이 많은 다른

972
00:28:53,668 --> 00:28:56,374


973
00:28:55,574 --> 00:28:57,713
경쟁 가설은 설명 할 수있다.

974
00:28:56,374 --> 00:28:58,513


975
00:28:57,713 --> 00:28:59,158
너의 관찰,

976
00:28:59,158 --> 00:29:01,039
당신은 일반적으로 더 간단한 것을 선호해야합니다,

977
00:28:59,958 --> 00:29:01,839


978
00:29:01,039 --> 00:29:03,399
그것이 그 가능성이 더 높은 설명이기 때문입니다.

979
00:29:01,839 --> 00:29:04,199


980
00:29:03,399 --> 00:29:06,801
미래의 새로운 관측에 일반화하기.

981
00:29:04,199 --> 00:29:07,601


982
00:29:06,801 --> 00:29:08,714
그리고 우리가이 직감을 조작하는 방법

983
00:29:07,601 --> 00:29:09,514


984
00:29:08,714 --> 00:29:10,977
기계 학습에서 일반적으로 몇 가지 명시 적으로

985
00:29:09,514 --> 00:29:11,777


986
00:29:10,977 --> 00:29:12,538
정규화 벌금

987
00:29:12,538 --> 00:29:15,121
그것은 종종 R로 기록됩니다.

988
00:29:13,338 --> 00:29:15,921


989
00:29:16,312 --> 00:29:18,687
그러면 표준 손실 함수

990
00:29:17,112 --> 00:29:19,487


991
00:29:18,687 --> 00:29:20,409
일반적으로이 두 가지 용어가 있습니다.

992
00:29:20,409 --> 00:29:22,417
데이터 손실 및 정규화 손실,

993
00:29:21,209 --> 00:29:23,217


994
00:29:22,417 --> 00:29:25,130
여기에 하이퍼 파라미터가 있습니다, 람다,

995
00:29:23,217 --> 00:29:25,930


996
00:29:25,130 --> 00:29:27,200
그 둘 사이에 거래됩니다.

997
00:29:25,930 --> 00:29:28,000


998
00:29:27,200 --> 00:29:28,952
그리고 우리는 하이퍼 파라미터에 대해 이야기했습니다.

999
00:29:28,952 --> 00:29:31,054
마지막 강연에서 상호 유효성 검사,

1000
00:29:29,752 --> 00:29:31,854


1001
00:29:31,054 --> 00:29:33,820
그래서이 정규화 하이퍼 파라미터 람다

1002
00:29:31,854 --> 00:29:34,620


1003
00:29:33,820 --> 00:29:35,797
더 중요한 것 중 하나가 될 것이다.

1004
00:29:34,620 --> 00:29:36,597


1005
00:29:35,797 --> 00:29:37,280
훈련 할 때 조정할 필요가있다.

1006
00:29:37,280 --> 00:29:39,363
실제로 이러한 모델.

1007
00:29:38,080 --> 00:29:40,163


1008
00:29:40,229 --> 00:29:41,062
문제?

1009
00:29:41,029 --> 00:29:41,862


1010
00:29:42,097 --> 00:29:44,679
- [학생] 그 람다 R 용어는 무엇입니까?

1011
00:29:42,897 --> 00:29:45,479


1012
00:29:44,679 --> 00:29:48,846
희미하게 말하기와 관련이 있습니다.

1013
00:29:45,479 --> 00:29:49,646


1014
00:29:50,685 --> 00:29:51,941
- 그래, 문제는,

1015
00:29:51,941 --> 00:29:53,850
이 람다 R W 용어 사이의 연결은 무엇인가?

1016
00:29:52,741 --> 00:29:54,650


1017
00:29:53,850 --> 00:29:55,405
실제로이 위그 라인을 강요합니다.

1018
00:29:55,405 --> 00:29:58,072
똑바로 푸른 선이 되려면?

1019
00:29:56,205 --> 00:29:58,872


1020
00:29:59,912 --> 00:30:01,319
나는 이것에 관한 파생을 겪고 싶지 않았다.

1021
00:30:01,319 --> 00:30:03,550
나는 그것이 우리를 너무 멀게
이끌 것이라고 생각했기 때문에,

1022
00:30:02,119 --> 00:30:04,350


1023
00:30:03,550 --> 00:30:04,768
그러나 당신은 상상할 수 있습니다.

1024
00:30:04,768 --> 00:30:06,310
어쩌면 당신은 회귀 문제를 겪고 있습니다.

1025
00:30:06,310 --> 00:30:09,830
다른 다항식 기초 함수의 관점에서,

1026
00:30:07,110 --> 00:30:10,630


1027
00:30:09,830 --> 00:30:13,921
이 회귀 패널티를 추가하는 경우,

1028
00:30:10,630 --> 00:30:14,721


1029
00:30:13,921 --> 00:30:15,775
어쩌면 모델은 다항식에 접근 할 수 있습니다.

1030
00:30:14,721 --> 00:30:16,575


1031
00:30:15,775 --> 00:30:18,316
매우 높은 수준이지만,이 회귀 기간을 통해

1032
00:30:16,575 --> 00:30:19,116


1033
00:30:18,316 --> 00:30:20,715
당신은 모델이 다항식을 선호하도록 장려 할 수 있습니다.

1034
00:30:19,116 --> 00:30:21,515


1035
00:30:20,715 --> 00:30:23,649
낮은 학위의, 그들은 적절하게 데이터에 맞는 경우,

1036
00:30:21,515 --> 00:30:24,449


1037
00:30:23,649 --> 00:30:25,569
또는 데이터가 비교적 잘 맞는지 여부.

1038
00:30:24,449 --> 00:30:26,369


1039
00:30:25,569 --> 00:30:29,021
그래서 이것을 할 수있는 두 가지
방법이 있다고 상상할 수 있습니다.

1040
00:30:26,369 --> 00:30:29,821


1041
00:30:29,021 --> 00:30:30,681
모델 클래스를 제한 할 수 있습니다.

1042
00:30:30,681 --> 00:30:32,897
더 강력한,

1043
00:30:31,481 --> 00:30:33,697


1044
00:30:32,897 --> 00:30:36,337
더 복잡한 모델을 만들거나이 소프트
페널티를 추가 할 수 있습니다.

1045
00:30:33,697 --> 00:30:37,137


1046
00:30:36,337 --> 00:30:40,846
모델이 여전히 더 복잡한 모델에 액세스 할 수있는 경우,

1047
00:30:37,137 --> 00:30:41,646


1048
00:30:40,846 --> 00:30:43,112
이 경우에는 다차원 다항식 일 수도 있지만,

1049
00:30:41,646 --> 00:30:43,912


1050
00:30:43,112 --> 00:30:44,664
하지만이 부드러운 제약 조건을 추가하면됩니다.

1051
00:30:44,664 --> 00:30:47,921
이 복잡한 모델을 사용하기를 원한다면,

1052
00:30:45,464 --> 00:30:48,721


1053
00:30:47,921 --> 00:30:49,648
이 형벌을 극복해야합니다.

1054
00:30:49,648 --> 00:30:51,316
그들의 복잡성을 사용합니다.

1055
00:30:51,316 --> 00:30:52,949
여기는 연결입니다.

1056
00:30:52,949 --> 00:30:55,282
그건 꽤 선형 분류되지 않습니다,

1057
00:30:53,749 --> 00:30:56,082


1058
00:30:55,282 --> 00:30:57,858
이것은 많은 사람들이 염두에두고있는 그림입니다.

1059
00:30:56,082 --> 00:30:58,658


1060
00:30:57,858 --> 00:31:01,691
적어도 정규화에 대해 생각할 때.

1061
00:30:58,658 --> 00:31:02,491


1062
00:31:02,731 --> 00:31:04,643
그래서 실제로는 여러 가지 유형이 있습니다.

1063
00:31:03,531 --> 00:31:05,443


1064
00:31:04,643 --> 00:31:06,917
실제로 사용되는 정규화.

1065
00:31:05,443 --> 00:31:07,717


1066
00:31:06,917 --> 00:31:09,874
가장 보편적 인 것은 아마 L2 정규화 일 것이다.

1067
00:31:07,717 --> 00:31:10,674


1068
00:31:09,874 --> 00:31:11,022
또는 체중 감량.

1069
00:31:11,022 --> 00:31:13,905
그러나 당신이 볼 수있는 다른 것들이 많이 있습니다.

1070
00:31:11,822 --> 00:31:14,705


1071
00:31:13,905 --> 00:31:17,973
이 L2 정규화는 단지 유클리드 표준입니다.

1072
00:31:14,705 --> 00:31:18,773


1073
00:31:17,973 --> 00:31:19,470
이 가중 벡터 (W)

1074
00:31:19,470 --> 00:31:21,892
때로는 제곱 된 규범입니다.

1075
00:31:20,270 --> 00:31:22,692


1076
00:31:21,892 --> 00:31:24,021
또는 때때로 제곱 된 표준의 절반

1077
00:31:22,692 --> 00:31:24,821


1078
00:31:24,021 --> 00:31:25,480
파생 상품을 없애기 때문에

1079
00:31:25,480 --> 00:31:26,738
조금 더 좋네.

1080
00:31:26,738 --> 00:31:28,832
그러나 L2 정규화의 아이디어

1081
00:31:27,538 --> 00:31:29,632


1082
00:31:28,832 --> 00:31:30,702
당신은 단지 유클리드 표준을 처벌하고 있습니까?

1083
00:31:29,632 --> 00:31:31,502


1084
00:31:30,702 --> 00:31:32,650
이 가중치 벡터의

1085
00:31:31,502 --> 00:31:33,450


1086
00:31:32,650 --> 00:31:35,593
또한 때로는 L1 정규화를 볼 수도 있습니다.

1087
00:31:33,450 --> 00:31:36,393


1088
00:31:35,593 --> 00:31:38,957
여기서 우리는 가중치 벡터의 L1 표준에 불이익을 당하고,

1089
00:31:36,393 --> 00:31:39,757


1090
00:31:38,957 --> 00:31:42,525
L1 정규화에는 좋은 속성이 있습니다.

1091
00:31:39,757 --> 00:31:43,325


1092
00:31:42,525 --> 00:31:46,401
이 행렬에서 격려하는 희소성처럼.

1093
00:31:43,325 --> 00:31:47,201


1094
00:31:46,401 --> 00:31:47,692
다른 것들을 볼 수도 있습니다.

1095
00:31:47,692 --> 00:31:49,815
이 탄성 망 정규화일까요?

1096
00:31:48,492 --> 00:31:50,615


1097
00:31:49,815 --> 00:31:52,744
L1과 L2의 조합입니다.

1098
00:31:50,615 --> 00:31:53,544


1099
00:31:52,744 --> 00:31:55,837
때로는 최대 표준 정규화를 볼 수 있습니다.

1100
00:31:53,544 --> 00:31:56,637


1101
00:31:55,837 --> 00:32:00,004
L1 또는 L2 표준이 아닌 최대 표준을 페널티.

1102
00:31:56,637 --> 00:32:00,804


1103
00:32:01,119 --> 00:32:03,025
그러나 이러한 정례화

1104
00:32:01,919 --> 00:32:03,825


1105
00:32:03,025 --> 00:32:05,792
당신이 깊은 학습에서 보는 것뿐만 아니라,

1106
00:32:03,825 --> 00:32:06,592


1107
00:32:05,792 --> 00:32:07,374
그러나 기계 학습의 많은 분야에 걸쳐

1108
00:32:07,374 --> 00:32:10,997
심지어는 더 광범위하게 최적화 할 수도 있습니다.

1109
00:32:08,174 --> 00:32:11,797


1110
00:32:10,997 --> 00:32:13,091
이후의 강의에서는

1111
00:32:11,797 --> 00:32:13,891


1112
00:32:13,091 --> 00:32:15,726
특정 유형의 정규화 유형

1113
00:32:13,891 --> 00:32:16,526


1114
00:32:15,726 --> 00:32:17,138
깊은 학습.

1115
00:32:17,138 --> 00:32:19,602
예를 들어 드롭 아웃을 위해, 우리는
몇 가지 강의를 보게 될 것입니다.

1116
00:32:17,938 --> 00:32:20,402


1117
00:32:19,602 --> 00:32:22,882
또는 배치 정규화, 확률 적 깊이,

1118
00:32:20,402 --> 00:32:23,682


1119
00:32:22,882 --> 00:32:25,157
최근 몇 년 동안 이러한 일들이 미쳐 버렸습니다.

1120
00:32:23,682 --> 00:32:25,957


1121
00:32:25,157 --> 00:32:26,761
그러나 정규화의 전체 아이디어

1122
00:32:26,761 --> 00:32:29,329
당신이 당신의 모델에 대해하는 일이라면,

1123
00:32:27,561 --> 00:32:30,129


1124
00:32:29,329 --> 00:32:32,557
그런 종류의 모델은 모델의 복잡성을 어떻게 든 처벌합니다.

1125
00:32:30,129 --> 00:32:33,357


1126
00:32:32,557 --> 00:32:37,061
명시 적으로 훈련 데이터에 맞추려고하지 않고

1127
00:32:33,357 --> 00:32:37,861


1128
00:32:37,061 --> 00:32:38,306
문제?

1129
00:32:38,306 --> 00:32:41,889
[희미하게 말하는 학생]

1130
00:32:39,106 --> 00:32:42,689


1131
00:32:44,858 --> 00:32:46,104
그래, 문제는,

1132
00:32:46,104 --> 00:32:48,604
L2 정규화는 복잡성을 어떻게 측정합니까?

1133
00:32:46,904 --> 00:32:49,404


1134
00:32:48,604 --> 00:32:50,186
모델의?

1135
00:32:50,186 --> 00:32:52,035
고맙게도 우리는 바로 여기에 그 예가 있습니다.

1136
00:32:50,986 --> 00:32:52,835


1137
00:32:52,035 --> 00:32:54,202
어쩌면 우리는 걸을 수 있습니다.

1138
00:32:52,835 --> 00:32:55,002


1139
00:32:55,437 --> 00:32:57,779
그래서 여기서 우리는 아마도
훈련 예를 가질 것입니다, x,

1140
00:32:56,237 --> 00:32:58,579


1141
00:32:57,779 --> 00:33:00,058
우리가 고려하고있는 두 가지 W가 있습니다.

1142
00:32:58,579 --> 00:33:00,858


1143
00:33:00,058 --> 00:33:02,673
그래서 x는이 4 개의 벡터입니다.

1144
00:33:00,858 --> 00:33:03,473


1145
00:33:02,673 --> 00:33:06,324
우리는이 두 가지 가능성을 고려하고 있습니다.

1146
00:33:03,473 --> 00:33:07,124


1147
00:33:06,324 --> 00:33:07,157
W.

1148
00:33:07,157 --> 00:33:08,978
하나는 처음에는 하나, 하나는 하나입니다.

1149
00:33:07,957 --> 00:33:09,778


1150
00:33:08,978 --> 00:33:10,367
그리고 세 개의 0,

1151
00:33:10,367 --> 00:33:12,530
다른 하나는이 0.25의 퍼짐을 가로 지르게됩니다

1152
00:33:11,167 --> 00:33:13,330


1153
00:33:12,530 --> 00:33:14,191
네 가지 항목.

1154
00:33:14,191 --> 00:33:15,898
그리고 지금, 선형 분류를 할 때,

1155
00:33:15,898 --> 00:33:17,299
우리는 실제로 점 제품을 복용하고 있습니다.

1156
00:33:17,299 --> 00:33:19,702
우리 x와 우리 W 사이.

1157
00:33:18,099 --> 00:33:20,502


1158
00:33:19,702 --> 00:33:22,533
그래서 선형 분류의 관점에서,

1159
00:33:20,502 --> 00:33:23,333


1160
00:33:22,533 --> 00:33:24,747
이 두 개의 W는 동일합니다.

1161
00:33:23,333 --> 00:33:25,547


1162
00:33:24,747 --> 00:33:26,319
그들은 같은 결과를주기 때문에

1163
00:33:26,319 --> 00:33:28,302
x로 도트를 찍을 때.

1164
00:33:27,119 --> 00:33:29,102


1165
00:33:28,302 --> 00:33:29,635
그러나 이제 문제는,

1166
00:33:29,635 --> 00:33:31,300
이 두 가지 예를 보면,

1167
00:33:31,300 --> 00:33:34,383
어느 것이 L2 회귀를 선호합니까?

1168
00:33:32,100 --> 00:33:35,183


1169
00:33:36,052 --> 00:33:39,300
그래, L2 회귀는 W2보다 더 좋아질거야.

1170
00:33:36,852 --> 00:33:40,100


1171
00:33:39,300 --> 00:33:41,030
왜냐하면 그것은 더 작은 규범을 가지고 있기 때문입니다.

1172
00:33:41,030 --> 00:33:44,854
그래서 대답은 L2 회귀

1173
00:33:41,830 --> 00:33:45,654


1174
00:33:44,854 --> 00:33:47,017
분류기의 복잡성을 측정합니다.

1175
00:33:45,654 --> 00:33:47,817


1176
00:33:47,017 --> 00:33:49,440
이 비교적 거친 방식으로,

1177
00:33:47,817 --> 00:33:50,240


1178
00:33:49,440 --> 00:33:51,644
그 아이디어는,

1179
00:33:50,240 --> 00:33:52,444


1180
00:33:51,644 --> 00:33:54,557
선형 분류의 W를 기억하십시오.

1181
00:33:52,444 --> 00:33:55,357


1182
00:33:54,557 --> 00:33:56,915
얼마나 많이 해석 했는가?

1183
00:33:55,357 --> 00:33:57,715


1184
00:33:56,915 --> 00:33:59,551
이 벡터의 값 x

1185
00:33:57,715 --> 00:34:00,351


1186
00:33:59,551 --> 00:34:01,920
이 출력 클래스에 해당합니까?

1187
00:34:00,351 --> 00:34:02,720


1188
00:34:01,920 --> 00:34:04,202
그래서 L2 정규화가 말하고 있습니다.

1189
00:34:02,720 --> 00:34:05,002


1190
00:34:04,202 --> 00:34:06,245
그 영향력을 퍼뜨리는 것이 더 좋아

1191
00:34:05,002 --> 00:34:07,045


1192
00:34:06,245 --> 00:34:09,080
x의 모든 다른 값에 걸쳐.

1193
00:34:07,045 --> 00:34:09,880


1194
00:34:09,080 --> 00:34:11,038
어쩌면 이것이 더 강력 할 수도 있습니다.

1195
00:34:09,880 --> 00:34:11,838


1196
00:34:11,039 --> 00:34:14,585
당신이 변화하는 xs를 생각해 내면,

1197
00:34:11,839 --> 00:34:15,385


1198
00:34:14,585 --> 00:34:16,687
그러면 우리의 결정이 확산됩니다.

1199
00:34:15,385 --> 00:34:17,487


1200
00:34:16,687 --> 00:34:18,359
전체 x 벡터에 의존하며,

1201
00:34:18,359 --> 00:34:20,205
특정 요소에만 의존하기보다

1202
00:34:19,159 --> 00:34:21,005


1203
00:34:20,205 --> 00:34:22,059
x 벡터의.

1204
00:34:21,005 --> 00:34:22,859


1205
00:34:22,060 --> 00:34:23,946
그런데, L1 정규화

1206
00:34:22,860 --> 00:34:24,746


1207
00:34:23,946 --> 00:34:26,839
이 반대 해석이있다.

1208
00:34:24,746 --> 00:34:27,639


1209
00:34:26,839 --> 00:34:29,357
그래서 우리가 L1 정규화를 사용한다면,

1210
00:34:27,639 --> 00:34:30,157


1211
00:34:29,357 --> 00:34:32,946
그러면 실제로 W2보다 W1을 선호 할 것입니다.

1212
00:34:30,157 --> 00:34:33,746


1213
00:34:32,946 --> 00:34:35,595
L1 정규화가이 다른 개념을 가지고 있기 때문에

1214
00:34:33,746 --> 00:34:36,395


1215
00:34:35,595 --> 00:34:39,762
복잡성에 대해서는 모델이 덜 복잡하고,

1216
00:34:36,395 --> 00:34:40,562


1217
00:34:41,569 --> 00:34:44,867
어쩌면 우리는 0의 수로 모델의
복잡성을 측정 할 것입니다.

1218
00:34:42,369 --> 00:34:45,667


1219
00:34:44,867 --> 00:34:46,080
상기 웨이트 벡터에서,

1220
00:34:46,080 --> 00:34:49,332
그래서 우리는 어떻게 복잡성을 측정 할 것인가에 대한 질문

1221
00:34:46,880 --> 00:34:50,132


1222
00:34:49,332 --> 00:34:51,917
L2는 어떻게 복잡성을 측정합니까?

1223
00:34:50,132 --> 00:34:52,717


1224
00:34:51,917 --> 00:34:54,196
그들은 일종의 문제에 의존합니다.

1225
00:34:52,717 --> 00:34:54,996


1226
00:34:54,196 --> 00:34:57,126
그리고 특정 설정에 대해 생각해야합니다.

1227
00:34:54,996 --> 00:34:57,926


1228
00:34:57,126 --> 00:34:58,921
특정 모델 및 데이터의 경우,

1229
00:34:58,921 --> 00:35:01,754
그 복잡성을 어떻게 측정해야한다고 생각하니?

1230
00:34:59,721 --> 00:35:02,554


1231
00:35:01,754 --> 00:35:02,837
이 일에?

1232
00:35:02,554 --> 00:35:03,637


1233
00:35:03,921 --> 00:35:04,788
문제?

1234
00:35:04,788 --> 00:35:07,040
- [Student] 그러면 L1이 W1을 선호하는 이유는 무엇입니까?

1235
00:35:05,588 --> 00:35:07,840


1236
00:35:07,040 --> 00:35:09,129
그들은 같은 사람과 합쳐지지 않습니까?

1237
00:35:07,840 --> 00:35:09,929


1238
00:35:09,129 --> 00:35:10,385
- 네, 맞습니다.

1239
00:35:10,385 --> 00:35:12,330
따라서이 경우 L1은 실제로 동일합니다.

1240
00:35:11,185 --> 00:35:13,130


1241
00:35:12,330 --> 00:35:13,830
이 둘 사이.

1242
00:35:13,130 --> 00:35:14,630


1243
00:35:15,193 --> 00:35:18,018
그러나 이것과 비슷한 예를 만들 수 있습니다.

1244
00:35:15,993 --> 00:35:18,818


1245
00:35:18,018 --> 00:35:21,546
여기서 W1은 L1 정규화에 의해 선호 될 것이다.

1246
00:35:18,818 --> 00:35:22,346


1247
00:35:21,546 --> 00:35:24,643
나는 L1 뒤의 일반적인 직관력을 추측한다.

1248
00:35:22,346 --> 00:35:25,443


1249
00:35:24,643 --> 00:35:26,908
일반적으로 스파 스 솔루션을 선호한다는 것입니다.

1250
00:35:25,443 --> 00:35:27,708


1251
00:35:26,908 --> 00:35:30,558
W의 모든 항목을 0으로 만듭니다.

1252
00:35:27,708 --> 00:35:31,358


1253
00:35:30,558 --> 00:35:31,954
커플의 경우를 제외하고는

1254
00:35:31,954 --> 00:35:35,016
여기서 0에서 벗어나는 것이 허용됩니다.

1255
00:35:32,754 --> 00:35:35,816


1256
00:35:35,016 --> 00:35:37,945
L1의 복잡성을 측정하는 방법

1257
00:35:35,816 --> 00:35:38,745


1258
00:35:37,945 --> 00:35:40,191
어쩌면 0이 아닌 항목의 수,

1259
00:35:38,745 --> 00:35:40,991


1260
00:35:40,191 --> 00:35:43,677
그리고 나서 L2에 대해서, 그것은 W를 퍼뜨리는 것들

1261
00:35:40,991 --> 00:35:44,477


1262
00:35:43,677 --> 00:35:45,682
모든 값에 걸쳐 덜 복잡한 있습니다.

1263
00:35:44,477 --> 00:35:46,482


1264
00:35:45,682 --> 00:35:48,920
따라서 귀하의 데이터에 따라 다르며
귀하의 문제에 달려 있습니다.

1265
00:35:46,482 --> 00:35:49,720


1266
00:35:48,920 --> 00:35:51,593
오 그런데, 만일 당신이 하드 코어 베이지안이라면,

1267
00:35:49,720 --> 00:35:52,393


1268
00:35:51,593 --> 00:35:54,584
다음 L2 정규화를 사용 하여이 좋은 해석이 있습니다

1269
00:35:52,393 --> 00:35:55,384


1270
00:35:54,584 --> 00:35:57,206
이전의 Gaussian에 의한 MAP 추론

1271
00:35:55,384 --> 00:35:58,006


1272
00:35:57,206 --> 00:35:58,897
매개 변수 벡터에.

1273
00:35:58,897 --> 00:36:00,430
나는 그것에 관한 숙제 문제가 있다고 생각한다.

1274
00:36:00,430 --> 00:36:02,496
229 년에 우리는 그것에 대해 이야기하지 않을 것입니다.

1275
00:36:01,230 --> 00:36:03,296


1276
00:36:02,496 --> 00:36:05,343
나머지 분기 동안.

1277
00:36:03,296 --> 00:36:06,143


1278
00:36:05,343 --> 00:36:08,105
그것은 나의 길고 깊은 잠수의 종류 다.

1279
00:36:06,143 --> 00:36:08,905


1280
00:36:08,105 --> 00:36:11,400
다중 클래스 SVM 손실로.

1281
00:36:08,905 --> 00:36:12,200


1282
00:36:11,400 --> 00:36:12,783
문제?

1283
00:36:12,783 --> 00:36:14,399
- [Student] 그래, 아직도 혼란 스럽네.

1284
00:36:14,399 --> 00:36:17,646
내가해야 할 일들에 대해

1285
00:36:15,199 --> 00:36:18,446


1286
00:36:17,646 --> 00:36:20,979
선형 대 다항식 일 때,

1287
00:36:18,446 --> 00:36:21,779


1288
00:36:24,590 --> 00:36:27,673
이 손실 함수의 사용

1289
00:36:25,390 --> 00:36:28,473


1290
00:36:29,007 --> 00:36:32,111
당신이하고있는 사실을 바꾸지 않을 것입니다.

1291
00:36:29,807 --> 00:36:32,911


1292
00:36:32,111 --> 00:36:33,982
당신은 선형 분류자를보고 있습니다, 맞습니까?

1293
00:36:32,911 --> 00:36:34,782


1294
00:36:33,982 --> 00:36:36,905
- 그래, 문제는 그게,

1295
00:36:34,782 --> 00:36:37,705


1296
00:36:36,905 --> 00:36:37,825
정규화 추가

1297
00:36:37,825 --> 00:36:39,798
가설 수업을 변경하지 않을 것입니다.

1298
00:36:38,625 --> 00:36:40,598


1299
00:36:39,798 --> 00:36:44,036
이것은 선형 분류 자로부터 우리를
멀어지게 만들지 않을 것입니다.

1300
00:36:40,598 --> 00:36:44,836


1301
00:36:44,036 --> 00:36:45,857
아이디어는 아마도이 예제입니다.

1302
00:36:44,836 --> 00:36:46,657


1303
00:36:45,857 --> 00:36:47,491
이 다항식 회귀 분석

1304
00:36:47,491 --> 00:36:50,038
확실히 선형 회귀가 아닙니다.

1305
00:36:48,291 --> 00:36:50,838


1306
00:36:50,038 --> 00:36:52,174
그것은 선형 회귀로 볼 수 있습니다.

1307
00:36:50,838 --> 00:36:52,974


1308
00:36:52,174 --> 00:36:56,826
입력의 다항식 확장 위에,

1309
00:36:52,974 --> 00:36:57,626


1310
00:36:56,826 --> 00:36:59,712
그리고이 경우에,이 회귀는

1311
00:36:57,626 --> 00:37:00,512


1312
00:36:59,712 --> 00:37:03,232
당신은 많은 다항식을 사용할 수 없다는 것을

1313
00:37:00,512 --> 00:37:04,032


1314
00:37:03,232 --> 00:37:05,802
아마도 계수가 있어야합니다.

1315
00:37:04,032 --> 00:37:06,602


1316
00:37:05,802 --> 00:37:07,385
맞아, 네가 상상할 수 있듯이,

1317
00:37:07,385 --> 00:37:09,290
당신이 다항식 회귀를 할 때,

1318
00:37:08,185 --> 00:37:10,090


1319
00:37:09,290 --> 00:37:11,762
당신은 x의 f로서 다항식을 쓸 수 있습니다.

1320
00:37:10,090 --> 00:37:12,562


1321
00:37:11,762 --> 00:37:16,187
같음 A 제로 더하기 A 하나 x 더하기 두 x 제곱

1322
00:37:12,562 --> 00:37:16,987


1323
00:37:16,187 --> 00:37:17,963
플러스 3 x 무엇이든,

1324
00:37:17,963 --> 00:37:20,343
이 경우 매개 변수, W,

1325
00:37:18,763 --> 00:37:21,143


1326
00:37:20,343 --> 00:37:23,093
이것들은 As 일 것입니다. 그 경우,

1327
00:37:21,143 --> 00:37:23,893


1328
00:37:24,211 --> 00:37:26,154
W에게 벌칙을 주면 그것을 강요 할 수있다.

1329
00:37:25,011 --> 00:37:26,954


1330
00:37:26,154 --> 00:37:28,190
낮은 차수의 다항식쪽으로.

1331
00:37:26,954 --> 00:37:28,990


1332
00:37:28,190 --> 00:37:30,139
다항식 회귀의 경우를 제외하고,

1333
00:37:28,990 --> 00:37:30,939


1334
00:37:30,139 --> 00:37:31,491
당신은 실제로 매개 변수화하고 싶지 않다.

1335
00:37:31,491 --> 00:37:33,526
As의 측면에서 보면 다른 매개 변수가 있습니다.

1336
00:37:32,291 --> 00:37:34,326


1337
00:37:33,526 --> 00:37:34,725
당신이 사용하고 싶은,

1338
00:37:34,725 --> 00:37:36,364
그러나 그것은 일반적인 생각입니다,

1339
00:37:36,364 --> 00:37:38,871
당신이 모델의 매개 변수를 처벌하는 것과 같습니다.

1340
00:37:37,164 --> 00:37:39,671


1341
00:37:38,871 --> 00:37:41,868
더 간단한 가설을 향해 그것을 강요하는 것

1342
00:37:39,671 --> 00:37:42,668


1343
00:37:41,868 --> 00:37:44,285
당신의 가설 수업 내에서.

1344
00:37:42,668 --> 00:37:45,085


1345
00:37:45,229 --> 00:37:46,956
그리고 아마 우리는 이것을 오프라인으로 가져갈 수 있습니다.

1346
00:37:46,956 --> 00:37:50,340
그래도 조금 혼란 스럽다면.

1347
00:37:47,756 --> 00:37:51,140


1348
00:37:50,340 --> 00:37:54,589
그래서 우리는이 다중 클래스 SVM 손실을 보았습니다.

1349
00:37:51,140 --> 00:37:55,389


1350
00:37:54,589 --> 00:37:56,349
그리고 옆의 쪽지로서 단지 길 옆에서,

1351
00:37:56,349 --> 00:38:00,516
이것은 하나의 확장 또는 SVM 손실의 일반화입니다.

1352
00:37:57,149 --> 00:38:01,316


1353
00:38:01,924 --> 00:38:03,097
여러 클래스에,

1354
00:38:03,097 --> 00:38:04,347
실제로 몇 가지 다른 공식이 있습니다.

1355
00:38:04,347 --> 00:38:06,596
문학에서 주위를 볼 수있는,

1356
00:38:05,147 --> 00:38:07,396


1357
00:38:06,596 --> 00:38:09,848
하지만 내 직감이란 모두가 일하는
경향이 있다는 것을 의미합니다.

1358
00:38:07,396 --> 00:38:10,648


1359
00:38:09,848 --> 00:38:11,755
실제로 마찬가지로,

1360
00:38:10,648 --> 00:38:12,555


1361
00:38:11,755 --> 00:38:13,813
적어도 깊은 학습의 맥락에서.

1362
00:38:12,555 --> 00:38:14,613


1363
00:38:13,813 --> 00:38:16,449
그래서 우리는이 특별한 공식을 고수 할 것입니다.

1364
00:38:14,613 --> 00:38:17,249


1365
00:38:16,449 --> 00:38:19,949
이 클래스의 다중 클래스 SVM 손실

1366
00:38:17,249 --> 00:38:20,749


1367
00:38:21,061 --> 00:38:23,145
물론 다양한 손실 함수가 있습니다.

1368
00:38:21,861 --> 00:38:23,945


1369
00:38:23,145 --> 00:38:25,158
당신은 상상할 수도 있습니다.

1370
00:38:23,945 --> 00:38:25,958


1371
00:38:25,158 --> 00:38:27,270
그리고 또 다른 인기있는 선택,

1372
00:38:25,958 --> 00:38:28,070


1373
00:38:27,270 --> 00:38:30,603
다중 클래스 SVM 손실 외에도,

1374
00:38:28,070 --> 00:38:31,403


1375
00:38:31,761 --> 00:38:33,758
깊은 학습에서 또 다른 인기있는 선택

1376
00:38:32,561 --> 00:38:34,558


1377
00:38:33,758 --> 00:38:36,269
이 다항식 로지스틱 회귀 분석은,

1378
00:38:34,558 --> 00:38:37,069


1379
00:38:36,269 --> 00:38:37,769
또는 소프트 맥스 손실.

1380
00:38:37,069 --> 00:38:38,569


1381
00:38:39,405 --> 00:38:41,298
그리고 이것은 실제로 좀 더 일반적 일 것입니다.

1382
00:38:40,205 --> 00:38:42,098


1383
00:38:41,298 --> 00:38:43,222
깊은 학습의 맥락에서,

1384
00:38:42,098 --> 00:38:44,022


1385
00:38:43,222 --> 00:38:48,127
그러나 나는 무엇인가의 이유로이
순간을 발표하기로 결정했다.

1386
00:38:44,022 --> 00:38:48,927


1387
00:38:48,127 --> 00:38:51,742
멀티 클래스 SVM 손실이라는 맥락에서 기억하십시오.

1388
00:38:48,927 --> 00:38:52,542


1389
00:38:51,742 --> 00:38:53,651
우리는 실제로 해석이 없었습니다.

1390
00:38:52,542 --> 00:38:54,451


1391
00:38:53,651 --> 00:38:55,096
그 점수는.

1392
00:38:55,096 --> 00:38:56,900
우리가 어떤 분류를 할 때,

1393
00:38:55,896 --> 00:38:57,700


1394
00:38:56,900 --> 00:39:00,524
우리의 모델 F는이 10 개의 숫자를 뱉어냅니다.

1395
00:38:57,700 --> 00:39:01,324


1396
00:39:00,524 --> 00:39:02,670
수업에 대한 우리 점수 인

1397
00:39:01,324 --> 00:39:03,470


1398
00:39:02,670 --> 00:39:04,681
다중 클래스 SVM의 경우,

1399
00:39:03,470 --> 00:39:05,481


1400
00:39:04,681 --> 00:39:07,787
우리는 실제로 그 점수에 많은 해석을하지 않았습니다.

1401
00:39:05,481 --> 00:39:08,587


1402
00:39:07,787 --> 00:39:09,726
우리는 단지 우리가 진정한 점수를 원한다고 말했다.

1403
00:39:08,587 --> 00:39:10,526


1404
00:39:09,726 --> 00:39:11,112
올바른 클래스의 점수

1405
00:39:11,112 --> 00:39:13,497
잘못된 클래스보다 커야합니다.

1406
00:39:11,912 --> 00:39:14,297


1407
00:39:13,497 --> 00:39:17,712
그 이상으로 우리는 그 점수가 무엇을
의미하는지 실제로 말하지 않습니다.

1408
00:39:14,297 --> 00:39:18,512


1409
00:39:17,712 --> 00:39:21,712
그러나 이제는 다항 로지스틱 회귀 분석에서

1410
00:39:18,512 --> 00:39:22,512


1411
00:39:23,258 --> 00:39:25,625
우리는 실제로 이러한 점수를 부여 할 것입니다.

1412
00:39:24,058 --> 00:39:26,425


1413
00:39:25,625 --> 00:39:27,668
몇 가지 추가적인 의미가 있습니다.

1414
00:39:26,425 --> 00:39:28,468


1415
00:39:27,668 --> 00:39:29,715
그리고 특히 우리는 그 점수를 사용할 것입니다.

1416
00:39:28,468 --> 00:39:30,515


1417
00:39:29,715 --> 00:39:32,264
확률 분포 계산

1418
00:39:30,515 --> 00:39:33,064


1419
00:39:32,264 --> 00:39:33,907
우리 수업 이상.

1420
00:39:33,907 --> 00:39:37,324
그래서 우리는 소위 softmax 함수를 사용합니다.

1421
00:39:34,707 --> 00:39:38,124


1422
00:39:37,324 --> 00:39:39,775
우리가 우리의 모든 점수를받는 곳에,

1423
00:39:38,124 --> 00:39:40,575


1424
00:39:39,775 --> 00:39:43,192
우리는 그것들이 이제는
긍정적이게되도록 그들을 배가시킵니다.

1425
00:39:40,575 --> 00:39:43,992


1426
00:39:43,192 --> 00:39:46,540
그런 다음 이들 지수의 합으로 다시 정규화합니다.

1427
00:39:43,992 --> 00:39:47,340


1428
00:39:46,540 --> 00:39:49,140
그래서 지금 우리가 점수를 보낸 후

1429
00:39:47,340 --> 00:39:49,940


1430
00:39:49,140 --> 00:39:50,636
이 softmax 기능을 통해,

1431
00:39:50,636 --> 00:39:53,053
이제 우리는이 확률 분포로 끝나고,

1432
00:39:51,436 --> 00:39:53,853


1433
00:39:53,053 --> 00:39:55,792
지금 우리는 우리 수업보다 확률이 높습니다.

1434
00:39:53,853 --> 00:39:56,592


1435
00:39:55,792 --> 00:39:58,193
각각의 확률은 0과 1 사이이며,

1436
00:39:56,592 --> 00:39:58,993


1437
00:39:58,193 --> 00:40:01,370
모든 클래스에 걸친 확률의 합

1438
00:39:58,993 --> 00:40:02,170


1439
00:40:01,370 --> 00:40:02,287
하나에 합계.

1440
00:40:02,170 --> 00:40:03,087


1441
00:40:03,954 --> 00:40:07,169
그리고 이제 해석은 우리가 원하는 것입니다.

1442
00:40:04,754 --> 00:40:07,969


1443
00:40:07,169 --> 00:40:10,113
이 계산 된 확률 분포가 있습니다.

1444
00:40:07,969 --> 00:40:10,913


1445
00:40:10,113 --> 00:40:12,020
그것은 우리의 점수에 의해 암시 된,

1446
00:40:10,913 --> 00:40:12,820


1447
00:40:12,020 --> 00:40:14,650
우리는 이것을 목표와 비교하기를 원합니다.

1448
00:40:12,820 --> 00:40:15,450


1449
00:40:14,650 --> 00:40:17,138
또는 진정한 확률 분포.

1450
00:40:15,450 --> 00:40:17,938


1451
00:40:17,138 --> 00:40:19,166
우리가 그 고양이가 고양이라는 것을 알면,

1452
00:40:17,938 --> 00:40:19,966


1453
00:40:19,166 --> 00:40:22,083
목표 확률 분포

1454
00:40:19,966 --> 00:40:22,883


1455
00:40:22,083 --> 00:40:24,735
고양이에 모든 확률 질량을 넣을 것이고,

1456
00:40:22,883 --> 00:40:25,535


1457
00:40:24,735 --> 00:40:26,904
그래서 우리는 고양이가 1과 같을 확률을 가질 것입니다.

1458
00:40:25,535 --> 00:40:27,704


1459
00:40:26,904 --> 00:40:29,754
다른 모든 클래스에 대해서는 확률이 0입니다.

1460
00:40:27,704 --> 00:40:30,554


1461
00:40:29,754 --> 00:40:31,612
이제 우리가하고 싶은 일은 격려입니다.

1462
00:40:30,554 --> 00:40:32,412


1463
00:40:31,612 --> 00:40:33,528
우리의 계산 된 확률 분포

1464
00:40:32,412 --> 00:40:34,328


1465
00:40:33,528 --> 00:40:35,574
이 softmax 함수에서 나온 것입니다.

1466
00:40:34,328 --> 00:40:36,374


1467
00:40:35,574 --> 00:40:38,376
이 목표 확률 분포와 일치시킨다.

1468
00:40:36,374 --> 00:40:39,176


1469
00:40:38,376 --> 00:40:40,671
올바른 클래스의 모든 질량을가집니다.

1470
00:40:39,176 --> 00:40:41,471


1471
00:40:40,671 --> 00:40:42,175
그리고 우리가 이렇게하는 방법,

1472
00:40:42,175 --> 00:40:45,181
내 말은, 당신은 여러 방면에서이
방정식을 할 수 있다는 것입니다.

1473
00:40:42,975 --> 00:40:45,981


1474
00:40:45,181 --> 00:40:46,795
당신은 KL 발산으로 이것을 할 수 있습니다.

1475
00:40:46,795 --> 00:40:48,283
표적 사이

1476
00:40:48,283 --> 00:40:51,102
상기 계산 된 확률 분포,

1477
00:40:49,083 --> 00:40:51,902


1478
00:40:51,102 --> 00:40:53,221
최대 우도 추정치로이를 수행 할 수 있습니다.

1479
00:40:51,902 --> 00:40:54,021


1480
00:40:53,221 --> 00:40:54,466
그러나 하루가 끝날 때,

1481
00:40:54,466 --> 00:40:56,474
우리가 정말로 원하는 것은 확률

1482
00:40:55,266 --> 00:40:57,274


1483
00:40:56,474 --> 00:41:00,839
진정한 계급은 높고 하나에 가깝습니다.

1484
00:40:57,274 --> 00:41:01,639


1485
00:41:00,839 --> 00:41:04,015
그래서 우리의 손실은 이제 음의 로그가 될 것입니다.

1486
00:41:01,639 --> 00:41:04,815


1487
00:41:04,015 --> 00:41:06,389
진실한 계급의 확율의.

1488
00:41:04,815 --> 00:41:07,189


1489
00:41:06,389 --> 00:41:08,151
이것은 우리가 이것을 넣기 때문에 혼란 스럽습니다.

1490
00:41:08,151 --> 00:41:09,707
여러 다른 것들을 통해,

1491
00:41:09,707 --> 00:41:11,745
그러나 우리는 확률을 원했다는 것을 기억하십시오.

1492
00:41:10,507 --> 00:41:12,545


1493
00:41:11,745 --> 00:41:13,524
하나에 가까워지면,

1494
00:41:13,524 --> 00:41:17,071
그래서 이제 로그는 단조로운 함수입니다.

1495
00:41:14,324 --> 00:41:17,871


1496
00:41:17,071 --> 00:41:18,414
수학적으로 밝혀졌습니다.

1497
00:41:18,414 --> 00:41:20,840
로그를 최대화하는 것이 더 쉽다.

1498
00:41:19,214 --> 00:41:21,640


1499
00:41:20,840 --> 00:41:23,277
원시 확률을 최대화하는 것보다,

1500
00:41:21,640 --> 00:41:24,077


1501
00:41:23,277 --> 00:41:25,604
그래서 우리는 일지를 고수합니다.

1502
00:41:24,077 --> 00:41:26,404


1503
00:41:25,604 --> 00:41:26,984
그리고 이제 로그는 단조롭지 만,

1504
00:41:26,984 --> 00:41:30,244
그래서 우리가 정확한 클래스의 로그 P를 최대화한다면,

1505
00:41:27,784 --> 00:41:31,044


1506
00:41:30,244 --> 00:41:32,599
그건 우리가 그걸 원한다는 뜻이고,

1507
00:41:31,044 --> 00:41:33,399


1508
00:41:32,599 --> 00:41:36,024
그러나 손실 함수는 좋지 않음을 측정한다.

1509
00:41:33,399 --> 00:41:36,824


1510
00:41:36,024 --> 00:41:37,454
그래서 우리는 마이너스 1을 넣을 필요가있다.

1511
00:41:37,454 --> 00:41:40,051
그것이 올바른 방향으로 나아갈 수 있도록.

1512
00:41:38,254 --> 00:41:40,851


1513
00:41:40,051 --> 00:41:42,314
SVM에 대한 우리의 손실 함수

1514
00:41:40,851 --> 00:41:43,114


1515
00:41:42,314 --> 00:41:44,648
확률의 마이너스 로그가 될 것입니다.

1516
00:41:43,114 --> 00:41:45,448


1517
00:41:44,648 --> 00:41:46,148
진정한 계급의

1518
00:41:45,448 --> 00:41:46,948


1519
00:41:48,909 --> 00:41:51,322
네, 여기 그것이 요약입니다.

1520
00:41:49,709 --> 00:41:52,122


1521
00:41:51,322 --> 00:41:53,782
우리가 점수를 받고, 우리가 softmax를 돌며,

1522
00:41:52,122 --> 00:41:54,582


1523
00:41:53,782 --> 00:41:56,075
이제 우리의 손실은 확률의 로그를 뺀 것이다.

1524
00:41:54,582 --> 00:41:56,875


1525
00:41:56,075 --> 00:41:57,575
진정한 계급의

1526
00:41:56,875 --> 00:41:58,375


1527
00:42:01,697 --> 00:42:03,743
좋아, 그럼이게 어떻게 생겼는지 봐.

1528
00:42:02,497 --> 00:42:04,543


1529
00:42:03,743 --> 00:42:05,043
구체적인 예를 들면,

1530
00:42:05,043 --> 00:42:07,749
그 다음 우리는 우리 아주 좋아하는 아름다운 고양이

1531
00:42:05,843 --> 00:42:08,549


1532
00:42:07,749 --> 00:42:10,634
세 가지 예를 통해 우리는이 세 가지 점수를 얻었습니다.

1533
00:42:08,549 --> 00:42:11,434


1534
00:42:10,634 --> 00:42:14,486
우리 선형 분류기에서 나오는

1535
00:42:11,434 --> 00:42:15,286


1536
00:42:14,486 --> 00:42:16,296
이 점수는 정확하게 그들이했던 방식입니다.

1537
00:42:15,286 --> 00:42:17,096


1538
00:42:16,296 --> 00:42:18,712
SVM 손실의 맥락에서

1539
00:42:17,096 --> 00:42:19,512


1540
00:42:18,712 --> 00:42:20,826
하지만 이제는이 점수를받는 것보다

1541
00:42:19,512 --> 00:42:21,626


1542
00:42:20,826 --> 00:42:22,817
그들을 우리의 손실 함수에 직접적으로 넣는 것,

1543
00:42:21,626 --> 00:42:23,617


1544
00:42:22,817 --> 00:42:25,422
우리는 그것들을 모두 가져 와서 그들을 압도 할 것입니다.

1545
00:42:23,617 --> 00:42:26,222


1546
00:42:25,422 --> 00:42:26,990
그래서 그들은 모두 긍정적입니다,

1547
00:42:26,990 --> 00:42:29,095
그리고 우리는 그들을 정상화시켜 확실하게 할 것입니다.

1548
00:42:27,790 --> 00:42:29,895


1549
00:42:29,095 --> 00:42:31,025
그들 모두가 하나가된다.

1550
00:42:29,895 --> 00:42:31,825


1551
00:42:31,025 --> 00:42:33,788
그리고 이제 우리의 손실은 마이너스 로그가 될 것입니다.

1552
00:42:31,825 --> 00:42:34,588


1553
00:42:33,788 --> 00:42:35,788
진실한 학급 점수의.

1554
00:42:34,588 --> 00:42:36,588


1555
00:42:36,643 --> 00:42:38,893
그래서 그것은 softmax 손실입니다,

1556
00:42:37,443 --> 00:42:39,693


1557
00:42:40,156 --> 00:42:43,823
다항 로지스틱 회귀 (multinomial
logistic regression)라고도합니다.

1558
00:42:40,956 --> 00:42:44,623


1559
00:42:45,496 --> 00:42:47,253
이제 몇 가지 질문을했습니다.

1560
00:42:47,253 --> 00:42:50,750
다중 클래스 SVM 손실에 대한
직감을 얻으려고 시도 할 때,

1561
00:42:48,053 --> 00:42:51,550


1562
00:42:50,750 --> 00:42:53,778
동일한 질문에 대해 생각하는 것이 유용합니다.

1563
00:42:51,550 --> 00:42:54,578


1564
00:42:53,778 --> 00:42:57,360
softmax 손실과 대비됩니다.

1565
00:42:54,578 --> 00:42:58,160


1566
00:42:57,360 --> 00:42:58,614
그럼 질문은,

1567
00:42:58,614 --> 00:43:02,697
Softmax 손실의 최소 및 최대 값은 얼마입니까?

1568
00:42:59,414 --> 00:43:03,497


1569
00:43:04,984 --> 00:43:06,785
좋아, 아마 그렇게 확신 할 수는 없어.

1570
00:43:05,784 --> 00:43:07,585


1571
00:43:06,785 --> 00:43:08,303
너무 많은 로그와 합계가 있습니다.

1572
00:43:08,303 --> 00:43:09,720
여기 들어가.

1573
00:43:09,103 --> 00:43:10,520


1574
00:43:11,298 --> 00:43:13,759
그래서 대답은 분 손실이 0이라는 것입니다.

1575
00:43:12,098 --> 00:43:14,559


1576
00:43:13,759 --> 00:43:15,430
최대 손실은 무한대입니다.

1577
00:43:15,430 --> 00:43:18,263
그리고 당신이 이것을 볼 수있는 방법,

1578
00:43:16,230 --> 00:43:19,063


1579
00:43:19,422 --> 00:43:21,165
우리가 원하는 확률 분포

1580
00:43:21,165 --> 00:43:24,467
올바른 클래스에 하나, 잘못된 클래스에 0,

1581
00:43:21,965 --> 00:43:25,267


1582
00:43:24,467 --> 00:43:25,842
우리가하는 방식은,

1583
00:43:25,842 --> 00:43:27,199
그래서 그 경우라면,

1584
00:43:27,199 --> 00:43:31,366
로그 안에있는이 것은 결국 하나가 될 것이고,

1585
00:43:27,999 --> 00:43:32,166


1586
00:43:33,662 --> 00:43:36,750
실제 클래스의 로그 확률이기 때문에,

1587
00:43:34,462 --> 00:43:37,550


1588
00:43:36,750 --> 00:43:40,917
하나의 로그가 0 일 때, 하나의
로그를 뺀 것이 여전히 0입니다.

1589
00:43:37,550 --> 00:43:41,717


1590
00:43:41,893 --> 00:43:44,001
그래서 우리가 그 일을 완전히 올바르게한다면,

1591
00:43:42,693 --> 00:43:44,801


1592
00:43:44,001 --> 00:43:46,515
우리의 손실은 0이 될 것입니다.

1593
00:43:44,801 --> 00:43:47,315


1594
00:43:46,515 --> 00:43:50,249
그러나 그런데, 그 일을 완전히 옳게하기 위해서,

1595
00:43:47,315 --> 00:43:51,049


1596
00:43:50,249 --> 00:43:53,582
우리 점수는 어떻게 생겼을까요?

1597
00:43:51,049 --> 00:43:54,382


1598
00:43:55,963 --> 00:43:57,252
불평, 불평.

1599
00:43:57,252 --> 00:44:00,135
그래서 점수는 실제로 상당히 극단적으로되어야 할 것입니다.

1600
00:43:58,052 --> 00:44:00,935


1601
00:44:00,135 --> 00:44:01,572
무한으로 향한 것처럼.

1602
00:44:01,572 --> 00:44:04,384
그래서 우리는 실제로이 지수를 가지고 있기 때문에,

1603
00:44:02,372 --> 00:44:05,184


1604
00:44:04,384 --> 00:44:06,098
이 정규화, 유일한 방법

1605
00:44:06,098 --> 00:44:09,029
우리는 실제로 하나의 확률 분포를 얻을 수있다.

1606
00:44:06,898 --> 00:44:09,829


1607
00:44:09,029 --> 00:44:11,970
그리고 제로는 실제로 무한 점수를 넣고 있습니다.

1608
00:44:09,829 --> 00:44:12,770


1609
00:44:11,970 --> 00:44:16,006
올바른 클래스의 경우 및 무한대 점수를 뺀 경우

1610
00:44:12,770 --> 00:44:16,806


1611
00:44:16,006 --> 00:44:17,509
잘못된 모든 클래스에 대해.

1612
00:44:17,509 --> 00:44:20,652
그리고 컴퓨터는 무한 성으로 잘하지 못합니다.

1613
00:44:18,309 --> 00:44:21,452


1614
00:44:20,652 --> 00:44:22,239
그래서 실제로, 당신은 결코 손실이 없어 질 것입니다.

1615
00:44:22,239 --> 00:44:24,108
유한 정밀도를 가진이 물건에.

1616
00:44:23,039 --> 00:44:24,908


1617
00:44:24,108 --> 00:44:25,755
그러나 당신은 여전히이 해석을 가지고 있습니다.

1618
00:44:25,755 --> 00:44:29,223
여기서 0은 이론상의 최소 손실입니다.

1619
00:44:26,555 --> 00:44:30,023


1620
00:44:29,223 --> 00:44:31,607
그리고 최대 손실은 제한되지 않습니다.

1621
00:44:30,023 --> 00:44:32,407


1622
00:44:31,607 --> 00:44:35,180
우리가 확률 질량이 0 일 때

1623
00:44:32,407 --> 00:44:35,980


1624
00:44:35,180 --> 00:44:39,483
올바른 클래스에서 마이너스 로그를 얻습니다.

1625
00:44:35,980 --> 00:44:40,283


1626
00:44:39,483 --> 00:44:42,643
0의 로그는 마이너스 무한대이며,

1627
00:44:40,283 --> 00:44:43,443


1628
00:44:42,643 --> 00:44:46,283
그래서 0의 마이너스 로그는 플러스 무한대가됩니다.

1629
00:44:43,443 --> 00:44:47,083


1630
00:44:46,283 --> 00:44:47,334
그래서 그것은 정말로 나쁘다.

1631
00:44:47,334 --> 00:44:49,071
그러나 다시, 당신은 결코 여기에 결코 도착하지 않을 것입니다.

1632
00:44:49,071 --> 00:44:53,748
이 확률을 실제로 얻을 수있는 유일한 방법이기 때문에

1633
00:44:49,871 --> 00:44:54,548


1634
00:44:53,748 --> 00:44:58,563
0이 되려면 올바른 클래스 점수에 대한 e가 0이고,

1635
00:44:54,548 --> 00:44:59,363


1636
00:44:58,563 --> 00:45:00,093
그 올바른 클래스 점수가

1637
00:45:00,093 --> 00:45:01,630
음의 무한대입니다.

1638
00:45:01,630 --> 00:45:04,034
다시 한번 말하지만, 당신은 실제로 이러한
최소한의 것을 얻지 못할 것입니다.

1639
00:45:02,430 --> 00:45:04,834


1640
00:45:04,034 --> 00:45:07,117
유한 정밀도의 최대 값.

1641
00:45:04,834 --> 00:45:07,917


1642
00:45:08,863 --> 00:45:11,032
그럼 우리가이 디버깅을했는지 기억해.

1643
00:45:09,663 --> 00:45:11,832


1644
00:45:11,032 --> 00:45:14,340
다중 클래스 SVM의 컨텍스트에서 온 전성
체크 (sanity check) 질문,

1645
00:45:11,832 --> 00:45:15,140


1646
00:45:14,340 --> 00:45:16,401
우리는 softmax에 대해서도 같은 질문을 할 수 있습니다.

1647
00:45:15,140 --> 00:45:17,201


1648
00:45:16,401 --> 00:45:19,138
모든 S가 작고 약 0이면,

1649
00:45:17,201 --> 00:45:19,938


1650
00:45:19,138 --> 00:45:21,287
그러면 여기서 손실이 무엇입니까?

1651
00:45:19,938 --> 00:45:22,087


1652
00:45:21,287 --> 00:45:22,292
그래, 대답?

1653
00:45:22,292 --> 00:45:24,363
- [학생] C에서 1을 제외한 로그.

1654
00:45:23,092 --> 00:45:25,163


1655
00:45:24,363 --> 00:45:27,045
- C에서 1의 로그를 뺀거야?

1656
00:45:25,163 --> 00:45:27,845


1657
00:45:27,045 --> 00:45:28,795
내 생각 엔 그래,

1658
00:45:27,845 --> 00:45:29,595


1659
00:45:30,026 --> 00:45:33,352
그래서 C를 넘어서는 하나의 로그를 뺀 것입니다.

1660
00:45:30,826 --> 00:45:34,152


1661
00:45:33,352 --> 00:45:34,693
로그가 그 일을 뒤집을 수 있기 때문에

1662
00:45:34,693 --> 00:45:36,526
그러면 C의 로그 일뿐입니다.

1663
00:45:35,493 --> 00:45:37,326


1664
00:45:36,526 --> 00:45:38,079
네, 그래서 C의 로그 일뿐입니다.

1665
00:45:38,079 --> 00:45:39,909
그리고 다시, 이것은 훌륭한 디버깅 일입니다.

1666
00:45:38,879 --> 00:45:40,709


1667
00:45:39,909 --> 00:45:41,911
이 softmax 손실로 모델을 훈련하는 경우,

1668
00:45:40,709 --> 00:45:42,711


1669
00:45:41,911 --> 00:45:43,977
첫 번째 반복을 확인해야합니다.

1670
00:45:42,711 --> 00:45:44,777


1671
00:45:43,977 --> 00:45:47,894
로그 C가 아니면 뭔가 잘못되었습니다.

1672
00:45:44,777 --> 00:45:48,694


1673
00:45:50,051 --> 00:45:53,257
그래서 우리는이 두 손실 함수를
비교하고 대조 할 수 있습니다.

1674
00:45:50,851 --> 00:45:54,057


1675
00:45:53,257 --> 00:45:54,600
약간.

1676
00:45:54,600 --> 00:45:56,111
선형 분류의 관점에서,

1677
00:45:56,111 --> 00:45:57,532
이 설정은 동일하게 보입니다.

1678
00:45:57,532 --> 00:45:59,246
곱해진 W 행렬이 있습니다.

1679
00:45:59,246 --> 00:46:02,072
이 유령의 유령을 산출하기위한 우리의 의견에 반하여,

1680
00:46:00,046 --> 00:46:02,872


1681
00:46:02,072 --> 00:46:04,046
이제 두 손실 함수의 차이점

1682
00:46:02,872 --> 00:46:04,846


1683
00:46:04,046 --> 00:46:06,434
우리가 점수를 해석하는 방법은

1684
00:46:04,846 --> 00:46:07,234


1685
00:46:06,434 --> 00:46:09,327
나중에 불량을 정량적으로 측정 할 수 있습니다.

1686
00:46:07,234 --> 00:46:10,127


1687
00:46:09,327 --> 00:46:11,562
그래서 SVM을 위해 우리는
들어가서 마진을 살펴볼 것입니다.

1688
00:46:10,127 --> 00:46:12,362


1689
00:46:11,562 --> 00:46:14,987
올바른 수업의 점수 사이에

1690
00:46:12,362 --> 00:46:15,787


1691
00:46:14,987 --> 00:46:17,138
잘못된 수업의 점수,

1692
00:46:15,787 --> 00:46:17,938


1693
00:46:17,138 --> 00:46:20,256
이 소프트 맥스 또는 크로스 엔트로피 손실에 대해서는,

1694
00:46:17,938 --> 00:46:21,056


1695
00:46:20,256 --> 00:46:22,703
우리는 가야하고 확률 분포를 계산할 것입니다.

1696
00:46:21,056 --> 00:46:23,503


1697
00:46:22,703 --> 00:46:24,980
그런 다음 마이너스 로그 확률을 살펴보십시오.

1698
00:46:23,503 --> 00:46:25,780


1699
00:46:24,980 --> 00:46:26,663
올바른 클래스의.

1700
00:46:26,663 --> 00:46:28,996
그래서 때때로 당신이 보면,

1701
00:46:27,463 --> 00:46:29,796


1702
00:46:30,198 --> 00:46:33,216
관점에서, 나는 그 점을 건너 뛸 것이다.

1703
00:46:30,998 --> 00:46:34,016


1704
00:46:33,216 --> 00:46:34,917
[웃음]

1705
00:46:34,917 --> 00:46:36,358
흥미로운 또 다른 질문입니다.

1706
00:46:36,358 --> 00:46:40,854
이 두 손실 함수를 대조하면 생각할 때,

1707
00:46:37,158 --> 00:46:41,654


1708
00:46:40,854 --> 00:46:44,241
이 예제 포인트가 있다고 가정 해 보겠습니다.

1709
00:46:41,654 --> 00:46:45,041


1710
00:46:44,241 --> 00:46:46,058
당신이 그것의 점수를 바꾸면,

1711
00:46:45,041 --> 00:46:46,858


1712
00:46:46,058 --> 00:46:49,975
우리는이 점에 대해 세 가지 점수가 있다고 가정합니다.

1713
00:46:46,858 --> 00:46:50,775


1714
00:46:52,859 --> 00:46:54,042
바닥에있는 부분을 무시하십시오.

1715
00:46:54,042 --> 00:46:56,558
그러나이 예제로 돌아 가면

1716
00:46:54,842 --> 00:46:57,358


1717
00:46:56,558 --> 00:46:59,814
여기서 다중 - 클래스 SVM 손실에서,

1718
00:46:57,358 --> 00:47:00,614


1719
00:46:59,814 --> 00:47:04,144
우리가 차를 가지고 있었을 때,
차 점수가 훨씬 좋았습니다.

1720
00:47:00,614 --> 00:47:04,944


1721
00:47:04,144 --> 00:47:06,293
모든 잘못된 수업보다

1722
00:47:04,944 --> 00:47:07,093


1723
00:47:06,293 --> 00:47:08,546
그 차 이미지의 점수를 흔들어 쓴다.

1724
00:47:07,093 --> 00:47:09,346


1725
00:47:08,546 --> 00:47:11,246
다중 클래스 SVM 손실을 전혀 변경하지 않았습니다.

1726
00:47:09,346 --> 00:47:12,046


1727
00:47:11,246 --> 00:47:13,141
유일한 이유는 SVM 손실

1728
00:47:12,046 --> 00:47:13,941


1729
00:47:13,141 --> 00:47:15,282
그 정확한 점수를 얻는 것에 관심이있었습니다.

1730
00:47:13,941 --> 00:47:16,082


1731
00:47:15,282 --> 00:47:18,359
잘못된 점수보다 큰 여백보다 커야합니다.

1732
00:47:16,082 --> 00:47:19,159


1733
00:47:18,359 --> 00:47:20,392
하지만 이제 softmax 손실은 실제로 상당히 다릅니다.

1734
00:47:19,159 --> 00:47:21,192


1735
00:47:20,392 --> 00:47:21,726
이 점에서.

1736
00:47:21,726 --> 00:47:24,174
softmax 손실은 실제로 항상 운전하기를 원합니다.

1737
00:47:22,526 --> 00:47:24,974


1738
00:47:24,174 --> 00:47:26,438
그 확률 덩어리가 하나가 될 때까지.

1739
00:47:24,974 --> 00:47:27,238


1740
00:47:26,438 --> 00:47:29,771
따라서 점수가 매우 높더라도

1741
00:47:27,238 --> 00:47:30,571


1742
00:47:31,143 --> 00:47:32,606
올바른 수업에, 그리고 매우 낮은 점수

1743
00:47:32,606 --> 00:47:34,298
모든 잘못된 수업에

1744
00:47:34,298 --> 00:47:36,852
softmax 당신이 더 많은
확률 질량을 쌓기를 원할 것입니다

1745
00:47:35,098 --> 00:47:37,652


1746
00:47:36,852 --> 00:47:40,044
올바른 수업을 듣고 점수를 계속 누르십시오.

1747
00:47:37,652 --> 00:47:40,844


1748
00:47:40,044 --> 00:47:42,350
무한대쪽으로 올라가는 올바른 클래스의

1749
00:47:40,844 --> 00:47:43,150


1750
00:47:42,350 --> 00:47:44,152
잘못된 수업의 점수

1751
00:47:43,150 --> 00:47:44,952


1752
00:47:44,152 --> 00:47:46,138
음의 무한대쪽으로 내려 갔다.

1753
00:47:44,952 --> 00:47:46,938


1754
00:47:46,138 --> 00:47:47,435
그래서 그것은 흥미로운 차이입니다.

1755
00:47:47,435 --> 00:47:49,968
실제로이 두 손실 함수 사이.

1756
00:47:48,235 --> 00:47:50,768


1757
00:47:49,968 --> 00:47:53,530
그 SVM, 막대 위에이 데이터 포인트를 가져올거야.

1758
00:47:50,768 --> 00:47:54,330


1759
00:47:53,530 --> 00:47:55,920
정확하게 분류하고 그냥 포기하면

1760
00:47:54,330 --> 00:47:56,720


1761
00:47:55,920 --> 00:47:57,739
그 데이터 포인트에 대해서는 더 이상 신경 쓰지 않습니다.

1762
00:47:56,720 --> 00:47:58,539


1763
00:47:57,739 --> 00:48:00,296
softmax는 항상 지속적으로 개선하려고 노력하지만

1764
00:47:58,539 --> 00:48:01,096


1765
00:48:00,296 --> 00:48:01,968
모든 단일 데이터 요소가 더 좋아지고 나아질 것입니다.

1766
00:48:01,968 --> 00:48:03,838
그리고 더 좋고 더 낫다.

1767
00:48:02,768 --> 00:48:04,638


1768
00:48:03,838 --> 00:48:05,405
그래서 그것은 흥미로운 차이입니다.

1769
00:48:05,405 --> 00:48:07,378
이 두 기능 사이.

1770
00:48:06,205 --> 00:48:08,178


1771
00:48:07,378 --> 00:48:09,974
실제로, 나는 그것이 큰 차이를 만들어
내지 않는 경향이 있다고 생각한다.

1772
00:48:08,178 --> 00:48:10,774


1773
00:48:09,974 --> 00:48:12,240
당신이 선택하는 것은 그들이 수행하는 경향이 있습니다.

1774
00:48:10,774 --> 00:48:13,040


1775
00:48:12,240 --> 00:48:14,040
꽤 유사하게,

1776
00:48:13,040 --> 00:48:14,840


1777
00:48:14,040 --> 00:48:15,966
적어도 많은 깊은 학습 응용 프로그램.

1778
00:48:14,840 --> 00:48:16,766


1779
00:48:15,966 --> 00:48:19,137
그러나 이러한 차이를 유지하는 것이 매우 유용합니다.

1780
00:48:16,766 --> 00:48:19,937


1781
00:48:19,137 --> 00:48:19,970
마음에.

1782
00:48:19,937 --> 00:48:20,770


1783
00:48:23,054 --> 00:48:26,176
네, 여기에서 우리가 어디로 왔는지 다시 정리하려면,

1784
00:48:23,854 --> 00:48:26,976


1785
00:48:26,176 --> 00:48:29,585
xs와 ys의 데이터 세트를 가지고 있다는 것입니다.

1786
00:48:26,976 --> 00:48:30,385


1787
00:48:29,585 --> 00:48:33,018
우리는 우리의 선형 분류기를 사용하여
몇 가지 점수 함수를 얻습니다.

1788
00:48:30,385 --> 00:48:33,818


1789
00:48:33,018 --> 00:48:36,595
우리의 점수 S를 계산하기 위해서, 우리의 입력에서 x,

1790
00:48:33,818 --> 00:48:37,395


1791
00:48:36,595 --> 00:48:38,311
그리고 나서 우리는 손실 함수를 사용할 것입니다,

1792
00:48:38,311 --> 00:48:41,134
어쩌면 softmax 또는 SVM
또는 일부 다른 손실 함수

1793
00:48:39,111 --> 00:48:41,934


1794
00:48:41,134 --> 00:48:45,997
양적으로 얼마나 나쁜지를 예측하는 것이었다.

1795
00:48:41,934 --> 00:48:46,797


1796
00:48:45,997 --> 00:48:48,954
이 땅에 비하면 진정한 목표, y.

1797
00:48:46,797 --> 00:48:49,754


1798
00:48:48,954 --> 00:48:52,410
그리고 나서 우리는 종종이 손실 함수를 증가시킬 것입니다.

1799
00:48:49,754 --> 00:48:53,210


1800
00:48:52,410 --> 00:48:53,849
정규화 용어로,

1801
00:48:53,849 --> 00:48:56,174
훈련 자료를 맞추는 사이에

1802
00:48:54,649 --> 00:48:56,974


1803
00:48:56,174 --> 00:48:59,059
더 단순한 모델을 선호합니다.

1804
00:48:56,974 --> 00:48:59,859


1805
00:48:59,059 --> 00:49:01,490
그래서 이것은 꽤 일반적인 개요입니다.

1806
00:48:59,859 --> 00:49:02,290


1807
00:49:01,490 --> 00:49:03,966
감독 학습이라고 불리는 많은 것을

1808
00:49:02,290 --> 00:49:04,766


1809
00:49:03,966 --> 00:49:07,065
우리가 앞으로 나아갈 때 깊은 학습에서 볼 수있는 것은,

1810
00:49:04,766 --> 00:49:07,865


1811
00:49:07,065 --> 00:49:10,888
일반적으로 당신은 어떤 함수
f를 지정하기를 원할 것입니다.

1812
00:49:07,865 --> 00:49:11,688


1813
00:49:10,888 --> 00:49:12,664
구조가 매우 복잡 할 수 있습니다.

1814
00:49:12,664 --> 00:49:14,489
결정하는 몇 가지 손실 함수를 지정하십시오.

1815
00:49:13,464 --> 00:49:15,289


1816
00:49:14,489 --> 00:49:18,028
당신의 알고리즘이 얼마나 잘하고 있는지,

1817
00:49:15,289 --> 00:49:18,828


1818
00:49:18,028 --> 00:49:19,645
파라미터의 임의의 값이 주어지면,

1819
00:49:19,645 --> 00:49:21,053
일부 정규화 용어

1820
00:49:21,053 --> 00:49:24,260
모델 복잡성을 처벌하는 방법

1821
00:49:21,853 --> 00:49:25,060


1822
00:49:24,260 --> 00:49:26,141
그런 다음이 것들을 하나로 결합합니다.

1823
00:49:25,060 --> 00:49:26,941


1824
00:49:26,141 --> 00:49:27,624
W를 찾으려고 노력해.

1825
00:49:27,624 --> 00:49:30,866
이 최종 손실 함수를 최소화합니다.

1826
00:49:28,424 --> 00:49:31,666


1827
00:49:30,866 --> 00:49:32,120
그러나 그때 질문은,

1828
00:49:32,120 --> 00:49:33,636
우리가 실제로 그 일을 어떻게 수행할까요?

1829
00:49:33,636 --> 00:49:37,132
손실을 최소화하는이 W를 실제로 어떻게 찾을 수 있습니까?

1830
00:49:34,436 --> 00:49:37,932


1831
00:49:37,132 --> 00:49:40,461
그리고 그것은 우리를 최적화 주제로 이끌고 있습니다.

1832
00:49:37,932 --> 00:49:41,261


1833
00:49:40,461 --> 00:49:43,033
그래서 우리가 최적화를 할 때,

1834
00:49:41,261 --> 00:49:43,833


1835
00:49:43,033 --> 00:49:45,495
나는 보통 걷는 관점에서 생각한다.

1836
00:49:43,833 --> 00:49:46,295


1837
00:49:45,495 --> 00:49:47,482
큰 계곡 주위에.

1838
00:49:46,295 --> 00:49:48,282


1839
00:49:47,482 --> 00:49:51,951
그래서이 큰 계곡을 걷고 있다는 생각이 들었습니다.

1840
00:49:48,282 --> 00:49:52,751


1841
00:49:51,951 --> 00:49:54,183
다른 산과 계곡과 시내와 함께

1842
00:49:52,751 --> 00:49:54,983


1843
00:49:54,183 --> 00:49:56,903
물건들, 그리고이 풍경의 모든 지점

1844
00:49:54,983 --> 00:49:57,703


1845
00:49:56,903 --> 00:50:00,729
파라미터 W의 일부 설정에 대응한다.

1846
00:49:57,703 --> 00:50:01,529


1847
00:50:00,729 --> 00:50:03,054
그리고이 계곡을 걸어 다니는이 작은 녀석입니다.

1848
00:50:01,529 --> 00:50:03,854


1849
00:50:03,054 --> 00:50:04,517
그리고 당신은 찾으려고 노력하고 있습니다.

1850
00:50:04,517 --> 00:50:06,216
이들 각 점의 높이,

1851
00:50:06,216 --> 00:50:10,728
미안하지만, W의 설정에 의해 초래 된 손실과 같습니다.

1852
00:50:07,016 --> 00:50:11,528


1853
00:50:10,728 --> 00:50:12,879
그리고이 작은 남자로서의 당신 직업

1854
00:50:11,528 --> 00:50:13,679


1855
00:50:12,879 --> 00:50:14,181
이 풍경을 돌아 다니며,

1856
00:50:14,181 --> 00:50:18,000
당신은 어떻게 든이 계곡의 바닥을 찾을 필요가 있습니다.

1857
00:50:14,981 --> 00:50:18,800


1858
00:50:18,000 --> 00:50:20,517
그리고 이것은 일반적으로 어려운 문제입니다.

1859
00:50:18,800 --> 00:50:21,317


1860
00:50:20,517 --> 00:50:22,851
너는 아마도 내가 정말로 영리하다고 생각할지도 모른다.

1861
00:50:21,317 --> 00:50:23,651


1862
00:50:22,851 --> 00:50:25,223
분석 속성에 대해 정말 열심히 생각할 수 있습니다.

1863
00:50:23,651 --> 00:50:26,023


1864
00:50:25,223 --> 00:50:27,379
내 손실 기능, 내 모든 정규화,

1865
00:50:26,023 --> 00:50:28,179


1866
00:50:27,379 --> 00:50:30,246
어쩌면 나는 미니 마이저를 적어 두거나,

1867
00:50:28,179 --> 00:50:31,046


1868
00:50:30,246 --> 00:50:33,089
그리고 그것은 마술처럼 순간 이동에 해당합니다

1869
00:50:31,046 --> 00:50:33,889


1870
00:50:33,089 --> 00:50:35,509
이 골짜기의 바닥까지.

1871
00:50:33,889 --> 00:50:36,309


1872
00:50:35,509 --> 00:50:38,740
그러나 실제로, 일단 당신의 예측 함수, f,

1873
00:50:36,309 --> 00:50:39,540


1874
00:50:38,740 --> 00:50:40,725
당신의 손실 함수와 당신의 정규식,

1875
00:50:39,540 --> 00:50:41,525


1876
00:50:40,725 --> 00:50:42,442
일단 이러한 것들이 크고 복잡 해지면

1877
00:50:42,442 --> 00:50:44,609
신경 네트워크를 사용하여,

1878
00:50:43,242 --> 00:50:45,409


1879
00:50:46,190 --> 00:50:48,055
적어 두려는 데별로 희망이 없습니다.

1880
00:50:46,990 --> 00:50:48,855


1881
00:50:48,055 --> 00:50:49,698
노골적인 분석 솔루션

1882
00:50:49,698 --> 00:50:52,017
그것은 당신을 미니 마에 직접 데려갑니다.

1883
00:50:50,498 --> 00:50:52,817


1884
00:50:52,017 --> 00:50:53,271
그래서 실제로

1885
00:50:53,271 --> 00:50:55,485
우리는 다양한 유형의 반복적 인
방법을 사용하는 경향이 있습니다

1886
00:50:54,071 --> 00:50:56,285


1887
00:50:55,485 --> 00:50:57,390
우리는 몇 가지 해결책으로 시작한다.

1888
00:50:56,285 --> 00:50:58,190


1889
00:50:57,390 --> 00:51:00,524
시간이 지남에 따라 서서히 개선하십시오.

1890
00:50:58,190 --> 00:51:01,324


1891
00:51:00,524 --> 00:51:03,357
그래서 아주 첫 번째, 어리석은 일

1892
00:51:01,324 --> 00:51:04,157


1893
00:51:04,527 --> 00:51:06,985
당신이 상상할 수있는 것은 무작위 검색입니다.

1894
00:51:05,327 --> 00:51:07,785


1895
00:51:06,985 --> 00:51:09,024
그것은 단지 W의 무리를 취할 것입니다,

1896
00:51:07,785 --> 00:51:09,824


1897
00:51:09,024 --> 00:51:12,180
무작위로 샘플링하여 손실 함수에 던져 넣습니다.

1898
00:51:09,824 --> 00:51:12,980


1899
00:51:12,180 --> 00:51:14,963
그들이 얼마나 잘하는지보십시오.

1900
00:51:12,980 --> 00:51:15,763


1901
00:51:14,963 --> 00:51:17,416
그래서 스포일러 경고, 이것은 정말 나쁜 알고리즘입니다,

1902
00:51:15,763 --> 00:51:18,216


1903
00:51:17,416 --> 00:51:18,828
당신은 아마 이것을 사용하면 안됩니다.

1904
00:51:18,828 --> 00:51:23,323
그러나 적어도 그것은 당신이 시도하는
것을 상상할 수도있는 한 가지입니다.

1905
00:51:19,628 --> 00:51:24,123


1906
00:51:23,323 --> 00:51:25,180
그리고 우리는 실제로 이것을 할 수 있습니다.

1907
00:51:24,123 --> 00:51:25,980


1908
00:51:25,180 --> 00:51:27,856
선형 분류기를 실제로 훈련 할 수 있습니다.

1909
00:51:25,980 --> 00:51:28,656


1910
00:51:27,856 --> 00:51:30,813
무작위 검색을 통해 CIFAR-10

1911
00:51:28,656 --> 00:51:31,613


1912
00:51:30,813 --> 00:51:34,152
그리고 이것을 위해 10 개의 클래스가 있습니다.

1913
00:51:31,613 --> 00:51:34,952


1914
00:51:34,152 --> 00:51:35,997
그래서 무작위 확률은 10 %입니다.

1915
00:51:34,952 --> 00:51:36,797


1916
00:51:35,997 --> 00:51:39,768
우리가 몇 가지 무작위 시도를했다면,

1917
00:51:36,797 --> 00:51:40,568


1918
00:51:39,768 --> 00:51:42,212
우리는 단지 투명한 바보 같은 운을 통하여 단지 발견했다.

1919
00:51:40,568 --> 00:51:43,012


1920
00:51:42,212 --> 00:51:45,645
어쩌면 15 %의 정확도를 가진 W의 일부 설정.

1921
00:51:43,012 --> 00:51:46,445


1922
00:51:45,645 --> 00:51:48,019
그래서 무작위보다는 낫다.

1923
00:51:46,445 --> 00:51:48,819


1924
00:51:48,019 --> 00:51:50,238
그러나 미술 수준은 아마 95 %

1925
00:51:48,819 --> 00:51:51,038


1926
00:51:50,238 --> 00:51:53,831
그래서 여기에 약간의 격차가 있습니다.

1927
00:51:51,038 --> 00:51:54,631


1928
00:51:53,831 --> 00:51:56,748
다시 한번 말하지만 실제로 이것을 사용하지 마십시오.

1929
00:51:54,631 --> 00:51:57,548


1930
00:51:56,748 --> 00:51:58,138
하지만 당신은 이것이 뭔가 있다고 상상할 수 있습니다.

1931
00:51:58,138 --> 00:52:00,677
당신은 잠재적으로 할 수 있습니다.

1932
00:51:58,938 --> 00:52:01,477


1933
00:52:00,677 --> 00:52:02,467
따라서 실제로는 더 나은 전략 일 것입니다.

1934
00:52:02,467 --> 00:52:04,664
실제로 일부 로컬 지오메트리를 사용하고 있습니다.

1935
00:52:03,267 --> 00:52:05,464


1936
00:52:04,664 --> 00:52:06,168
이 풍경.

1937
00:52:06,168 --> 00:52:07,614
그래서 당신이 걷고있는이 작은 녀석이라면

1938
00:52:07,614 --> 00:52:09,910
이 풍경 주변,

1939
00:52:08,414 --> 00:52:10,710


1940
00:52:09,910 --> 00:52:12,178
어쩌면 당신은 직접 경로를 볼 수 없습니다.

1941
00:52:10,710 --> 00:52:12,978


1942
00:52:12,178 --> 00:52:13,802
골짜기의 바닥에 이르기까지,

1943
00:52:13,802 --> 00:52:16,031
그러나 당신이 할 수있는 것은 당신의 발로 느끼는 것입니다.

1944
00:52:14,602 --> 00:52:16,831


1945
00:52:16,031 --> 00:52:19,531
로컬 지오메트리가 무엇인지 파악하고,

1946
00:52:16,831 --> 00:52:20,331


1947
00:52:20,697 --> 00:52:21,927
내가 여기 서 있으면,

1948
00:52:21,927 --> 00:52:24,145
어떤 방법으로 내리막 길을 조금 걸릴까요?

1949
00:52:22,727 --> 00:52:24,945


1950
00:52:24,145 --> 00:52:25,595
그래서 당신은 당신의 발로 느낄 수 있습니다.

1951
00:52:25,595 --> 00:52:28,136
지상의 기울기가 어디인지 느껴보십시오.

1952
00:52:26,395 --> 00:52:28,936


1953
00:52:28,136 --> 00:52:30,861
이 방향으로 조금 나를 쓰러 뜨 렸어?

1954
00:52:28,936 --> 00:52:31,661


1955
00:52:30,861 --> 00:52:32,649
그리고 그 방향으로 나아갈 수 있습니다.

1956
00:52:32,649 --> 00:52:34,037
그리고 너는 조금 내려갈 것이다.

1957
00:52:34,037 --> 00:52:36,260
당신의 발로 다시 어떤 느낌이 내려 졌는지 알아 내려고,

1958
00:52:34,837 --> 00:52:37,060


1959
00:52:36,260 --> 00:52:37,704
그런 다음 반복해서 반복하십시오.

1960
00:52:37,704 --> 00:52:39,529
당신이 바닥에 끝나기를 희망합니다.

1961
00:52:38,504 --> 00:52:40,329


1962
00:52:39,529 --> 00:52:41,526
결국 계곡의.

1963
00:52:40,329 --> 00:52:42,326


1964
00:52:41,526 --> 00:52:45,296
이것은 또한 상대적으로 간단한 알고리즘처럼 보입니다.

1965
00:52:42,326 --> 00:52:46,096


1966
00:52:45,296 --> 00:52:47,236
하지만 실제로 이것은 정말 잘 작동하는 경향이 있습니다.

1967
00:52:46,096 --> 00:52:48,036


1968
00:52:47,236 --> 00:52:50,195
실제로 모든 세부 사항을 올바르게 얻는다면.

1969
00:52:48,036 --> 00:52:50,995


1970
00:52:50,195 --> 00:52:52,209
이것이 일반적으로 우리가 따라야 할 전략입니다.

1971
00:52:50,995 --> 00:52:53,009


1972
00:52:52,209 --> 00:52:53,963
이 거대한 신경 네트워크를 훈련 할 때

1973
00:52:53,963 --> 00:52:57,028
선형 분류 자 및 다른 것들.

1974
00:52:54,763 --> 00:52:57,828


1975
00:52:57,028 --> 00:52:58,769
그래서, 그것은 물결 모양의 작은 손이었습니다.

1976
00:52:58,769 --> 00:52:59,952
그래서 사면은 무엇입니까?

1977
00:52:59,952 --> 00:53:02,337
미적분 클래스를 기억한다면,

1978
00:53:00,752 --> 00:53:03,137


1979
00:53:02,337 --> 00:53:03,842
적어도 하나의 차원에서,

1980
00:53:03,842 --> 00:53:07,673
기울기는이 함수의 미분 값입니다.

1981
00:53:04,642 --> 00:53:08,473


1982
00:53:07,673 --> 00:53:09,971
그래서 우리가 1 차원 함수 f를 가지면,

1983
00:53:08,473 --> 00:53:10,771


1984
00:53:09,971 --> 00:53:12,969
스칼라 x를 취한 다음 높이를 출력합니다.

1985
00:53:10,771 --> 00:53:13,769


1986
00:53:12,969 --> 00:53:16,460
곡선의 일부를 구하면 기울기를 계산할 수 있습니다.

1987
00:53:13,769 --> 00:53:17,260


1988
00:53:16,460 --> 00:53:19,717
또는 파생 상품을 언제든지 상상할 수 있습니다.

1989
00:53:17,260 --> 00:53:20,517


1990
00:53:19,717 --> 00:53:23,467
우리가 작은 걸음을 내딛으면 어떤 방향 으로든,

1991
00:53:20,517 --> 00:53:24,267


1992
00:53:26,298 --> 00:53:28,057
작은 걸음 걸음, 그리고 그 차이를 비교해 보라.

1993
00:53:28,057 --> 00:53:29,798
해당 단계의 함수 값에서

1994
00:53:29,798 --> 00:53:31,679
단계 크기를 0으로 드래그하고,

1995
00:53:30,598 --> 00:53:32,479


1996
00:53:31,679 --> 00:53:33,237
그 함수의 기울기를 우리에게 줄 것이다.

1997
00:53:33,237 --> 00:53:34,895
그 시점에서.

1998
00:53:34,895 --> 00:53:36,094
그리고 이것은 아주 자연스럽게 일반화됩니다.

1999
00:53:36,094 --> 00:53:38,333
다중 변수 기능도 제공합니다.

2000
00:53:36,894 --> 00:53:39,133


2001
00:53:38,333 --> 00:53:41,377
그래서 실제로 x는 스칼라가 아닙니다.

2002
00:53:39,133 --> 00:53:42,177


2003
00:53:41,377 --> 00:53:42,612
그러나 전체 벡터,

2004
00:53:42,612 --> 00:53:46,445
기억하기 때문에, x는 전체 벡터 일 수 있습니다.

2005
00:53:43,412 --> 00:53:47,245


2006
00:53:47,532 --> 00:53:49,063
그래서 우리는이 개념을 일반화 할 필요가있다.

2007
00:53:49,063 --> 00:53:51,941
다중 변수에 이르기까지.

2008
00:53:49,863 --> 00:53:52,741


2009
00:53:51,941 --> 00:53:54,658
그리고 우리가 파생 상품을 사용하는 일반화

2010
00:53:52,741 --> 00:53:55,458


2011
00:53:54,658 --> 00:53:57,896
다중 변수 설정에서 그라데이션,

2012
00:53:55,458 --> 00:53:58,696


2013
00:53:57,896 --> 00:54:01,168
그래디언트는 부분 미분의 벡터입니다.

2014
00:53:58,696 --> 00:54:01,968


2015
00:54:01,168 --> 00:54:04,409
그래디언트는 x와 동일한 모양을 갖습니다.

2016
00:54:01,968 --> 00:54:05,209


2017
00:54:04,409 --> 00:54:07,473
그라디언트의 각 요소는 우리에게 알려줍니다.

2018
00:54:05,209 --> 00:54:08,273


2019
00:54:07,473 --> 00:54:09,595
함수 f의 기울기는 얼마인가?

2020
00:54:08,273 --> 00:54:10,395


2021
00:54:09,595 --> 00:54:12,391
우리가 그 좌표 방향으로 움직인다면.

2022
00:54:10,395 --> 00:54:13,191


2023
00:54:12,391 --> 00:54:13,899
그리고 그라디언트가 밝혀졌습니다.

2024
00:54:13,899 --> 00:54:16,816
이 아주 좋은 재산을 가지려면,

2025
00:54:14,699 --> 00:54:17,616


2026
00:54:18,373 --> 00:54:21,036
그래디언트는 이제 부분 미분의 벡터입니다.

2027
00:54:19,173 --> 00:54:21,836


2028
00:54:21,036 --> 00:54:23,228
그러나 그것은 가장 큰 증가의 방향을 가리킨다.

2029
00:54:21,836 --> 00:54:24,028


2030
00:54:23,228 --> 00:54:25,657
따라서,

2031
00:54:24,028 --> 00:54:26,457


2032
00:54:25,657 --> 00:54:27,545
음의 그래디언트 방향을 보면,

2033
00:54:26,457 --> 00:54:28,345


2034
00:54:27,545 --> 00:54:29,770
그것은 당신에게 가장 큰 감소의 방향을 제시합니다.

2035
00:54:28,345 --> 00:54:30,570


2036
00:54:29,770 --> 00:54:31,612
함수의.

2037
00:54:30,570 --> 00:54:32,412


2038
00:54:31,612 --> 00:54:34,210
그리고 더 일반적으로, 당신이 알고 싶다면,

2039
00:54:32,412 --> 00:54:35,010


2040
00:54:34,210 --> 00:54:37,418
어떤 방향 으로든 내 풍경의 경사는 무엇입니까?

2041
00:54:35,010 --> 00:54:38,218


2042
00:54:37,418 --> 00:54:39,357
그러면 그라데이션의 내적과 같습니다.

2043
00:54:38,218 --> 00:54:40,157


2044
00:54:39,357 --> 00:54:42,693
그 방향을 설명하는 단위 벡터와 함께.

2045
00:54:40,157 --> 00:54:43,493


2046
00:54:42,693 --> 00:54:44,549
그래서이 그라데이션은 매우 중요합니다.

2047
00:54:43,493 --> 00:54:45,349


2048
00:54:44,549 --> 00:54:47,719
이 선형 일차 근사를 제공하기 때문에

2049
00:54:45,349 --> 00:54:48,519


2050
00:54:47,719 --> 00:54:50,198
귀하의 현재 지점에서 귀하의 기능에.

2051
00:54:48,519 --> 00:54:50,998


2052
00:54:50,198 --> 00:54:51,382
그래서 실제로, 많은 깊은 학습

2053
00:54:51,382 --> 00:54:53,661
함수의 그라데이션 계산하기

2054
00:54:52,182 --> 00:54:54,461


2055
00:54:53,661 --> 00:54:56,290
그런 그라디언트를 사용하여 반복적으로 업데이트

2056
00:54:54,461 --> 00:54:57,090


2057
00:54:56,290 --> 00:54:58,123
매개 변수 벡터.

2058
00:54:57,090 --> 00:54:58,923


2059
00:54:59,204 --> 00:55:02,195
당신이 상상할 수있는 순진한 방법 하나

2060
00:55:00,004 --> 00:55:02,995


2061
00:55:02,195 --> 00:55:04,812
실제로 컴퓨터에서이 그라디언트를 평가하면

2062
00:55:02,995 --> 00:55:05,612


2063
00:55:04,812 --> 00:55:06,955
유한 차이의 방법을 사용하고 있습니다.

2064
00:55:05,612 --> 00:55:07,755


2065
00:55:06,955 --> 00:55:09,488
그래디언트의 한계 정의로 돌아갑니다.

2066
00:55:07,755 --> 00:55:10,288


2067
00:55:09,488 --> 00:55:12,621
그래서 왼쪽에서 우리는 우리의 현재 W

2068
00:55:10,288 --> 00:55:13,421


2069
00:55:12,621 --> 00:55:14,012
이 매개 변수 벡터입니다.

2070
00:55:14,012 --> 00:55:17,432
어쩌면 우리에게 어쩌면 1.25의
현재 손실을 줄 수 있습니다.

2071
00:55:14,812 --> 00:55:18,232


2072
00:55:17,432 --> 00:55:21,127
우리의 목표는 기울기를 계산하는 것입니다, dW,

2073
00:55:18,232 --> 00:55:21,927


2074
00:55:21,127 --> 00:55:23,922
이것은 W와 같은 모양의 벡터가 될 것입니다.

2075
00:55:21,927 --> 00:55:24,722


2076
00:55:23,922 --> 00:55:26,021
그 그라디언트의 각 슬롯은 우리에게 알려줄 것입니다.

2077
00:55:24,722 --> 00:55:26,821


2078
00:55:26,021 --> 00:55:29,050
얼마나 많은 손실이 발생할 것인가?

2079
00:55:26,821 --> 00:55:29,850


2080
00:55:29,050 --> 00:55:31,734
그 좌표 방향으로 미미한 양.

2081
00:55:29,850 --> 00:55:32,534


2082
00:55:31,734 --> 00:55:33,241
그래서 당신이 상상할 수있는 한가지

2083
00:55:33,241 --> 00:55:35,741
이 유한 차분을 계산하는 것입니다.

2084
00:55:34,041 --> 00:55:36,541


2085
00:55:35,741 --> 00:55:38,336
우리가 W를 가지고 있다면, 우리는

2086
00:55:36,541 --> 00:55:39,136


2087
00:55:38,336 --> 00:55:41,902
W의 첫 번째 원소, 작은 값, h,

2088
00:55:39,136 --> 00:55:42,702


2089
00:55:41,902 --> 00:55:44,210
손실 함수를 사용하여 손실을 다시 계산하십시오.

2090
00:55:42,702 --> 00:55:45,010


2091
00:55:44,210 --> 00:55:45,842
우리의 분류 자와 모든 것.

2092
00:55:45,842 --> 00:55:48,112
그리고 아마도이 상황에서 우리가 조금 움직이면

2093
00:55:46,642 --> 00:55:48,912


2094
00:55:48,112 --> 00:55:50,792
첫 번째 차원에서 손실이 줄어들 것입니다.

2095
00:55:48,912 --> 00:55:51,592


2096
00:55:50,792 --> 00:55:53,792
1.2534에서 1.25322로 조금 증가했습니다.

2097
00:55:51,592 --> 00:55:54,592


2098
00:55:55,945 --> 00:55:57,574
그런 다음이 한계 정의를 사용할 수 있습니다.

2099
00:55:57,574 --> 00:56:01,363
이 유한 차분 근사값을 생각해 내야한다.

2100
00:55:58,374 --> 00:56:02,163


2101
00:56:01,363 --> 00:56:04,378
이 첫 번째 차원의 그래디언트로

2102
00:56:02,163 --> 00:56:05,178


2103
00:56:04,378 --> 00:56:06,194
이제이 절차를 반복하는 것을 상상할 수 있습니다.

2104
00:56:05,178 --> 00:56:06,994


2105
00:56:06,194 --> 00:56:07,795
두 번째 차원에서,

2106
00:56:07,795 --> 00:56:09,371
이제 우리는 첫 번째 차원을 취하고,

2107
00:56:09,371 --> 00:56:11,029
원래 값으로 다시 설정하십시오.

2108
00:56:11,029 --> 00:56:13,728
작은 단계로 두 번째 방향을 증가시킵니다.

2109
00:56:11,829 --> 00:56:14,528


2110
00:56:13,728 --> 00:56:15,294
그리고 다시, 우리는 손실을 계산합니다.

2111
00:56:15,294 --> 00:56:17,593
이 유한 차분 근사법을 사용하십시오.

2112
00:56:16,094 --> 00:56:18,393


2113
00:56:17,593 --> 00:56:19,444
그라데이션에 대한 근사치를 계산하는

2114
00:56:18,393 --> 00:56:20,244


2115
00:56:19,444 --> 00:56:21,165
두 번째 슬롯에서.

2116
00:56:21,165 --> 00:56:22,843
그리고 이제 세 번째로 이것을 반복하십시오.

2117
00:56:22,843 --> 00:56:25,135
그리고 계속해서.

2118
00:56:23,643 --> 00:56:25,935


2119
00:56:25,135 --> 00:56:27,683
그래서 이것은 실제로 끔찍한 생각입니다.

2120
00:56:25,935 --> 00:56:28,483


2121
00:56:27,683 --> 00:56:29,150
왜냐하면 그것은 매우 느리기 때문입니다.

2122
00:56:29,150 --> 00:56:31,980
그래서 여러분은이 함수 f를 계산하면,

2123
00:56:29,950 --> 00:56:32,780


2124
00:56:31,980 --> 00:56:34,230
그것이 크다면 실제로는 매우 느릴지도 모른다.

2125
00:56:32,780 --> 00:56:35,030


2126
00:56:34,230 --> 00:56:35,920
길쌈 신경 네트워크.

2127
00:56:35,920 --> 00:56:38,369
그리고이 매개 변수 벡터 W는,

2128
00:56:36,720 --> 00:56:39,169


2129
00:56:38,369 --> 00:56:40,693
아마 여기에있는 것처럼 10 개의 항목이 없을 것입니다.

2130
00:56:39,169 --> 00:56:41,493


2131
00:56:40,693 --> 00:56:42,266
수천만 명이 될 수도있다.

2132
00:56:42,266 --> 00:56:44,344
또는이 수백만 달러 중 일부는 수백만 달러,

2133
00:56:43,066 --> 00:56:45,144


2134
00:56:44,344 --> 00:56:46,446
복잡한 심층 학습 모델.

2135
00:56:45,144 --> 00:56:47,246


2136
00:56:46,446 --> 00:56:48,482
따라서 실제적으로, 당신은 결코
계산을 원하지 않을 것입니다.

2137
00:56:47,246 --> 00:56:49,282


2138
00:56:48,482 --> 00:56:50,381
당신의 유한 차이에 대한 당신의 그라디언트,

2139
00:56:49,282 --> 00:56:51,181


2140
00:56:50,381 --> 00:56:52,864
당신이 수억 명을 기다려야하기 때문에

2141
00:56:51,181 --> 00:56:53,664


2142
00:56:52,864 --> 00:56:54,749
잠재적으로 기능 평가

2143
00:56:53,664 --> 00:56:55,549


2144
00:56:54,749 --> 00:56:56,908
하나의 그래디언트를 얻는다면 그것은 매우 느려질 것입니다.

2145
00:56:55,549 --> 00:56:57,708


2146
00:56:56,908 --> 00:56:58,075
슈퍼 나쁜.

2147
00:56:57,708 --> 00:56:58,875


2148
00:56:59,351 --> 00:57:02,524
그러나 고맙게도 우리는 그렇게 할 필요가 없습니다.

2149
00:57:00,151 --> 00:57:03,324


2150
00:57:02,524 --> 00:57:03,676
바라건대 당신은 미적분 과정을 택했을 것입니다.

2151
00:57:03,676 --> 00:57:05,315
네 인생의 어느 시점에서,

2152
00:57:05,315 --> 00:57:08,146
그래서 당신은이 사람들 덕분에,

2153
00:57:06,115 --> 00:57:08,946


2154
00:57:08,146 --> 00:57:11,206
우리는 손실에 대한 표현을 적어 둘 수 있습니다.

2155
00:57:08,946 --> 00:57:12,006


2156
00:57:11,206 --> 00:57:13,658
미적분학의 마법 망치를 사용하십시오.

2157
00:57:12,006 --> 00:57:14,458


2158
00:57:13,658 --> 00:57:15,584
표현을 적어 두는 것

2159
00:57:14,458 --> 00:57:16,384


2160
00:57:15,584 --> 00:57:17,372
이 기울기가 무엇을 위해 있어야합니다.

2161
00:57:17,372 --> 00:57:18,708
그리고 이것은 훨씬 더 효율적 일 것입니다.

2162
00:57:18,708 --> 00:57:20,245
분석적으로 계산하는 것보다

2163
00:57:20,245 --> 00:57:21,658
유한 한 차이를 통해.

2164
00:57:21,658 --> 00:57:22,929
하나, 정확 할거야.

2165
00:57:22,929 --> 00:57:25,433
둘째, 계산이 필요하기 때문에 훨씬 빠릅니다.

2166
00:57:23,729 --> 00:57:26,233


2167
00:57:25,433 --> 00:57:27,350
이 단일 표현.

2168
00:57:26,233 --> 00:57:28,150


2169
00:57:28,945 --> 00:57:31,405
그래서 이것이 어떻게 생겼는지는 지금입니다.

2170
00:57:29,745 --> 00:57:32,205


2171
00:57:31,405 --> 00:57:33,513
우리가 현재의 W의 그림으로 돌아 가면,

2172
00:57:32,205 --> 00:57:34,313


2173
00:57:33,513 --> 00:57:36,848
W의 모든 차원을 반복하는 것이 아니라,

2174
00:57:34,313 --> 00:57:37,648


2175
00:57:36,848 --> 00:57:38,311
우리가 미리 알아낼거야.

2176
00:57:38,311 --> 00:57:40,653
그래디언트의 분석 식은 무엇입니까?

2177
00:57:39,111 --> 00:57:41,453


2178
00:57:40,653 --> 00:57:44,279
그런 다음 그것을 적어서 W에서 직접 이동하십시오.

2179
00:57:41,453 --> 00:57:45,079


2180
00:57:44,279 --> 00:57:47,337
하나의 단계에서 dW 또는
그래디언트를 계산할 수 있습니다.

2181
00:57:45,079 --> 00:57:48,137


2182
00:57:47,337 --> 00:57:50,875
그리고 그것은 실제로 더 나아질 것입니다.

2183
00:57:48,137 --> 00:57:51,675


2184
00:57:50,875 --> 00:57:53,846
요약하자면이 수치 그라디언트

2185
00:57:51,675 --> 00:57:54,646


2186
00:57:53,846 --> 00:57:56,738
간단하고 의미있는 것입니다.

2187
00:57:54,646 --> 00:57:57,538


2188
00:57:56,738 --> 00:57:58,745
그러나 실제로는 실제로 사용하지 않을 것입니다.

2189
00:57:57,538 --> 00:57:59,545


2190
00:57:58,745 --> 00:58:01,794
실제로는 분석 그라디언트를 항상 사용합니다.

2191
00:57:59,545 --> 00:58:02,594


2192
00:58:01,794 --> 00:58:03,039
그것을 사용하십시오.

2193
00:58:03,039 --> 00:58:05,301
실제로 이러한 그래디언트 계산을 수행 할 때

2194
00:58:03,839 --> 00:58:06,101


2195
00:58:05,301 --> 00:58:06,951
그러나 흥미로운 점은

2196
00:58:06,951 --> 00:58:09,360
이 숫자 그라디언트는 실제로 매우 유용합니다.

2197
00:58:07,751 --> 00:58:10,160


2198
00:58:09,360 --> 00:58:10,610
디버깅 도구.

2199
00:58:10,160 --> 00:58:11,410


2200
00:58:12,572 --> 00:58:13,841
몇 가지 코드를 작성했다고 가정 해 보겠습니다.

2201
00:58:13,841 --> 00:58:16,169
당신은 손실을 계산하는 코드를 작성했습니다.

2202
00:58:14,641 --> 00:58:16,969


2203
00:58:16,169 --> 00:58:17,770
손실의 기울기,

2204
00:58:17,770 --> 00:58:19,562
그렇다면 어떻게 디버깅합니까?

2205
00:58:19,562 --> 00:58:21,909
이 분석식이

2206
00:58:20,362 --> 00:58:22,709


2207
00:58:21,909 --> 00:58:24,085
코드에서 파생하고 적어 둔

2208
00:58:22,709 --> 00:58:24,885


2209
00:58:24,085 --> 00:58:25,684
실제로 맞습니까?

2210
00:58:25,684 --> 00:58:28,443
따라서 이러한 것들을위한 일반적인 디버깅 전략

2211
00:58:26,484 --> 00:58:29,243


2212
00:58:28,443 --> 00:58:31,159
방법으로 숫자 그라디언트를 사용하는 것입니다.

2213
00:58:29,243 --> 00:58:31,959


2214
00:58:31,159 --> 00:58:32,823
확실한 단위 테스트의 일종으로

2215
00:58:32,823 --> 00:58:35,141
분석 기울기가 맞는지 확인하십시오.

2216
00:58:33,623 --> 00:58:35,941


2217
00:58:35,141 --> 00:58:38,320
다시 말하지만, 이것은 매우 느리고 정확하지 않기 때문에,

2218
00:58:35,941 --> 00:58:39,120


2219
00:58:38,320 --> 00:58:41,435
이 숫자 그라디언트 검사를 수행 할 때,

2220
00:58:39,120 --> 00:58:42,235


2221
00:58:41,435 --> 00:58:43,739
호출 될 때 매개 변수의 크기를 줄이는 경향이 있습니다.

2222
00:58:42,235 --> 00:58:44,539


2223
00:58:43,739 --> 00:58:45,184
문제가 실제로 실행되도록

2224
00:58:45,184 --> 00:58:46,755
합리적인 시간에

2225
00:58:46,755 --> 00:58:49,376
하지만 이것은 유용한 유용한 디버깅 전략이됩니다.

2226
00:58:47,555 --> 00:58:50,176


2227
00:58:49,376 --> 00:58:51,721
자신의 그라디언트 계산을 작성할 때.

2228
00:58:50,176 --> 00:58:52,521


2229
00:58:51,721 --> 00:58:54,112
그래서 실제로 이것은 실제로 실제로 많이 사용됩니다.

2230
00:58:52,521 --> 00:58:54,912


2231
00:58:54,112 --> 00:58:58,610
그리고 당신은 당신의 과제에도 이것을 할 것입니다.

2232
00:58:54,912 --> 00:58:59,410


2233
00:58:58,610 --> 00:59:01,834
그래서 일단 그라디언트를 계산하는 방법을 알게되면,

2234
00:58:59,410 --> 00:59:02,634


2235
00:59:01,834 --> 00:59:04,547
그러면 우리를이 초간단 단순 알고리즘으로 이끈다.

2236
00:59:02,634 --> 00:59:05,347


2237
00:59:04,547 --> 00:59:06,990
그것은 세 줄과 같지만 마음 속에있는 것으로 밝혀졌습니다.

2238
00:59:05,347 --> 00:59:07,790


2239
00:59:06,990 --> 00:59:09,480
우리가 이처럼 매우 큰 것을 훈련하는 방법에 대해서,

2240
00:59:07,790 --> 00:59:10,280


2241
00:59:09,480 --> 00:59:11,607
가장 복잡한 심층 학습 알고리즘,

2242
00:59:10,280 --> 00:59:12,407


2243
00:59:11,607 --> 00:59:13,152
그것은 구배 강하입니다.

2244
00:59:13,152 --> 00:59:16,991
그래디언트 강하가 먼저 W를 초기화합니다.

2245
00:59:13,952 --> 00:59:17,791


2246
00:59:16,991 --> 00:59:19,544
어떤 무작위로, 그렇다면 사실,

2247
00:59:17,791 --> 00:59:20,344


2248
00:59:19,544 --> 00:59:21,555
우리는 우리의 손실과 그라디언트를 계산할 것입니다.

2249
00:59:20,344 --> 00:59:22,355


2250
00:59:21,555 --> 00:59:24,521
우리는 우리의 가중치를 업데이트 할 것입니다.

2251
00:59:22,355 --> 00:59:25,321


2252
00:59:24,521 --> 00:59:27,547
그레디언트 방향의 반대 방향에서,

2253
00:59:25,321 --> 00:59:28,347


2254
00:59:27,547 --> 00:59:28,722
그라디언트를 기억하십시오.

2255
00:59:28,722 --> 00:59:30,710
가장 큰 증가의 방향을 가리키고 있었다.

2256
00:59:29,522 --> 00:59:31,510


2257
00:59:30,710 --> 00:59:32,305
함수의 - 그래디언트 빼기

2258
00:59:32,305 --> 00:59:34,047
가장 큰 감소 방향의 포인트,

2259
00:59:34,047 --> 00:59:36,727
그래서 우리는 방향으로 조금 나아갈 것입니다.

2260
00:59:34,847 --> 00:59:37,527


2261
00:59:36,727 --> 00:59:39,262
빼기 그라디언트를 사용하고 영원히 이것을 반복하십시오.

2262
00:59:37,527 --> 00:59:40,062


2263
00:59:39,262 --> 00:59:40,774
결국 네트워크가 수렴 할 것입니다.

2264
00:59:40,774 --> 00:59:43,255
희망적으로 당신은 매우 행복 할 것입니다.

2265
00:59:41,574 --> 00:59:44,055


2266
00:59:43,255 --> 00:59:45,596
그러나이 단계 크기는 실제로 하이퍼 매개 변수입니다.

2267
00:59:44,055 --> 00:59:46,396


2268
00:59:45,596 --> 00:59:48,219
이것은 그라디언트를 계산할 때마다,

2269
00:59:46,396 --> 00:59:49,019


2270
00:59:48,219 --> 00:59:50,842
우리는 그 방향으로 얼마나 멀리 나아갈 것입니다.

2271
00:59:49,019 --> 00:59:51,642


2272
00:59:50,842 --> 00:59:53,602
그리고이 단계 크기는 학습 속도라고도하며,

2273
00:59:51,642 --> 00:59:54,402


2274
00:59:53,602 --> 00:59:55,445
아마 가장 중요한 것 중 하나 일 것입니다.

2275
00:59:54,402 --> 00:59:56,245


2276
00:59:55,445 --> 00:59:57,378
설정해야하는 하이퍼 매개 변수

2277
00:59:56,245 --> 00:59:58,178


2278
00:59:57,378 --> 01:00:00,033
실제로 이러한 것들을 실제로 훈련 할 때.

2279
00:59:58,178 --> 01:00:00,833


2280
01:00:00,033 --> 01:00:02,117
사실 나는이 일을 훈련 할 때 나를 위해,

2281
01:00:00,833 --> 01:00:02,917


2282
01:00:02,117 --> 01:00:04,200
이 단계 크기를 알아 내려고

2283
01:00:02,917 --> 01:00:05,000


2284
01:00:04,200 --> 01:00:06,340
또는이 학습 률은 첫 번째 하이퍼 파라미터입니다.

2285
01:00:05,000 --> 01:00:07,140


2286
01:00:06,340 --> 01:00:07,502
나는 항상 확인한다.

2287
01:00:07,502 --> 01:00:10,949
모형 크기 또는 정규화 강도와 같은 것

2288
01:00:08,302 --> 01:00:11,749


2289
01:00:10,949 --> 01:00:12,434
나중에 조금 떠날 때까지,

2290
01:00:12,434 --> 01:00:15,408
학습 속도 또는 단계 크기를 올바르게 얻는 방법

2291
01:00:13,234 --> 01:00:16,208


2292
01:00:15,408 --> 01:00:19,361
처음에 설정하려고하는 첫 번째 것입니다.

2293
01:00:16,208 --> 01:00:20,161


2294
01:00:19,361 --> 01:00:23,015
그래서 그림처럼 보이는 것입니다.

2295
01:00:20,161 --> 01:00:23,815


2296
01:00:23,015 --> 01:00:25,212
여기에 2 차원의 간단한 예제가 있습니다.

2297
01:00:23,815 --> 01:00:26,012


2298
01:00:25,212 --> 01:00:28,004
그래서 여기에 우리는 아마도이 사발을 가지고있을 것입니다.

2299
01:00:26,012 --> 01:00:28,804


2300
01:00:28,004 --> 01:00:30,051
우리의 손실 함수를 보여주고있다.

2301
01:00:28,804 --> 01:00:30,851


2302
01:00:30,051 --> 01:00:33,635
중앙의이 빨간 지역

2303
01:00:30,851 --> 01:00:34,435


2304
01:00:33,635 --> 01:00:36,598
우리가 가고 싶은 낮은 손실의이 지역

2305
01:00:34,435 --> 01:00:37,398


2306
01:00:36,598 --> 01:00:38,852
에지쪽으로 청색 및 녹색 영역

2307
01:00:37,398 --> 01:00:39,652


2308
01:00:38,852 --> 01:00:41,187
우리가 피하고자하는 손실이 더 큽니다.

2309
01:00:39,652 --> 01:00:41,987


2310
01:00:41,187 --> 01:00:43,204
이제 우리는 우리의 W를 시작할 것입니다.

2311
01:00:41,987 --> 01:00:44,004


2312
01:00:43,204 --> 01:00:44,750
우주의 어떤 임의의 지점에서,

2313
01:00:44,750 --> 01:00:47,536
음의 그래디언트 방향을 계산하고,

2314
01:00:45,550 --> 01:00:48,336


2315
01:00:47,536 --> 01:00:49,680
바라건대 우리를 방향으로 안내 할 것입니다.

2316
01:00:48,336 --> 01:00:50,480


2317
01:00:49,680 --> 01:00:51,387
결국 미니 마의.

2318
01:00:51,387 --> 01:00:53,171
그리고 우리가 이것을 반복해서 반복한다면,

2319
01:00:53,171 --> 01:00:56,407
우리는 최후에 정확한 미니 마를 얻을 수 있기를 바랍니다.

2320
01:00:53,971 --> 01:00:57,207


2321
01:00:56,407 --> 01:01:00,283
실제로 이것이 어떻게 생겼는지는,

2322
01:00:57,207 --> 01:01:01,083


2323
01:01:00,283 --> 01:01:03,140
오,이 쥐 문제가 다시 발생했습니다.

2324
01:01:01,083 --> 01:01:03,940


2325
01:01:03,140 --> 01:01:04,358
그래서 이것이 실제로 어떻게 생겼는지

2326
01:01:04,358 --> 01:01:09,250
우리가이 일을 반복해서 반복한다면,

2327
01:01:05,158 --> 01:01:10,050


2328
01:01:09,250 --> 01:01:12,061
그러면 우리는 어느 시점부터 시작합니다.

2329
01:01:10,050 --> 01:01:12,861


2330
01:01:12,061 --> 01:01:15,266
결국에는 매번 작은 그래디언트 단계를 거치면서,

2331
01:01:12,861 --> 01:01:16,066


2332
01:01:15,266 --> 01:01:19,221
매개 변수가 중심을 향해 원호 모양으로 표시되고,

2333
01:01:16,066 --> 01:01:20,021


2334
01:01:19,221 --> 01:01:20,626
이 미니 마의 영역,

2335
01:01:20,626 --> 01:01:22,236
그리고 그것은 당신이 정말로 원하는 것입니다.

2336
01:01:22,236 --> 01:01:24,241
낮은 손실을 원하기 때문입니다.

2337
01:01:23,036 --> 01:01:25,041


2338
01:01:24,241 --> 01:01:26,812
그리고 그런데, 티저의 비트로서,

2339
01:01:25,041 --> 01:01:27,612


2340
01:01:26,812 --> 01:01:28,139
우리는 이전 슬라이드에서 보았습니다.

2341
01:01:28,139 --> 01:01:30,905
아주 간단한 그래디언트 디센트의이 예제,

2342
01:01:28,939 --> 01:01:31,705


2343
01:01:30,905 --> 01:01:32,705
모든 단계에서 우리는 방향으로 나아가고 있습니다.

2344
01:01:31,705 --> 01:01:33,505


2345
01:01:32,705 --> 01:01:33,998
그라디언트의

2346
01:01:33,998 --> 01:01:35,997
그러나 실제로, 다음 강의에서,

2347
01:01:34,798 --> 01:01:36,797


2348
01:01:35,997 --> 01:01:38,679
우리는 약간 더 매끈한 단계가 있음을 볼 것입니다,

2349
01:01:36,797 --> 01:01:39,479


2350
01:01:38,679 --> 01:01:40,885
그들이이 업데이트 규칙이라고 부르는 것,

2351
01:01:39,479 --> 01:01:41,685


2352
01:01:40,885 --> 01:01:43,598
약간 애호가를 취할 수있는 곳

2353
01:01:41,685 --> 01:01:44,398


2354
01:01:43,598 --> 01:01:46,037
여러 시간 단계에 걸쳐 그라디언트 통합

2355
01:01:44,398 --> 01:01:46,837


2356
01:01:46,037 --> 01:01:48,206
그리고 그런 것들은 조금 더 잘 작동하는 경향이 있습니다.

2357
01:01:46,837 --> 01:01:49,006


2358
01:01:48,206 --> 01:01:50,836
실제로는 훨씬 더 일반적으로 사용됩니다.

2359
01:01:49,006 --> 01:01:51,636


2360
01:01:50,836 --> 01:01:52,513
이 바닐라 그라데이션 하강보다

2361
01:01:52,513 --> 01:01:54,610
실제로 이러한 것들을 훈련 할 때.

2362
01:01:53,313 --> 01:01:55,410


2363
01:01:54,610 --> 01:01:55,877
그리고 약간의 미리보기로서,

2364
01:01:55,877 --> 01:01:59,101
우리는이 약간의 애호가 방법 중 일부를 볼 수 있습니다.

2365
01:01:56,677 --> 01:01:59,901


2366
01:01:59,101 --> 01:02:01,054
동일한 문제를 최적화합니다.

2367
01:01:59,901 --> 01:02:01,854


2368
01:02:01,054 --> 01:02:04,701
다시 검정은이 같은 그라디언트 계산이 될 것입니다.

2369
01:02:01,854 --> 01:02:05,501


2370
01:02:04,701 --> 01:02:07,530
그리고 이것들은 그들이 어떤 색인지 잊어 버렸습니다.

2371
01:02:05,501 --> 01:02:08,330


2372
01:02:07,530 --> 01:02:08,871
그러나이 두 개의 다른 곡선

2373
01:02:08,871 --> 01:02:11,580
약간 더 까다로운 업데이트 규칙을 사용하고 있습니다.

2374
01:02:09,671 --> 01:02:12,380


2375
01:02:11,580 --> 01:02:13,960
그래디언트 정보를 사용하는 방법을 정확하게 결정하는

2376
01:02:12,380 --> 01:02:14,760


2377
01:02:13,960 --> 01:02:15,929
우리의 다음 단계를 만들기 위해.

2378
01:02:14,760 --> 01:02:16,729


2379
01:02:15,929 --> 01:02:20,451
따라서 이들 중 하나는 기세가있는 그라데이션 강하입니다.

2380
01:02:16,729 --> 01:02:21,251


2381
01:02:20,451 --> 01:02:22,835
다른 하나는이 Adam Optimizer입니다.

2382
01:02:21,251 --> 01:02:23,635


2383
01:02:22,835 --> 01:02:24,273
자세한 내용은

2384
01:02:24,273 --> 01:02:25,389
나중에 코스에서.

2385
01:02:25,389 --> 01:02:28,300
그러나 아이디어는 우리가이 아주 기본적인
알고리즘을 가지고 있다는 것입니다.

2386
01:02:26,189 --> 01:02:29,100


2387
01:02:28,300 --> 01:02:29,795
그라디언트 디센트라는,

2388
01:02:29,795 --> 01:02:31,544
매 단계마다 그라디언트를 사용합니다.

2389
01:02:31,544 --> 01:02:33,424
다음으로 나아갈 위치를 결정하기 위해,

2390
01:02:32,344 --> 01:02:34,224


2391
01:02:33,424 --> 01:02:35,874
우리에게 알려주는 다른 업데이트 규칙이 있습니다.

2392
01:02:34,224 --> 01:02:36,674


2393
01:02:35,874 --> 01:02:38,559
그 기울기 정보를 얼마나 정확하게 사용하는지.

2394
01:02:36,674 --> 01:02:39,359


2395
01:02:38,559 --> 01:02:40,391
그러나 그것은 모두 동일한 기본 알고리즘입니다.

2396
01:02:39,359 --> 01:02:41,191


2397
01:02:40,391 --> 01:02:44,058
매 단계마다 내리막 길을 가려고했다.

2398
01:02:41,191 --> 01:02:44,858


2399
01:02:50,022 --> 01:02:51,852
하지만 실제로는 조금 더 주름살이 있습니다.

2400
01:02:50,822 --> 01:02:52,652


2401
01:02:51,852 --> 01:02:53,212
우리가 얘기해야 할 것.

2402
01:02:53,212 --> 01:02:56,818
그래서 우리가 손실 함수를 정의했음을 기억하십시오.

2403
01:02:54,012 --> 01:02:57,618


2404
01:02:56,818 --> 01:02:59,356
우리는 얼마나 나쁜지를 계산하는 손실을 정의했다.

2405
01:02:57,618 --> 01:03:00,156


2406
01:02:59,356 --> 01:03:02,237
어떤 단일 훈련 예에서 우리
분류 자 (classifier)

2407
01:03:00,156 --> 01:03:03,037


2408
01:03:02,237 --> 01:03:04,536
데이터 세트에 대한 우리의 모든 손실이

2409
01:03:03,037 --> 01:03:05,336


2410
01:03:04,536 --> 01:03:06,077
평균 손실이 될거야.

2411
01:03:06,077 --> 01:03:08,314
전체 교육 세트에서

2412
01:03:06,877 --> 01:03:09,114


2413
01:03:08,314 --> 01:03:12,572
그러나 실제로이 N은 매우 커질 수 있습니다.

2414
01:03:09,114 --> 01:03:13,372


2415
01:03:12,572 --> 01:03:15,274
예를 들어 이미지 넷 데이터 세트를 사용하는 경우,

2416
01:03:13,372 --> 01:03:16,074


2417
01:03:15,274 --> 01:03:17,010
우리가 첫 번째 강의에서 이야기 한 내용,

2418
01:03:17,010 --> 01:03:19,199
N은 130 만 명,

2419
01:03:17,810 --> 01:03:19,999


2420
01:03:19,199 --> 01:03:21,403
그래서 실제로이 손실을 계산합니다.

2421
01:03:19,999 --> 01:03:22,203


2422
01:03:21,403 --> 01:03:23,208
실제로는 매우 비쌀 수있다.

2423
01:03:22,203 --> 01:03:24,008


2424
01:03:23,208 --> 01:03:26,081
아마도 수백만 가지 평가 계산이 필요합니다.

2425
01:03:24,008 --> 01:03:26,881


2426
01:03:26,081 --> 01:03:28,206
이 함수의

2427
01:03:26,881 --> 01:03:29,006


2428
01:03:28,206 --> 01:03:29,821
그래서 그것은 정말로 느릴 수 있습니다.

2429
01:03:29,821 --> 01:03:32,094
실제로 그라디언트는 선형 연산자이므로,

2430
01:03:30,621 --> 01:03:32,894


2431
01:03:32,094 --> 01:03:34,109
그라디언트를 실제로 계산하려고 할 때

2432
01:03:32,894 --> 01:03:34,909


2433
01:03:34,109 --> 01:03:37,034
이 표현에서 우리는 손실의 기울기

2434
01:03:34,909 --> 01:03:37,834


2435
01:03:37,034 --> 01:03:39,504
이제는 손실 그라디언트의 합계입니다.

2436
01:03:37,834 --> 01:03:40,304


2437
01:03:39,504 --> 01:03:41,461
각 개별 용어에 대해

2438
01:03:40,304 --> 01:03:42,261


2439
01:03:41,461 --> 01:03:43,734
이제 그라디언트를 다시 계산하려면,

2440
01:03:42,261 --> 01:03:44,534


2441
01:03:43,734 --> 01:03:45,248
그것은 우리에게 반복을 요구합니다.

2442
01:03:45,248 --> 01:03:46,930
전체 교육 데이터 세트에 대해

2443
01:03:46,930 --> 01:03:48,469
이 모든 N 개의 예제.

2444
01:03:48,469 --> 01:03:50,390
그래서 만약 우리 N이 백만 달러라면,

2445
01:03:49,269 --> 01:03:51,190


2446
01:03:50,390 --> 01:03:51,960
이것은 슈퍼 슈퍼 천천히,

2447
01:03:51,960 --> 01:03:54,078
우리는 아주 오랜 시간을 기다려야 할 것입니다.

2448
01:03:52,760 --> 01:03:54,878


2449
01:03:54,078 --> 01:03:56,978
우리가 W를 개별적으로 업데이트하기 전에.

2450
01:03:54,878 --> 01:03:57,778


2451
01:03:56,978 --> 01:03:58,577
그래서 실제로, 우리는

2452
01:03:58,577 --> 01:04:00,728
확률 적 구배 강하라고 불리는 것,

2453
01:03:59,377 --> 01:04:01,528


2454
01:04:00,728 --> 01:04:04,061
손실 및 기울기 계산보다는

2455
01:04:01,528 --> 01:04:04,861


2456
01:04:04,061 --> 01:04:05,697
전체 훈련 세트에서,

2457
01:04:05,697 --> 01:04:08,938
대신 매 반복마다 샘플 세트

2458
01:04:06,497 --> 01:04:09,738


2459
01:04:08,938 --> 01:04:12,540
minibatch라고하는 훈련 예를들 수 있습니다.

2460
01:04:09,738 --> 01:04:13,340


2461
01:04:12,540 --> 01:04:14,213
보통 이것은 관습에 따라 2의 힘입니다.

2462
01:04:14,213 --> 01:04:17,705
32, 64, 128은 일반적인 숫자입니다.

2463
01:04:15,013 --> 01:04:18,505


2464
01:04:17,705 --> 01:04:19,887
그런 다음이 작은 미니 바를 사용합니다.

2465
01:04:18,505 --> 01:04:20,687


2466
01:04:19,887 --> 01:04:22,483
전체 합계의 추정치를 계산하기 위해,

2467
01:04:20,687 --> 01:04:23,283


2468
01:04:22,483 --> 01:04:25,047
그리고 진정한 그라데이션의 추정치.

2469
01:04:23,283 --> 01:04:25,847


2470
01:04:25,047 --> 01:04:27,703
그리고 이것은 당신이 이것을 볼
수 있기 때문에 확률 적입니다.

2471
01:04:25,847 --> 01:04:28,503


2472
01:04:27,703 --> 01:04:31,845
몬테 카를로의 기대치에 대한 추정치 일 수도 있습니다.

2473
01:04:28,503 --> 01:04:32,645


2474
01:04:31,845 --> 01:04:33,345
진정한 가치의

2475
01:04:32,645 --> 01:04:34,145


2476
01:04:34,716 --> 01:04:37,318
그래서 이것은 우리의 알고리즘을
약간 더 매끈하게 만듭니다.

2477
01:04:35,516 --> 01:04:38,118


2478
01:04:37,318 --> 01:04:38,945
하지만 여전히 4 줄 밖에 없습니다.

2479
01:04:38,945 --> 01:04:43,112
이제 데이터가 무작위로 추출됩니다.

2480
01:04:39,745 --> 01:04:43,912


2481
01:04:44,291 --> 01:04:46,682
minibatch에서 손실 및 그라디언트를 평가하고,

2482
01:04:45,091 --> 01:04:47,482


2483
01:04:46,682 --> 01:04:48,690
이제 매개 변수를 업데이트하십시오.

2484
01:04:47,482 --> 01:04:49,490


2485
01:04:48,690 --> 01:04:51,113
이 손실 추정에 기초하여,

2486
01:04:49,490 --> 01:04:51,913


2487
01:04:51,113 --> 01:04:53,661
이 기울기의 추정치.

2488
01:04:51,913 --> 01:04:54,461


2489
01:04:53,661 --> 01:04:56,769
그리고 다시 약간 더 매끄러운 업데이트 규칙을 보겠습니다.

2490
01:04:54,461 --> 01:04:57,569


2491
01:04:56,769 --> 01:04:59,811
여러 가지 그라디언트를 통합하는 방법을 정확히 설명합니다.

2492
01:04:57,569 --> 01:05:00,611


2493
01:04:59,811 --> 01:05:02,780
시간이지 나면서,하지만 이것은 기본적인 훈련 알고리즘입니다

2494
01:05:00,611 --> 01:05:03,580


2495
01:05:02,780 --> 01:05:04,948
우리가 거의 모든 심층 신경 네트워크에 사용하는

2496
01:05:03,580 --> 01:05:05,748


2497
01:05:04,948 --> 01:05:05,948
실제로.

2498
01:05:05,748 --> 01:05:06,748


2499
01:05:06,875 --> 01:05:10,235
그래서 우리는 또 다른 대화 형 웹 데모를 가지고 있습니다.

2500
01:05:07,675 --> 01:05:11,035


2501
01:05:10,235 --> 01:05:12,625
선형 분류기로 실제로 놀고,

2502
01:05:11,035 --> 01:05:13,425


2503
01:05:12,625 --> 01:05:14,970
확률 적 구배 강하를 통해 이러한 것들을 훈련하고,

2504
01:05:13,425 --> 01:05:15,770


2505
01:05:14,970 --> 01:05:17,782
그러나 웹 데모가 얼마나 비참한지를 생각해 보면,

2506
01:05:15,770 --> 01:05:18,582


2507
01:05:17,782 --> 01:05:20,122
나는 실제로 링크를 열지 않을 것이다.

2508
01:05:18,582 --> 01:05:20,922


2509
01:05:20,122 --> 01:05:23,269
대신, 나는이 비디오를 재생할 것입니다.

2510
01:05:20,922 --> 01:05:24,069


2511
01:05:23,269 --> 01:05:25,339
[웃음]

2512
01:05:24,069 --> 01:05:26,139


2513
01:05:25,339 --> 01:05:26,594
하지만 이걸 확인해 보시길 바랍니다.

2514
01:05:26,594 --> 01:05:27,749
온라인으로 게임을 즐기고,

2515
01:05:27,749 --> 01:05:29,256
실제로 어떤 직감을 구축하는 데 도움이되기 때문에

2516
01:05:29,256 --> 01:05:31,036
선형 분류기에 대해 배우고 훈련하기

2517
01:05:31,036 --> 01:05:32,735
그라데이션 강하를 통해.

2518
01:05:32,735 --> 01:05:34,919
여기 왼쪽에서 볼 수 있습니다.

2519
01:05:33,535 --> 01:05:35,719


2520
01:05:34,919 --> 01:05:37,485
우리가 분류하고있는이 문제가 있습니다.

2521
01:05:35,719 --> 01:05:38,285


2522
01:05:37,485 --> 01:05:40,146
3 개의 다른 종류,

2523
01:05:38,285 --> 01:05:40,946


2524
01:05:40,146 --> 01:05:42,551
우리는이 녹색, 파란색 및 빨간색 점을 가지고 있습니다.

2525
01:05:40,946 --> 01:05:43,351


2526
01:05:42,551 --> 01:05:45,753
그것은이 세 가지 수업에서 얻은 우리의 훈련 견본입니다.

2527
01:05:43,351 --> 01:05:46,553


2528
01:05:45,753 --> 01:05:48,351
그리고 이제 우리는 결정 경계를 이끌어 냈습니다.

2529
01:05:46,553 --> 01:05:49,151


2530
01:05:48,351 --> 01:05:52,068
컬러 백그라운드 영역 인 이러한 클래스의 경우,

2531
01:05:49,151 --> 01:05:52,868


2532
01:05:52,068 --> 01:05:54,270
이러한 방향뿐만 아니라,

2533
01:05:52,868 --> 01:05:55,070


2534
01:05:54,270 --> 01:05:57,487
학급 점수 인상 방향을 알려줍니다.

2535
01:05:55,070 --> 01:05:58,287


2536
01:05:57,487 --> 01:05:59,007
이 세 가지 클래스 각각에 대해.

2537
01:05:59,007 --> 01:06:03,108
그리고 지금 보시다시피, 실제로 가서 놀면

2538
01:05:59,807 --> 01:06:03,908


2539
01:06:03,108 --> 01:06:04,814
이 물건을 온라인으로

2540
01:06:04,814 --> 01:06:07,579
당신은 우리가 가서 W를 조정할 수
있다는 것을 볼 수 있습니다.

2541
01:06:05,614 --> 01:06:08,379


2542
01:06:07,579 --> 01:06:09,442
Ws 값 변경

2543
01:06:08,379 --> 01:06:10,242


2544
01:06:09,442 --> 01:06:12,176
이러한 결정 경계가 회전하게됩니다.

2545
01:06:10,242 --> 01:06:12,976


2546
01:06:12,176 --> 01:06:14,189
편향을 변경하면 결정 경계

2547
01:06:12,976 --> 01:06:14,989


2548
01:06:14,189 --> 01:06:17,476
회전하지 않고 대신 좌우로 움직입니다.

2549
01:06:14,989 --> 01:06:18,276


2550
01:06:17,476 --> 01:06:18,662
또는 위 아래로.

2551
01:06:18,662 --> 01:06:19,989
그러면 실제로 단계를 밟을 수 있습니다.

2552
01:06:19,989 --> 01:06:21,855
이 손실을 업데이트하려고 시도하는

2553
01:06:20,789 --> 01:06:22,655


2554
01:06:21,855 --> 01:06:23,984
또는이 슬라이더로 스텝 크기를 변경할 수 있습니다.

2555
01:06:22,655 --> 01:06:24,784


2556
01:06:23,984 --> 01:06:26,009
이 버튼을 눌러 실제 작동시킬 수 있습니다.

2557
01:06:24,784 --> 01:06:26,809


2558
01:06:26,009 --> 01:06:27,296
이제 큰 단계 크기로,

2559
01:06:27,296 --> 01:06:29,076
우리는 현재 그라디언트 디센트를 실행 중입니다.

2560
01:06:29,076 --> 01:06:30,624
이러한 결정의 경계는

2561
01:06:30,624 --> 01:06:32,874
데이터를 맞추려고합니다.

2562
01:06:31,424 --> 01:06:33,674


2563
01:06:34,553 --> 01:06:36,432
이제는 괜찮아요.

2564
01:06:35,353 --> 01:06:37,232


2565
01:06:36,432 --> 01:06:39,890
실제 손실 함수를 실시간으로 변경할 수 있습니다.

2566
01:06:37,232 --> 01:06:40,690


2567
01:06:39,890 --> 01:06:41,845
이 서로 다른 SVM 공식들 사이

2568
01:06:40,690 --> 01:06:42,645


2569
01:06:41,845 --> 01:06:43,567
및 다른 softmax.

2570
01:06:43,567 --> 01:06:44,754
그리고 당신은 볼 수 있습니다.

2571
01:06:44,754 --> 01:06:47,695
손실 함수의 이들 상이한 공식들 사이에서,

2572
01:06:45,554 --> 01:06:48,495


2573
01:06:47,695 --> 01:06:50,219
그것은 일반적으로 똑같은 일을합니다.

2574
01:06:48,495 --> 01:06:51,019


2575
01:06:50,219 --> 01:06:52,340
우리의 의사 결정 영역은 대부분 같은 장소에 있습니다.

2576
01:06:51,019 --> 01:06:53,140


2577
01:06:52,340 --> 01:06:54,945
그러나 그들이 어떻게 서로에 관해서는 결국

2578
01:06:53,140 --> 01:06:55,745


2579
01:06:54,945 --> 01:06:56,263
정확하게 상충되는 점은 무엇입니까?

2580
01:06:56,263 --> 01:06:59,139
이 다른 것들을 범주화하는 것

2581
01:06:57,063 --> 01:06:59,939


2582
01:06:59,139 --> 01:07:00,743
조금 바뀐다.

2583
01:07:00,743 --> 01:07:02,137
그래서 나는 정말로 당신이 온라인에 가기를 권장합니다.

2584
01:07:02,137 --> 01:07:03,877
이 일로 약간의 직감을 얻으려고 노력한다.

2585
01:07:03,877 --> 01:07:05,699
실제로 어떻게 생겼는지

2586
01:07:04,677 --> 01:07:06,499


2587
01:07:05,699 --> 01:07:07,428
이러한 선형 분류자를 훈련 시키려고

2588
01:07:07,428 --> 01:07:09,178
그라데이션 강하를 통해.

2589
01:07:08,228 --> 01:07:09,978


2590
01:07:12,343 --> 01:07:16,245
이제는 제쳐두고, 저는 다른 생각에
대해 이야기하고 싶습니다.

2591
01:07:13,143 --> 01:07:17,045


2592
01:07:16,245 --> 01:07:18,102
그것은 이미지 기능의 것입니다.

2593
01:07:17,045 --> 01:07:18,902


2594
01:07:18,102 --> 01:07:20,668
지금까지 선형 분류기에 대해 이야기했습니다.

2595
01:07:18,902 --> 01:07:21,468


2596
01:07:20,668 --> 01:07:23,032
그냥 우리의 원시 이미지 픽셀을 복용하는 것입니다

2597
01:07:21,468 --> 01:07:23,832


2598
01:07:23,032 --> 01:07:25,184
원시 픽셀 자체를 공급하는 단계

2599
01:07:23,832 --> 01:07:25,984


2600
01:07:25,184 --> 01:07:27,434
우리의 선형 분류 자로.

2601
01:07:25,984 --> 01:07:28,234


2602
01:07:28,464 --> 01:07:31,075
그러나 마지막 강연에서 우리가 이야기 한 것처럼,

2603
01:07:29,264 --> 01:07:31,875


2604
01:07:31,075 --> 01:07:33,343
이것은 아마도 그렇게 대단한 것이 아니며,

2605
01:07:31,875 --> 01:07:34,143


2606
01:07:33,343 --> 01:07:36,233
멀티 - 모달과 같은 것들 때문에.

2607
01:07:34,143 --> 01:07:37,033


2608
01:07:36,233 --> 01:07:39,368
따라서 실제로는 원본 픽셀 값을 실제로 공급합니다.

2609
01:07:37,033 --> 01:07:40,168


2610
01:07:39,368 --> 01:07:42,789
선형 분류기로 변환하는 것이 잘
작동하지 않는 경향이 있습니다.

2611
01:07:40,168 --> 01:07:43,589


2612
01:07:42,789 --> 01:07:45,742
그래서 그것은 지배 이전에 실제로 일반적이었습니다.

2613
01:07:43,589 --> 01:07:46,542


2614
01:07:45,742 --> 01:07:47,145
깊은 신경 네트워크,

2615
01:07:47,145 --> 01:07:49,592
대신에이 2 단계 접근법을 사용하는 것이 었습니다.

2616
01:07:47,945 --> 01:07:50,392


2617
01:07:49,592 --> 01:07:51,105
먼저, 당신은 당신의 이미지를 찍을 것입니다.

2618
01:07:51,105 --> 01:07:53,916
다양한 특징 표현을 계산할 수 있습니다.

2619
01:07:51,905 --> 01:07:54,716


2620
01:07:53,916 --> 01:07:55,886
어쩌면 계산중인 이미지의

2621
01:07:54,716 --> 01:07:56,686


2622
01:07:55,886 --> 01:07:59,219
외관과 관련된 여러 종류의 양

2623
01:07:56,686 --> 01:08:00,019


2624
01:07:59,219 --> 01:08:00,179
이미지의

2625
01:08:00,179 --> 01:08:02,117
이들 서로 다른 특징 벡터들을 연결

2626
01:08:00,979 --> 01:08:02,917


2627
01:08:02,117 --> 01:08:05,137
이미지의 일부 기능 표현을 제공하려면,

2628
01:08:02,917 --> 01:08:05,937


2629
01:08:05,137 --> 01:08:07,019
이제 이미지의이 특징 표현

2630
01:08:05,937 --> 01:08:07,819


2631
01:08:07,019 --> 01:08:08,836
선형 분류기에 공급 될 것이고,

2632
01:08:07,819 --> 01:08:09,636


2633
01:08:08,836 --> 01:08:10,683
원시 픽셀 자체를 먹이기보다는

2634
01:08:09,636 --> 01:08:11,483


2635
01:08:10,683 --> 01:08:12,902
분류기에 넣습니다.

2636
01:08:11,483 --> 01:08:13,702


2637
01:08:12,902 --> 01:08:15,671
그리고 여기서의 동기는,

2638
01:08:13,702 --> 01:08:16,471


2639
01:08:15,671 --> 01:08:17,700
그래서 왼쪽에 훈련 데이터 세트가 있다고 상상해보십시오.

2640
01:08:16,471 --> 01:08:18,500


2641
01:08:17,701 --> 01:08:20,193
이 빨간 점들과 중간에 빨간 점들

2642
01:08:18,501 --> 01:08:20,993


2643
01:08:20,193 --> 01:08:22,244
그리고 그 주변의 푸른 점.

2644
01:08:20,993 --> 01:08:23,044


2645
01:08:22,244 --> 01:08:23,693
그리고 이러한 종류의 데이터 세트의 경우,

2646
01:08:23,693 --> 01:08:26,440
선형 결정 경계를 그릴 수있는 방법이 없습니다.

2647
01:08:24,493 --> 01:08:27,240


2648
01:08:26,441 --> 01:08:29,157
빨간색 점과 파란색 점을 구분합니다.

2649
01:08:27,241 --> 01:08:29,957


2650
01:08:29,157 --> 01:08:32,155
그리고 우리는 마지막 강의에서 이것에
대한 더 많은 예를 보았습니다.

2651
01:08:29,957 --> 01:08:32,955


2652
01:08:32,156 --> 01:08:34,459
그러나 영리한 피처 변환을 사용한다면,

2653
01:08:32,956 --> 01:08:35,259


2654
01:08:34,459 --> 01:08:36,660
이 경우에는 극좌표로 변환,

2655
01:08:35,259 --> 01:08:37,460


2656
01:08:36,660 --> 01:08:39,079
이제 우리가 피쳐 변환을 한 후에,

2657
01:08:37,460 --> 01:08:39,879


2658
01:08:39,079 --> 01:08:42,361
이 복잡한 데이터 세트는 실제로

2659
01:08:39,879 --> 01:08:43,161


2660
01:08:42,361 --> 01:08:43,677
선형으로 분리 가능한,

2661
01:08:43,677 --> 01:08:45,160
실제로 정확하게 분류 될 수있다.

2662
01:08:45,160 --> 01:08:46,858
선형 분류기에 의해

2663
01:08:46,858 --> 01:08:48,435
그리고 여기있는 모든 트릭은 이제 알아내는 것입니다.

2664
01:08:48,436 --> 01:08:51,034
올바른 피쳐 변환은 무엇입니까?

2665
01:08:49,236 --> 01:08:51,834


2666
01:08:51,034 --> 01:08:53,197
그것은 올바른 양의 계산입니다.

2667
01:08:51,834 --> 01:08:53,997


2668
01:08:53,197 --> 01:08:55,129
당신이 걱정하는 문제에 대해서.

2669
01:08:53,997 --> 01:08:55,929


2670
01:08:55,129 --> 01:08:58,017
따라서 이미지의 경우 픽셀을 변환 할 수 있습니다.

2671
01:08:55,929 --> 01:08:58,817


2672
01:08:58,017 --> 01:08:59,751
극좌표로, 이해가 안 돼,

2673
01:08:59,751 --> 01:09:01,505
그러나 실제로 당신은 적어 내려고 노력할 수 있습니다.

2674
01:09:01,505 --> 01:09:03,084
이미지의 특징 표현

2675
01:09:03,085 --> 01:09:04,749
그게 말이 되겠지,

2676
01:09:04,749 --> 01:09:06,458
실제로 너를 도울지도 모른다.

2677
01:09:06,458 --> 01:09:08,385
원시 픽셀을 넣는 것보다 효과적 일 수 있습니다.

2678
01:09:07,258 --> 01:09:09,185


2679
01:09:08,385 --> 01:09:10,391
분류기에 넣습니다.

2680
01:09:09,185 --> 01:09:11,191


2681
01:09:10,392 --> 01:09:13,157
그래서 이런 종류의 피쳐 표현의 한 예

2682
01:09:11,192 --> 01:09:13,957


2683
01:09:13,157 --> 01:09:16,343
그것은 매우 간단합니다. 색상 히스토그램의 아이디어입니다.

2684
01:09:13,957 --> 01:09:17,143


2685
01:09:16,343 --> 01:09:18,526
그래서 당신은 아마 각 픽셀을 가져갈 것입니다,

2686
01:09:17,143 --> 01:09:19,326


2687
01:09:18,526 --> 01:09:21,188
당신은이 색조 스펙트럼을 취할 것입니다.

2688
01:09:19,326 --> 01:09:21,988


2689
01:09:21,188 --> 01:09:23,985
버킷으로 나눈 다음 모든 픽셀에 대해

2690
01:09:21,988 --> 01:09:24,785


2691
01:09:23,986 --> 01:09:26,425
당신은 그 색 버킷 중 하나에 그것을 매핑 할 것입니다.

2692
01:09:24,786 --> 01:09:27,225


2693
01:09:26,425 --> 01:09:28,535
얼마나 많은 픽셀을 카운트하는지

2694
01:09:27,225 --> 01:09:29,335


2695
01:09:28,536 --> 01:09:31,162
이 각각의 버킷으로 떨어지십시오.

2696
01:09:29,336 --> 01:09:31,962


2697
01:09:31,162 --> 01:09:34,638
따라서 이미지에 어떤 색상이 있는지
전 세계적으로 알 수 있습니다.

2698
01:09:31,962 --> 01:09:35,438


2699
01:09:34,639 --> 01:09:36,278
어쩌면이 개구리의 예가

2700
01:09:36,278 --> 01:09:37,500
이 특징 벡터는 우리에게

2701
01:09:37,500 --> 01:09:39,076
녹색 물건이 많이 있어요.

2702
01:09:39,076 --> 01:09:40,938
어쩌면 자주색이나 붉은 색이 아닌 것도있을 것입니다.

2703
01:09:39,876 --> 01:09:41,738


2704
01:09:40,938 --> 01:09:43,043
그리고 이것은 여러분이 볼 수있는 단순한 특징 벡터입니다

2705
01:09:41,738 --> 01:09:43,843


2706
01:09:43,043 --> 01:09:44,043
실제로.

2707
01:09:43,843 --> 01:09:44,843


2708
01:09:45,108 --> 01:09:47,720
우리가 본 또 다른 공통 특징 벡터

2709
01:09:45,908 --> 01:09:48,520


2710
01:09:47,720 --> 01:09:49,430
신경 네트워크의 부상 전에,

2711
01:09:49,431 --> 01:09:50,983
또는 신경 네트워크의 지배 이전에

2712
01:09:50,983 --> 01:09:53,219
지향 그라디언트의 히스토그램이었습니다.

2713
01:09:51,783 --> 01:09:54,019


2714
01:09:53,220 --> 01:09:54,952
첫 번째 강의에서 기억하십시오.

2715
01:09:54,952 --> 01:09:57,829
Hubel과 Wiesel은 이러한
지향성 가장자리를 발견했습니다.

2716
01:09:55,752 --> 01:09:58,629


2717
01:09:57,829 --> 01:10:00,046
인간 시각 시스템에서 정말로 중요합니다.

2718
01:09:58,629 --> 01:10:00,846


2719
01:10:00,046 --> 01:10:02,209
지향성 그라디언트의 히스토그램

2720
01:10:00,846 --> 01:10:03,009


2721
01:10:02,209 --> 01:10:04,690
피쳐 표현은 캡쳐를 시도한다.

2722
01:10:03,009 --> 01:10:05,490


2723
01:10:04,690 --> 01:10:07,680
똑같은 직감과 현지 오리엔테이션 측정

2724
01:10:05,490 --> 01:10:08,480


2725
01:10:07,680 --> 01:10:09,974
이미지 가장자리.

2726
01:10:08,480 --> 01:10:10,774


2727
01:10:09,974 --> 01:10:11,280
그래서이 일이 무엇을 할 것인지,

2728
01:10:11,280 --> 01:10:13,286
우리의 이미지를 받아 그것을 나눕니다.

2729
01:10:12,080 --> 01:10:14,086


2730
01:10:13,286 --> 01:10:16,354
이 작은 8x8 픽셀 영역으로

2731
01:10:14,086 --> 01:10:17,154


2732
01:10:16,354 --> 01:10:19,142
그리고 나서, 각각의 8 × 8 픽셀 영역 내에서,

2733
01:10:17,154 --> 01:10:19,942


2734
01:10:19,142 --> 01:10:22,268
지배적 인 에지 방향을 계산합니다.

2735
01:10:19,942 --> 01:10:23,068


2736
01:10:22,268 --> 01:10:24,921
각 픽셀의 에지 방향

2737
01:10:23,068 --> 01:10:25,721


2738
01:10:24,921 --> 01:10:27,776
여러 버킷에 넣은 다음 각 영역 내에서

2739
01:10:25,721 --> 01:10:28,576


2740
01:10:27,776 --> 01:10:31,857
이들 서로 다른 에지 방향에
대해 히스토그램을 계산하십시오.

2741
01:10:28,576 --> 01:10:32,657


2742
01:10:31,857 --> 01:10:33,417
이제 전체 기능 벡터

2743
01:10:33,417 --> 01:10:35,797
이러한 서로 다른 버킷 히스토그램이됩니다.

2744
01:10:34,217 --> 01:10:36,597


2745
01:10:35,797 --> 01:10:37,382
가장자리 방위의

2746
01:10:37,382 --> 01:10:39,121
8 개 지역마다 8 개 지역에 걸쳐

2747
01:10:39,121 --> 01:10:40,204
이미지에서.

2748
01:10:39,921 --> 01:10:41,004


2749
01:10:41,660 --> 01:10:43,450
그래서 이것은 어떤면에서는 이중적인 것입니다.

2750
01:10:43,450 --> 01:10:47,029
이전에 본 색상 히스토그램 분류기로

2751
01:10:44,250 --> 01:10:47,829


2752
01:10:47,029 --> 01:10:49,704
그래서 색 막대 그래프는 전 세계적으로 어떤 색

2753
01:10:47,829 --> 01:10:50,504


2754
01:10:49,704 --> 01:10:51,082
이미지에 존재한다.

2755
01:10:51,082 --> 01:10:53,751
이것은 전반적으로 어떤 유형의 에지 정보

2756
01:10:51,882 --> 01:10:54,551


2757
01:10:53,751 --> 01:10:55,305
이미지에 존재합니다.

2758
01:10:55,305 --> 01:10:57,991
그리고 심지어 이미지의 다른 부분에 국한되어,

2759
01:10:56,105 --> 01:10:58,791


2760
01:10:57,991 --> 01:11:01,191
다른 영역에 어떤 유형의 모서리가 있는지.

2761
01:10:58,791 --> 01:11:01,991


2762
01:11:01,191 --> 01:11:03,546
어쩌면 왼쪽에있는이 개구리의 경우,

2763
01:11:01,991 --> 01:11:04,346


2764
01:11:03,546 --> 01:11:04,938
당신은 그가 잎에 앉아있는 것을 볼 수 있습니다,

2765
01:11:04,938 --> 01:11:07,561
이 나뭇잎들은이 지배적 인 대각선 모서리를 가지며,

2766
01:11:05,738 --> 01:11:08,361


2767
01:11:07,561 --> 01:11:10,480
방향 그라디언트의 히스토그램을 시각화하면

2768
01:11:08,361 --> 01:11:11,280


2769
01:11:10,480 --> 01:11:12,833
기능을 사용하면이 지역에서

2770
01:11:11,280 --> 01:11:13,633


2771
01:11:12,833 --> 01:11:14,667
우리는 대각선 가장자리가 많습니다.

2772
01:11:13,633 --> 01:11:15,467


2773
01:11:14,667 --> 01:11:16,227
방향성 그라디언트의 히스토그램

2774
01:11:16,227 --> 01:11:19,340
특징 표현의 캡쳐.

2775
01:11:17,027 --> 01:11:20,140


2776
01:11:19,340 --> 01:11:21,509
그래서 이것은 매우 일반적인 특징 표현이었습니다.

2777
01:11:20,140 --> 01:11:22,309


2778
01:11:21,509 --> 01:11:23,535
물체 인식에 많이 사용되었습니다.

2779
01:11:22,309 --> 01:11:24,335


2780
01:11:23,535 --> 01:11:25,702
사실 너무 오래 전에.

2781
01:11:24,335 --> 01:11:26,502


2782
01:11:26,573 --> 01:11:29,789
거기에서 볼 수있는 또 다른 특징 표현

2783
01:11:27,373 --> 01:11:30,589


2784
01:11:29,789 --> 01:11:32,810
단어의 가방이 아이디어입니다.

2785
01:11:30,589 --> 01:11:33,610


2786
01:11:32,810 --> 01:11:34,202
그래서 이것은 영감을 얻고 있습니다.

2787
01:11:34,202 --> 01:11:36,355
자연 언어 처리에서.

2788
01:11:35,002 --> 01:11:37,155


2789
01:11:36,355 --> 01:11:38,220
그래서 단락이 있다면,

2790
01:11:37,155 --> 01:11:39,020


2791
01:11:38,220 --> 01:11:40,799
그런 다음 단락을 나타낼 수있는 방법

2792
01:11:39,020 --> 01:11:41,599


2793
01:11:40,799 --> 01:11:43,398
특징 벡터에 의해 발생 횟수를 세고있다.

2794
01:11:41,599 --> 01:11:44,198


2795
01:11:43,398 --> 01:11:45,732
그 단락의 다른 단어들.

2796
01:11:44,198 --> 01:11:46,532


2797
01:11:45,732 --> 01:11:47,666
그래서 우리는 그 직감을 받아 적용하고 싶습니다.

2798
01:11:46,532 --> 01:11:48,466


2799
01:11:47,666 --> 01:11:49,664
어떤 식 으로든 이미지에.

2800
01:11:48,466 --> 01:11:50,464


2801
01:11:49,664 --> 01:11:51,708
그러나 문제는 실제로는 단순한 것이 아니라,

2802
01:11:50,464 --> 01:11:52,508


2803
01:11:51,708 --> 01:11:54,288
이미지에 대한 단어의 직접적인 유추,

2804
01:11:52,508 --> 01:11:55,088


2805
01:11:54,288 --> 01:11:56,632
그래서 우리는 우리 자신의 어휘를 정의 할 필요가있다.

2806
01:11:55,088 --> 01:11:57,432


2807
01:11:56,632 --> 01:11:57,965
시각적 단어의

2808
01:11:57,432 --> 01:11:58,765


2809
01:11:58,880 --> 01:12:01,106
그래서 우리는이 2 단계 접근법을 취합니다.

2810
01:11:59,680 --> 01:12:01,906


2811
01:12:01,106 --> 01:12:04,318
먼저 우리는 많은 이미지를 얻을 것입니다.

2812
01:12:01,906 --> 01:12:05,118


2813
01:12:04,318 --> 01:12:06,455
작은 무작위 농작물을 한꺼번에 채취하다.

2814
01:12:05,118 --> 01:12:07,255


2815
01:12:06,455 --> 01:12:07,995
그 이미지들로부터

2816
01:12:07,995 --> 01:12:09,723
K와 같은 것을 사용하는 것을 의미합니다.

2817
01:12:09,723 --> 01:12:12,820
이 다른 클러스터 센터를 생각해 내야한다.

2818
01:12:10,523 --> 01:12:13,620


2819
01:12:12,820 --> 01:12:15,189
어쩌면 다른 유형을 나타내는 것입니다.

2820
01:12:13,620 --> 01:12:15,989


2821
01:12:15,189 --> 01:12:17,139
이미지의 시각적 단어.

2822
01:12:15,989 --> 01:12:17,939


2823
01:12:17,139 --> 01:12:19,341
이 예제를 여기 오른쪽에서 보면,

2824
01:12:17,939 --> 01:12:20,141


2825
01:12:19,341 --> 01:12:21,160
이것은 클러스터링의 실제 예입니다.

2826
01:12:20,141 --> 01:12:21,960


2827
01:12:21,160 --> 01:12:23,335
이미지에서 실제로 다른 이미지 패치,

2828
01:12:21,960 --> 01:12:24,135


2829
01:12:23,335 --> 01:12:25,627
이 클러스터링 단계 후에,

2830
01:12:24,135 --> 01:12:26,427


2831
01:12:25,627 --> 01:12:28,450
우리의 시각적 인 단어는이 다른 색을 포착합니다.

2832
01:12:26,427 --> 01:12:29,250


2833
01:12:28,450 --> 01:12:30,552
빨간색과 파란색과 노란색처럼,

2834
01:12:29,250 --> 01:12:31,352


2835
01:12:30,552 --> 01:12:32,553
이러한 다양한 유형의 지향 에지

2836
01:12:31,352 --> 01:12:33,353


2837
01:12:32,553 --> 01:12:34,556
다른 방향으로,

2838
01:12:33,353 --> 01:12:35,356


2839
01:12:34,556 --> 01:12:36,482
우리가보기 시작한 것이 흥미 롭습니다.

2840
01:12:35,356 --> 01:12:37,282


2841
01:12:36,482 --> 01:12:38,909
이러한 지향 에지는 데이터에서 나옵니다.

2842
01:12:37,282 --> 01:12:39,709


2843
01:12:38,909 --> 01:12:40,480
데이터 중심 방식으로

2844
01:12:40,480 --> 01:12:43,097
그리고 이제 우리가 이러한 일련의
시각적 단어들을 얻게되면,

2845
01:12:41,280 --> 01:12:43,897


2846
01:12:43,097 --> 01:12:44,291
또한 코드북이라고 불리는,

2847
01:12:44,291 --> 01:12:47,249
그러면 우리는 우리의 이미지를 인 코드 할 수 있습니다.

2848
01:12:45,091 --> 01:12:48,049


2849
01:12:47,249 --> 01:12:48,862
이러한 시각적 단어들 각각에 대해,

2850
01:12:48,862 --> 01:12:52,468
이 시각적 단어는 이미지에서 얼마나 발생합니까?

2851
01:12:49,662 --> 01:12:53,268


2852
01:12:52,468 --> 01:12:54,082
그리고 이제 이것은 우리에게 다시금,

2853
01:12:54,082 --> 01:12:55,463
약간 다른 정보

2854
01:12:55,463 --> 01:12:59,427
이 이미지의 시각적 모양은 무엇입니까?

2855
01:12:56,263 --> 01:13:00,227


2856
01:12:59,427 --> 01:13:02,124
실제로 이것은 일종의 특징 표현입니다.

2857
01:13:00,227 --> 01:13:02,924


2858
01:13:02,124 --> 01:13:04,638
황비홍 (Fei-Fei)은 대학원생이었을 때 일했고,

2859
01:13:02,924 --> 01:13:05,438


2860
01:13:04,638 --> 01:13:07,555
그래서 이것은 당신이 실제로 보았던 무언가입니다.

2861
01:13:05,438 --> 01:13:08,355


2862
01:13:07,555 --> 01:13:08,972
그리 오래 전 아니에요.

2863
01:13:08,355 --> 01:13:09,772


2864
01:13:10,783 --> 01:13:13,033
그래서 약간의 티저로,

2865
01:13:11,583 --> 01:13:13,833


2866
01:13:14,951 --> 01:13:16,837
이 모든 것을 다시 묶어서,

2867
01:13:15,751 --> 01:13:17,637


2868
01:13:16,837 --> 01:13:19,743
이 이미지 분류 파이프 라인

2869
01:13:17,637 --> 01:13:20,543


2870
01:13:19,743 --> 01:13:20,886
같이 보일지도 모른다.

2871
01:13:20,886 --> 01:13:22,725
어쩌면 5 년에서 10 년 전쯤에

2872
01:13:21,686 --> 01:13:23,525


2873
01:13:22,725 --> 01:13:24,421
당신이 당신의 이미지를 찍을 것이고,

2874
01:13:24,421 --> 01:13:26,677
그런 다음 이러한 다양한 특징 표현을 계산할 수 있습니다.

2875
01:13:25,221 --> 01:13:27,477


2876
01:13:26,677 --> 01:13:28,809
너의 심상의, 낱말의 부대 같이 것,

2877
01:13:27,477 --> 01:13:29,609


2878
01:13:28,809 --> 01:13:31,173
또는 방향 그라디언트의 히스토그램,

2879
01:13:29,609 --> 01:13:31,973


2880
01:13:31,173 --> 01:13:33,381
모든 기능을 함께 연결하고,

2881
01:13:31,973 --> 01:13:34,181


2882
01:13:33,381 --> 01:13:35,519
이러한 피쳐 추출기로 피드

2883
01:13:34,181 --> 01:13:36,319


2884
01:13:35,519 --> 01:13:38,590
일부 선형 분류기로

2885
01:13:36,319 --> 01:13:39,390


2886
01:13:38,590 --> 01:13:39,461
나는 조금 단순화하고있다.

2887
01:13:39,461 --> 01:13:42,018
파이프 라인은 그보다 조금 더 복잡했습니다.

2888
01:13:40,261 --> 01:13:42,818


2889
01:13:42,018 --> 01:13:44,576
그러나 이것은 일반적인 직감입니다.

2890
01:13:42,818 --> 01:13:45,376


2891
01:13:44,576 --> 01:13:48,158
그리고 그 아이디어는 당신이 추출한 후였습니다.

2892
01:13:45,376 --> 01:13:48,958


2893
01:13:48,158 --> 01:13:50,196
이러한 기능들,이 피쳐 추출기

2894
01:13:48,958 --> 01:13:50,996


2895
01:13:50,196 --> 01:13:52,326
업데이트되지 않는 고정 된 블록이 될 것입니다.

2896
01:13:50,996 --> 01:13:53,126


2897
01:13:52,326 --> 01:13:53,563
훈련 도중.

2898
01:13:53,563 --> 01:13:54,396
그리고 훈련 도중,

2899
01:13:54,396 --> 01:13:55,933
선형 분류 자만 업데이트하면됩니다.

2900
01:13:55,933 --> 01:13:57,907
기능 상단에서 작업하는 경우

2901
01:13:56,733 --> 01:13:58,707


2902
01:13:57,907 --> 01:13:59,972
그리고 실제로, 저는 일단 우리가 움직이면

2903
01:13:58,707 --> 01:14:00,772


2904
01:13:59,972 --> 01:14:01,774
길쌈 신경망 (convolutional neural networks)

2905
01:14:00,772 --> 01:14:02,574


2906
01:14:01,774 --> 01:14:03,140
이 깊은 신경 네트워크,

2907
01:14:03,140 --> 01:14:06,486
실제로 다른 것을 보지 못합니다.

2908
01:14:03,940 --> 01:14:07,286


2909
01:14:06,486 --> 01:14:08,651
유일한 차이점은 쓰기보다는

2910
01:14:07,286 --> 01:14:09,451


2911
01:14:08,651 --> 01:14:10,322
미리 기능,

2912
01:14:10,322 --> 01:14:12,687
우리는 데이터에서 직접 기능을 배우려고합니다.

2913
01:14:11,122 --> 01:14:13,487


2914
01:14:12,687 --> 01:14:15,916
그래서 우리는 원시 픽셀을 가져다가 먹일 것입니다.

2915
01:14:13,487 --> 01:14:16,716


2916
01:14:15,916 --> 01:14:17,530
이것을 컨볼 루션 네트워크에 연결함으로써,

2917
01:14:17,530 --> 01:14:19,687
이것은 여러 다른 레이어를 통해 컴퓨팅을 끝낼 것입니다.

2918
01:14:18,330 --> 01:14:20,487


2919
01:14:19,687 --> 01:14:21,488
어떤 유형의 피쳐 표현

2920
01:14:20,487 --> 01:14:22,288


2921
01:14:21,488 --> 01:14:23,459
데이터에 의해 주도되고 실제로 훈련하게됩니다.

2922
01:14:22,288 --> 01:14:24,259


2923
01:14:23,459 --> 01:14:26,120
이 전체 네트워크에 대한이 전체 가중치,

2924
01:14:24,259 --> 01:14:26,920


2925
01:14:26,120 --> 01:14:27,954
선형 분류기의 가중치가 아닌

2926
01:14:26,920 --> 01:14:28,754


2927
01:14:27,954 --> 01:14:28,787
위에.

2928
01:14:28,754 --> 01:14:29,587


2929
01:14:30,329 --> 01:14:32,970
그럼, 다음에 우리는이 아이디어에 뛰어들 것입니다.

2930
01:14:31,129 --> 01:14:33,770


2931
01:14:32,970 --> 01:14:36,131
좀 더 자세히 살펴보면, 우리는 몇
가지 신경망을 소개 할 것이며,

2932
01:14:33,770 --> 01:14:36,931


2933
01:14:36,131 --> -00:00:00,800
backpropagation에
대해서도 이야기를 시작하십시오.

2934
01:14:36,931 --> 00:00:00,000

