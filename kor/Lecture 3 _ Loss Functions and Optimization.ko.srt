1
00:00:07,755 --> 00:00:11,682
CS231N 3강입니다.

2
00:00:11,682 --> 00:00:14,594
오늘은 손실함수(Loss functions)와
최적화(Optimization)을 배울겁니다.

3
00:00:14,594 --> 00:00:20,129
진도 나가기에 앞서 공지사항을 전달합니다.

4
00:00:21,094 --> 00:00:24,547
먼저, 첫 번째 과제가 생겼습니다.

5
00:00:24,547 --> 00:00:26,889
웹사이트에서 확인하실 수 있습니다.

6
00:00:26,889 --> 00:00:30,086
과제를 좀 늦게 내드렸기 때문에

7
00:00:30,086 --> 00:00:35,264
4월 20일 목요일 오후 11시 59분으로 기한을 변경합니다.

8
00:00:36,374 --> 00:00:42,702
앞으로 2주정도 라고 보시면 됩니다.

9
00:00:42,702 --> 00:00:49,087
그리고 변경된 기한으로 강의계획서를 업데이트하겠습니다.

10
00:00:49,087 --> 00:00:54,617
과제를 완료하면 zip파일로 Canvas에 업로드해야 하며

11
00:00:54,617 --> 00:00:57,779
가능한 빨리 채점을 하도록 하겠습니다.

12
00:00:58,799 --> 00:01:04,879
다음 사항은 여러분이 공지사항을 확인하려면
항상 Piazza를 체크해야 한다는 것입니다.

13
00:01:04,879 --> 00:01:11,432
이번주에 Piazza에 몇가지 예제 프로젝트를 
공고해 놓을 것입니다.

14
00:01:11,432 --> 00:01:17,220
그래서 Stanford 커뮤니티와, 혹은 Stanford와 연계된 
분들로부터 프로젝트 아이디와 관련된 예시를 부탁드렸는데,

15
00:01:17,220 --> 00:01:24,583
그리고 그 분들이 이 수업을 듣는 학생들과 함께 프로젝트를 
진행하고 싶다는 제안을 해주셨습니다.

16
00:01:24,583 --> 00:01:26,986
그러니 Pizza에 공고를 확인해 보시고

17
00:01:26,986 --> 00:01:34,231
하고싶은 프로젝트가 있으면 자유롭게 
프로젝트 멘토와 직접 얘기하면 되겠습니다.

18
00:01:34,231 --> 00:01:37,090
추가적으로 강의 홈피에 근무시간을
게시하였습니다.

19
00:01:37,090 --> 00:01:45,077
구글 캔린더로 게시하였고 학생들이 근무시간에 대해
많이 물어봐서 이번에 게시하였습니다.

20
00:01:45,077 --> 00:01:48,307
마지막 공지는 구글 클라우드 관련된 것입니다.

21
00:01:48,307 --> 00:01:54,331
구글 클라우드에서 우리 강의를 지원하므로

22
00:01:54,331 --> 00:02:00,687
여러분의 과제와 프로젝트를 수행하기 위해
구글 클라우드 크래딧 100$를 지급하였으며

23
00:02:00,687 --> 00:02:05,246
사용하는 방법은 추후 Piazza에 공지하겠습니다.

24
00:02:05,246 --> 00:02:11,977
공지사항에 대한 질문이 없으면 진도를 나가겠습니다.

25
00:02:13,440 --> 00:02:14,273
좋습니다.

26
00:02:15,559 --> 00:02:17,997
저번 2강을 복습해보면

27
00:02:17,997 --> 00:02:20,412
인식에서의 어려운 점을 살펴보았고

28
00:02:20,412 --> 00:02:24,476
데이터 중심의 방식에 초점을 마춰보았습니다.

29
00:02:24,476 --> 00:02:29,160
그리고 이미지 분류에 대해 배웠고 
왜 이미지 분류가 어려운지, 그리고

30
00:02:29,160 --> 00:02:35,812
컴퓨터가 보는것과 사람이 보는 것의 차이가 있다는 것도 배웠습니다.

31
00:02:35,812 --> 00:02:42,124
그리고 분류를 어렵게하는 조명의 변화, 변형에 대해 다뤘고 
왜 이게 그토록 어려운지에 대해 배웠습니다.

32
00:02:42,124 --> 00:02:47,912
인간의 시각체계는 이 일을 엄청 쉽게 하는데도 말이죠

33
00:02:47,912 --> 00:02:50,421
그리고 KNN 분류기도 배웠습니다.

34
00:02:50,421 --> 00:02:55,309
데이터 중심 접근방법 중에 가장 단순한 방법이었죠.

35
00:02:55,309 --> 00:03:00,824
그리고 CIFAR-10도 배웠습니다. 저기 좌상단에 보이는
이미지들이죠.

36
00:03:00,824 --> 00:03:05,787
CIFAR-10은 비행기, 자동차 등 10개의 카테고리가 있습니다.

37
00:03:05,787 --> 00:03:11,202
그리고 어떻게 KNN을 이용해서 학습 데이터를 가지고 
각 카테고리 클래스를 분류하는

38
00:03:11,202 --> 00:03:15,746
결정 경계를 학습시킬 수 있는지도 배웠습니다.

39
00:03:15,746 --> 00:03:25,190
또한 크로스 벨리데이션데 대해서도 배웠고 트레인, 벨리데이션,
테스트셋으로 나눠서 하이퍼파라미터를 찾는 법도 배웠습니다.

40
00:03:25,190 --> 00:03:32,410
그리고 2강 마지막에는 Linear classification에 대해 배웠
습니다. Linear classifier는 뉴럴넷의 기본요소입니다.

41
00:03:32,410 --> 00:03:38,538
Linear classifier는 parametric classifier의
일종입니다. parametric classifier라는 것은

42
00:03:38,538 --> 00:03:45,444
트레이닝 데이터의 정보가 파라미터인  "행렬 W"로 축약된
다는 것을 뜻합니다. 그리고 W가 학습되는 것입니다.

43
00:03:45,444 --> 00:03:48,448
그리고 이 Linear classifier는 엄청 간단합니다.

44
00:03:48,448 --> 00:03:51,810
이미지를 입력받으면 하나의 긴 벡터로 폅니다.

45
00:03:51,810 --> 00:03:54,974
여기 이미지 x가 있고,

46
00:03:54,974 --> 00:04:02,918
이 이미지는 32x32x3 픽셀이고, 
이를 긴 열벡터로 펼치게 됩니다.

47
00:04:04,344 --> 00:04:09,722
32x32는 이미지의 높이와 너비이며 3은
이미지의 채널 red, green, blue 입니다.

48
00:04:09,722 --> 00:04:13,561
그리고 파라미터 "행렬 W"가 있는데

49
00:04:13,561 --> 00:04:18,517
이 행렬은 아까 만들었던 이미지 픽셀과 연산하여

50
00:04:18,517 --> 00:04:24,387
CIFAR-10의 각 10개의 클래스에 해당하는
클래스 스코어를 만들어줍니다.

51
00:04:24,387 --> 00:04:29,617
그리고 해석해보면 스코어가 더 큰 값은

52
00:04:29,617 --> 00:04:34,881
가령 고양이 클래스의 스코어가 더 크다는 것은 
분류기가 이 이미지가 고양이일것 같다고 생각하는 것입니다.

53
00:04:34,881 --> 00:04:42,443
반대로 개나 자동차 클래스의 스코어가 더 낮다는 것은 
이 이미지가 개나 자동차일 확률이 낮다는 것을 의미하는 것입니다.

54
00:04:42,443 --> 00:04:51,625
또한 저번 수업에서 설명이 좀 부족했던 부분이 있었습니다. 
Linear clssification을 각 클래스 템플릿으로 보는 것인데

55
00:04:51,625 --> 00:05:02,444
좌하단의 다이어그램을 보시면, 행렬 W에는 각 10개의 클래스와 
이미지의 모든 픽셀에 대응하되는 하나씩의 요소가 있습니다.

56
00:05:02,444 --> 00:05:06,554
그리고 이 것은 "이 픽셀이 클래스를 결정하는데 
얼마나 중요한 역할을 하는지"를 의미합니다.

57
00:05:06,554 --> 00:05:12,412
행렬 W의 각 행들이 해당하는 그 클래스의 
템플릿이 되는 것입니다.

58
00:05:12,412 --> 00:05:22,551
행렬 W의 각 행은 "이미지의 픽셀 값"과 "해당 클래스" 사이의
가중치가 되기 때문에

59
00:05:22,551 --> 00:05:29,900
그렇기 때문에 우리가 각 행을 풀어서 다시 한 이미지로 재구성하면
각 클래스에 대응하는 학습된 템플릿을 볼 수 있었습니다.

60
00:05:29,900 --> 00:05:35,399
그리고 또한 Linear clssification이 고차원 공간에서의 
일종의 "결정 경계"를 학습한다는 측면으로 볼 수 있는데,

61
00:05:35,399 --> 00:05:43,774
그 공간의 차원은 이미지의 픽셀 값에 해당하는 것입니다.

62
00:05:43,774 --> 00:05:47,571
지난 시간에 여기까지 진행했습니다.

63
00:05:47,571 --> 00:05:54,141
지난 시간에는 Linear classifier에 대한 간략한 아이디어만
배우고 끝났습니다.

64
00:05:54,141 --> 00:05:57,554
그때는 실제로 행렬W를 어떻게 만드는지는
다루지 않았습니다.

65
00:05:57,554 --> 00:06:02,628
가장 좋은 행렬W를 구하는데 어떻게 트레이닝 데이터를
활용해야 하는지는 언급하지 않았었습니다.

66
00:06:02,628 --> 00:06:08,292
지난 시간에는 임의의 행렬 W를 사용했었고,

67
00:06:08,292 --> 00:06:12,068
그 행렬 W를 가지고 각 이미지에 해당하는
10개의 클래스 스코어를 계산하였습니다.

68
00:06:12,068 --> 00:06:15,597
이 때의 클래스스코어는 좋을수도, 나쁠수도 있었습니다. 
(임의로 정한 것이니까)

69
00:06:15,597 --> 00:06:20,833
여기 간단한 예시가 있는데 세개의 트레이닝 데이터에 대한

70
00:06:20,833 --> 00:06:26,046
임의의 행렬W를 가지고 예측한 10개의 클래스 스코어입니다.

71
00:06:26,046 --> 00:06:29,506
굵게 표시한 각 스코어를 보면 더 좋은것도 있고 
나쁜 것도 있다는 것을 볼 수 있습니다.

72
00:06:29,506 --> 00:06:35,924
예컨데 왼쪽의 이미지를 보면 고양이이죠 
우리는 인간이니까 아주 쉽게 알 수 있죠.

73
00:06:35,924 --> 00:06:41,068
하지만 고양이에 부여된 확률을 보면, 
(확률이 아니라 스코어죠)

74
00:06:41,068 --> 00:06:48,082
이 분류기가 cat에는 2.9점을 줬습니다.

75
00:06:48,082 --> 00:06:51,018
반면 flog에는 3.78점을 줬군요

76
00:06:51,018 --> 00:06:57,920
이 분류기는 별로 좋아보이지 않습니다. 우리는 정답 클래스(고양이)
가 가장 높은 점수가 되는 분류기를 원합니다.

77
00:06:57,920 --> 00:07:02,729
반면 다른 예를 살펴봅시다. 차동차를 보면

78
00:07:02,729 --> 00:07:06,819
자동차 이미지에서는 자동차의 스코어가 제일 높습니다.
좋은 거죠

79
00:07:06,819 --> 00:07:14,357
개구리의 예를 보면 스코어가 -4입니다. 오히려 
다른 스코어보다도 훨씬 낮습니다. 엄청 않좋은 것입니다.

80
00:07:14,357 --> 00:07:16,531
이렇게 분석하는 방법은 좋지 않습니다.

81
00:07:16,531 --> 00:07:20,654
스코어를 눈으로 훑으면서 어느게 좋고
어느게 나쁜지 살펴보기만 하는것은 좋은 생각이 아닙니다.

82
00:07:20,654 --> 00:07:25,264
이런 알고리즘을 만들고, 어떤 W가 가장 좋은지를
결정하기 위해서는

83
00:07:25,264 --> 00:07:31,032
지금 만든 W가 좋은지 나쁜지를 정량화 할 방법이 필요합니다.

84
00:07:31,032 --> 00:07:41,987
W를 입력으로 받아서 각 스코어를 확인하고 이 W가 지금 얼마나 
거지같은지를 정량적으로 말해주는 것이 바로 손실함수입니다.

85
00:07:41,987 --> 00:07:49,782
이번 강의에서는 Image classification에서 쓸만한
몇가지 손실함수를 소개해 드리도록 하겠습니다.

86
00:07:49,782 --> 00:07:52,683
손실함수가 어떤 일을 해야하는지 한번 생각해 보세요.

87
00:07:52,683 --> 00:07:58,498
임의의 값 W가 얼마나 좋은지 나쁜지를 정량화해 줘야 합니다.

88
00:07:58,498 --> 00:08:04,770
우리가 실제로 원하는 것은, 행렬 W가 될 수 있는
모든 경우의 수에 대해서

89
00:08:04,770 --> 00:08:10,688
"가장 덜 구린"  W가 무엇인지를 찾고싶은 것입니다.

90
00:08:10,688 --> 00:08:12,860
이 과정이 바로 "최적화 과정" 이며

91
00:08:12,860 --> 00:08:16,276
최적화에 대해서는 좀 있다가 다시 다루겠습니다.

92
00:08:16,276 --> 00:08:21,003
일단 문제를 좀 줄여보겠습니다. 클래스 10개는 
다루기가 불편하니깐요

93
00:08:21,003 --> 00:08:28,886
이번 강의에서는 클래스를 3개로 하는 
toy 데이터셋을 사용하겠습니다.

94
00:08:28,886 --> 00:08:44,425
이 예제에서 "고양이" 클래스는 잘 분류되지 못했고 "자동차" 는 잘됐고 
"개구리"는 최악입니다. 개구리 점수는 다른 것보다 더 낮습니다.

95
00:08:44,425 --> 00:08:48,817
이를 좀 더 공식화해 봅시다.
손실 함수에 대해 생각해보면

96
00:08:48,817 --> 00:08:52,870
트레이닝 데이터 X와 Y가 있고

97
00:08:52,870 --> 00:08:56,196
보통 X는 알고리즘의 입력에 해당하고

98
00:08:56,196 --> 00:09:05,407
Image classification 알고리즘이라면 X는 이미지가 될 것이고
 Y는 예측하고자 하는 것이 될 것입니다.

99
00:09:05,407 --> 00:09:08,930
보통은 Y는 레이블이나 타겟이라고 합니다.

100
00:09:08,930 --> 00:09:16,797
Image classification의 경우라면 각 이미지를 CIFAR-10
의 10개의 카테고리 중 하나로 분류하는 것이고,

101
00:09:16,797 --> 00:09:24,414
여기에서 레이블 y는 1에서 10 사이의 정수 값이 됩니다. 
프로그래밍 언어에 따라 0에서 9 일수도 있습니다.

102
00:09:24,414 --> 00:09:30,270
어쨌든 이 y라는 정수값은 각 이미지 x의
정답 카테고리를 의미합니다.

103
00:09:30,270 --> 00:09:42,838
앞서 예측 함수를 정의했었죠 입력 이미지 x와 행렬 W를 입력으로 받아서
y를 예측하는 것입니다.

104
00:09:42,838 --> 00:09:46,446
Image classification 문제라면 y는 10개가 되겠습니다. 
(CIFAR-10의 경우)

105
00:09:46,446 --> 00:09:49,938
그 다음 손실함수 L_i를 정의합니다.

106
00:09:49,938 --> 00:09:56,804
앞서 말씀드린 예측함수 f와 정답 값 Y를 입력으로 받아서는

107
00:09:56,804 --> 00:10:02,510
이 트레이닝 샘플을 얼마나 구리게 예측하는지를
정량화 시켜 줍니다.

108
00:10:02,510 --> 00:10:10,478
그리고 최종 Loss인 "L"은 우리 데이터 셋에서 
각 N개의 샘플들의 Loss의 평균이 됩니다.

109
00:10:10,478 --> 00:10:13,432
이 함수는 아주 일반적인 공식입니다.

110
00:10:13,432 --> 00:10:16,421
그리고 Image classification 외에도 다양하게
확장할 수 있습니다.

111
00:10:16,421 --> 00:10:24,426
좀 더 나아가서 다른 딥러닝 알고리즘을 살펴보자면 
어떤 알고리즘이던 가장 일반적으로 진행되는 일은,

112
00:10:24,426 --> 00:10:33,535
일단 어떤 X와 Y가 존재하고, 여러분이 만들 파라미터 W에 
W가 얼마나 좋은지를 정량화하는 손실 함수를 만드는 것입니다.

113
00:10:33,535 --> 00:10:39,982
결국은 W의 공간을 탐색하면서 의트레이닝 데이터의
 Loss를 최소화하는 어떤 W를 찾게 될 것입니다.

114
00:10:41,081 --> 00:10:49,322
구체적으로 한 손실함수의 예를 들어보겠습니다.
이 손실함수는 Image classification에 아주 적합하기도 합니다.

115
00:10:49,322 --> 00:10:52,755
multi-class SVM loss를 대해 알아보겠습니다.

116
00:10:52,755 --> 00:10:59,602
아마 CS229에서 이진 SVM, sopport vector machine을 
본 적이 있을 것입니다.

117
00:11:00,662 --> 00:11:05,263
multi-class SVM은 여러 클래스를 다루기 위한
이진 SVM의 일반화된 형태입니다.

118
00:11:05,263 --> 00:11:11,797
CS229에서 다뤘던 이진 SVM은 두개의 클래스만 다룹니다.

119
00:11:11,797 --> 00:11:15,159
각 데이터는 Positive 또는 Negative로 분류될 뿐입니다.

120
00:11:15,159 --> 00:11:20,762
하지만 여긴 10개의 카테고리가 있기 때문에
여러개의 클래스를 다루려면 이 개념을 좀더 일반화 시켜야 합니다.

121
00:11:20,762 --> 00:11:24,854
그렇기 때문에 multi-class SVM의 손실함수는
조금 재밌게 생겼습니다.

122
00:11:24,854 --> 00:11:29,801
지금부터는 이에 대해 좀 더 자세히 알아보도록 하겠습니다.

123
00:11:29,801 --> 00:11:35,584
각각의 트레이닝 데이터에서 Loss L_i
를 구하는 방법 부터 살펴보겠습니다.

124
00:11:35,584 --> 00:11:43,373
L_i를 구하기 위해 우선  "True인 카테고리" 를 제외한 
"나머지 카테고리 Y"의 합을 구합니다.

125
00:11:43,373 --> 00:11:46,204
다시말해 맞지 않는 카테고리를 전부 합치는 것입니다.

126
00:11:46,204 --> 00:11:52,509
그리고 올바른 카테고리의 스코어와 올바르지 않은 카테고리의 
스코어를 비교해 봅니다.

127
00:11:52,509 --> 00:11:59,589
만약 올바른 카테고리의 점수가 올바르지 않은 카테고리의
점수보다 더 높으면

128
00:11:59,589 --> 00:12:03,814
그리고 그 격차가 일정 마진(safeth margin) 이상이라면,
이 예시에서는 그 마진을 1로 두었죠

129
00:12:03,814 --> 00:12:14,661
이 경우는 True인 스코어가 다른 false 카테고리보다 훨씬 더
크다는 것을 의미합니다.

130
00:12:14,661 --> 00:12:17,663
그렇게 되면 Loss는 0이 됩니다.

131
00:12:17,663 --> 00:12:21,703
이미지 내 정답이 아닌 카테고리의 모든 값들을 합치면

132
00:12:21,703 --> 00:12:27,155
그 값이 바로 한 이미지의 최종 Loss 가 되는 것입니다.

133
00:12:27,155 --> 00:12:31,994
그리고 전체 트레이닝 데이터 셋에서 그 Loss들의 평균을 구합니다.

134
00:12:31,994 --> 00:12:41,263
이걸 수식화 시키면 if-then 으로 표현 할 수 있습니다. 
(if:)정답 클래스의 스코어의 점수가 제일 높으면

135
00:12:41,263 --> 00:12:49,327
그럼 "then" 은 다음과 같이 나타낼 수 있습니다. 
then, max(0,s_j - s_yj + 1).

136
00:12:50,496 --> 00:12:54,294
그리고 수식을 보다가 헷갈리 때가 있는데

137
00:12:54,294 --> 00:13:00,920
그러면 직접 손으로 써보면서 이해하면 도움이 됩니다.

138
00:13:00,920 --> 00:13:06,805
그리고 0과 다른 값의 최댓값, Max(0, value) 
과 같은 식으로 손실 함수를 만드는데,

139
00:13:06,805 --> 00:13:14,627
이런 류의 손실 함수를 "hinge loss"(경첩)라고 부르기도 합니다.
그래프의 모양때문에 붙혀진 이름입니다.

140
00:13:14,627 --> 00:13:22,275
여기에서 x축은 S_Yi입니다.
실제 정답 클래스의 스코어입니다.

141
00:13:22,275 --> 00:13:25,353
그리고 y축은 Loss입니다.

142
00:13:25,353 --> 00:13:34,174
정답 카테고리의 점수가 올라갈수록 Loss가 선형적으로
줄어드는 것을 알 수 있습니다.

143
00:13:34,174 --> 00:13:40,235
이 로스는 0이 된 이후에도 Safety margin 을 
넘어설 때 까지 더 줄어듭니다.

144
00:13:40,235 --> 00:13:44,402
Loss가 0이 됐다는 건 클래스를 잘 분류했다는 뜻입니다.

145
00:13:45,550 --> 00:13:47,836
질문 있나요?

146
00:13:47,836 --> 00:13:52,410
[죄송합니다만, S_Yi가 뭔가요?]

147
00:13:52,410 --> 00:13:55,170
[정답 스코어인가요? ]

148
00:13:55,170 --> 00:13:56,321
네 질문은

149
00:13:56,321 --> 00:14:00,097
S가 어떤 것이고 S_Yi가 어떤 것인지 입니다.

150
00:14:00,097 --> 00:14:07,246
S는 분류기의 출력으로 나온 예측된 스코어 입니다.

151
00:14:07,246 --> 00:14:13,942
가령 1이 고양이고 2가 개면 
S_1은 고양이 스코어 S_2는 개 스코어 인 것이죠

152
00:14:13,942 --> 00:14:21,056
Y_i는 이미지의 실제 정답 카테고리 입니다. 정수 값이죠

153
00:14:21,056 --> 00:14:32,683
그러니 S_Y_i는 트레이닝 셋의 i번째 이미지의
정답 클래스의 스코어인 것입니다.

154
00:14:32,683 --> 00:14:33,870
질문 있으십니까?

155
00:14:33,870 --> 00:14:35,921
[질문 중]

156
00:14:35,921 --> 00:14:38,677
질문은, 그럼 정확히 무엇을 계산하는 것인지 입니다.

157
00:14:38,677 --> 00:14:44,590
무엇을 계산하는지는 흥미로운 주제입니다. 
앞으로 예제를 살펴보면 답을 찾을 수 있을 것입니다.

158
00:14:44,590 --> 00:14:51,984
이 Loss가 말하고자 하는 것은 정답 스코어가
다른 스코어들보다 높으면 좋다는 것입니다.

159
00:14:51,984 --> 00:14:58,364
정답 스코어는 safty margin(여기에서 1) 을 두고 
다른 스코어들 보다 훨씬 더 높아야 합니다.

160
00:14:58,364 --> 00:15:06,534
충분히 높지 않으면 Loss가 높아지게 됩니다.

161
00:15:06,534 --> 00:15:13,223
3개의 트레이닝 데이터셋을 가지고 예를 들어보면 
좀 더 감을 익힐 수 있을 것입니다.

162
00:15:13,223 --> 00:15:19,513
여기서는 case space notation을 제거하고 
zero one notation을 사용했습니다.

163
00:15:19,513 --> 00:15:26,186
맨 왼쪽의 예시를 보면서 multi-class SVM loss가
계산 되는 과정을 살펴보자면

164
00:15:26,186 --> 00:15:31,913
우선 정답이 아닌 클래스를 순회합니다.

165
00:15:31,913 --> 00:15:39,089
Cat은 정답 클래스이니, Car와  Flog 클래스를 순회합니다. 
이제 다시 Car의 경우를 살펴보면

166
00:15:41,088 --> 00:15:48,782
이제 (Car 스코어) 5.1 - (Cat 스코어) 3.2 + 1(magin)
을 구합니다. Cat과 Car를 비교할때 짐작할 수 있는 것은

167
00:15:49,969 --> 00:15:55,134
Car가 Cat보다 더 높으니까 Loss가 발생할 것이라는 것입니다.

168
00:15:55,134 --> 00:15:59,153
이 고양이 이미지의 예시를 보면

169
00:15:59,153 --> 00:16:03,387
2.9의 손실이 발생합니다.
Cat-Car 와 Car-Flog를 비교해보면

170
00:16:03,387 --> 00:16:08,480
Car는 3.2, Flog는 -1.7의 에러가 발생합니다.

171
00:16:08,480 --> 00:16:16,409
Cat 스코어는 Frog 스코어보다 훨씬 그므로 
Loss는 0이라고 할 수 있습니다.

172
00:16:16,409 --> 00:16:23,094
고양이 이미지의 Multiclass-SVM Loss는 
이런 클래스쌍의 Loss의 합이 되며

173
00:16:23,094 --> 00:16:27,112
즉 2.9 + 0 = 2.9 가 되는 것입니다.

174
00:16:27,112 --> 00:16:34,567
여기에서 2.9라는 숫자가 "얼마나 분류기가 이 이미지를
구리게 분류하는지" 에 대한 척도가 되겠습니다.

175
00:16:35,795 --> 00:16:41,574
이런 작업은 여러번 반복합니다.
 Car 클래스에 다시한번 해보면

176
00:16:41,574 --> 00:16:52,072
Car와 Cat을 비교해 볼 것입니다. Car 스코어가 Cat 스코어보다 높기
때문에 여기에서 Loss는 0이 될 것입니다.

177
00:16:52,072 --> 00:16:57,557
Car 와 Frog도 비교해보면 Car의 스코어가 Frog보다 훨씬
더 크다는 것을 알 수 있습니다.

178
00:16:57,557 --> 00:17:02,993
그러므로 Car 의 경우는 전체 Loss도 0이 되겠습니다.

179
00:17:02,993 --> 00:17:09,051
이미 다들 감을 잡으셨겠지만 개구리, frog의 경우도 한번 봅시다.

180
00:17:09,051 --> 00:17:14,895
Frog와 Cat을 보면 엄청 큰 Loss가 발생함을 알 수 있습니다.
Flog스코어가 엄청 낮기 때문이죠. Car와의 경우도 같습니다.

181
00:17:14,895 --> 00:17:19,697
또한 Frog 스코어 자체가 엄청 낮기 때문이죠. 
결국 전체 Loss는 12.9가 됩니다.

182
00:17:21,150 --> 00:17:26,578
그리고 전체 트레이닝 셋의 최종 Loss는 
각 트레이닝 이미지의 Loss들의 평균이 되겠습니다.

183
00:17:26,578 --> 00:17:29,277
대략 5.3정도 나오는 걸 볼 수 있습니다.

184
00:17:29,277 --> 00:17:34,929
이것이 의미하는 바는, 우리의 분류기가 5.3점 만큼 이 트레이닝 셋을
구리게 분류하고 있다는 "정량적 지표" 가 되는 것입니다.

185
00:17:34,929 --> 00:17:36,732
질문 있으십니까?

186
00:17:36,732 --> 00:17:39,152
[질문 중]

187
00:17:39,152 --> 00:17:41,920
질문은 바로, "1을 더하는 것"은 어떻게 결정하는 것인지 입니다. 
(safety margin 이 1이었음)

188
00:17:41,920 --> 00:17:43,574
아무 좋은 질문입니다.

189
00:17:43,574 --> 00:17:46,662
이게 아무 숫자나 막 끼워 넣은 것 같아 보이고

190
00:17:46,662 --> 00:17:52,532
이게 또 손실 함수에서 상수 텀으로 자리잡고 있고
그래서 보기에 불쾌할 수도 있는데요

191
00:17:52,532 --> 00:18:00,305
이게 임의로 선택한 숫자같아 보이긴 하지만, 우리는 사실 손실함수의
"스코어가 정확이 몇인지"는 신경쓰지 않습니다.

192
00:18:00,305 --> 00:18:05,389
우리가 궁금한건 여러 스코어 간의 상대적인 차이인 것입니다.

193
00:18:05,389 --> 00:18:08,944
우리가 관심있어 하는건 오로지 정답 스코어가 다른 스코어에 비해
얼마나 더 큰 스코어를 가지고 있는지 입니다.

194
00:18:08,944 --> 00:18:14,742
행렬 W를 전체적으로 스케일링한다고 생각해 보면
결과 스코어도 이에 따라 스케일이 바뀔 것입니다.

195
00:18:14,742 --> 00:18:20,808
이와 관련된 내용이 온라인 코스 노트에 자세히 나와 있습니다.

196
00:18:20,808 --> 00:18:24,362
그걸 보면 1 이라는게 별 상관 없다는 것을 알게 될 것입니다.

197
00:18:24,362 --> 00:18:32,625
1이라는 파라미터는 없어지고, W의 스케일에 의해 상쇄됩니다.

198
00:18:32,625 --> 00:18:35,489
자세한 내용은 온라인 코스노트를 확인해 주시면 되겠습니다.

199
00:18:37,953 --> 00:18:45,374
그 Loss가 실제무 무슨 일을 하는지를 직관적으로 이해하는 
것이 중요하므로 다른 질문들로 다시 접근해 보겠습니다.

200
00:18:45,374 --> 00:18:53,349
만약 Car 스코어가 조금 변하면 Loss에는 무슨 일이 일어날까요?

201
00:18:53,349 --> 00:18:54,182
아이디어 있으십니까?

202
00:18:56,479 --> 00:18:59,856
다들 대답하기 무서워 하는것 같네요

203
00:18:59,856 --> 00:19:00,689
정답?

204
00:19:00,689 --> 00:19:04,272
[대답 하는 학생]

205
00:19:06,983 --> 00:19:13,333
대답은 우리가 Car의 스코어를 조금 바꾸더라고
Loss가 바뀌지 않을 것이라고 대답했습니다.

206
00:19:13,333 --> 00:19:21,918
다시 SVM loss를 상기해보면 이 loss는 오직 정답 스코어와 
그 외의 스코어와의 차이만 고려했습니다. 따라서 이 경우에는

207
00:19:21,918 --> 00:19:25,506
Car 스코어가 이미 다른 스코어들보다 엄청 높기 때문에

208
00:19:25,506 --> 00:19:35,437
여기 스코어를 조금 바꾼다고 해도, 서로 간의 간격(Margin)은 여전히
유지될 것이고, 결국 Loss는 변하지 않습니다. 계속 0일 것입니다.

209
00:19:36,870 --> 00:19:40,150
다음 질문은 SVM Loss가 가질 수 있는
최댓/최솟값이 어떻게 될까요?

210
00:19:43,265 --> 00:19:44,445
[대답하는 학생]

211
00:19:44,445 --> 00:19:45,764
잘 못들었습니다?

212
00:19:45,764 --> 00:19:52,018
최솟값은 0이 되겠죠. 왜냐하면 모든 클래스에 걸쳐서

213
00:19:52,018 --> 00:19:57,357
정답 클래스의 스코어가 제일 크면 
모든 트레이닝 데이터에서 loss가 0이 될테니깐요

214
00:19:57,357 --> 00:20:03,495
그리고 다시 이 손실 함수가 hinge loss 모양이라는 점을 고려해 보면,

215
00:20:03,495 --> 00:20:07,981
만약 정답 클래스 스코어가 엄청 낮은 음수 값을 가지고
있다고 생각해보면, 아마 Loss가 무한대 일 것입니다.

216
00:20:07,981 --> 00:20:11,538
그러니 최솟값은 0이고 최댓값을 무한대 일 것입니다.

217
00:20:11,538 --> 00:20:14,427
또 다른 질문으로, 이러한 것들을 초기화 할 때 일종의

218
00:20:14,427 --> 00:20:16,120
처음부터 교육을 시작하십시오.

219
00:20:16,120 --> 00:20:18,015
보통 당신은 일종의 W를 초기화합니다.

220
00:20:18,015 --> 00:20:21,242
결과적으로 당신의 점수가 작은 무작위 값으로

221
00:20:21,242 --> 00:20:23,942
작고 균일 한 무작위 값인 경향이있다.

222
00:20:23,942 --> 00:20:25,535
훈련 시작시.

223
00:20:25,535 --> 00:20:27,926
그리고 질문은 만약 당신의 모든 S,

224
00:20:27,926 --> 00:20:30,064
모든 점수가 거의 0 인 경우

225
00:20:30,064 --> 00:20:31,449
대략 동일하고,

226
00:20:31,449 --> 00:20:32,815
그러면 어떤 종류의 손실이 예상됩니까?

227
00:20:32,815 --> 00:20:35,741
멀티 클래스 SVM을 사용할 때?

228
00:20:35,741 --> 00:20:37,502
- [학생] 수업 수에서 1을 뺍니다.

229
00:20:37,502 --> 00:20:42,448
네, 대답은 수업 수에서 1을 뺀 것입니다.

230
00:20:42,448 --> 00:20:45,871
우리가 반복한다면

231
00:20:45,871 --> 00:20:48,225
모든 잘못된 클래스들, 그래서 우리는 반복하고있다.

232
00:20:48,225 --> 00:20:51,489
C 클래스에서 하나의 클래스를 뺀 클래스

233
00:20:51,489 --> 00:20:53,738
두 개의 S는 거의 같을 것이다.

234
00:20:53,738 --> 00:20:55,096
그래서 우리는 1의 손실을 얻을 것이다.

235
00:20:55,096 --> 00:20:57,536
마진 때문에 우리는 C 빼기를 얻습니다.

236
00:20:57,536 --> 00:21:00,359
그래서 이것은 실제로 유용합니다. 왜냐하면 여러분,

237
00:21:00,359 --> 00:21:01,964
이것은 유용한 디버깅 전략이다.

238
00:21:01,964 --> 00:21:03,243
당신이 이런 것들을 사용할 때,

239
00:21:03,243 --> 00:21:04,734
훈련을 시작할 때,

240
00:21:04,734 --> 00:21:08,082
당신은 당신의 상실을 기대하는 것에 대해 생각해야합니다.

241
00:21:08,082 --> 00:21:10,931
그리고 훈련이 시작될 때 실제로 본 손실이

242
00:21:10,931 --> 00:21:13,784
첫 번째 반복에서 C에서 1을 뺀 것과 같습니다.

243
00:21:13,784 --> 00:21:14,999
이 경우,

244
00:21:14,999 --> 00:21:16,353
그건 아마 버그가 있다는 것을 의미하고 체크를해야합니다.

245
00:21:16,353 --> 00:21:18,655
귀하의 코드, 그래서 이것은 실제로 유용한 일종의

246
00:21:18,655 --> 00:21:20,905
실제로 확인하는 것.

247
00:21:21,886 --> 00:21:25,252
또 다른 질문은, 만약에 일어나면, 그래서 나는 우리가

248
00:21:25,252 --> 00:21:30,010
잘못된 클래스에 대한 SVM,

249
00:21:30,010 --> 00:21:31,487
올바른 클래스를 넘었다.

250
00:21:31,487 --> 00:21:34,323
우리가 모든 것을 다 통과한다면?

251
00:21:34,323 --> 00:21:36,034
- [학생] 손실이 1 증가합니다.

252
00:21:36,034 --> 00:21:39,326
그래, 대답은 손실이 하나씩 증가한다는 것이다.

253
00:21:39,326 --> 00:21:41,929
우리가 실제로이 일을하는 이유는

254
00:21:41,929 --> 00:21:45,052
일반적으로 제로의 손실은 일종의, 좋은이있다

255
00:21:45,052 --> 00:21:47,356
당신이 전혀 잃지 않는다는 해석,

256
00:21:47,356 --> 00:21:51,363
그래서 좋네요, 그래서 나는 당신의 대답을 생각합니다.

257
00:21:51,363 --> 00:21:52,755
정말 변하지 않을거야.

258
00:21:52,755 --> 00:21:54,512
당신은 결국 같은 분류자를 찾는 것입니다.

259
00:21:54,512 --> 00:21:56,484
실제로 모든 범주를 반복하면

260
00:21:56,484 --> 00:21:59,979
하지만 규칙에 따라 올바른 클래스를 생략하면

261
00:21:59,979 --> 00:22:02,729
우리의 최소 손실은 0이됩니다.

262
00:22:04,931 --> 00:22:06,341
그래서 또 다른 질문은, 만약 우리가 평균

263
00:22:06,341 --> 00:22:08,008
여기에 합계 대신에?

264
00:22:09,943 --> 00:22:11,233
- [학생] 변경되지 않습니다.

265
00:22:11,233 --> 00:22:13,076
- 네, 대답은 변하지 않습니다.

266
00:22:13,076 --> 00:22:15,788
따라서 클래스 수는 미리 고정 될 것입니다.

267
00:22:15,788 --> 00:22:18,075
우리가 우리의 데이터 세트를 선택할
때, 그것은 단지 재 스케일링입니다.

268
00:22:18,075 --> 00:22:20,500
상수에 의한 전체 손실 함수,

269
00:22:20,500 --> 00:22:22,334
그래서 그것은별로 중요하지 않습니다.

270
00:22:22,334 --> 00:22:23,991
다른 모든 규모의 것들과

271
00:22:23,991 --> 00:22:25,754
왜냐하면 우리는 실제로 진정한 가치에
관심을 갖지 않기 때문입니다.

272
00:22:25,754 --> 00:22:28,331
또는 손실의 진정한 가치

273
00:22:28,331 --> 00:22:29,664
그 문제에 대한.

274
00:22:30,541 --> 00:22:32,710
이제 다른 예가 있습니다.

275
00:22:32,710 --> 00:22:35,935
이 손실 공식 및 우리는 실제로 사각 항을 더했습니다.

276
00:22:35,935 --> 00:22:37,934
이 최대의 상단에?

277
00:22:37,934 --> 00:22:40,066
이것이 결국 같은 문제가 될까?

278
00:22:40,066 --> 00:22:42,968
아니면 다른 분류 알고리즘이 될까요?

279
00:22:42,968 --> 00:22:44,178
- [학생] 다르다.

280
00:22:44,178 --> 00:22:45,213
- 그래, 이건 다를거야.

281
00:22:45,213 --> 00:22:47,296
그래서 여기서 아이디어는 우리가
일종의 변화라고 생각합니다.

282
00:22:47,296 --> 00:22:49,145
선과 악의 절충

283
00:22:49,145 --> 00:22:50,903
일종의 비선형 방식으로,

284
00:22:50,903 --> 00:22:52,433
그래서 이것은 결국 실제로 컴퓨팅을 끝낼 것입니다.

285
00:22:52,433 --> 00:22:54,264
다른 손실 함수.

286
00:22:54,264 --> 00:22:57,296
제곱 된 경첩 손실이라는이 아이디어는 실제로 사용됩니다.

287
00:22:57,296 --> 00:22:59,915
때로는 실제로, 그래서 또 다른 속임수입니다

288
00:22:59,915 --> 00:23:01,597
너가 위로 만들 때 너의 부대 안에있을 것이다

289
00:23:01,597 --> 00:23:05,066
자신의 문제에 대한 자신의 손실 함수.

290
00:23:05,066 --> 00:23:07,831
이제 끝낼 것입니다, 오, 질문 있니?

291
00:23:07,831 --> 00:23:09,126
- [학생] 왜 제곱 된 손실을 사용합니까?

292
00:23:09,126 --> 00:23:11,596
제곱이 아닌 손실 대신에?

293
00:23:11,596 --> 00:23:14,018
- 네, 그럼 질문은 왜 당신이

294
00:23:14,018 --> 00:23:16,760
비 제곱 손실 대신 제곱 손실을 사용합니까?

295
00:23:16,760 --> 00:23:18,519
그리고 손실 함수의 전체 요점

296
00:23:18,519 --> 00:23:22,281
실수가 얼마나 다른지를 정량화하는 것입니다.

297
00:23:22,281 --> 00:23:25,143
그리고 분류자가 여러 종류의 실수를 저지르고 있다면,

298
00:23:25,143 --> 00:23:26,995
우리는 어떻게 서로 다른 장단점을 달랠 수 있을까요?

299
00:23:26,995 --> 00:23:28,940
서로 다른 유형의 실수 사이에서 분류 자

300
00:23:28,940 --> 00:23:30,141
만들겠습니까?

301
00:23:30,141 --> 00:23:32,265
그래서 제곱 된 손실을 사용한다면,

302
00:23:32,265 --> 00:23:36,371
그 종류의 말은 매우 나쁜 것들이

303
00:23:36,371 --> 00:23:37,834
이제 제곱이 나빠질거야.

304
00:23:37,834 --> 00:23:39,192
그래서 그것은 정말로, 정말로 나쁘고,

305
00:23:39,192 --> 00:23:40,711
우리는 아무것도 원하지 않는 것처럼

306
00:23:40,711 --> 00:23:43,623
그것은 완전히 파국적으로 잘못 분류 된 것입니다.

307
00:23:43,623 --> 00:23:47,321
반면에이 경첩 손실을 사용하는 경우,

308
00:23:47,321 --> 00:23:49,556
우리는 조금 틀린 것 사이에서 실제로 신경 쓰지 않는다.

309
00:23:49,556 --> 00:23:52,553
그리고 많은 잘못되고, 틀린 종류의,

310
00:23:52,553 --> 00:23:55,340
예를 많이 잘못 생각하면 그것을 증가시킵니다.

311
00:23:55,340 --> 00:23:57,051
조금만 잘못하면

312
00:23:57,051 --> 00:23:59,608
그것은 예제와 같은 장점입니다.

313
00:23:59,608 --> 00:24:02,603
그것은 조금 잘못되어 다시 증가시키는 것입니다.

314
00:24:02,603 --> 00:24:04,375
조금 더 옳다.

315
00:24:04,375 --> 00:24:06,268
그래서 약간 물결 모양의 물결 모양입니다.

316
00:24:06,268 --> 00:24:08,785
그러나 선형 대 정사각형을 사용하는이 아이디어

317
00:24:08,785 --> 00:24:11,198
우리가 얼마나 신경을 쓰는지를 수치화하는 방법입니다.

318
00:24:11,198 --> 00:24:13,597
서로 다른 범주의 오류에 대해

319
00:24:13,597 --> 00:24:15,375
그리고 이것은 확실히 당신이 생각해야 할 것입니다.

320
00:24:15,375 --> 00:24:17,938
실제로 이러한 것들을 실제로 적용 할 때,

321
00:24:17,938 --> 00:24:20,245
손실 함수가 방법이기 때문에

322
00:24:20,245 --> 00:24:22,770
알고리즘에 어떤 유형의 오류

323
00:24:22,770 --> 00:24:24,104
당신이 신경을 쓰고 어떤 종류의 오류

324
00:24:24,104 --> 00:24:26,267
그것은 반대해야합니다.

325
00:24:26,267 --> 00:24:27,907
실제로 실제로는 매우 중요합니다.

326
00:24:27,907 --> 00:24:30,407
귀하의 응용 프로그램에 따라.

327
00:24:32,315 --> 00:24:35,017
여기 벡터화 된 코드 종류의 작은 조각이 있습니다.

328
00:24:35,017 --> 00:24:37,566
numpy로 끝내면 결국 구현됩니다.

329
00:24:37,566 --> 00:24:40,962
첫 번째 임무를 위해 이와 비슷한 것,

330
00:24:40,962 --> 00:24:43,593
그러나 이런 종류의 감각은 당신에게이 합계

331
00:24:43,593 --> 00:24:44,952
실제로는 꽤 쉽다.

332
00:24:44,952 --> 00:24:46,798
numpy로 구현하려면 몇 줄 밖에 걸리지 않습니다.

333
00:24:46,798 --> 00:24:48,768
벡터화 된 코드

334
00:24:48,768 --> 00:24:50,926
그리고 실제로 하나의 멋진 트릭을 볼 수 있습니다.

335
00:24:50,926 --> 00:24:55,904
실제로 여기에 들어가서 여백을
제로화 할 수 있다는 것입니다.

336
00:24:55,904 --> 00:24:57,856
올바른 클래스에 해당하는

337
00:24:57,856 --> 00:25:00,750
그리고 그것은 바로 그때에 쉽게,

338
00:25:00,750 --> 00:25:04,345
그것은 건너 뛰는 멋진 벡터화 된 트릭의 일종이며,

339
00:25:04,345 --> 00:25:06,069
하나의 클래스를 제외한 모든 클래스를 반복합니다.

340
00:25:06,069 --> 00:25:07,891
당신은 건너 뛰고 싶은 사람을 제로로 해줍니다.

341
00:25:07,891 --> 00:25:09,854
그리고 어쨌든 합계를 계산하면 좋은 트릭입니다.

342
00:25:09,854 --> 00:25:13,315
당신은 과제에 대한 사용을 고려할 수 있습니다.

343
00:25:13,315 --> 00:25:17,106
자, 이제이 손실 함수에 대한 또 다른 질문입니다.

344
00:25:17,106 --> 00:25:19,439
당신이 W를 찾을만큼 충분히 운이 좋다면

345
00:25:19,439 --> 00:25:21,629
그 손실은 0입니다. 당신은 전혀 손실되지 않습니다.

346
00:25:21,629 --> 00:25:22,745
너는 완전히 이기고있다.

347
00:25:22,745 --> 00:25:24,139
이 손실 함수는 그것을 분쇄하고 있습니다.

348
00:25:24,139 --> 00:25:27,819
하지만 질문이 있습니다.이 독특한 W입니다.

349
00:25:27,819 --> 00:25:29,057
또는 다른 W가 있었나요

350
00:25:29,057 --> 00:25:32,390
그것은 또한 손실을 0으로 만들 수 있었습니까?

351
00:25:33,251 --> 00:25:34,271
- [학생] 다른 W가 있습니다.

352
00:25:34,271 --> 00:25:37,444
- 답변, 예, 그래서 다른 W가 있습니다.

353
00:25:37,444 --> 00:25:39,998
그리고 특히, 우리가 조금 이야기했기 때문에

354
00:25:39,998 --> 00:25:43,976
전체 문제를 위 또는 아래로 조정하는 것에 관한 것

355
00:25:43,976 --> 00:25:46,709
W에 따라 실제로 W를 취할 수 있습니다.

356
00:25:46,709 --> 00:25:50,876
두 배로 곱해졌고 W가 두 배로
증가했습니다 (지금 쿼드 U입니까?

357
00:25:51,984 --> 00:25:52,978
나도 몰라.)

358
00:25:52,978 --> 00:25:54,110
[웃음]

359
00:25:54,110 --> 00:25:56,601
이것은 또한 손실을 0으로 만들 것입니다.

360
00:25:56,601 --> 00:25:58,568
그래서 이것의 구체적인 예로서,

361
00:25:58,568 --> 00:26:00,066
좋아하는 예제로 돌아갈 수 있습니다.

362
00:26:00,066 --> 00:26:01,184
어쩌면 숫자를 통해 일할 수도 있습니다.

363
00:26:01,184 --> 00:26:02,351
나중에 조금,

364
00:26:02,351 --> 00:26:05,309
그러나 당신이 W를 가져 가고 우리가 W를 두 배로한다면,

365
00:26:05,309 --> 00:26:09,129
올바른 점수와 잘못된 점수 사이의 마진

366
00:26:09,129 --> 00:26:10,583
또한 두 배가됩니다.

367
00:26:10,583 --> 00:26:12,190
그래서이 모든 마진이

368
00:26:12,190 --> 00:26:14,521
이미 하나보다 컸고 우리는 그들을 두배로 늘 렸습니다.

369
00:26:14,521 --> 00:26:16,274
그들은 여전히 하나보다 커질 것입니다.

370
00:26:16,274 --> 00:26:18,857
그래서 당신은 여전히 손실이 없을 것입니다.

371
00:26:20,180 --> 00:26:22,182
그리고 이것은 재미있는 일종의,

372
00:26:22,182 --> 00:26:24,572
왜냐하면 우리의 손실 함수

373
00:26:24,572 --> 00:26:27,428
우리가 우리가 원하는 W를 우리
분류 자에게 말하는 방식입니다.

374
00:26:27,428 --> 00:26:29,045
그리고 우리가 신경 쓰는 W,

375
00:26:29,045 --> 00:26:30,333
이것은 조금 이상하다.

376
00:26:30,333 --> 00:26:31,795
지금이 불일치가 있습니다.

377
00:26:31,795 --> 00:26:35,197
분류 자의 선택 방법

378
00:26:35,197 --> 00:26:36,862
이들 서로 다른 버전의 W 사이

379
00:26:36,862 --> 00:26:39,112
모두 제로 손실을 달성합니까?

380
00:26:40,099 --> 00:26:42,210
그리고 그것은 우리가 여기서 한 일이

381
00:26:42,210 --> 00:26:45,303
데이터 측면에서 손실 만 기록됩니다.

382
00:26:45,303 --> 00:26:48,136
우리는 분류 자에게만 말했습니다.

383
00:26:48,982 --> 00:26:50,448
W를 찾아야한다고

384
00:26:50,448 --> 00:26:52,239
교육 자료에 맞는

385
00:26:52,239 --> 00:26:54,233
하지만 실제로 실제로, 우리는 실제로 신경 쓰지 않습니다.

386
00:26:54,233 --> 00:26:56,465
훈련 자료를 맞추는 것에 관한 것,

387
00:26:56,465 --> 00:26:57,760
기계 학습의 요점

388
00:26:57,760 --> 00:27:01,553
우리가 훈련 데이터를 사용하여
일부 분류자를 찾는 것입니다.

389
00:27:01,553 --> 00:27:04,222
그런 다음 테스트 데이터에 적용 할 것입니다.

390
00:27:04,222 --> 00:27:06,858
따라서 우리는 교육 데이터 성능에
대해 실제로 신경 쓰지 않습니다.

391
00:27:06,858 --> 00:27:08,503
우리는 정말 성능에 신경을 썼다.

392
00:27:08,503 --> 00:27:10,928
테스트 데이터에 대한이 분류 기준.

393
00:27:10,928 --> 00:27:12,785
결과적으로 유일한 경우

394
00:27:12,785 --> 00:27:14,671
우리는 분류 자에게 할 일을 알려줍니다.

395
00:27:14,671 --> 00:27:16,176
훈련 자료에 적합하다.

396
00:27:16,176 --> 00:27:18,053
우리는 스스로를 인도 할 수있다.

397
00:27:18,053 --> 00:27:20,307
때로는이 이상한 상황에 빠지기도합니다.

398
00:27:20,307 --> 00:27:24,079
분류기는 직관적이지 않은 동작을 할 수 있습니다.

399
00:27:24,079 --> 00:27:27,786
그래서 이런 종류의 구체적이고 표준적인 예는,

400
00:27:27,786 --> 00:27:29,985
그건 그렇고, 이것은 더 이상 선형 분류가 아니며,

401
00:27:29,985 --> 00:27:31,426
이것은 좀 더 일반적인 것입니다.

402
00:27:31,426 --> 00:27:32,918
기계 학습 개념,

403
00:27:32,918 --> 00:27:35,815
우리가이 파란 점의 데이터 세트를 가지고 있다고 가정하면,

404
00:27:35,815 --> 00:27:38,668
우리는 훈련 데이터에 약간의 곡선을 맞출 것입니다.

405
00:27:38,668 --> 00:27:39,827
파란 점들,

406
00:27:39,827 --> 00:27:42,783
그런 다음 우리가 분류 자에게 말했던 유일한 것

407
00:27:42,783 --> 00:27:44,532
훈련 자료를 시험하고 맞추는 것,

408
00:27:44,532 --> 00:27:46,451
들어가서 아주 휘어지는 커브를 가질 수 있습니다.

409
00:27:46,451 --> 00:27:47,812
완벽하게 분류하려고 시도하다.

410
00:27:47,812 --> 00:27:49,647
모든 교육 데이터 포인트.

411
00:27:49,647 --> 00:27:52,235
그러나 우리가 실제로 걱정하지 않기 때문에 이것은 나쁘다.

412
00:27:52,235 --> 00:27:53,519
이 성능에 대해,

413
00:27:53,519 --> 00:27:56,329
우리는 테스트 데이터의 성능에 신경을 썼다.

414
00:27:56,329 --> 00:27:58,379
이제 새로운 데이터가 있으면

415
00:27:58,379 --> 00:28:00,710
그 종류의 추세는 같은 경향을 따르고 있습니다.

416
00:28:00,710 --> 00:28:02,267
이 아주 휘황 찬란 선

417
00:28:02,267 --> 00:28:03,872
완전히 틀리게 될 것입니다.

418
00:28:03,872 --> 00:28:05,601
그리고 사실 우리가 선호했을만한 것은

419
00:28:05,601 --> 00:28:07,806
할 분류 기준은 아마도 예측할 수 있습니다.

420
00:28:07,806 --> 00:28:09,334
이 똑바로 녹색 선,

421
00:28:09,334 --> 00:28:11,683
이 복잡한 위그 라인보다

422
00:28:11,683 --> 00:28:15,171
모든 교육 데이터에 완벽하게 맞춰야합니다.

423
00:28:15,171 --> 00:28:17,821
그리고 이것은 근본적인 근본적인 문제입니다.

424
00:28:17,821 --> 00:28:19,232
기계 학습에서,

425
00:28:19,232 --> 00:28:20,662
우리가 일반적으로 해결하는 방법,

426
00:28:20,662 --> 00:28:22,816
이 정례화의 개념입니다.

427
00:28:22,816 --> 00:28:25,159
여기에 추가 용어를 추가 할 것입니다.

428
00:28:25,159 --> 00:28:26,443
손실 함수.

429
00:28:26,443 --> 00:28:27,832
데이터 손실 외에도,

430
00:28:27,832 --> 00:28:30,255
그것은 우리 분류 자에게 적합하다고 말해 줄 것입니다.

431
00:28:30,255 --> 00:28:32,448
교육 데이터는 일반적으로 추가 할 것입니다.

432
00:28:32,448 --> 00:28:34,309
손실 함수에 대한 또 다른 용어

433
00:28:34,309 --> 00:28:36,057
정규화 용어 (regularization term)

434
00:28:36,057 --> 00:28:40,691
모델이 어떻게 든 더 단순한 W를 선택하도록 장려한다.

435
00:28:40,691 --> 00:28:42,492
단순한 개념

436
00:28:42,492 --> 00:28:45,992
종류는 작업과 모델에 따라 다릅니다.

437
00:28:47,925 --> 00:28:49,639
Occam의 면도날에 대한 아이디어가 있습니다.

438
00:28:49,639 --> 00:28:52,868
이것은 과학적 발견에서의이 근본적인 생각이다.

439
00:28:52,868 --> 00:28:55,574
더 넓게, 그것은 당신이 많은 다른

440
00:28:55,574 --> 00:28:57,713
경쟁 가설은 설명 할 수있다.

441
00:28:57,713 --> 00:28:59,158
너의 관찰,

442
00:28:59,158 --> 00:29:01,039
당신은 일반적으로 더 간단한 것을 선호해야합니다,

443
00:29:01,039 --> 00:29:03,399
그것이 그 가능성이 더 높은 설명이기 때문입니다.

444
00:29:03,399 --> 00:29:06,801
미래의 새로운 관측에 일반화하기.

445
00:29:06,801 --> 00:29:08,714
그리고 우리가이 직감을 조작하는 방법

446
00:29:08,714 --> 00:29:10,977
기계 학습에서 일반적으로 몇 가지 명시 적으로

447
00:29:10,977 --> 00:29:12,538
정규화 벌금

448
00:29:12,538 --> 00:29:15,121
그것은 종종 R로 기록됩니다.

449
00:29:16,312 --> 00:29:18,687
그러면 표준 손실 함수

450
00:29:18,687 --> 00:29:20,409
일반적으로이 두 가지 용어가 있습니다.

451
00:29:20,409 --> 00:29:22,417
데이터 손실 및 정규화 손실,

452
00:29:22,417 --> 00:29:25,130
여기에 하이퍼 파라미터가 있습니다, 람다,

453
00:29:25,130 --> 00:29:27,200
그 둘 사이에 거래됩니다.

454
00:29:27,200 --> 00:29:28,952
그리고 우리는 하이퍼 파라미터에 대해 이야기했습니다.

455
00:29:28,952 --> 00:29:31,054
마지막 강연에서 상호 유효성 검사,

456
00:29:31,054 --> 00:29:33,820
그래서이 정규화 하이퍼 파라미터 람다

457
00:29:33,820 --> 00:29:35,797
더 중요한 것 중 하나가 될 것이다.

458
00:29:35,797 --> 00:29:37,280
훈련 할 때 조정할 필요가있다.

459
00:29:37,280 --> 00:29:39,363
실제로 이러한 모델.

460
00:29:40,229 --> 00:29:41,062
문제?

461
00:29:42,097 --> 00:29:44,679
- [학생] 그 람다 R 용어는 무엇입니까?

462
00:29:44,679 --> 00:29:48,846
희미하게 말하기와 관련이 있습니다.

463
00:29:50,685 --> 00:29:51,941
- 그래, 문제는,

464
00:29:51,941 --> 00:29:53,850
이 람다 R W 용어 사이의 연결은 무엇인가?

465
00:29:53,850 --> 00:29:55,405
실제로이 위그 라인을 강요합니다.

466
00:29:55,405 --> 00:29:58,072
똑바로 푸른 선이 되려면?

467
00:29:59,912 --> 00:30:01,319
나는 이것에 관한 파생을 겪고 싶지 않았다.

468
00:30:01,319 --> 00:30:03,550
나는 그것이 우리를 너무 멀게
이끌 것이라고 생각했기 때문에,

469
00:30:03,550 --> 00:30:04,768
그러나 당신은 상상할 수 있습니다.

470
00:30:04,768 --> 00:30:06,310
어쩌면 당신은 회귀 문제를 겪고 있습니다.

471
00:30:06,310 --> 00:30:09,830
다른 다항식 기초 함수의 관점에서,

472
00:30:09,830 --> 00:30:13,921
이 회귀 패널티를 추가하는 경우,

473
00:30:13,921 --> 00:30:15,775
어쩌면 모델은 다항식에 접근 할 수 있습니다.

474
00:30:15,775 --> 00:30:18,316
매우 높은 수준이지만,이 회귀 기간을 통해

475
00:30:18,316 --> 00:30:20,715
당신은 모델이 다항식을 선호하도록 장려 할 수 있습니다.

476
00:30:20,715 --> 00:30:23,649
낮은 학위의, 그들은 적절하게 데이터에 맞는 경우,

477
00:30:23,649 --> 00:30:25,569
또는 데이터가 비교적 잘 맞는지 여부.

478
00:30:25,569 --> 00:30:29,021
그래서 이것을 할 수있는 두 가지
방법이 있다고 상상할 수 있습니다.

479
00:30:29,021 --> 00:30:30,681
모델 클래스를 제한 할 수 있습니다.

480
00:30:30,681 --> 00:30:32,897
더 강력한,

481
00:30:32,897 --> 00:30:36,337
더 복잡한 모델을 만들거나이 소프트
페널티를 추가 할 수 있습니다.

482
00:30:36,337 --> 00:30:40,846
모델이 여전히 더 복잡한 모델에 액세스 할 수있는 경우,

483
00:30:40,846 --> 00:30:43,112
이 경우에는 다차원 다항식 일 수도 있지만,

484
00:30:43,112 --> 00:30:44,664
하지만이 부드러운 제약 조건을 추가하면됩니다.

485
00:30:44,664 --> 00:30:47,921
이 복잡한 모델을 사용하기를 원한다면,

486
00:30:47,921 --> 00:30:49,648
이 형벌을 극복해야합니다.

487
00:30:49,648 --> 00:30:51,316
그들의 복잡성을 사용합니다.

488
00:30:51,316 --> 00:30:52,949
여기는 연결입니다.

489
00:30:52,949 --> 00:30:55,282
그건 꽤 선형 분류되지 않습니다,

490
00:30:55,282 --> 00:30:57,858
이것은 많은 사람들이 염두에두고있는 그림입니다.

491
00:30:57,858 --> 00:31:01,691
적어도 정규화에 대해 생각할 때.

492
00:31:02,731 --> 00:31:04,643
그래서 실제로는 여러 가지 유형이 있습니다.

493
00:31:04,643 --> 00:31:06,917
실제로 사용되는 정규화.

494
00:31:06,917 --> 00:31:09,874
가장 보편적 인 것은 아마 L2 정규화 일 것이다.

495
00:31:09,874 --> 00:31:11,022
또는 체중 감량.

496
00:31:11,022 --> 00:31:13,905
그러나 당신이 볼 수있는 다른 것들이 많이 있습니다.

497
00:31:13,905 --> 00:31:17,973
이 L2 정규화는 단지 유클리드 표준입니다.

498
00:31:17,973 --> 00:31:19,470
이 가중 벡터 (W)

499
00:31:19,470 --> 00:31:21,892
때로는 제곱 된 규범입니다.

500
00:31:21,892 --> 00:31:24,021
또는 때때로 제곱 된 표준의 절반

501
00:31:24,021 --> 00:31:25,480
파생 상품을 없애기 때문에

502
00:31:25,480 --> 00:31:26,738
조금 더 좋네.

503
00:31:26,738 --> 00:31:28,832
그러나 L2 정규화의 아이디어

504
00:31:28,832 --> 00:31:30,702
당신은 단지 유클리드 표준을 처벌하고 있습니까?

505
00:31:30,702 --> 00:31:32,650
이 가중치 벡터의

506
00:31:32,650 --> 00:31:35,593
또한 때로는 L1 정규화를 볼 수도 있습니다.

507
00:31:35,593 --> 00:31:38,957
여기서 우리는 가중치 벡터의 L1 표준에 불이익을 당하고,

508
00:31:38,957 --> 00:31:42,525
L1 정규화에는 좋은 속성이 있습니다.

509
00:31:42,525 --> 00:31:46,401
이 행렬에서 격려하는 희소성처럼.

510
00:31:46,401 --> 00:31:47,692
다른 것들을 볼 수도 있습니다.

511
00:31:47,692 --> 00:31:49,815
이 탄성 망 정규화일까요?

512
00:31:49,815 --> 00:31:52,744
L1과 L2의 조합입니다.

513
00:31:52,744 --> 00:31:55,837
때로는 최대 표준 정규화를 볼 수 있습니다.

514
00:31:55,837 --> 00:32:00,004
L1 또는 L2 표준이 아닌 최대 표준을 페널티.

515
00:32:01,119 --> 00:32:03,025
그러나 이러한 정례화

516
00:32:03,025 --> 00:32:05,792
당신이 깊은 학습에서 보는 것뿐만 아니라,

517
00:32:05,792 --> 00:32:07,374
그러나 기계 학습의 많은 분야에 걸쳐

518
00:32:07,374 --> 00:32:10,997
심지어는 더 광범위하게 최적화 할 수도 있습니다.

519
00:32:10,997 --> 00:32:13,091
이후의 강의에서는

520
00:32:13,091 --> 00:32:15,726
특정 유형의 정규화 유형

521
00:32:15,726 --> 00:32:17,138
깊은 학습.

522
00:32:17,138 --> 00:32:19,602
예를 들어 드롭 아웃을 위해, 우리는
몇 가지 강의를 보게 될 것입니다.

523
00:32:19,602 --> 00:32:22,882
또는 배치 정규화, 확률 적 깊이,

524
00:32:22,882 --> 00:32:25,157
최근 몇 년 동안 이러한 일들이 미쳐 버렸습니다.

525
00:32:25,157 --> 00:32:26,761
그러나 정규화의 전체 아이디어

526
00:32:26,761 --> 00:32:29,329
당신이 당신의 모델에 대해하는 일이라면,

527
00:32:29,329 --> 00:32:32,557
그런 종류의 모델은 모델의 복잡성을 어떻게 든 처벌합니다.

528
00:32:32,557 --> 00:32:37,061
명시 적으로 훈련 데이터에 맞추려고하지 않고

529
00:32:37,061 --> 00:32:38,306
문제?

530
00:32:38,306 --> 00:32:41,889
[희미하게 말하는 학생]

531
00:32:44,858 --> 00:32:46,104
그래, 문제는,

532
00:32:46,104 --> 00:32:48,604
L2 정규화는 복잡성을 어떻게 측정합니까?

533
00:32:48,604 --> 00:32:50,186
모델의?

534
00:32:50,186 --> 00:32:52,035
고맙게도 우리는 바로 여기에 그 예가 있습니다.

535
00:32:52,035 --> 00:32:54,202
어쩌면 우리는 걸을 수 있습니다.

536
00:32:55,437 --> 00:32:57,779
그래서 여기서 우리는 아마도
훈련 예를 가질 것입니다, x,

537
00:32:57,779 --> 00:33:00,058
우리가 고려하고있는 두 가지 W가 있습니다.

538
00:33:00,058 --> 00:33:02,673
그래서 x는이 4 개의 벡터입니다.

539
00:33:02,673 --> 00:33:06,324
우리는이 두 가지 가능성을 고려하고 있습니다.

540
00:33:06,324 --> 00:33:07,157
W.

541
00:33:07,157 --> 00:33:08,978
하나는 처음에는 하나, 하나는 하나입니다.

542
00:33:08,978 --> 00:33:10,367
그리고 세 개의 0,

543
00:33:10,367 --> 00:33:12,530
다른 하나는이 0.25의 퍼짐을 가로 지르게됩니다

544
00:33:12,530 --> 00:33:14,191
네 가지 항목.

545
00:33:14,191 --> 00:33:15,898
그리고 지금, 선형 분류를 할 때,

546
00:33:15,898 --> 00:33:17,299
우리는 실제로 점 제품을 복용하고 있습니다.

547
00:33:17,299 --> 00:33:19,702
우리 x와 우리 W 사이.

548
00:33:19,702 --> 00:33:22,533
그래서 선형 분류의 관점에서,

549
00:33:22,533 --> 00:33:24,747
이 두 개의 W는 동일합니다.

550
00:33:24,747 --> 00:33:26,319
그들은 같은 결과를주기 때문에

551
00:33:26,319 --> 00:33:28,302
x로 도트를 찍을 때.

552
00:33:28,302 --> 00:33:29,635
그러나 이제 문제는,

553
00:33:29,635 --> 00:33:31,300
이 두 가지 예를 보면,

554
00:33:31,300 --> 00:33:34,383
어느 것이 L2 회귀를 선호합니까?

555
00:33:36,052 --> 00:33:39,300
그래, L2 회귀는 W2보다 더 좋아질거야.

556
00:33:39,300 --> 00:33:41,030
왜냐하면 그것은 더 작은 규범을 가지고 있기 때문입니다.

557
00:33:41,030 --> 00:33:44,854
그래서 대답은 L2 회귀

558
00:33:44,854 --> 00:33:47,017
분류기의 복잡성을 측정합니다.

559
00:33:47,017 --> 00:33:49,440
이 비교적 거친 방식으로,

560
00:33:49,440 --> 00:33:51,644
그 아이디어는,

561
00:33:51,644 --> 00:33:54,557
선형 분류의 W를 기억하십시오.

562
00:33:54,557 --> 00:33:56,915
얼마나 많이 해석 했는가?

563
00:33:56,915 --> 00:33:59,551
이 벡터의 값 x

564
00:33:59,551 --> 00:34:01,920
이 출력 클래스에 해당합니까?

565
00:34:01,920 --> 00:34:04,202
그래서 L2 정규화가 말하고 있습니다.

566
00:34:04,202 --> 00:34:06,245
그 영향력을 퍼뜨리는 것이 더 좋아

567
00:34:06,245 --> 00:34:09,080
x의 모든 다른 값에 걸쳐.

568
00:34:09,080 --> 00:34:11,038
어쩌면 이것이 더 강력 할 수도 있습니다.

569
00:34:11,039 --> 00:34:14,585
당신이 변화하는 xs를 생각해 내면,

570
00:34:14,585 --> 00:34:16,687
그러면 우리의 결정이 확산됩니다.

571
00:34:16,687 --> 00:34:18,359
전체 x 벡터에 의존하며,

572
00:34:18,359 --> 00:34:20,205
특정 요소에만 의존하기보다

573
00:34:20,205 --> 00:34:22,059
x 벡터의.

574
00:34:22,060 --> 00:34:23,946
그런데, L1 정규화

575
00:34:23,946 --> 00:34:26,839
이 반대 해석이있다.

576
00:34:26,839 --> 00:34:29,357
그래서 우리가 L1 정규화를 사용한다면,

577
00:34:29,357 --> 00:34:32,946
그러면 실제로 W2보다 W1을 선호 할 것입니다.

578
00:34:32,946 --> 00:34:35,595
L1 정규화가이 다른 개념을 가지고 있기 때문에

579
00:34:35,595 --> 00:34:39,762
복잡성에 대해서는 모델이 덜 복잡하고,

580
00:34:41,569 --> 00:34:44,867
어쩌면 우리는 0의 수로 모델의
복잡성을 측정 할 것입니다.

581
00:34:44,867 --> 00:34:46,080
상기 웨이트 벡터에서,

582
00:34:46,080 --> 00:34:49,332
그래서 우리는 어떻게 복잡성을 측정 할 것인가에 대한 질문

583
00:34:49,332 --> 00:34:51,917
L2는 어떻게 복잡성을 측정합니까?

584
00:34:51,917 --> 00:34:54,196
그들은 일종의 문제에 의존합니다.

585
00:34:54,196 --> 00:34:57,126
그리고 특정 설정에 대해 생각해야합니다.

586
00:34:57,126 --> 00:34:58,921
특정 모델 및 데이터의 경우,

587
00:34:58,921 --> 00:35:01,754
그 복잡성을 어떻게 측정해야한다고 생각하니?

588
00:35:01,754 --> 00:35:02,837
이 일에?

589
00:35:03,921 --> 00:35:04,788
문제?

590
00:35:04,788 --> 00:35:07,040
- [Student] 그러면 L1이 W1을 선호하는 이유는 무엇입니까?

591
00:35:07,040 --> 00:35:09,129
그들은 같은 사람과 합쳐지지 않습니까?

592
00:35:09,129 --> 00:35:10,385
- 네, 맞습니다.

593
00:35:10,385 --> 00:35:12,330
따라서이 경우 L1은 실제로 동일합니다.

594
00:35:12,330 --> 00:35:13,830
이 둘 사이.

595
00:35:15,193 --> 00:35:18,018
그러나 이것과 비슷한 예를 만들 수 있습니다.

596
00:35:18,018 --> 00:35:21,546
여기서 W1은 L1 정규화에 의해 선호 될 것이다.

597
00:35:21,546 --> 00:35:24,643
나는 L1 뒤의 일반적인 직관력을 추측한다.

598
00:35:24,643 --> 00:35:26,908
일반적으로 스파 스 솔루션을 선호한다는 것입니다.

599
00:35:26,908 --> 00:35:30,558
W의 모든 항목을 0으로 만듭니다.

600
00:35:30,558 --> 00:35:31,954
커플의 경우를 제외하고는

601
00:35:31,954 --> 00:35:35,016
여기서 0에서 벗어나는 것이 허용됩니다.

602
00:35:35,016 --> 00:35:37,945
L1의 복잡성을 측정하는 방법

603
00:35:37,945 --> 00:35:40,191
어쩌면 0이 아닌 항목의 수,

604
00:35:40,191 --> 00:35:43,677
그리고 나서 L2에 대해서, 그것은 W를 퍼뜨리는 것들

605
00:35:43,677 --> 00:35:45,682
모든 값에 걸쳐 덜 복잡한 있습니다.

606
00:35:45,682 --> 00:35:48,920
따라서 귀하의 데이터에 따라 다르며
귀하의 문제에 달려 있습니다.

607
00:35:48,920 --> 00:35:51,593
오 그런데, 만일 당신이 하드 코어 베이지안이라면,

608
00:35:51,593 --> 00:35:54,584
다음 L2 정규화를 사용 하여이 좋은 해석이 있습니다

609
00:35:54,584 --> 00:35:57,206
이전의 Gaussian에 의한 MAP 추론

610
00:35:57,206 --> 00:35:58,897
매개 변수 벡터에.

611
00:35:58,897 --> 00:36:00,430
나는 그것에 관한 숙제 문제가 있다고 생각한다.

612
00:36:00,430 --> 00:36:02,496
229 년에 우리는 그것에 대해 이야기하지 않을 것입니다.

613
00:36:02,496 --> 00:36:05,343
나머지 분기 동안.

614
00:36:05,343 --> 00:36:08,105
그것은 나의 길고 깊은 잠수의 종류 다.

615
00:36:08,105 --> 00:36:11,400
다중 클래스 SVM 손실로.

616
00:36:11,400 --> 00:36:12,783
문제?

617
00:36:12,783 --> 00:36:14,399
- [Student] 그래, 아직도 혼란 스럽네.

618
00:36:14,399 --> 00:36:17,646
내가해야 할 일들에 대해

619
00:36:17,646 --> 00:36:20,979
선형 대 다항식 일 때,

620
00:36:24,590 --> 00:36:27,673
이 손실 함수의 사용

621
00:36:29,007 --> 00:36:32,111
당신이하고있는 사실을 바꾸지 않을 것입니다.

622
00:36:32,111 --> 00:36:33,982
당신은 선형 분류자를보고 있습니다, 맞습니까?

623
00:36:33,982 --> 00:36:36,905
- 그래, 문제는 그게,

624
00:36:36,905 --> 00:36:37,825
정규화 추가

625
00:36:37,825 --> 00:36:39,798
가설 수업을 변경하지 않을 것입니다.

626
00:36:39,798 --> 00:36:44,036
이것은 선형 분류 자로부터 우리를
멀어지게 만들지 않을 것입니다.

627
00:36:44,036 --> 00:36:45,857
아이디어는 아마도이 예제입니다.

628
00:36:45,857 --> 00:36:47,491
이 다항식 회귀 분석

629
00:36:47,491 --> 00:36:50,038
확실히 선형 회귀가 아닙니다.

630
00:36:50,038 --> 00:36:52,174
그것은 선형 회귀로 볼 수 있습니다.

631
00:36:52,174 --> 00:36:56,826
입력의 다항식 확장 위에,

632
00:36:56,826 --> 00:36:59,712
그리고이 경우에,이 회귀는

633
00:36:59,712 --> 00:37:03,232
당신은 많은 다항식을 사용할 수 없다는 것을

634
00:37:03,232 --> 00:37:05,802
아마도 계수가 있어야합니다.

635
00:37:05,802 --> 00:37:07,385
맞아, 네가 상상할 수 있듯이,

636
00:37:07,385 --> 00:37:09,290
당신이 다항식 회귀를 할 때,

637
00:37:09,290 --> 00:37:11,762
당신은 x의 f로서 다항식을 쓸 수 있습니다.

638
00:37:11,762 --> 00:37:16,187
같음 A 제로 더하기 A 하나 x 더하기 두 x 제곱

639
00:37:16,187 --> 00:37:17,963
플러스 3 x 무엇이든,

640
00:37:17,963 --> 00:37:20,343
이 경우 매개 변수, W,

641
00:37:20,343 --> 00:37:23,093
이것들은 As 일 것입니다. 그 경우,

642
00:37:24,211 --> 00:37:26,154
W에게 벌칙을 주면 그것을 강요 할 수있다.

643
00:37:26,154 --> 00:37:28,190
낮은 차수의 다항식쪽으로.

644
00:37:28,190 --> 00:37:30,139
다항식 회귀의 경우를 제외하고,

645
00:37:30,139 --> 00:37:31,491
당신은 실제로 매개 변수화하고 싶지 않다.

646
00:37:31,491 --> 00:37:33,526
As의 측면에서 보면 다른 매개 변수가 있습니다.

647
00:37:33,526 --> 00:37:34,725
당신이 사용하고 싶은,

648
00:37:34,725 --> 00:37:36,364
그러나 그것은 일반적인 생각입니다,

649
00:37:36,364 --> 00:37:38,871
당신이 모델의 매개 변수를 처벌하는 것과 같습니다.

650
00:37:38,871 --> 00:37:41,868
더 간단한 가설을 향해 그것을 강요하는 것

651
00:37:41,868 --> 00:37:44,285
당신의 가설 수업 내에서.

652
00:37:45,229 --> 00:37:46,956
그리고 아마 우리는 이것을 오프라인으로 가져갈 수 있습니다.

653
00:37:46,956 --> 00:37:50,340
그래도 조금 혼란 스럽다면.

654
00:37:50,340 --> 00:37:54,589
그래서 우리는이 다중 클래스 SVM 손실을 보았습니다.

655
00:37:54,589 --> 00:37:56,349
그리고 옆의 쪽지로서 단지 길 옆에서,

656
00:37:56,349 --> 00:38:00,516
이것은 하나의 확장 또는 SVM 손실의 일반화입니다.

657
00:38:01,924 --> 00:38:03,097
여러 클래스에,

658
00:38:03,097 --> 00:38:04,347
실제로 몇 가지 다른 공식이 있습니다.

659
00:38:04,347 --> 00:38:06,596
문학에서 주위를 볼 수있는,

660
00:38:06,596 --> 00:38:09,848
하지만 내 직감이란 모두가 일하는
경향이 있다는 것을 의미합니다.

661
00:38:09,848 --> 00:38:11,755
실제로 마찬가지로,

662
00:38:11,755 --> 00:38:13,813
적어도 깊은 학습의 맥락에서.

663
00:38:13,813 --> 00:38:16,449
그래서 우리는이 특별한 공식을 고수 할 것입니다.

664
00:38:16,449 --> 00:38:19,949
이 클래스의 다중 클래스 SVM 손실

665
00:38:21,061 --> 00:38:23,145
물론 다양한 손실 함수가 있습니다.

666
00:38:23,145 --> 00:38:25,158
당신은 상상할 수도 있습니다.

667
00:38:25,158 --> 00:38:27,270
그리고 또 다른 인기있는 선택,

668
00:38:27,270 --> 00:38:30,603
다중 클래스 SVM 손실 외에도,

669
00:38:31,761 --> 00:38:33,758
깊은 학습에서 또 다른 인기있는 선택

670
00:38:33,758 --> 00:38:36,269
이 다항식 로지스틱 회귀 분석은,

671
00:38:36,269 --> 00:38:37,769
또는 소프트 맥스 손실.

672
00:38:39,405 --> 00:38:41,298
그리고 이것은 실제로 좀 더 일반적 일 것입니다.

673
00:38:41,298 --> 00:38:43,222
깊은 학습의 맥락에서,

674
00:38:43,222 --> 00:38:48,127
그러나 나는 무엇인가의 이유로이
순간을 발표하기로 결정했다.

675
00:38:48,127 --> 00:38:51,742
멀티 클래스 SVM 손실이라는 맥락에서 기억하십시오.

676
00:38:51,742 --> 00:38:53,651
우리는 실제로 해석이 없었습니다.

677
00:38:53,651 --> 00:38:55,096
그 점수는.

678
00:38:55,096 --> 00:38:56,900
우리가 어떤 분류를 할 때,

679
00:38:56,900 --> 00:39:00,524
우리의 모델 F는이 10 개의 숫자를 뱉어냅니다.

680
00:39:00,524 --> 00:39:02,670
수업에 대한 우리 점수 인

681
00:39:02,670 --> 00:39:04,681
다중 클래스 SVM의 경우,

682
00:39:04,681 --> 00:39:07,787
우리는 실제로 그 점수에 많은 해석을하지 않았습니다.

683
00:39:07,787 --> 00:39:09,726
우리는 단지 우리가 진정한 점수를 원한다고 말했다.

684
00:39:09,726 --> 00:39:11,112
올바른 클래스의 점수

685
00:39:11,112 --> 00:39:13,497
잘못된 클래스보다 커야합니다.

686
00:39:13,497 --> 00:39:17,712
그 이상으로 우리는 그 점수가 무엇을
의미하는지 실제로 말하지 않습니다.

687
00:39:17,712 --> 00:39:21,712
그러나 이제는 다항 로지스틱 회귀 분석에서

688
00:39:23,258 --> 00:39:25,625
우리는 실제로 이러한 점수를 부여 할 것입니다.

689
00:39:25,625 --> 00:39:27,668
몇 가지 추가적인 의미가 있습니다.

690
00:39:27,668 --> 00:39:29,715
그리고 특히 우리는 그 점수를 사용할 것입니다.

691
00:39:29,715 --> 00:39:32,264
확률 분포 계산

692
00:39:32,264 --> 00:39:33,907
우리 수업 이상.

693
00:39:33,907 --> 00:39:37,324
그래서 우리는 소위 softmax 함수를 사용합니다.

694
00:39:37,324 --> 00:39:39,775
우리가 우리의 모든 점수를받는 곳에,

695
00:39:39,775 --> 00:39:43,192
우리는 그것들이 이제는
긍정적이게되도록 그들을 배가시킵니다.

696
00:39:43,192 --> 00:39:46,540
그런 다음 이들 지수의 합으로 다시 정규화합니다.

697
00:39:46,540 --> 00:39:49,140
그래서 지금 우리가 점수를 보낸 후

698
00:39:49,140 --> 00:39:50,636
이 softmax 기능을 통해,

699
00:39:50,636 --> 00:39:53,053
이제 우리는이 확률 분포로 끝나고,

700
00:39:53,053 --> 00:39:55,792
지금 우리는 우리 수업보다 확률이 높습니다.

701
00:39:55,792 --> 00:39:58,193
각각의 확률은 0과 1 사이이며,

702
00:39:58,193 --> 00:40:01,370
모든 클래스에 걸친 확률의 합

703
00:40:01,370 --> 00:40:02,287
하나에 합계.

704
00:40:03,954 --> 00:40:07,169
그리고 이제 해석은 우리가 원하는 것입니다.

705
00:40:07,169 --> 00:40:10,113
이 계산 된 확률 분포가 있습니다.

706
00:40:10,113 --> 00:40:12,020
그것은 우리의 점수에 의해 암시 된,

707
00:40:12,020 --> 00:40:14,650
우리는 이것을 목표와 비교하기를 원합니다.

708
00:40:14,650 --> 00:40:17,138
또는 진정한 확률 분포.

709
00:40:17,138 --> 00:40:19,166
우리가 그 고양이가 고양이라는 것을 알면,

710
00:40:19,166 --> 00:40:22,083
목표 확률 분포

711
00:40:22,083 --> 00:40:24,735
고양이에 모든 확률 질량을 넣을 것이고,

712
00:40:24,735 --> 00:40:26,904
그래서 우리는 고양이가 1과 같을 확률을 가질 것입니다.

713
00:40:26,904 --> 00:40:29,754
다른 모든 클래스에 대해서는 확률이 0입니다.

714
00:40:29,754 --> 00:40:31,612
이제 우리가하고 싶은 일은 격려입니다.

715
00:40:31,612 --> 00:40:33,528
우리의 계산 된 확률 분포

716
00:40:33,528 --> 00:40:35,574
이 softmax 함수에서 나온 것입니다.

717
00:40:35,574 --> 00:40:38,376
이 목표 확률 분포와 일치시킨다.

718
00:40:38,376 --> 00:40:40,671
올바른 클래스의 모든 질량을가집니다.

719
00:40:40,671 --> 00:40:42,175
그리고 우리가 이렇게하는 방법,

720
00:40:42,175 --> 00:40:45,181
내 말은, 당신은 여러 방면에서이
방정식을 할 수 있다는 것입니다.

721
00:40:45,181 --> 00:40:46,795
당신은 KL 발산으로 이것을 할 수 있습니다.

722
00:40:46,795 --> 00:40:48,283
표적 사이

723
00:40:48,283 --> 00:40:51,102
상기 계산 된 확률 분포,

724
00:40:51,102 --> 00:40:53,221
최대 우도 추정치로이를 수행 할 수 있습니다.

725
00:40:53,221 --> 00:40:54,466
그러나 하루가 끝날 때,

726
00:40:54,466 --> 00:40:56,474
우리가 정말로 원하는 것은 확률

727
00:40:56,474 --> 00:41:00,839
진정한 계급은 높고 하나에 가깝습니다.

728
00:41:00,839 --> 00:41:04,015
그래서 우리의 손실은 이제 음의 로그가 될 것입니다.

729
00:41:04,015 --> 00:41:06,389
진실한 계급의 확율의.

730
00:41:06,389 --> 00:41:08,151
이것은 우리가 이것을 넣기 때문에 혼란 스럽습니다.

731
00:41:08,151 --> 00:41:09,707
여러 다른 것들을 통해,

732
00:41:09,707 --> 00:41:11,745
그러나 우리는 확률을 원했다는 것을 기억하십시오.

733
00:41:11,745 --> 00:41:13,524
하나에 가까워지면,

734
00:41:13,524 --> 00:41:17,071
그래서 이제 로그는 단조로운 함수입니다.

735
00:41:17,071 --> 00:41:18,414
수학적으로 밝혀졌습니다.

736
00:41:18,414 --> 00:41:20,840
로그를 최대화하는 것이 더 쉽다.

737
00:41:20,840 --> 00:41:23,277
원시 확률을 최대화하는 것보다,

738
00:41:23,277 --> 00:41:25,604
그래서 우리는 일지를 고수합니다.

739
00:41:25,604 --> 00:41:26,984
그리고 이제 로그는 단조롭지 만,

740
00:41:26,984 --> 00:41:30,244
그래서 우리가 정확한 클래스의 로그 P를 최대화한다면,

741
00:41:30,244 --> 00:41:32,599
그건 우리가 그걸 원한다는 뜻이고,

742
00:41:32,599 --> 00:41:36,024
그러나 손실 함수는 좋지 않음을 측정한다.

743
00:41:36,024 --> 00:41:37,454
그래서 우리는 마이너스 1을 넣을 필요가있다.

744
00:41:37,454 --> 00:41:40,051
그것이 올바른 방향으로 나아갈 수 있도록.

745
00:41:40,051 --> 00:41:42,314
SVM에 대한 우리의 손실 함수

746
00:41:42,314 --> 00:41:44,648
확률의 마이너스 로그가 될 것입니다.

747
00:41:44,648 --> 00:41:46,148
진정한 계급의

748
00:41:48,909 --> 00:41:51,322
네, 여기 그것이 요약입니다.

749
00:41:51,322 --> 00:41:53,782
우리가 점수를 받고, 우리가 softmax를 돌며,

750
00:41:53,782 --> 00:41:56,075
이제 우리의 손실은 확률의 로그를 뺀 것이다.

751
00:41:56,075 --> 00:41:57,575
진정한 계급의

752
00:42:01,697 --> 00:42:03,743
좋아, 그럼이게 어떻게 생겼는지 봐.

753
00:42:03,743 --> 00:42:05,043
구체적인 예를 들면,

754
00:42:05,043 --> 00:42:07,749
그 다음 우리는 우리 아주 좋아하는 아름다운 고양이

755
00:42:07,749 --> 00:42:10,634
세 가지 예를 통해 우리는이 세 가지 점수를 얻었습니다.

756
00:42:10,634 --> 00:42:14,486
우리 선형 분류기에서 나오는

757
00:42:14,486 --> 00:42:16,296
이 점수는 정확하게 그들이했던 방식입니다.

758
00:42:16,296 --> 00:42:18,712
SVM 손실의 맥락에서

759
00:42:18,712 --> 00:42:20,826
하지만 이제는이 점수를받는 것보다

760
00:42:20,826 --> 00:42:22,817
그들을 우리의 손실 함수에 직접적으로 넣는 것,

761
00:42:22,817 --> 00:42:25,422
우리는 그것들을 모두 가져 와서 그들을 압도 할 것입니다.

762
00:42:25,422 --> 00:42:26,990
그래서 그들은 모두 긍정적입니다,

763
00:42:26,990 --> 00:42:29,095
그리고 우리는 그들을 정상화시켜 확실하게 할 것입니다.

764
00:42:29,095 --> 00:42:31,025
그들 모두가 하나가된다.

765
00:42:31,025 --> 00:42:33,788
그리고 이제 우리의 손실은 마이너스 로그가 될 것입니다.

766
00:42:33,788 --> 00:42:35,788
진실한 학급 점수의.

767
00:42:36,643 --> 00:42:38,893
그래서 그것은 softmax 손실입니다,

768
00:42:40,156 --> 00:42:43,823
다항 로지스틱 회귀 (multinomial
logistic regression)라고도합니다.

769
00:42:45,496 --> 00:42:47,253
이제 몇 가지 질문을했습니다.

770
00:42:47,253 --> 00:42:50,750
다중 클래스 SVM 손실에 대한
직감을 얻으려고 시도 할 때,

771
00:42:50,750 --> 00:42:53,778
동일한 질문에 대해 생각하는 것이 유용합니다.

772
00:42:53,778 --> 00:42:57,360
softmax 손실과 대비됩니다.

773
00:42:57,360 --> 00:42:58,614
그럼 질문은,

774
00:42:58,614 --> 00:43:02,697
Softmax 손실의 최소 및 최대 값은 얼마입니까?

775
00:43:04,984 --> 00:43:06,785
좋아, 아마 그렇게 확신 할 수는 없어.

776
00:43:06,785 --> 00:43:08,303
너무 많은 로그와 합계가 있습니다.

777
00:43:08,303 --> 00:43:09,720
여기 들어가.

778
00:43:11,298 --> 00:43:13,759
그래서 대답은 분 손실이 0이라는 것입니다.

779
00:43:13,759 --> 00:43:15,430
최대 손실은 무한대입니다.

780
00:43:15,430 --> 00:43:18,263
그리고 당신이 이것을 볼 수있는 방법,

781
00:43:19,422 --> 00:43:21,165
우리가 원하는 확률 분포

782
00:43:21,165 --> 00:43:24,467
올바른 클래스에 하나, 잘못된 클래스에 0,

783
00:43:24,467 --> 00:43:25,842
우리가하는 방식은,

784
00:43:25,842 --> 00:43:27,199
그래서 그 경우라면,

785
00:43:27,199 --> 00:43:31,366
로그 안에있는이 것은 결국 하나가 될 것이고,

786
00:43:33,662 --> 00:43:36,750
실제 클래스의 로그 확률이기 때문에,

787
00:43:36,750 --> 00:43:40,917
하나의 로그가 0 일 때, 하나의
로그를 뺀 것이 여전히 0입니다.

788
00:43:41,893 --> 00:43:44,001
그래서 우리가 그 일을 완전히 올바르게한다면,

789
00:43:44,001 --> 00:43:46,515
우리의 손실은 0이 될 것입니다.

790
00:43:46,515 --> 00:43:50,249
그러나 그런데, 그 일을 완전히 옳게하기 위해서,

791
00:43:50,249 --> 00:43:53,582
우리 점수는 어떻게 생겼을까요?

792
00:43:55,963 --> 00:43:57,252
불평, 불평.

793
00:43:57,252 --> 00:44:00,135
그래서 점수는 실제로 상당히 극단적으로되어야 할 것입니다.

794
00:44:00,135 --> 00:44:01,572
무한으로 향한 것처럼.

795
00:44:01,572 --> 00:44:04,384
그래서 우리는 실제로이 지수를 가지고 있기 때문에,

796
00:44:04,384 --> 00:44:06,098
이 정규화, 유일한 방법

797
00:44:06,098 --> 00:44:09,029
우리는 실제로 하나의 확률 분포를 얻을 수있다.

798
00:44:09,029 --> 00:44:11,970
그리고 제로는 실제로 무한 점수를 넣고 있습니다.

799
00:44:11,970 --> 00:44:16,006
올바른 클래스의 경우 및 무한대 점수를 뺀 경우

800
00:44:16,006 --> 00:44:17,509
잘못된 모든 클래스에 대해.

801
00:44:17,509 --> 00:44:20,652
그리고 컴퓨터는 무한 성으로 잘하지 못합니다.

802
00:44:20,652 --> 00:44:22,239
그래서 실제로, 당신은 결코 손실이 없어 질 것입니다.

803
00:44:22,239 --> 00:44:24,108
유한 정밀도를 가진이 물건에.

804
00:44:24,108 --> 00:44:25,755
그러나 당신은 여전히이 해석을 가지고 있습니다.

805
00:44:25,755 --> 00:44:29,223
여기서 0은 이론상의 최소 손실입니다.

806
00:44:29,223 --> 00:44:31,607
그리고 최대 손실은 제한되지 않습니다.

807
00:44:31,607 --> 00:44:35,180
우리가 확률 질량이 0 일 때

808
00:44:35,180 --> 00:44:39,483
올바른 클래스에서 마이너스 로그를 얻습니다.

809
00:44:39,483 --> 00:44:42,643
0의 로그는 마이너스 무한대이며,

810
00:44:42,643 --> 00:44:46,283
그래서 0의 마이너스 로그는 플러스 무한대가됩니다.

811
00:44:46,283 --> 00:44:47,334
그래서 그것은 정말로 나쁘다.

812
00:44:47,334 --> 00:44:49,071
그러나 다시, 당신은 결코 여기에 결코 도착하지 않을 것입니다.

813
00:44:49,071 --> 00:44:53,748
이 확률을 실제로 얻을 수있는 유일한 방법이기 때문에

814
00:44:53,748 --> 00:44:58,563
0이 되려면 올바른 클래스 점수에 대한 e가 0이고,

815
00:44:58,563 --> 00:45:00,093
그 올바른 클래스 점수가

816
00:45:00,093 --> 00:45:01,630
음의 무한대입니다.

817
00:45:01,630 --> 00:45:04,034
다시 한번 말하지만, 당신은 실제로 이러한
최소한의 것을 얻지 못할 것입니다.

818
00:45:04,034 --> 00:45:07,117
유한 정밀도의 최대 값.

819
00:45:08,863 --> 00:45:11,032
그럼 우리가이 디버깅을했는지 기억해.

820
00:45:11,032 --> 00:45:14,340
다중 클래스 SVM의 컨텍스트에서 온 전성
체크 (sanity check) 질문,

821
00:45:14,340 --> 00:45:16,401
우리는 softmax에 대해서도 같은 질문을 할 수 있습니다.

822
00:45:16,401 --> 00:45:19,138
모든 S가 작고 약 0이면,

823
00:45:19,138 --> 00:45:21,287
그러면 여기서 손실이 무엇입니까?

824
00:45:21,287 --> 00:45:22,292
그래, 대답?

825
00:45:22,292 --> 00:45:24,363
- [학생] C에서 1을 제외한 로그.

826
00:45:24,363 --> 00:45:27,045
- C에서 1의 로그를 뺀거야?

827
00:45:27,045 --> 00:45:28,795
내 생각 엔 그래,

828
00:45:30,026 --> 00:45:33,352
그래서 C를 넘어서는 하나의 로그를 뺀 것입니다.

829
00:45:33,352 --> 00:45:34,693
로그가 그 일을 뒤집을 수 있기 때문에

830
00:45:34,693 --> 00:45:36,526
그러면 C의 로그 일뿐입니다.

831
00:45:36,526 --> 00:45:38,079
네, 그래서 C의 로그 일뿐입니다.

832
00:45:38,079 --> 00:45:39,909
그리고 다시, 이것은 훌륭한 디버깅 일입니다.

833
00:45:39,909 --> 00:45:41,911
이 softmax 손실로 모델을 훈련하는 경우,

834
00:45:41,911 --> 00:45:43,977
첫 번째 반복을 확인해야합니다.

835
00:45:43,977 --> 00:45:47,894
로그 C가 아니면 뭔가 잘못되었습니다.

836
00:45:50,051 --> 00:45:53,257
그래서 우리는이 두 손실 함수를
비교하고 대조 할 수 있습니다.

837
00:45:53,257 --> 00:45:54,600
약간.

838
00:45:54,600 --> 00:45:56,111
선형 분류의 관점에서,

839
00:45:56,111 --> 00:45:57,532
이 설정은 동일하게 보입니다.

840
00:45:57,532 --> 00:45:59,246
곱해진 W 행렬이 있습니다.

841
00:45:59,246 --> 00:46:02,072
이 유령의 유령을 산출하기위한 우리의 의견에 반하여,

842
00:46:02,072 --> 00:46:04,046
이제 두 손실 함수의 차이점

843
00:46:04,046 --> 00:46:06,434
우리가 점수를 해석하는 방법은

844
00:46:06,434 --> 00:46:09,327
나중에 불량을 정량적으로 측정 할 수 있습니다.

845
00:46:09,327 --> 00:46:11,562
그래서 SVM을 위해 우리는
들어가서 마진을 살펴볼 것입니다.

846
00:46:11,562 --> 00:46:14,987
올바른 수업의 점수 사이에

847
00:46:14,987 --> 00:46:17,138
잘못된 수업의 점수,

848
00:46:17,138 --> 00:46:20,256
이 소프트 맥스 또는 크로스 엔트로피 손실에 대해서는,

849
00:46:20,256 --> 00:46:22,703
우리는 가야하고 확률 분포를 계산할 것입니다.

850
00:46:22,703 --> 00:46:24,980
그런 다음 마이너스 로그 확률을 살펴보십시오.

851
00:46:24,980 --> 00:46:26,663
올바른 클래스의.

852
00:46:26,663 --> 00:46:28,996
그래서 때때로 당신이 보면,

853
00:46:30,198 --> 00:46:33,216
관점에서, 나는 그 점을 건너 뛸 것이다.

854
00:46:33,216 --> 00:46:34,917
[웃음]

855
00:46:34,917 --> 00:46:36,358
흥미로운 또 다른 질문입니다.

856
00:46:36,358 --> 00:46:40,854
이 두 손실 함수를 대조하면 생각할 때,

857
00:46:40,854 --> 00:46:44,241
이 예제 포인트가 있다고 가정 해 보겠습니다.

858
00:46:44,241 --> 00:46:46,058
당신이 그것의 점수를 바꾸면,

859
00:46:46,058 --> 00:46:49,975
우리는이 점에 대해 세 가지 점수가 있다고 가정합니다.

860
00:46:52,859 --> 00:46:54,042
바닥에있는 부분을 무시하십시오.

861
00:46:54,042 --> 00:46:56,558
그러나이 예제로 돌아 가면

862
00:46:56,558 --> 00:46:59,814
여기서 다중 - 클래스 SVM 손실에서,

863
00:46:59,814 --> 00:47:04,144
우리가 차를 가지고 있었을 때,
차 점수가 훨씬 좋았습니다.

864
00:47:04,144 --> 00:47:06,293
모든 잘못된 수업보다

865
00:47:06,293 --> 00:47:08,546
그 차 이미지의 점수를 흔들어 쓴다.

866
00:47:08,546 --> 00:47:11,246
다중 클래스 SVM 손실을 전혀 변경하지 않았습니다.

867
00:47:11,246 --> 00:47:13,141
유일한 이유는 SVM 손실

868
00:47:13,141 --> 00:47:15,282
그 정확한 점수를 얻는 것에 관심이있었습니다.

869
00:47:15,282 --> 00:47:18,359
잘못된 점수보다 큰 여백보다 커야합니다.

870
00:47:18,359 --> 00:47:20,392
하지만 이제 softmax 손실은 실제로 상당히 다릅니다.

871
00:47:20,392 --> 00:47:21,726
이 점에서.

872
00:47:21,726 --> 00:47:24,174
softmax 손실은 실제로 항상 운전하기를 원합니다.

873
00:47:24,174 --> 00:47:26,438
그 확률 덩어리가 하나가 될 때까지.

874
00:47:26,438 --> 00:47:29,771
따라서 점수가 매우 높더라도

875
00:47:31,143 --> 00:47:32,606
올바른 수업에, 그리고 매우 낮은 점수

876
00:47:32,606 --> 00:47:34,298
모든 잘못된 수업에

877
00:47:34,298 --> 00:47:36,852
softmax 당신이 더 많은
확률 질량을 쌓기를 원할 것입니다

878
00:47:36,852 --> 00:47:40,044
올바른 수업을 듣고 점수를 계속 누르십시오.

879
00:47:40,044 --> 00:47:42,350
무한대쪽으로 올라가는 올바른 클래스의

880
00:47:42,350 --> 00:47:44,152
잘못된 수업의 점수

881
00:47:44,152 --> 00:47:46,138
음의 무한대쪽으로 내려 갔다.

882
00:47:46,138 --> 00:47:47,435
그래서 그것은 흥미로운 차이입니다.

883
00:47:47,435 --> 00:47:49,968
실제로이 두 손실 함수 사이.

884
00:47:49,968 --> 00:47:53,530
그 SVM, 막대 위에이 데이터 포인트를 가져올거야.

885
00:47:53,530 --> 00:47:55,920
정확하게 분류하고 그냥 포기하면

886
00:47:55,920 --> 00:47:57,739
그 데이터 포인트에 대해서는 더 이상 신경 쓰지 않습니다.

887
00:47:57,739 --> 00:48:00,296
softmax는 항상 지속적으로 개선하려고 노력하지만

888
00:48:00,296 --> 00:48:01,968
모든 단일 데이터 요소가 더 좋아지고 나아질 것입니다.

889
00:48:01,968 --> 00:48:03,838
그리고 더 좋고 더 낫다.

890
00:48:03,838 --> 00:48:05,405
그래서 그것은 흥미로운 차이입니다.

891
00:48:05,405 --> 00:48:07,378
이 두 기능 사이.

892
00:48:07,378 --> 00:48:09,974
실제로, 나는 그것이 큰 차이를 만들어
내지 않는 경향이 있다고 생각한다.

893
00:48:09,974 --> 00:48:12,240
당신이 선택하는 것은 그들이 수행하는 경향이 있습니다.

894
00:48:12,240 --> 00:48:14,040
꽤 유사하게,

895
00:48:14,040 --> 00:48:15,966
적어도 많은 깊은 학습 응용 프로그램.

896
00:48:15,966 --> 00:48:19,137
그러나 이러한 차이를 유지하는 것이 매우 유용합니다.

897
00:48:19,137 --> 00:48:19,970
마음에.

898
00:48:23,054 --> 00:48:26,176
네, 여기에서 우리가 어디로 왔는지 다시 정리하려면,

899
00:48:26,176 --> 00:48:29,585
xs와 ys의 데이터 세트를 가지고 있다는 것입니다.

900
00:48:29,585 --> 00:48:33,018
우리는 우리의 선형 분류기를 사용하여
몇 가지 점수 함수를 얻습니다.

901
00:48:33,018 --> 00:48:36,595
우리의 점수 S를 계산하기 위해서, 우리의 입력에서 x,

902
00:48:36,595 --> 00:48:38,311
그리고 나서 우리는 손실 함수를 사용할 것입니다,

903
00:48:38,311 --> 00:48:41,134
어쩌면 softmax 또는 SVM
또는 일부 다른 손실 함수

904
00:48:41,134 --> 00:48:45,997
양적으로 얼마나 나쁜지를 예측하는 것이었다.

905
00:48:45,997 --> 00:48:48,954
이 땅에 비하면 진정한 목표, y.

906
00:48:48,954 --> 00:48:52,410
그리고 나서 우리는 종종이 손실 함수를 증가시킬 것입니다.

907
00:48:52,410 --> 00:48:53,849
정규화 용어로,

908
00:48:53,849 --> 00:48:56,174
훈련 자료를 맞추는 사이에

909
00:48:56,174 --> 00:48:59,059
더 단순한 모델을 선호합니다.

910
00:48:59,059 --> 00:49:01,490
그래서 이것은 꽤 일반적인 개요입니다.

911
00:49:01,490 --> 00:49:03,966
감독 학습이라고 불리는 많은 것을

912
00:49:03,966 --> 00:49:07,065
우리가 앞으로 나아갈 때 깊은 학습에서 볼 수있는 것은,

913
00:49:07,065 --> 00:49:10,888
일반적으로 당신은 어떤 함수
f를 지정하기를 원할 것입니다.

914
00:49:10,888 --> 00:49:12,664
구조가 매우 복잡 할 수 있습니다.

915
00:49:12,664 --> 00:49:14,489
결정하는 몇 가지 손실 함수를 지정하십시오.

916
00:49:14,489 --> 00:49:18,028
당신의 알고리즘이 얼마나 잘하고 있는지,

917
00:49:18,028 --> 00:49:19,645
파라미터의 임의의 값이 주어지면,

918
00:49:19,645 --> 00:49:21,053
일부 정규화 용어

919
00:49:21,053 --> 00:49:24,260
모델 복잡성을 처벌하는 방법

920
00:49:24,260 --> 00:49:26,141
그런 다음이 것들을 하나로 결합합니다.

921
00:49:26,141 --> 00:49:27,624
W를 찾으려고 노력해.

922
00:49:27,624 --> 00:49:30,866
이 최종 손실 함수를 최소화합니다.

923
00:49:30,866 --> 00:49:32,120
그러나 그때 질문은,

924
00:49:32,120 --> 00:49:33,636
우리가 실제로 그 일을 어떻게 수행할까요?

925
00:49:33,636 --> 00:49:37,132
손실을 최소화하는이 W를 실제로 어떻게 찾을 수 있습니까?

926
00:49:37,132 --> 00:49:40,461
그리고 그것은 우리를 최적화 주제로 이끌고 있습니다.

927
00:49:40,461 --> 00:49:43,033
그래서 우리가 최적화를 할 때,

928
00:49:43,033 --> 00:49:45,495
나는 보통 걷는 관점에서 생각한다.

929
00:49:45,495 --> 00:49:47,482
큰 계곡 주위에.

930
00:49:47,482 --> 00:49:51,951
그래서이 큰 계곡을 걷고 있다는 생각이 들었습니다.

931
00:49:51,951 --> 00:49:54,183
다른 산과 계곡과 시내와 함께

932
00:49:54,183 --> 00:49:56,903
물건들, 그리고이 풍경의 모든 지점

933
00:49:56,903 --> 00:50:00,729
파라미터 W의 일부 설정에 대응한다.

934
00:50:00,729 --> 00:50:03,054
그리고이 계곡을 걸어 다니는이 작은 녀석입니다.

935
00:50:03,054 --> 00:50:04,517
그리고 당신은 찾으려고 노력하고 있습니다.

936
00:50:04,517 --> 00:50:06,216
이들 각 점의 높이,

937
00:50:06,216 --> 00:50:10,728
미안하지만, W의 설정에 의해 초래 된 손실과 같습니다.

938
00:50:10,728 --> 00:50:12,879
그리고이 작은 남자로서의 당신 직업

939
00:50:12,879 --> 00:50:14,181
이 풍경을 돌아 다니며,

940
00:50:14,181 --> 00:50:18,000
당신은 어떻게 든이 계곡의 바닥을 찾을 필요가 있습니다.

941
00:50:18,000 --> 00:50:20,517
그리고 이것은 일반적으로 어려운 문제입니다.

942
00:50:20,517 --> 00:50:22,851
너는 아마도 내가 정말로 영리하다고 생각할지도 모른다.

943
00:50:22,851 --> 00:50:25,223
분석 속성에 대해 정말 열심히 생각할 수 있습니다.

944
00:50:25,223 --> 00:50:27,379
내 손실 기능, 내 모든 정규화,

945
00:50:27,379 --> 00:50:30,246
어쩌면 나는 미니 마이저를 적어 두거나,

946
00:50:30,246 --> 00:50:33,089
그리고 그것은 마술처럼 순간 이동에 해당합니다

947
00:50:33,089 --> 00:50:35,509
이 골짜기의 바닥까지.

948
00:50:35,509 --> 00:50:38,740
그러나 실제로, 일단 당신의 예측 함수, f,

949
00:50:38,740 --> 00:50:40,725
당신의 손실 함수와 당신의 정규식,

950
00:50:40,725 --> 00:50:42,442
일단 이러한 것들이 크고 복잡 해지면

951
00:50:42,442 --> 00:50:44,609
신경 네트워크를 사용하여,

952
00:50:46,190 --> 00:50:48,055
적어 두려는 데별로 희망이 없습니다.

953
00:50:48,055 --> 00:50:49,698
노골적인 분석 솔루션

954
00:50:49,698 --> 00:50:52,017
그것은 당신을 미니 마에 직접 데려갑니다.

955
00:50:52,017 --> 00:50:53,271
그래서 실제로

956
00:50:53,271 --> 00:50:55,485
우리는 다양한 유형의 반복적 인
방법을 사용하는 경향이 있습니다

957
00:50:55,485 --> 00:50:57,390
우리는 몇 가지 해결책으로 시작한다.

958
00:50:57,390 --> 00:51:00,524
시간이 지남에 따라 서서히 개선하십시오.

959
00:51:00,524 --> 00:51:03,357
그래서 아주 첫 번째, 어리석은 일

960
00:51:04,527 --> 00:51:06,985
당신이 상상할 수있는 것은 무작위 검색입니다.

961
00:51:06,985 --> 00:51:09,024
그것은 단지 W의 무리를 취할 것입니다,

962
00:51:09,024 --> 00:51:12,180
무작위로 샘플링하여 손실 함수에 던져 넣습니다.

963
00:51:12,180 --> 00:51:14,963
그들이 얼마나 잘하는지보십시오.

964
00:51:14,963 --> 00:51:17,416
그래서 스포일러 경고, 이것은 정말 나쁜 알고리즘입니다,

965
00:51:17,416 --> 00:51:18,828
당신은 아마 이것을 사용하면 안됩니다.

966
00:51:18,828 --> 00:51:23,323
그러나 적어도 그것은 당신이 시도하는
것을 상상할 수도있는 한 가지입니다.

967
00:51:23,323 --> 00:51:25,180
그리고 우리는 실제로 이것을 할 수 있습니다.

968
00:51:25,180 --> 00:51:27,856
선형 분류기를 실제로 훈련 할 수 있습니다.

969
00:51:27,856 --> 00:51:30,813
무작위 검색을 통해 CIFAR-10

970
00:51:30,813 --> 00:51:34,152
그리고 이것을 위해 10 개의 클래스가 있습니다.

971
00:51:34,152 --> 00:51:35,997
그래서 무작위 확률은 10 %입니다.

972
00:51:35,997 --> 00:51:39,768
우리가 몇 가지 무작위 시도를했다면,

973
00:51:39,768 --> 00:51:42,212
우리는 단지 투명한 바보 같은 운을 통하여 단지 발견했다.

974
00:51:42,212 --> 00:51:45,645
어쩌면 15 %의 정확도를 가진 W의 일부 설정.

975
00:51:45,645 --> 00:51:48,019
그래서 무작위보다는 낫다.

976
00:51:48,019 --> 00:51:50,238
그러나 미술 수준은 아마 95 %

977
00:51:50,238 --> 00:51:53,831
그래서 여기에 약간의 격차가 있습니다.

978
00:51:53,831 --> 00:51:56,748
다시 한번 말하지만 실제로 이것을 사용하지 마십시오.

979
00:51:56,748 --> 00:51:58,138
하지만 당신은 이것이 뭔가 있다고 상상할 수 있습니다.

980
00:51:58,138 --> 00:52:00,677
당신은 잠재적으로 할 수 있습니다.

981
00:52:00,677 --> 00:52:02,467
따라서 실제로는 더 나은 전략 일 것입니다.

982
00:52:02,467 --> 00:52:04,664
실제로 일부 로컬 지오메트리를 사용하고 있습니다.

983
00:52:04,664 --> 00:52:06,168
이 풍경.

984
00:52:06,168 --> 00:52:07,614
그래서 당신이 걷고있는이 작은 녀석이라면

985
00:52:07,614 --> 00:52:09,910
이 풍경 주변,

986
00:52:09,910 --> 00:52:12,178
어쩌면 당신은 직접 경로를 볼 수 없습니다.

987
00:52:12,178 --> 00:52:13,802
골짜기의 바닥에 이르기까지,

988
00:52:13,802 --> 00:52:16,031
그러나 당신이 할 수있는 것은 당신의 발로 느끼는 것입니다.

989
00:52:16,031 --> 00:52:19,531
로컬 지오메트리가 무엇인지 파악하고,

990
00:52:20,697 --> 00:52:21,927
내가 여기 서 있으면,

991
00:52:21,927 --> 00:52:24,145
어떤 방법으로 내리막 길을 조금 걸릴까요?

992
00:52:24,145 --> 00:52:25,595
그래서 당신은 당신의 발로 느낄 수 있습니다.

993
00:52:25,595 --> 00:52:28,136
지상의 기울기가 어디인지 느껴보십시오.

994
00:52:28,136 --> 00:52:30,861
이 방향으로 조금 나를 쓰러 뜨 렸어?

995
00:52:30,861 --> 00:52:32,649
그리고 그 방향으로 나아갈 수 있습니다.

996
00:52:32,649 --> 00:52:34,037
그리고 너는 조금 내려갈 것이다.

997
00:52:34,037 --> 00:52:36,260
당신의 발로 다시 어떤 느낌이 내려 졌는지 알아 내려고,

998
00:52:36,260 --> 00:52:37,704
그런 다음 반복해서 반복하십시오.

999
00:52:37,704 --> 00:52:39,529
당신이 바닥에 끝나기를 희망합니다.

1000
00:52:39,529 --> 00:52:41,526
결국 계곡의.

1001
00:52:41,526 --> 00:52:45,296
이것은 또한 상대적으로 간단한 알고리즘처럼 보입니다.

1002
00:52:45,296 --> 00:52:47,236
하지만 실제로 이것은 정말 잘 작동하는 경향이 있습니다.

1003
00:52:47,236 --> 00:52:50,195
실제로 모든 세부 사항을 올바르게 얻는다면.

1004
00:52:50,195 --> 00:52:52,209
이것이 일반적으로 우리가 따라야 할 전략입니다.

1005
00:52:52,209 --> 00:52:53,963
이 거대한 신경 네트워크를 훈련 할 때

1006
00:52:53,963 --> 00:52:57,028
선형 분류 자 및 다른 것들.

1007
00:52:57,028 --> 00:52:58,769
그래서, 그것은 물결 모양의 작은 손이었습니다.

1008
00:52:58,769 --> 00:52:59,952
그래서 사면은 무엇입니까?

1009
00:52:59,952 --> 00:53:02,337
미적분 클래스를 기억한다면,

1010
00:53:02,337 --> 00:53:03,842
적어도 하나의 차원에서,

1011
00:53:03,842 --> 00:53:07,673
기울기는이 함수의 미분 값입니다.

1012
00:53:07,673 --> 00:53:09,971
그래서 우리가 1 차원 함수 f를 가지면,

1013
00:53:09,971 --> 00:53:12,969
스칼라 x를 취한 다음 높이를 출력합니다.

1014
00:53:12,969 --> 00:53:16,460
곡선의 일부를 구하면 기울기를 계산할 수 있습니다.

1015
00:53:16,460 --> 00:53:19,717
또는 파생 상품을 언제든지 상상할 수 있습니다.

1016
00:53:19,717 --> 00:53:23,467
우리가 작은 걸음을 내딛으면 어떤 방향 으로든,

1017
00:53:26,298 --> 00:53:28,057
작은 걸음 걸음, 그리고 그 차이를 비교해 보라.

1018
00:53:28,057 --> 00:53:29,798
해당 단계의 함수 값에서

1019
00:53:29,798 --> 00:53:31,679
단계 크기를 0으로 드래그하고,

1020
00:53:31,679 --> 00:53:33,237
그 함수의 기울기를 우리에게 줄 것이다.

1021
00:53:33,237 --> 00:53:34,895
그 시점에서.

1022
00:53:34,895 --> 00:53:36,094
그리고 이것은 아주 자연스럽게 일반화됩니다.

1023
00:53:36,094 --> 00:53:38,333
다중 변수 기능도 제공합니다.

1024
00:53:38,333 --> 00:53:41,377
그래서 실제로 x는 스칼라가 아닙니다.

1025
00:53:41,377 --> 00:53:42,612
그러나 전체 벡터,

1026
00:53:42,612 --> 00:53:46,445
기억하기 때문에, x는 전체 벡터 일 수 있습니다.

1027
00:53:47,532 --> 00:53:49,063
그래서 우리는이 개념을 일반화 할 필요가있다.

1028
00:53:49,063 --> 00:53:51,941
다중 변수에 이르기까지.

1029
00:53:51,941 --> 00:53:54,658
그리고 우리가 파생 상품을 사용하는 일반화

1030
00:53:54,658 --> 00:53:57,896
다중 변수 설정에서 그라데이션,

1031
00:53:57,896 --> 00:54:01,168
그래디언트는 부분 미분의 벡터입니다.

1032
00:54:01,168 --> 00:54:04,409
그래디언트는 x와 동일한 모양을 갖습니다.

1033
00:54:04,409 --> 00:54:07,473
그라디언트의 각 요소는 우리에게 알려줍니다.

1034
00:54:07,473 --> 00:54:09,595
함수 f의 기울기는 얼마인가?

1035
00:54:09,595 --> 00:54:12,391
우리가 그 좌표 방향으로 움직인다면.

1036
00:54:12,391 --> 00:54:13,899
그리고 그라디언트가 밝혀졌습니다.

1037
00:54:13,899 --> 00:54:16,816
이 아주 좋은 재산을 가지려면,

1038
00:54:18,373 --> 00:54:21,036
그래디언트는 이제 부분 미분의 벡터입니다.

1039
00:54:21,036 --> 00:54:23,228
그러나 그것은 가장 큰 증가의 방향을 가리킨다.

1040
00:54:23,228 --> 00:54:25,657
따라서,

1041
00:54:25,657 --> 00:54:27,545
음의 그래디언트 방향을 보면,

1042
00:54:27,545 --> 00:54:29,770
그것은 당신에게 가장 큰 감소의 방향을 제시합니다.

1043
00:54:29,770 --> 00:54:31,612
함수의.

1044
00:54:31,612 --> 00:54:34,210
그리고 더 일반적으로, 당신이 알고 싶다면,

1045
00:54:34,210 --> 00:54:37,418
어떤 방향 으로든 내 풍경의 경사는 무엇입니까?

1046
00:54:37,418 --> 00:54:39,357
그러면 그라데이션의 내적과 같습니다.

1047
00:54:39,357 --> 00:54:42,693
그 방향을 설명하는 단위 벡터와 함께.

1048
00:54:42,693 --> 00:54:44,549
그래서이 그라데이션은 매우 중요합니다.

1049
00:54:44,549 --> 00:54:47,719
이 선형 일차 근사를 제공하기 때문에

1050
00:54:47,719 --> 00:54:50,198
귀하의 현재 지점에서 귀하의 기능에.

1051
00:54:50,198 --> 00:54:51,382
그래서 실제로, 많은 깊은 학습

1052
00:54:51,382 --> 00:54:53,661
함수의 그라데이션 계산하기

1053
00:54:53,661 --> 00:54:56,290
그런 그라디언트를 사용하여 반복적으로 업데이트

1054
00:54:56,290 --> 00:54:58,123
매개 변수 벡터.

1055
00:54:59,204 --> 00:55:02,195
당신이 상상할 수있는 순진한 방법 하나

1056
00:55:02,195 --> 00:55:04,812
실제로 컴퓨터에서이 그라디언트를 평가하면

1057
00:55:04,812 --> 00:55:06,955
유한 차이의 방법을 사용하고 있습니다.

1058
00:55:06,955 --> 00:55:09,488
그래디언트의 한계 정의로 돌아갑니다.

1059
00:55:09,488 --> 00:55:12,621
그래서 왼쪽에서 우리는 우리의 현재 W

1060
00:55:12,621 --> 00:55:14,012
이 매개 변수 벡터입니다.

1061
00:55:14,012 --> 00:55:17,432
어쩌면 우리에게 어쩌면 1.25의
현재 손실을 줄 수 있습니다.

1062
00:55:17,432 --> 00:55:21,127
우리의 목표는 기울기를 계산하는 것입니다, dW,

1063
00:55:21,127 --> 00:55:23,922
이것은 W와 같은 모양의 벡터가 될 것입니다.

1064
00:55:23,922 --> 00:55:26,021
그 그라디언트의 각 슬롯은 우리에게 알려줄 것입니다.

1065
00:55:26,021 --> 00:55:29,050
얼마나 많은 손실이 발생할 것인가?

1066
00:55:29,050 --> 00:55:31,734
그 좌표 방향으로 미미한 양.

1067
00:55:31,734 --> 00:55:33,241
그래서 당신이 상상할 수있는 한가지

1068
00:55:33,241 --> 00:55:35,741
이 유한 차분을 계산하는 것입니다.

1069
00:55:35,741 --> 00:55:38,336
우리가 W를 가지고 있다면, 우리는

1070
00:55:38,336 --> 00:55:41,902
W의 첫 번째 원소, 작은 값, h,

1071
00:55:41,902 --> 00:55:44,210
손실 함수를 사용하여 손실을 다시 계산하십시오.

1072
00:55:44,210 --> 00:55:45,842
우리의 분류 자와 모든 것.

1073
00:55:45,842 --> 00:55:48,112
그리고 아마도이 상황에서 우리가 조금 움직이면

1074
00:55:48,112 --> 00:55:50,792
첫 번째 차원에서 손실이 줄어들 것입니다.

1075
00:55:50,792 --> 00:55:53,792
1.2534에서 1.25322로 조금 증가했습니다.

1076
00:55:55,945 --> 00:55:57,574
그런 다음이 한계 정의를 사용할 수 있습니다.

1077
00:55:57,574 --> 00:56:01,363
이 유한 차분 근사값을 생각해 내야한다.

1078
00:56:01,363 --> 00:56:04,378
이 첫 번째 차원의 그래디언트로

1079
00:56:04,378 --> 00:56:06,194
이제이 절차를 반복하는 것을 상상할 수 있습니다.

1080
00:56:06,194 --> 00:56:07,795
두 번째 차원에서,

1081
00:56:07,795 --> 00:56:09,371
이제 우리는 첫 번째 차원을 취하고,

1082
00:56:09,371 --> 00:56:11,029
원래 값으로 다시 설정하십시오.

1083
00:56:11,029 --> 00:56:13,728
작은 단계로 두 번째 방향을 증가시킵니다.

1084
00:56:13,728 --> 00:56:15,294
그리고 다시, 우리는 손실을 계산합니다.

1085
00:56:15,294 --> 00:56:17,593
이 유한 차분 근사법을 사용하십시오.

1086
00:56:17,593 --> 00:56:19,444
그라데이션에 대한 근사치를 계산하는

1087
00:56:19,444 --> 00:56:21,165
두 번째 슬롯에서.

1088
00:56:21,165 --> 00:56:22,843
그리고 이제 세 번째로 이것을 반복하십시오.

1089
00:56:22,843 --> 00:56:25,135
그리고 계속해서.

1090
00:56:25,135 --> 00:56:27,683
그래서 이것은 실제로 끔찍한 생각입니다.

1091
00:56:27,683 --> 00:56:29,150
왜냐하면 그것은 매우 느리기 때문입니다.

1092
00:56:29,150 --> 00:56:31,980
그래서 여러분은이 함수 f를 계산하면,

1093
00:56:31,980 --> 00:56:34,230
그것이 크다면 실제로는 매우 느릴지도 모른다.

1094
00:56:34,230 --> 00:56:35,920
길쌈 신경 네트워크.

1095
00:56:35,920 --> 00:56:38,369
그리고이 매개 변수 벡터 W는,

1096
00:56:38,369 --> 00:56:40,693
아마 여기에있는 것처럼 10 개의 항목이 없을 것입니다.

1097
00:56:40,693 --> 00:56:42,266
수천만 명이 될 수도있다.

1098
00:56:42,266 --> 00:56:44,344
또는이 수백만 달러 중 일부는 수백만 달러,

1099
00:56:44,344 --> 00:56:46,446
복잡한 심층 학습 모델.

1100
00:56:46,446 --> 00:56:48,482
따라서 실제적으로, 당신은 결코
계산을 원하지 않을 것입니다.

1101
00:56:48,482 --> 00:56:50,381
당신의 유한 차이에 대한 당신의 그라디언트,

1102
00:56:50,381 --> 00:56:52,864
당신이 수억 명을 기다려야하기 때문에

1103
00:56:52,864 --> 00:56:54,749
잠재적으로 기능 평가

1104
00:56:54,749 --> 00:56:56,908
하나의 그래디언트를 얻는다면 그것은 매우 느려질 것입니다.

1105
00:56:56,908 --> 00:56:58,075
슈퍼 나쁜.

1106
00:56:59,351 --> 00:57:02,524
그러나 고맙게도 우리는 그렇게 할 필요가 없습니다.

1107
00:57:02,524 --> 00:57:03,676
바라건대 당신은 미적분 과정을 택했을 것입니다.

1108
00:57:03,676 --> 00:57:05,315
네 인생의 어느 시점에서,

1109
00:57:05,315 --> 00:57:08,146
그래서 당신은이 사람들 덕분에,

1110
00:57:08,146 --> 00:57:11,206
우리는 손실에 대한 표현을 적어 둘 수 있습니다.

1111
00:57:11,206 --> 00:57:13,658
미적분학의 마법 망치를 사용하십시오.

1112
00:57:13,658 --> 00:57:15,584
표현을 적어 두는 것

1113
00:57:15,584 --> 00:57:17,372
이 기울기가 무엇을 위해 있어야합니다.

1114
00:57:17,372 --> 00:57:18,708
그리고 이것은 훨씬 더 효율적 일 것입니다.

1115
00:57:18,708 --> 00:57:20,245
분석적으로 계산하는 것보다

1116
00:57:20,245 --> 00:57:21,658
유한 한 차이를 통해.

1117
00:57:21,658 --> 00:57:22,929
하나, 정확 할거야.

1118
00:57:22,929 --> 00:57:25,433
둘째, 계산이 필요하기 때문에 훨씬 빠릅니다.

1119
00:57:25,433 --> 00:57:27,350
이 단일 표현.

1120
00:57:28,945 --> 00:57:31,405
그래서 이것이 어떻게 생겼는지는 지금입니다.

1121
00:57:31,405 --> 00:57:33,513
우리가 현재의 W의 그림으로 돌아 가면,

1122
00:57:33,513 --> 00:57:36,848
W의 모든 차원을 반복하는 것이 아니라,

1123
00:57:36,848 --> 00:57:38,311
우리가 미리 알아낼거야.

1124
00:57:38,311 --> 00:57:40,653
그래디언트의 분석 식은 무엇입니까?

1125
00:57:40,653 --> 00:57:44,279
그런 다음 그것을 적어서 W에서 직접 이동하십시오.

1126
00:57:44,279 --> 00:57:47,337
하나의 단계에서 dW 또는
그래디언트를 계산할 수 있습니다.

1127
00:57:47,337 --> 00:57:50,875
그리고 그것은 실제로 더 나아질 것입니다.

1128
00:57:50,875 --> 00:57:53,846
요약하자면이 수치 그라디언트

1129
00:57:53,846 --> 00:57:56,738
간단하고 의미있는 것입니다.

1130
00:57:56,738 --> 00:57:58,745
그러나 실제로는 실제로 사용하지 않을 것입니다.

1131
00:57:58,745 --> 00:58:01,794
실제로는 분석 그라디언트를 항상 사용합니다.

1132
00:58:01,794 --> 00:58:03,039
그것을 사용하십시오.

1133
00:58:03,039 --> 00:58:05,301
실제로 이러한 그래디언트 계산을 수행 할 때

1134
00:58:05,301 --> 00:58:06,951
그러나 흥미로운 점은

1135
00:58:06,951 --> 00:58:09,360
이 숫자 그라디언트는 실제로 매우 유용합니다.

1136
00:58:09,360 --> 00:58:10,610
디버깅 도구.

1137
00:58:12,572 --> 00:58:13,841
몇 가지 코드를 작성했다고 가정 해 보겠습니다.

1138
00:58:13,841 --> 00:58:16,169
당신은 손실을 계산하는 코드를 작성했습니다.

1139
00:58:16,169 --> 00:58:17,770
손실의 기울기,

1140
00:58:17,770 --> 00:58:19,562
그렇다면 어떻게 디버깅합니까?

1141
00:58:19,562 --> 00:58:21,909
이 분석식이

1142
00:58:21,909 --> 00:58:24,085
코드에서 파생하고 적어 둔

1143
00:58:24,085 --> 00:58:25,684
실제로 맞습니까?

1144
00:58:25,684 --> 00:58:28,443
따라서 이러한 것들을위한 일반적인 디버깅 전략

1145
00:58:28,443 --> 00:58:31,159
방법으로 숫자 그라디언트를 사용하는 것입니다.

1146
00:58:31,159 --> 00:58:32,823
확실한 단위 테스트의 일종으로

1147
00:58:32,823 --> 00:58:35,141
분석 기울기가 맞는지 확인하십시오.

1148
00:58:35,141 --> 00:58:38,320
다시 말하지만, 이것은 매우 느리고 정확하지 않기 때문에,

1149
00:58:38,320 --> 00:58:41,435
이 숫자 그라디언트 검사를 수행 할 때,

1150
00:58:41,435 --> 00:58:43,739
호출 될 때 매개 변수의 크기를 줄이는 경향이 있습니다.

1151
00:58:43,739 --> 00:58:45,184
문제가 실제로 실행되도록

1152
00:58:45,184 --> 00:58:46,755
합리적인 시간에

1153
00:58:46,755 --> 00:58:49,376
하지만 이것은 유용한 유용한 디버깅 전략이됩니다.

1154
00:58:49,376 --> 00:58:51,721
자신의 그라디언트 계산을 작성할 때.

1155
00:58:51,721 --> 00:58:54,112
그래서 실제로 이것은 실제로 실제로 많이 사용됩니다.

1156
00:58:54,112 --> 00:58:58,610
그리고 당신은 당신의 과제에도 이것을 할 것입니다.

1157
00:58:58,610 --> 00:59:01,834
그래서 일단 그라디언트를 계산하는 방법을 알게되면,

1158
00:59:01,834 --> 00:59:04,547
그러면 우리를이 초간단 단순 알고리즘으로 이끈다.

1159
00:59:04,547 --> 00:59:06,990
그것은 세 줄과 같지만 마음 속에있는 것으로 밝혀졌습니다.

1160
00:59:06,990 --> 00:59:09,480
우리가 이처럼 매우 큰 것을 훈련하는 방법에 대해서,

1161
00:59:09,480 --> 00:59:11,607
가장 복잡한 심층 학습 알고리즘,

1162
00:59:11,607 --> 00:59:13,152
그것은 구배 강하입니다.

1163
00:59:13,152 --> 00:59:16,991
그래디언트 강하가 먼저 W를 초기화합니다.

1164
00:59:16,991 --> 00:59:19,544
어떤 무작위로, 그렇다면 사실,

1165
00:59:19,544 --> 00:59:21,555
우리는 우리의 손실과 그라디언트를 계산할 것입니다.

1166
00:59:21,555 --> 00:59:24,521
우리는 우리의 가중치를 업데이트 할 것입니다.

1167
00:59:24,521 --> 00:59:27,547
그레디언트 방향의 반대 방향에서,

1168
00:59:27,547 --> 00:59:28,722
그라디언트를 기억하십시오.

1169
00:59:28,722 --> 00:59:30,710
가장 큰 증가의 방향을 가리키고 있었다.

1170
00:59:30,710 --> 00:59:32,305
함수의 - 그래디언트 빼기

1171
00:59:32,305 --> 00:59:34,047
가장 큰 감소 방향의 포인트,

1172
00:59:34,047 --> 00:59:36,727
그래서 우리는 방향으로 조금 나아갈 것입니다.

1173
00:59:36,727 --> 00:59:39,262
빼기 그라디언트를 사용하고 영원히 이것을 반복하십시오.

1174
00:59:39,262 --> 00:59:40,774
결국 네트워크가 수렴 할 것입니다.

1175
00:59:40,774 --> 00:59:43,255
희망적으로 당신은 매우 행복 할 것입니다.

1176
00:59:43,255 --> 00:59:45,596
그러나이 단계 크기는 실제로 하이퍼 매개 변수입니다.

1177
00:59:45,596 --> 00:59:48,219
이것은 그라디언트를 계산할 때마다,

1178
00:59:48,219 --> 00:59:50,842
우리는 그 방향으로 얼마나 멀리 나아갈 것입니다.

1179
00:59:50,842 --> 00:59:53,602
그리고이 단계 크기는 학습 속도라고도하며,

1180
00:59:53,602 --> 00:59:55,445
아마 가장 중요한 것 중 하나 일 것입니다.

1181
00:59:55,445 --> 00:59:57,378
설정해야하는 하이퍼 매개 변수

1182
00:59:57,378 --> 01:00:00,033
실제로 이러한 것들을 실제로 훈련 할 때.

1183
01:00:00,033 --> 01:00:02,117
사실 나는이 일을 훈련 할 때 나를 위해,

1184
01:00:02,117 --> 01:00:04,200
이 단계 크기를 알아 내려고

1185
01:00:04,200 --> 01:00:06,340
또는이 학습 률은 첫 번째 하이퍼 파라미터입니다.

1186
01:00:06,340 --> 01:00:07,502
나는 항상 확인한다.

1187
01:00:07,502 --> 01:00:10,949
모형 크기 또는 정규화 강도와 같은 것

1188
01:00:10,949 --> 01:00:12,434
나중에 조금 떠날 때까지,

1189
01:00:12,434 --> 01:00:15,408
학습 속도 또는 단계 크기를 올바르게 얻는 방법

1190
01:00:15,408 --> 01:00:19,361
처음에 설정하려고하는 첫 번째 것입니다.

1191
01:00:19,361 --> 01:00:23,015
그래서 그림처럼 보이는 것입니다.

1192
01:00:23,015 --> 01:00:25,212
여기에 2 차원의 간단한 예제가 있습니다.

1193
01:00:25,212 --> 01:00:28,004
그래서 여기에 우리는 아마도이 사발을 가지고있을 것입니다.

1194
01:00:28,004 --> 01:00:30,051
우리의 손실 함수를 보여주고있다.

1195
01:00:30,051 --> 01:00:33,635
중앙의이 빨간 지역

1196
01:00:33,635 --> 01:00:36,598
우리가 가고 싶은 낮은 손실의이 지역

1197
01:00:36,598 --> 01:00:38,852
에지쪽으로 청색 및 녹색 영역

1198
01:00:38,852 --> 01:00:41,187
우리가 피하고자하는 손실이 더 큽니다.

1199
01:00:41,187 --> 01:00:43,204
이제 우리는 우리의 W를 시작할 것입니다.

1200
01:00:43,204 --> 01:00:44,750
우주의 어떤 임의의 지점에서,

1201
01:00:44,750 --> 01:00:47,536
음의 그래디언트 방향을 계산하고,

1202
01:00:47,536 --> 01:00:49,680
바라건대 우리를 방향으로 안내 할 것입니다.

1203
01:00:49,680 --> 01:00:51,387
결국 미니 마의.

1204
01:00:51,387 --> 01:00:53,171
그리고 우리가 이것을 반복해서 반복한다면,

1205
01:00:53,171 --> 01:00:56,407
우리는 최후에 정확한 미니 마를 얻을 수 있기를 바랍니다.

1206
01:00:56,407 --> 01:01:00,283
실제로 이것이 어떻게 생겼는지는,

1207
01:01:00,283 --> 01:01:03,140
오,이 쥐 문제가 다시 발생했습니다.

1208
01:01:03,140 --> 01:01:04,358
그래서 이것이 실제로 어떻게 생겼는지

1209
01:01:04,358 --> 01:01:09,250
우리가이 일을 반복해서 반복한다면,

1210
01:01:09,250 --> 01:01:12,061
그러면 우리는 어느 시점부터 시작합니다.

1211
01:01:12,061 --> 01:01:15,266
결국에는 매번 작은 그래디언트 단계를 거치면서,

1212
01:01:15,266 --> 01:01:19,221
매개 변수가 중심을 향해 원호 모양으로 표시되고,

1213
01:01:19,221 --> 01:01:20,626
이 미니 마의 영역,

1214
01:01:20,626 --> 01:01:22,236
그리고 그것은 당신이 정말로 원하는 것입니다.

1215
01:01:22,236 --> 01:01:24,241
낮은 손실을 원하기 때문입니다.

1216
01:01:24,241 --> 01:01:26,812
그리고 그런데, 티저의 비트로서,

1217
01:01:26,812 --> 01:01:28,139
우리는 이전 슬라이드에서 보았습니다.

1218
01:01:28,139 --> 01:01:30,905
아주 간단한 그래디언트 디센트의이 예제,

1219
01:01:30,905 --> 01:01:32,705
모든 단계에서 우리는 방향으로 나아가고 있습니다.

1220
01:01:32,705 --> 01:01:33,998
그라디언트의

1221
01:01:33,998 --> 01:01:35,997
그러나 실제로, 다음 강의에서,

1222
01:01:35,997 --> 01:01:38,679
우리는 약간 더 매끈한 단계가 있음을 볼 것입니다,

1223
01:01:38,679 --> 01:01:40,885
그들이이 업데이트 규칙이라고 부르는 것,

1224
01:01:40,885 --> 01:01:43,598
약간 애호가를 취할 수있는 곳

1225
01:01:43,598 --> 01:01:46,037
여러 시간 단계에 걸쳐 그라디언트 통합

1226
01:01:46,037 --> 01:01:48,206
그리고 그런 것들은 조금 더 잘 작동하는 경향이 있습니다.

1227
01:01:48,206 --> 01:01:50,836
실제로는 훨씬 더 일반적으로 사용됩니다.

1228
01:01:50,836 --> 01:01:52,513
이 바닐라 그라데이션 하강보다

1229
01:01:52,513 --> 01:01:54,610
실제로 이러한 것들을 훈련 할 때.

1230
01:01:54,610 --> 01:01:55,877
그리고 약간의 미리보기로서,

1231
01:01:55,877 --> 01:01:59,101
우리는이 약간의 애호가 방법 중 일부를 볼 수 있습니다.

1232
01:01:59,101 --> 01:02:01,054
동일한 문제를 최적화합니다.

1233
01:02:01,054 --> 01:02:04,701
다시 검정은이 같은 그라디언트 계산이 될 것입니다.

1234
01:02:04,701 --> 01:02:07,530
그리고 이것들은 그들이 어떤 색인지 잊어 버렸습니다.

1235
01:02:07,530 --> 01:02:08,871
그러나이 두 개의 다른 곡선

1236
01:02:08,871 --> 01:02:11,580
약간 더 까다로운 업데이트 규칙을 사용하고 있습니다.

1237
01:02:11,580 --> 01:02:13,960
그래디언트 정보를 사용하는 방법을 정확하게 결정하는

1238
01:02:13,960 --> 01:02:15,929
우리의 다음 단계를 만들기 위해.

1239
01:02:15,929 --> 01:02:20,451
따라서 이들 중 하나는 기세가있는 그라데이션 강하입니다.

1240
01:02:20,451 --> 01:02:22,835
다른 하나는이 Adam Optimizer입니다.

1241
01:02:22,835 --> 01:02:24,273
자세한 내용은

1242
01:02:24,273 --> 01:02:25,389
나중에 코스에서.

1243
01:02:25,389 --> 01:02:28,300
그러나 아이디어는 우리가이 아주 기본적인
알고리즘을 가지고 있다는 것입니다.

1244
01:02:28,300 --> 01:02:29,795
그라디언트 디센트라는,

1245
01:02:29,795 --> 01:02:31,544
매 단계마다 그라디언트를 사용합니다.

1246
01:02:31,544 --> 01:02:33,424
다음으로 나아갈 위치를 결정하기 위해,

1247
01:02:33,424 --> 01:02:35,874
우리에게 알려주는 다른 업데이트 규칙이 있습니다.

1248
01:02:35,874 --> 01:02:38,559
그 기울기 정보를 얼마나 정확하게 사용하는지.

1249
01:02:38,559 --> 01:02:40,391
그러나 그것은 모두 동일한 기본 알고리즘입니다.

1250
01:02:40,391 --> 01:02:44,058
매 단계마다 내리막 길을 가려고했다.

1251
01:02:50,022 --> 01:02:51,852
하지만 실제로는 조금 더 주름살이 있습니다.

1252
01:02:51,852 --> 01:02:53,212
우리가 얘기해야 할 것.

1253
01:02:53,212 --> 01:02:56,818
그래서 우리가 손실 함수를 정의했음을 기억하십시오.

1254
01:02:56,818 --> 01:02:59,356
우리는 얼마나 나쁜지를 계산하는 손실을 정의했다.

1255
01:02:59,356 --> 01:03:02,237
어떤 단일 훈련 예에서 우리
분류 자 (classifier)

1256
01:03:02,237 --> 01:03:04,536
데이터 세트에 대한 우리의 모든 손실이

1257
01:03:04,536 --> 01:03:06,077
평균 손실이 될거야.

1258
01:03:06,077 --> 01:03:08,314
전체 교육 세트에서

1259
01:03:08,314 --> 01:03:12,572
그러나 실제로이 N은 매우 커질 수 있습니다.

1260
01:03:12,572 --> 01:03:15,274
예를 들어 이미지 넷 데이터 세트를 사용하는 경우,

1261
01:03:15,274 --> 01:03:17,010
우리가 첫 번째 강의에서 이야기 한 내용,

1262
01:03:17,010 --> 01:03:19,199
N은 130 만 명,

1263
01:03:19,199 --> 01:03:21,403
그래서 실제로이 손실을 계산합니다.

1264
01:03:21,403 --> 01:03:23,208
실제로는 매우 비쌀 수있다.

1265
01:03:23,208 --> 01:03:26,081
아마도 수백만 가지 평가 계산이 필요합니다.

1266
01:03:26,081 --> 01:03:28,206
이 함수의

1267
01:03:28,206 --> 01:03:29,821
그래서 그것은 정말로 느릴 수 있습니다.

1268
01:03:29,821 --> 01:03:32,094
실제로 그라디언트는 선형 연산자이므로,

1269
01:03:32,094 --> 01:03:34,109
그라디언트를 실제로 계산하려고 할 때

1270
01:03:34,109 --> 01:03:37,034
이 표현에서 우리는 손실의 기울기

1271
01:03:37,034 --> 01:03:39,504
이제는 손실 그라디언트의 합계입니다.

1272
01:03:39,504 --> 01:03:41,461
각 개별 용어에 대해

1273
01:03:41,461 --> 01:03:43,734
이제 그라디언트를 다시 계산하려면,

1274
01:03:43,734 --> 01:03:45,248
그것은 우리에게 반복을 요구합니다.

1275
01:03:45,248 --> 01:03:46,930
전체 교육 데이터 세트에 대해

1276
01:03:46,930 --> 01:03:48,469
이 모든 N 개의 예제.

1277
01:03:48,469 --> 01:03:50,390
그래서 만약 우리 N이 백만 달러라면,

1278
01:03:50,390 --> 01:03:51,960
이것은 슈퍼 슈퍼 천천히,

1279
01:03:51,960 --> 01:03:54,078
우리는 아주 오랜 시간을 기다려야 할 것입니다.

1280
01:03:54,078 --> 01:03:56,978
우리가 W를 개별적으로 업데이트하기 전에.

1281
01:03:56,978 --> 01:03:58,577
그래서 실제로, 우리는

1282
01:03:58,577 --> 01:04:00,728
확률 적 구배 강하라고 불리는 것,

1283
01:04:00,728 --> 01:04:04,061
손실 및 기울기 계산보다는

1284
01:04:04,061 --> 01:04:05,697
전체 훈련 세트에서,

1285
01:04:05,697 --> 01:04:08,938
대신 매 반복마다 샘플 세트

1286
01:04:08,938 --> 01:04:12,540
minibatch라고하는 훈련 예를들 수 있습니다.

1287
01:04:12,540 --> 01:04:14,213
보통 이것은 관습에 따라 2의 힘입니다.

1288
01:04:14,213 --> 01:04:17,705
32, 64, 128은 일반적인 숫자입니다.

1289
01:04:17,705 --> 01:04:19,887
그런 다음이 작은 미니 바를 사용합니다.

1290
01:04:19,887 --> 01:04:22,483
전체 합계의 추정치를 계산하기 위해,

1291
01:04:22,483 --> 01:04:25,047
그리고 진정한 그라데이션의 추정치.

1292
01:04:25,047 --> 01:04:27,703
그리고 이것은 당신이 이것을 볼
수 있기 때문에 확률 적입니다.

1293
01:04:27,703 --> 01:04:31,845
몬테 카를로의 기대치에 대한 추정치 일 수도 있습니다.

1294
01:04:31,845 --> 01:04:33,345
진정한 가치의

1295
01:04:34,716 --> 01:04:37,318
그래서 이것은 우리의 알고리즘을
약간 더 매끈하게 만듭니다.

1296
01:04:37,318 --> 01:04:38,945
하지만 여전히 4 줄 밖에 없습니다.

1297
01:04:38,945 --> 01:04:43,112
이제 데이터가 무작위로 추출됩니다.

1298
01:04:44,291 --> 01:04:46,682
minibatch에서 손실 및 그라디언트를 평가하고,

1299
01:04:46,682 --> 01:04:48,690
이제 매개 변수를 업데이트하십시오.

1300
01:04:48,690 --> 01:04:51,113
이 손실 추정에 기초하여,

1301
01:04:51,113 --> 01:04:53,661
이 기울기의 추정치.

1302
01:04:53,661 --> 01:04:56,769
그리고 다시 약간 더 매끄러운 업데이트 규칙을 보겠습니다.

1303
01:04:56,769 --> 01:04:59,811
여러 가지 그라디언트를 통합하는 방법을 정확히 설명합니다.

1304
01:04:59,811 --> 01:05:02,780
시간이지 나면서,하지만 이것은 기본적인 훈련 알고리즘입니다

1305
01:05:02,780 --> 01:05:04,948
우리가 거의 모든 심층 신경 네트워크에 사용하는

1306
01:05:04,948 --> 01:05:05,948
실제로.

1307
01:05:06,875 --> 01:05:10,235
그래서 우리는 또 다른 대화 형 웹 데모를 가지고 있습니다.

1308
01:05:10,235 --> 01:05:12,625
선형 분류기로 실제로 놀고,

1309
01:05:12,625 --> 01:05:14,970
확률 적 구배 강하를 통해 이러한 것들을 훈련하고,

1310
01:05:14,970 --> 01:05:17,782
그러나 웹 데모가 얼마나 비참한지를 생각해 보면,

1311
01:05:17,782 --> 01:05:20,122
나는 실제로 링크를 열지 않을 것이다.

1312
01:05:20,122 --> 01:05:23,269
대신, 나는이 비디오를 재생할 것입니다.

1313
01:05:23,269 --> 01:05:25,339
[웃음]

1314
01:05:25,339 --> 01:05:26,594
하지만 이걸 확인해 보시길 바랍니다.

1315
01:05:26,594 --> 01:05:27,749
온라인으로 게임을 즐기고,

1316
01:05:27,749 --> 01:05:29,256
실제로 어떤 직감을 구축하는 데 도움이되기 때문에

1317
01:05:29,256 --> 01:05:31,036
선형 분류기에 대해 배우고 훈련하기

1318
01:05:31,036 --> 01:05:32,735
그라데이션 강하를 통해.

1319
01:05:32,735 --> 01:05:34,919
여기 왼쪽에서 볼 수 있습니다.

1320
01:05:34,919 --> 01:05:37,485
우리가 분류하고있는이 문제가 있습니다.

1321
01:05:37,485 --> 01:05:40,146
3 개의 다른 종류,

1322
01:05:40,146 --> 01:05:42,551
우리는이 녹색, 파란색 및 빨간색 점을 가지고 있습니다.

1323
01:05:42,551 --> 01:05:45,753
그것은이 세 가지 수업에서 얻은 우리의 훈련 견본입니다.

1324
01:05:45,753 --> 01:05:48,351
그리고 이제 우리는 결정 경계를 이끌어 냈습니다.

1325
01:05:48,351 --> 01:05:52,068
컬러 백그라운드 영역 인 이러한 클래스의 경우,

1326
01:05:52,068 --> 01:05:54,270
이러한 방향뿐만 아니라,

1327
01:05:54,270 --> 01:05:57,487
학급 점수 인상 방향을 알려줍니다.

1328
01:05:57,487 --> 01:05:59,007
이 세 가지 클래스 각각에 대해.

1329
01:05:59,007 --> 01:06:03,108
그리고 지금 보시다시피, 실제로 가서 놀면

1330
01:06:03,108 --> 01:06:04,814
이 물건을 온라인으로

1331
01:06:04,814 --> 01:06:07,579
당신은 우리가 가서 W를 조정할 수
있다는 것을 볼 수 있습니다.

1332
01:06:07,579 --> 01:06:09,442
Ws 값 변경

1333
01:06:09,442 --> 01:06:12,176
이러한 결정 경계가 회전하게됩니다.

1334
01:06:12,176 --> 01:06:14,189
편향을 변경하면 결정 경계

1335
01:06:14,189 --> 01:06:17,476
회전하지 않고 대신 좌우로 움직입니다.

1336
01:06:17,476 --> 01:06:18,662
또는 위 아래로.

1337
01:06:18,662 --> 01:06:19,989
그러면 실제로 단계를 밟을 수 있습니다.

1338
01:06:19,989 --> 01:06:21,855
이 손실을 업데이트하려고 시도하는

1339
01:06:21,855 --> 01:06:23,984
또는이 슬라이더로 스텝 크기를 변경할 수 있습니다.

1340
01:06:23,984 --> 01:06:26,009
이 버튼을 눌러 실제 작동시킬 수 있습니다.

1341
01:06:26,009 --> 01:06:27,296
이제 큰 단계 크기로,

1342
01:06:27,296 --> 01:06:29,076
우리는 현재 그라디언트 디센트를 실행 중입니다.

1343
01:06:29,076 --> 01:06:30,624
이러한 결정의 경계는

1344
01:06:30,624 --> 01:06:32,874
데이터를 맞추려고합니다.

1345
01:06:34,553 --> 01:06:36,432
이제는 괜찮아요.

1346
01:06:36,432 --> 01:06:39,890
실제 손실 함수를 실시간으로 변경할 수 있습니다.

1347
01:06:39,890 --> 01:06:41,845
이 서로 다른 SVM 공식들 사이

1348
01:06:41,845 --> 01:06:43,567
및 다른 softmax.

1349
01:06:43,567 --> 01:06:44,754
그리고 당신은 볼 수 있습니다.

1350
01:06:44,754 --> 01:06:47,695
손실 함수의 이들 상이한 공식들 사이에서,

1351
01:06:47,695 --> 01:06:50,219
그것은 일반적으로 똑같은 일을합니다.

1352
01:06:50,219 --> 01:06:52,340
우리의 의사 결정 영역은 대부분 같은 장소에 있습니다.

1353
01:06:52,340 --> 01:06:54,945
그러나 그들이 어떻게 서로에 관해서는 결국

1354
01:06:54,945 --> 01:06:56,263
정확하게 상충되는 점은 무엇입니까?

1355
01:06:56,263 --> 01:06:59,139
이 다른 것들을 범주화하는 것

1356
01:06:59,139 --> 01:07:00,743
조금 바뀐다.

1357
01:07:00,743 --> 01:07:02,137
그래서 나는 정말로 당신이 온라인에 가기를 권장합니다.

1358
01:07:02,137 --> 01:07:03,877
이 일로 약간의 직감을 얻으려고 노력한다.

1359
01:07:03,877 --> 01:07:05,699
실제로 어떻게 생겼는지

1360
01:07:05,699 --> 01:07:07,428
이러한 선형 분류자를 훈련 시키려고

1361
01:07:07,428 --> 01:07:09,178
그라데이션 강하를 통해.

1362
01:07:12,343 --> 01:07:16,245
이제는 제쳐두고, 저는 다른 생각에
대해 이야기하고 싶습니다.

1363
01:07:16,245 --> 01:07:18,102
그것은 이미지 기능의 것입니다.

1364
01:07:18,102 --> 01:07:20,668
지금까지 선형 분류기에 대해 이야기했습니다.

1365
01:07:20,668 --> 01:07:23,032
그냥 우리의 원시 이미지 픽셀을 복용하는 것입니다

1366
01:07:23,032 --> 01:07:25,184
원시 픽셀 자체를 공급하는 단계

1367
01:07:25,184 --> 01:07:27,434
우리의 선형 분류 자로.

1368
01:07:28,464 --> 01:07:31,075
그러나 마지막 강연에서 우리가 이야기 한 것처럼,

1369
01:07:31,075 --> 01:07:33,343
이것은 아마도 그렇게 대단한 것이 아니며,

1370
01:07:33,343 --> 01:07:36,233
멀티 - 모달과 같은 것들 때문에.

1371
01:07:36,233 --> 01:07:39,368
따라서 실제로는 원본 픽셀 값을 실제로 공급합니다.

1372
01:07:39,368 --> 01:07:42,789
선형 분류기로 변환하는 것이 잘
작동하지 않는 경향이 있습니다.

1373
01:07:42,789 --> 01:07:45,742
그래서 그것은 지배 이전에 실제로 일반적이었습니다.

1374
01:07:45,742 --> 01:07:47,145
깊은 신경 네트워크,

1375
01:07:47,145 --> 01:07:49,592
대신에이 2 단계 접근법을 사용하는 것이 었습니다.

1376
01:07:49,592 --> 01:07:51,105
먼저, 당신은 당신의 이미지를 찍을 것입니다.

1377
01:07:51,105 --> 01:07:53,916
다양한 특징 표현을 계산할 수 있습니다.

1378
01:07:53,916 --> 01:07:55,886
어쩌면 계산중인 이미지의

1379
01:07:55,886 --> 01:07:59,219
외관과 관련된 여러 종류의 양

1380
01:07:59,219 --> 01:08:00,179
이미지의

1381
01:08:00,179 --> 01:08:02,117
이들 서로 다른 특징 벡터들을 연결

1382
01:08:02,117 --> 01:08:05,137
이미지의 일부 기능 표현을 제공하려면,

1383
01:08:05,137 --> 01:08:07,019
이제 이미지의이 특징 표현

1384
01:08:07,019 --> 01:08:08,836
선형 분류기에 공급 될 것이고,

1385
01:08:08,836 --> 01:08:10,683
원시 픽셀 자체를 먹이기보다는

1386
01:08:10,683 --> 01:08:12,902
분류기에 넣습니다.

1387
01:08:12,902 --> 01:08:15,671
그리고 여기서의 동기는,

1388
01:08:15,671 --> 01:08:17,700
그래서 왼쪽에 훈련 데이터 세트가 있다고 상상해보십시오.

1389
01:08:17,701 --> 01:08:20,193
이 빨간 점들과 중간에 빨간 점들

1390
01:08:20,193 --> 01:08:22,244
그리고 그 주변의 푸른 점.

1391
01:08:22,244 --> 01:08:23,693
그리고 이러한 종류의 데이터 세트의 경우,

1392
01:08:23,693 --> 01:08:26,440
선형 결정 경계를 그릴 수있는 방법이 없습니다.

1393
01:08:26,441 --> 01:08:29,157
빨간색 점과 파란색 점을 구분합니다.

1394
01:08:29,157 --> 01:08:32,155
그리고 우리는 마지막 강의에서 이것에
대한 더 많은 예를 보았습니다.

1395
01:08:32,156 --> 01:08:34,459
그러나 영리한 피처 변환을 사용한다면,

1396
01:08:34,459 --> 01:08:36,660
이 경우에는 극좌표로 변환,

1397
01:08:36,660 --> 01:08:39,079
이제 우리가 피쳐 변환을 한 후에,

1398
01:08:39,079 --> 01:08:42,361
이 복잡한 데이터 세트는 실제로

1399
01:08:42,361 --> 01:08:43,677
선형으로 분리 가능한,

1400
01:08:43,677 --> 01:08:45,160
실제로 정확하게 분류 될 수있다.

1401
01:08:45,160 --> 01:08:46,858
선형 분류기에 의해

1402
01:08:46,858 --> 01:08:48,435
그리고 여기있는 모든 트릭은 이제 알아내는 것입니다.

1403
01:08:48,436 --> 01:08:51,034
올바른 피쳐 변환은 무엇입니까?

1404
01:08:51,034 --> 01:08:53,197
그것은 올바른 양의 계산입니다.

1405
01:08:53,197 --> 01:08:55,129
당신이 걱정하는 문제에 대해서.

1406
01:08:55,129 --> 01:08:58,017
따라서 이미지의 경우 픽셀을 변환 할 수 있습니다.

1407
01:08:58,017 --> 01:08:59,751
극좌표로, 이해가 안 돼,

1408
01:08:59,751 --> 01:09:01,505
그러나 실제로 당신은 적어 내려고 노력할 수 있습니다.

1409
01:09:01,505 --> 01:09:03,084
이미지의 특징 표현

1410
01:09:03,085 --> 01:09:04,749
그게 말이 되겠지,

1411
01:09:04,749 --> 01:09:06,458
실제로 너를 도울지도 모른다.

1412
01:09:06,458 --> 01:09:08,385
원시 픽셀을 넣는 것보다 효과적 일 수 있습니다.

1413
01:09:08,385 --> 01:09:10,391
분류기에 넣습니다.

1414
01:09:10,392 --> 01:09:13,157
그래서 이런 종류의 피쳐 표현의 한 예

1415
01:09:13,157 --> 01:09:16,343
그것은 매우 간단합니다. 색상 히스토그램의 아이디어입니다.

1416
01:09:16,343 --> 01:09:18,526
그래서 당신은 아마 각 픽셀을 가져갈 것입니다,

1417
01:09:18,526 --> 01:09:21,188
당신은이 색조 스펙트럼을 취할 것입니다.

1418
01:09:21,188 --> 01:09:23,985
버킷으로 나눈 다음 모든 픽셀에 대해

1419
01:09:23,986 --> 01:09:26,425
당신은 그 색 버킷 중 하나에 그것을 매핑 할 것입니다.

1420
01:09:26,425 --> 01:09:28,535
얼마나 많은 픽셀을 카운트하는지

1421
01:09:28,536 --> 01:09:31,162
이 각각의 버킷으로 떨어지십시오.

1422
01:09:31,162 --> 01:09:34,638
따라서 이미지에 어떤 색상이 있는지
전 세계적으로 알 수 있습니다.

1423
01:09:34,639 --> 01:09:36,278
어쩌면이 개구리의 예가

1424
01:09:36,278 --> 01:09:37,500
이 특징 벡터는 우리에게

1425
01:09:37,500 --> 01:09:39,076
녹색 물건이 많이 있어요.

1426
01:09:39,076 --> 01:09:40,938
어쩌면 자주색이나 붉은 색이 아닌 것도있을 것입니다.

1427
01:09:40,938 --> 01:09:43,043
그리고 이것은 여러분이 볼 수있는 단순한 특징 벡터입니다

1428
01:09:43,043 --> 01:09:44,043
실제로.

1429
01:09:45,108 --> 01:09:47,720
우리가 본 또 다른 공통 특징 벡터

1430
01:09:47,720 --> 01:09:49,430
신경 네트워크의 부상 전에,

1431
01:09:49,431 --> 01:09:50,983
또는 신경 네트워크의 지배 이전에

1432
01:09:50,983 --> 01:09:53,219
지향 그라디언트의 히스토그램이었습니다.

1433
01:09:53,220 --> 01:09:54,952
첫 번째 강의에서 기억하십시오.

1434
01:09:54,952 --> 01:09:57,829
Hubel과 Wiesel은 이러한
지향성 가장자리를 발견했습니다.

1435
01:09:57,829 --> 01:10:00,046
인간 시각 시스템에서 정말로 중요합니다.

1436
01:10:00,046 --> 01:10:02,209
지향성 그라디언트의 히스토그램

1437
01:10:02,209 --> 01:10:04,690
피쳐 표현은 캡쳐를 시도한다.

1438
01:10:04,690 --> 01:10:07,680
똑같은 직감과 현지 오리엔테이션 측정

1439
01:10:07,680 --> 01:10:09,974
이미지 가장자리.

1440
01:10:09,974 --> 01:10:11,280
그래서이 일이 무엇을 할 것인지,

1441
01:10:11,280 --> 01:10:13,286
우리의 이미지를 받아 그것을 나눕니다.

1442
01:10:13,286 --> 01:10:16,354
이 작은 8x8 픽셀 영역으로

1443
01:10:16,354 --> 01:10:19,142
그리고 나서, 각각의 8 × 8 픽셀 영역 내에서,

1444
01:10:19,142 --> 01:10:22,268
지배적 인 에지 방향을 계산합니다.

1445
01:10:22,268 --> 01:10:24,921
각 픽셀의 에지 방향

1446
01:10:24,921 --> 01:10:27,776
여러 버킷에 넣은 다음 각 영역 내에서

1447
01:10:27,776 --> 01:10:31,857
이들 서로 다른 에지 방향에
대해 히스토그램을 계산하십시오.

1448
01:10:31,857 --> 01:10:33,417
이제 전체 기능 벡터

1449
01:10:33,417 --> 01:10:35,797
이러한 서로 다른 버킷 히스토그램이됩니다.

1450
01:10:35,797 --> 01:10:37,382
가장자리 방위의

1451
01:10:37,382 --> 01:10:39,121
8 개 지역마다 8 개 지역에 걸쳐

1452
01:10:39,121 --> 01:10:40,204
이미지에서.

1453
01:10:41,660 --> 01:10:43,450
그래서 이것은 어떤면에서는 이중적인 것입니다.

1454
01:10:43,450 --> 01:10:47,029
이전에 본 색상 히스토그램 분류기로

1455
01:10:47,029 --> 01:10:49,704
그래서 색 막대 그래프는 전 세계적으로 어떤 색

1456
01:10:49,704 --> 01:10:51,082
이미지에 존재한다.

1457
01:10:51,082 --> 01:10:53,751
이것은 전반적으로 어떤 유형의 에지 정보

1458
01:10:53,751 --> 01:10:55,305
이미지에 존재합니다.

1459
01:10:55,305 --> 01:10:57,991
그리고 심지어 이미지의 다른 부분에 국한되어,

1460
01:10:57,991 --> 01:11:01,191
다른 영역에 어떤 유형의 모서리가 있는지.

1461
01:11:01,191 --> 01:11:03,546
어쩌면 왼쪽에있는이 개구리의 경우,

1462
01:11:03,546 --> 01:11:04,938
당신은 그가 잎에 앉아있는 것을 볼 수 있습니다,

1463
01:11:04,938 --> 01:11:07,561
이 나뭇잎들은이 지배적 인 대각선 모서리를 가지며,

1464
01:11:07,561 --> 01:11:10,480
방향 그라디언트의 히스토그램을 시각화하면

1465
01:11:10,480 --> 01:11:12,833
기능을 사용하면이 지역에서

1466
01:11:12,833 --> 01:11:14,667
우리는 대각선 가장자리가 많습니다.

1467
01:11:14,667 --> 01:11:16,227
방향성 그라디언트의 히스토그램

1468
01:11:16,227 --> 01:11:19,340
특징 표현의 캡쳐.

1469
01:11:19,340 --> 01:11:21,509
그래서 이것은 매우 일반적인 특징 표현이었습니다.

1470
01:11:21,509 --> 01:11:23,535
물체 인식에 많이 사용되었습니다.

1471
01:11:23,535 --> 01:11:25,702
사실 너무 오래 전에.

1472
01:11:26,573 --> 01:11:29,789
거기에서 볼 수있는 또 다른 특징 표현

1473
01:11:29,789 --> 01:11:32,810
단어의 가방이 아이디어입니다.

1474
01:11:32,810 --> 01:11:34,202
그래서 이것은 영감을 얻고 있습니다.

1475
01:11:34,202 --> 01:11:36,355
자연 언어 처리에서.

1476
01:11:36,355 --> 01:11:38,220
그래서 단락이 있다면,

1477
01:11:38,220 --> 01:11:40,799
그런 다음 단락을 나타낼 수있는 방법

1478
01:11:40,799 --> 01:11:43,398
특징 벡터에 의해 발생 횟수를 세고있다.

1479
01:11:43,398 --> 01:11:45,732
그 단락의 다른 단어들.

1480
01:11:45,732 --> 01:11:47,666
그래서 우리는 그 직감을 받아 적용하고 싶습니다.

1481
01:11:47,666 --> 01:11:49,664
어떤 식 으로든 이미지에.

1482
01:11:49,664 --> 01:11:51,708
그러나 문제는 실제로는 단순한 것이 아니라,

1483
01:11:51,708 --> 01:11:54,288
이미지에 대한 단어의 직접적인 유추,

1484
01:11:54,288 --> 01:11:56,632
그래서 우리는 우리 자신의 어휘를 정의 할 필요가있다.

1485
01:11:56,632 --> 01:11:57,965
시각적 단어의

1486
01:11:58,880 --> 01:12:01,106
그래서 우리는이 2 단계 접근법을 취합니다.

1487
01:12:01,106 --> 01:12:04,318
먼저 우리는 많은 이미지를 얻을 것입니다.

1488
01:12:04,318 --> 01:12:06,455
작은 무작위 농작물을 한꺼번에 채취하다.

1489
01:12:06,455 --> 01:12:07,995
그 이미지들로부터

1490
01:12:07,995 --> 01:12:09,723
K와 같은 것을 사용하는 것을 의미합니다.

1491
01:12:09,723 --> 01:12:12,820
이 다른 클러스터 센터를 생각해 내야한다.

1492
01:12:12,820 --> 01:12:15,189
어쩌면 다른 유형을 나타내는 것입니다.

1493
01:12:15,189 --> 01:12:17,139
이미지의 시각적 단어.

1494
01:12:17,139 --> 01:12:19,341
이 예제를 여기 오른쪽에서 보면,

1495
01:12:19,341 --> 01:12:21,160
이것은 클러스터링의 실제 예입니다.

1496
01:12:21,160 --> 01:12:23,335
이미지에서 실제로 다른 이미지 패치,

1497
01:12:23,335 --> 01:12:25,627
이 클러스터링 단계 후에,

1498
01:12:25,627 --> 01:12:28,450
우리의 시각적 인 단어는이 다른 색을 포착합니다.

1499
01:12:28,450 --> 01:12:30,552
빨간색과 파란색과 노란색처럼,

1500
01:12:30,552 --> 01:12:32,553
이러한 다양한 유형의 지향 에지

1501
01:12:32,553 --> 01:12:34,556
다른 방향으로,

1502
01:12:34,556 --> 01:12:36,482
우리가보기 시작한 것이 흥미 롭습니다.

1503
01:12:36,482 --> 01:12:38,909
이러한 지향 에지는 데이터에서 나옵니다.

1504
01:12:38,909 --> 01:12:40,480
데이터 중심 방식으로

1505
01:12:40,480 --> 01:12:43,097
그리고 이제 우리가 이러한 일련의
시각적 단어들을 얻게되면,

1506
01:12:43,097 --> 01:12:44,291
또한 코드북이라고 불리는,

1507
01:12:44,291 --> 01:12:47,249
그러면 우리는 우리의 이미지를 인 코드 할 수 있습니다.

1508
01:12:47,249 --> 01:12:48,862
이러한 시각적 단어들 각각에 대해,

1509
01:12:48,862 --> 01:12:52,468
이 시각적 단어는 이미지에서 얼마나 발생합니까?

1510
01:12:52,468 --> 01:12:54,082
그리고 이제 이것은 우리에게 다시금,

1511
01:12:54,082 --> 01:12:55,463
약간 다른 정보

1512
01:12:55,463 --> 01:12:59,427
이 이미지의 시각적 모양은 무엇입니까?

1513
01:12:59,427 --> 01:13:02,124
실제로 이것은 일종의 특징 표현입니다.

1514
01:13:02,124 --> 01:13:04,638
황비홍 (Fei-Fei)은 대학원생이었을 때 일했고,

1515
01:13:04,638 --> 01:13:07,555
그래서 이것은 당신이 실제로 보았던 무언가입니다.

1516
01:13:07,555 --> 01:13:08,972
그리 오래 전 아니에요.

1517
01:13:10,783 --> 01:13:13,033
그래서 약간의 티저로,

1518
01:13:14,951 --> 01:13:16,837
이 모든 것을 다시 묶어서,

1519
01:13:16,837 --> 01:13:19,743
이 이미지 분류 파이프 라인

1520
01:13:19,743 --> 01:13:20,886
같이 보일지도 모른다.

1521
01:13:20,886 --> 01:13:22,725
어쩌면 5 년에서 10 년 전쯤에

1522
01:13:22,725 --> 01:13:24,421
당신이 당신의 이미지를 찍을 것이고,

1523
01:13:24,421 --> 01:13:26,677
그런 다음 이러한 다양한 특징 표현을 계산할 수 있습니다.

1524
01:13:26,677 --> 01:13:28,809
너의 심상의, 낱말의 부대 같이 것,

1525
01:13:28,809 --> 01:13:31,173
또는 방향 그라디언트의 히스토그램,

1526
01:13:31,173 --> 01:13:33,381
모든 기능을 함께 연결하고,

1527
01:13:33,381 --> 01:13:35,519
이러한 피쳐 추출기로 피드

1528
01:13:35,519 --> 01:13:38,590
일부 선형 분류기로

1529
01:13:38,590 --> 01:13:39,461
나는 조금 단순화하고있다.

1530
01:13:39,461 --> 01:13:42,018
파이프 라인은 그보다 조금 더 복잡했습니다.

1531
01:13:42,018 --> 01:13:44,576
그러나 이것은 일반적인 직감입니다.

1532
01:13:44,576 --> 01:13:48,158
그리고 그 아이디어는 당신이 추출한 후였습니다.

1533
01:13:48,158 --> 01:13:50,196
이러한 기능들,이 피쳐 추출기

1534
01:13:50,196 --> 01:13:52,326
업데이트되지 않는 고정 된 블록이 될 것입니다.

1535
01:13:52,326 --> 01:13:53,563
훈련 도중.

1536
01:13:53,563 --> 01:13:54,396
그리고 훈련 도중,

1537
01:13:54,396 --> 01:13:55,933
선형 분류 자만 업데이트하면됩니다.

1538
01:13:55,933 --> 01:13:57,907
기능 상단에서 작업하는 경우

1539
01:13:57,907 --> 01:13:59,972
그리고 실제로, 저는 일단 우리가 움직이면

1540
01:13:59,972 --> 01:14:01,774
길쌈 신경망 (convolutional neural networks)

1541
01:14:01,774 --> 01:14:03,140
이 깊은 신경 네트워크,

1542
01:14:03,140 --> 01:14:06,486
실제로 다른 것을 보지 못합니다.

1543
01:14:06,486 --> 01:14:08,651
유일한 차이점은 쓰기보다는

1544
01:14:08,651 --> 01:14:10,322
미리 기능,

1545
01:14:10,322 --> 01:14:12,687
우리는 데이터에서 직접 기능을 배우려고합니다.

1546
01:14:12,687 --> 01:14:15,916
그래서 우리는 원시 픽셀을 가져다가 먹일 것입니다.

1547
01:14:15,916 --> 01:14:17,530
이것을 컨볼 루션 네트워크에 연결함으로써,

1548
01:14:17,530 --> 01:14:19,687
이것은 여러 다른 레이어를 통해 컴퓨팅을 끝낼 것입니다.

1549
01:14:19,687 --> 01:14:21,488
어떤 유형의 피쳐 표현

1550
01:14:21,488 --> 01:14:23,459
데이터에 의해 주도되고 실제로 훈련하게됩니다.

1551
01:14:23,459 --> 01:14:26,120
이 전체 네트워크에 대한이 전체 가중치,

1552
01:14:26,120 --> 01:14:27,954
선형 분류기의 가중치가 아닌

1553
01:14:27,954 --> 01:14:28,787
위에.

1554
01:14:30,329 --> 01:14:32,970
그럼, 다음에 우리는이 아이디어에 뛰어들 것입니다.

1555
01:14:32,970 --> 01:14:36,131
좀 더 자세히 살펴보면, 우리는 몇
가지 신경망을 소개 할 것이며,

1556
01:14:36,131 --> -00:00:00,800
backpropagation에
대해서도 이야기를 시작하십시오.

