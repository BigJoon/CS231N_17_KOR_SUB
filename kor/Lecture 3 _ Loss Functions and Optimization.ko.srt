1
00:00:07,755 --> 00:00:11,682
CS231N 3강입니다.

2
00:00:11,682 --> 00:00:14,594
오늘은 손실함수(Loss functions)와
최적화(Optimization)을 배울겁니다.

3
00:00:14,594 --> 00:00:20,129
진도 나가기에 앞서 공지사항을 전달합니다.

4
00:00:21,094 --> 00:00:24,547
먼저, 첫 번째 과제가 생겼습니다.

5
00:00:24,547 --> 00:00:26,889
웹사이트에서 확인하실 수 있습니다.

6
00:00:26,889 --> 00:00:30,086
과제를 좀 늦게 내드렸기 때문에

7
00:00:30,086 --> 00:00:35,264
4월 20일 목요일 오후 11시 59분으로 기한을 변경합니다.

8
00:00:36,374 --> 00:00:42,702
앞으로 2주정도 라고 보시면 됩니다.

9
00:00:42,702 --> 00:00:49,087
그리고 변경된 기한으로 강의계획서를 업데이트하겠습니다.

10
00:00:49,087 --> 00:00:54,617
과제를 완료하면 zip파일로 Canvas에 업로드해야 하며

11
00:00:54,617 --> 00:00:57,779
가능한 빨리 채점을 하도록 하겠습니다.

12
00:00:58,799 --> 00:01:04,879
다음 사항은 여러분이 공지사항을 확인하려면
항상 Piazza를 체크해야 한다는 것입니다.

13
00:01:04,879 --> 00:01:11,432
이번주에 Piazza에 몇가지 예제 프로젝트를 
공고해 놓을 것입니다.

14
00:01:11,432 --> 00:01:17,220
그래서 Stanford 커뮤니티와, 혹은 Stanford와 연계된 
분들로부터 프로젝트 아이디와 관련된 예시를 부탁드렸는데,

15
00:01:17,220 --> 00:01:24,583
그리고 그 분들이 이 수업을 듣는 학생들과 함께 프로젝트를 
진행하고 싶다는 제안을 해주셨습니다.

16
00:01:24,583 --> 00:01:26,986
그러니 Pizza에 공고를 확인해 보시고

17
00:01:26,986 --> 00:01:34,231
하고싶은 프로젝트가 있으면 자유롭게 
프로젝트 멘토와 직접 얘기하면 되겠습니다.

18
00:01:34,231 --> 00:01:37,090
추가적으로 강의 홈피에 근무시간을
게시하였습니다.

19
00:01:37,090 --> 00:01:45,077
구글 캔린더로 게시하였고 학생들이 근무시간에 대해
많이 물어봐서 이번에 게시하였습니다.

20
00:01:45,077 --> 00:01:48,307
마지막 공지는 구글 클라우드 관련된 것입니다.

21
00:01:48,307 --> 00:01:54,331
구글 클라우드에서 우리 강의를 지원하므로

22
00:01:54,331 --> 00:02:00,687
여러분의 과제와 프로젝트를 수행하기 위해
구글 클라우드 크래딧 100$를 지급하였으며

23
00:02:00,687 --> 00:02:05,246
사용하는 방법은 추후 Piazza에 공지하겠습니다.

24
00:02:05,246 --> 00:02:11,977
공지사항에 대한 질문이 없으면 진도를 나가겠습니다.

25
00:02:13,440 --> 00:02:14,273
좋습니다.

26
00:02:15,559 --> 00:02:17,997
저번 2강을 복습해보면

27
00:02:17,997 --> 00:02:20,412
인식에서의 어려운 점을 살펴보았고

28
00:02:20,412 --> 00:02:24,476
데이터 중심의 방식에 초점을 마춰보았습니다.

29
00:02:24,476 --> 00:02:29,160
그리고 이미지 분류에 대해 배웠고 
왜 이미지 분류가 어려운지, 그리고

30
00:02:29,160 --> 00:02:35,812
컴퓨터가 보는것과 사람이 보는 것의 차이가 있다는 것도 배웠습니다.

31
00:02:35,812 --> 00:02:42,124
그리고 분류를 어렵게하는 조명의 변화, 변형에 대해 다뤘고 
왜 이게 그토록 어려운지에 대해 배웠습니다.

32
00:02:42,124 --> 00:02:47,912
인간의 시각체계는 이 일을 엄청 쉽게 하는데도 말이죠

33
00:02:47,912 --> 00:02:50,421
그리고 KNN 분류기도 배웠습니다.

34
00:02:50,421 --> 00:02:55,309
데이터 중심 접근방법 중에 가장 단순한 방법이었죠.

35
00:02:55,309 --> 00:03:00,824
그리고 CIFAR-10도 배웠습니다. 저기 좌상단에 보이는
이미지들이죠.

36
00:03:00,824 --> 00:03:05,787
CIFAR-10은 비행기, 자동차 등 10개의 카테고리가 있습니다.

37
00:03:05,787 --> 00:03:11,202
그리고 어떻게 KNN을 이용해서 학습 데이터를 가지고 
각 카테고리 클래스를 분류하는

38
00:03:11,202 --> 00:03:15,746
결정 경계를 학습시킬 수 있는지도 배웠습니다.

39
00:03:15,746 --> 00:03:25,190
또한 크로스 벨리데이션데 대해서도 배웠고 트레인, 벨리데이션,
테스트셋으로 나눠서 하이퍼파라미터를 찾는 법도 배웠습니다.

40
00:03:25,190 --> 00:03:32,410
그리고 2강 마지막에는 Linear classification에 대해 배웠
습니다. Linear classifier는 뉴럴넷의 기본요소입니다.

41
00:03:32,410 --> 00:03:38,538
Linear classifier는 parametric classifier의
일종입니다. parametric classifier라는 것은

42
00:03:38,538 --> 00:03:45,444
트레이닝 데이터의 정보가 파라미터인  "행렬 W"로 축약된
다는 것을 뜻합니다. 그리고 W가 학습되는 것입니다.

43
00:03:45,444 --> 00:03:48,448
그리고 이 Linear classifier는 엄청 간단합니다.

44
00:03:48,448 --> 00:03:51,810
이미지를 입력받으면 하나의 긴 벡터로 폅니다.

45
00:03:51,810 --> 00:03:54,974
여기 이미지 x가 있고,

46
00:03:54,974 --> 00:04:02,918
이 이미지는 32x32x3 픽셀이고, 
이를 긴 열벡터로 펼치게 됩니다.

47
00:04:04,344 --> 00:04:09,722
32x32는 이미지의 높이와 너비이며 3은
이미지의 채널 red, green, blue 입니다.

48
00:04:09,722 --> 00:04:13,561
그리고 파라미터 "행렬 W"가 있는데

49
00:04:13,561 --> 00:04:18,517
이 행렬은 아까 만들었던 이미지 픽셀과 연산하여

50
00:04:18,517 --> 00:04:24,387
CIFAR-10의 각 10개의 클래스에 해당하는
클래스 스코어를 만들어줍니다.

51
00:04:24,387 --> 00:04:29,617
그리고 해석해보면 스코어가 더 큰 값은

52
00:04:29,617 --> 00:04:34,881
가령 고양이 클래스의 스코어가 더 크다는 것은 
분류기가 이 이미지가 고양이일것 같다고 생각하는 것입니다.

53
00:04:34,881 --> 00:04:42,443
반대로 개나 자동차 클래스의 스코어가 더 낮다는 것은 
이 이미지가 개나 자동차일 확률이 낮다는 것을 의미하는 것입니다.

54
00:04:42,443 --> 00:04:51,625
또한 저번 수업에서 설명이 좀 부족했던 부분이 있었습니다. 
Linear clssification을 각 클래스 템플릿으로 보는 것인데

55
00:04:51,625 --> 00:05:02,444
좌하단의 다이어그램을 보시면, 행렬 W에는 각 10개의 클래스와 
이미지의 모든 픽셀에 대응하되는 하나씩의 요소가 있습니다.

56
00:05:02,444 --> 00:05:06,554
그리고 이 것은 "이 픽셀이 클래스를 결정하는데 
얼마나 중요한 역할을 하는지"를 의미합니다.

57
00:05:06,554 --> 00:05:12,412
행렬 W의 각 행들이 해당하는 그 클래스의 
템플릿이 되는 것입니다.

58
00:05:12,412 --> 00:05:22,551
행렬 W의 각 행은 "이미지의 픽셀 값"과 "해당 클래스" 사이의
가중치가 되기 때문에

59
00:05:22,551 --> 00:05:29,900
그렇기 때문에 우리가 각 행을 풀어서 다시 한 이미지로 재구성하면
각 클래스에 대응하는 학습된 템플릿을 볼 수 있었습니다.

60
00:05:29,900 --> 00:05:35,399
그리고 또한 Linear clssification이 고차원 공간에서의 
일종의 "결정 경계"를 학습한다는 측면으로 볼 수 있는데,

61
00:05:35,399 --> 00:05:43,774
그 공간의 차원은 이미지의 픽셀 값에 해당하는 것입니다.

62
00:05:43,774 --> 00:05:47,571
지난 시간에 여기까지 진행했습니다.

63
00:05:47,571 --> 00:05:54,141
지난 시간에는 Linear classifier에 대한 간략한 아이디어만
배우고 끝났습니다.

64
00:05:54,141 --> 00:05:57,554
그때는 실제로 행렬W를 어떻게 만드는지는
다루지 않았습니다.

65
00:05:57,554 --> 00:06:02,628
가장 좋은 행렬W를 구하는데 어떻게 트레이닝 데이터를
활용해야 하는지는 언급하지 않았었습니다.

66
00:06:02,628 --> 00:06:08,292
지난 시간에는 임의의 행렬 W를 사용했었고,

67
00:06:08,292 --> 00:06:12,068
그 행렬 W를 가지고 각 이미지에 해당하는
10개의 클래스 스코어를 계산하였습니다.

68
00:06:12,068 --> 00:06:15,597
이 때의 클래스스코어는 좋을수도, 나쁠수도 있었습니다. 
(임의로 정한 것이니까)

69
00:06:15,597 --> 00:06:20,833
여기 간단한 예시가 있는데 세개의 트레이닝 데이터에 대한

70
00:06:20,833 --> 00:06:26,046
임의의 행렬W를 가지고 예측한 10개의 클래스 스코어입니다.

71
00:06:26,046 --> 00:06:29,506
굵게 표시한 각 스코어를 보면 더 좋은것도 있고 
나쁜 것도 있다는 것을 볼 수 있습니다.

72
00:06:29,506 --> 00:06:35,924
예컨데 왼쪽의 이미지를 보면 고양이이죠 
우리는 인간이니까 아주 쉽게 알 수 있죠.

73
00:06:35,924 --> 00:06:41,068
하지만 고양이에 부여된 확률을 보면, 
(확률이 아니라 스코어죠)

74
00:06:41,068 --> 00:06:48,082
이 분류기가 cat에는 2.9점을 줬습니다.

75
00:06:48,082 --> 00:06:51,018
반면 flog에는 3.78점을 줬군요

76
00:06:51,018 --> 00:06:57,920
이 분류기는 별로 좋아보이지 않습니다. 우리는 정답 클래스(고양이)
가 가장 높은 점수가 되는 분류기를 원합니다.

77
00:06:57,920 --> 00:07:02,729
반면 다른 예를 살펴봅시다. 차동차를 보면

78
00:07:02,729 --> 00:07:06,819
자동차 이미지에서는 자동차의 스코어가 제일 높습니다.
좋은 거죠

79
00:07:06,819 --> 00:07:14,357
개구리의 예를 보면 스코어가 -4입니다. 오히려 
다른 스코어보다도 훨씬 낮습니다. 엄청 않좋은 것입니다.

80
00:07:14,357 --> 00:07:16,531
이렇게 분석하는 방법은 좋지 않습니다.

81
00:07:16,531 --> 00:07:20,654
스코어를 눈으로 훑으면서 어느게 좋고
어느게 나쁜지 살펴보기만 하는것은 좋은 생각이 아닙니다.

82
00:07:20,654 --> 00:07:25,264
이런 알고리즘을 만들고, 어떤 W가 가장 좋은지를
결정하기 위해서는

83
00:07:25,264 --> 00:07:31,032
지금 만든 W가 좋은지 나쁜지를 정량화 할 방법이 필요합니다.

84
00:07:31,032 --> 00:07:41,987
W를 입력으로 받아서 각 스코어를 확인하고 이 W가 지금 얼마나 
거지같은지를 정량적으로 말해주는 것이 바로 손실함수입니다.

85
00:07:41,987 --> 00:07:49,782
이번 강의에서는 Image classification에서 쓸만한
몇가지 손실함수를 소개해 드리도록 하겠습니다.

86
00:07:49,782 --> 00:07:52,683
손실함수가 어떤 일을 해야하는지 한번 생각해 보세요.

87
00:07:52,683 --> 00:07:58,498
임의의 값 W가 얼마나 좋은지 나쁜지를 정량화해 줘야 합니다.

88
00:07:58,498 --> 00:08:04,770
우리가 실제로 원하는 것은, 행렬 W가 될 수 있는
모든 경우의 수에 대해서

89
00:08:04,770 --> 00:08:10,688
"가장 덜 구린"  W가 무엇인지를 찾고싶은 것입니다.

90
00:08:10,688 --> 00:08:12,860
이 과정이 바로 "최적화 과정" 이며

91
00:08:12,860 --> 00:08:16,276
최적화에 대해서는 좀 있다가 다시 다루겠습니다.

92
00:08:16,276 --> 00:08:21,003
일단 문제를 좀 줄여보겠습니다. 클래스 10개는 
다루기가 불편하니깐요

93
00:08:21,003 --> 00:08:28,886
이번 강의에서는 클래스를 3개로 하는 
toy 데이터셋을 사용하겠습니다.

94
00:08:28,886 --> 00:08:44,425
이 예제에서 "고양이" 클래스는 잘 분류되지 못했고 "자동차" 는 잘됐고 
"개구리"는 최악입니다. 개구리 점수는 다른 것보다 더 낮습니다.

95
00:08:44,425 --> 00:08:48,817
이를 좀 더 공식화해 봅시다.
손실 함수에 대해 생각해보면

96
00:08:48,817 --> 00:08:52,870
트레이닝 데이터 X와 Y가 있고

97
00:08:52,870 --> 00:08:56,196
보통 X는 알고리즘의 입력에 해당하고

98
00:08:56,196 --> 00:09:05,407
Image classification 알고리즘이라면 X는 이미지가 될 것이고
 Y는 예측하고자 하는 것이 될 것입니다.

99
00:09:05,407 --> 00:09:08,930
보통은 Y는 레이블이나 타겟이라고 합니다.

100
00:09:08,930 --> 00:09:16,797
Image classification의 경우라면 각 이미지를 CIFAR-10
의 10개의 카테고리 중 하나로 분류하는 것이고,

101
00:09:16,797 --> 00:09:24,414
여기에서 레이블 y는 1에서 10 사이의 정수 값이 됩니다. 
프로그래밍 언어에 따라 0에서 9 일수도 있습니다.

102
00:09:24,414 --> 00:09:30,270
어쨌든 이 y라는 정수값은 각 이미지 x의
정답 카테고리를 의미합니다.

103
00:09:30,270 --> 00:09:42,838
앞서 예측 함수를 정의했었죠 입력 이미지 x와 행렬 W를 입력으로 받아서
y를 예측하는 것입니다.

104
00:09:42,838 --> 00:09:46,446
Image classification 문제라면 y는 10개가 되겠습니다. 
(CIFAR-10의 경우)

105
00:09:46,446 --> 00:09:49,938
그 다음 손실함수 L_i를 정의합니다.

106
00:09:49,938 --> 00:09:56,804
앞서 말씀드린 예측함수 f와 정답 값 Y를 입력으로 받아서는

107
00:09:56,804 --> 00:10:02,510
이 트레이닝 샘플을 얼마나 구리게 예측하는지를
정량화 시켜 줍니다.

108
00:10:02,510 --> 00:10:10,478
그리고 최종 Loss인 "L"은 우리 데이터 셋에서 
각 N개의 샘플들의 Loss의 평균이 됩니다.

109
00:10:10,478 --> 00:10:13,432
이 함수는 아주 일반적인 공식입니다.

110
00:10:13,432 --> 00:10:16,421
그리고 Image classification 외에도 다양하게
확장할 수 있습니다.

111
00:10:16,421 --> 00:10:21,323


112
00:10:21,323 --> 00:10:24,426
일반적인 설정의 종류는 모든 작업에 대한 것입니다.

113
00:10:24,426 --> 00:10:26,839
너는 몇 xs와 ys가 있고 너는 적어두고 싶다.

114
00:10:26,839 --> 00:10:30,049
정확하게 얼마나 행복한지를 정량화하는 손실 함수

115
00:10:30,049 --> 00:10:33,535
당신은 당신의 특정 매개 변수 설정 W

116
00:10:33,535 --> 00:10:39,982
결국 W의 공간을 검색하게됩니다.
교육 자료의 손실을 최소화하는 W를 찾으십시오.

117
00:10:41,081 --> 00:10:45,530
그래서 구체적인 손실 함수의 첫 번째 예제

118
00:10:45,530 --> 00:10:49,322
그것은 이미지 분류에서 작동하는 좋은 점입니다.

119
00:10:49,322 --> 00:10:52,755
우리는 다중 클래스 SVM 손실에 대해 이야기 할 것입니다.

120
00:10:52,755 --> 00:10:56,269
바이너리 SVM, 우리의 지원 벡터

121
00:10:56,269 --> 00:10:59,602
CS 229의 기계 및 다중 선 SVM

122
00:11:00,662 --> 00:11:05,263
여러 클래스를 처리하는 일반화입니다.

123
00:11:05,263 --> 00:11:09,447
229에서 볼 수 있듯이 바이너리 SVM의 경우,

124
00:11:09,447 --> 00:11:11,797
당신은 오직 두 개의 클래스 만 가지고 있습니다, 각 예제 x

125
00:11:11,797 --> 00:11:13,750
긍정적 인 것으로 분류 될 것인가?

126
00:11:13,750 --> 00:11:15,159
또는 부정적인 예를 들면,

127
00:11:15,159 --> 00:11:17,615
그러나 이제 우리에게는 10 개의
범주가 있으므로 일반화해야합니다.

128
00:11:17,615 --> 00:11:20,762
이 개념은 여러 클래스를 처리합니다.

129
00:11:20,762 --> 00:11:24,854
따라서이 손실 함수는 재미있는
함수 형태를 가지고 있습니다.

130
00:11:24,854 --> 00:11:26,206
그래서 우리는 좀 더 자세히 살펴볼 것입니다.

131
00:11:26,206 --> 00:11:29,801
슬라이드의 다음 몇 가지에 대해 꽤 자세히 설명합니다.

132
00:11:29,801 --> 00:11:33,094
그러나 이것이 말하는 것은 손실 L_i

133
00:11:33,094 --> 00:11:35,584
모든 개별적인 예를 들어, 우리가 계산할 방법

134
00:11:35,584 --> 00:11:40,298
우리는 모든 범주에 걸쳐 합계를 수행 할 것인가? Y,

135
00:11:40,298 --> 00:11:43,373
실제 범주 Y_i를 제외하고,

136
00:11:43,373 --> 00:11:46,204
그래서 우리는 모든 잘못된 카테고리를 합산하려고합니다.

137
00:11:46,204 --> 00:11:48,442
우리는 점수를 비교하려고합니다.

138
00:11:48,442 --> 00:11:50,246
올바른 카테고리의 점수와 점수

139
00:11:50,246 --> 00:11:52,509
잘못된 카테고리의

140
00:11:52,509 --> 00:11:55,079
이제 올바른 카테고리의 점수가

141
00:11:55,079 --> 00:11:59,589
잘못된 카테고리의 점수보다 큽니다.

142
00:11:59,589 --> 00:12:03,814
일부 안전 여유로 잘못된 점수보다 큼

143
00:12:03,814 --> 00:12:07,149
우리가 1로 설정 한 경우에 해당하는 경우

144
00:12:07,149 --> 00:12:11,340
진실한 점수는 다량, 또는 진실한 종류를위한 점수

145
00:12:11,340 --> 00:12:14,661
그것이 거짓 범주들 중 어느 것보다 훨씬 큰 경우,

146
00:12:14,661 --> 00:12:17,663
그러면 우리는 0의 손실을 얻습니다.

147
00:12:17,663 --> 00:12:21,703
그리고 우리는 이것을 모든 잘못된
범주들에 대해 요약 할 것입니다.

148
00:12:21,703 --> 00:12:24,454
우리의 이미지를 위해 이것은 우리에게
우리의 마지막 손실을 줄 것이다.

149
00:12:24,454 --> 00:12:27,155
데이터 세트에서이 한 예를 들어 보겠습니다.

150
00:12:27,155 --> 00:12:29,711
그리고 다시 우리는이 손실의 평균을 취할 것입니다.

151
00:12:29,711 --> 00:12:31,994
전체 교육 데이터 세트에 대해

152
00:12:31,994 --> 00:12:36,937
그래서 이런 종류의 진술은 진실

153
00:12:36,937 --> 00:12:41,263
학급 점수는 다른 사람들보다 훨씬 큽니다.

154
00:12:41,263 --> 00:12:45,160
이런 종류의 제재가 우리가 종종 압축한다면

155
00:12:45,160 --> 00:12:49,327
이 단일 최대 S_j 마이너스
S_Yi 플러스 한 가지 더하기,

156
00:12:50,496 --> 00:12:53,049
그러나 나는 항상 그 표기법을 조금 혼란스럽게 생각합니다.

157
00:12:53,049 --> 00:12:54,294
항상 도움이됩니다.

158
00:12:54,294 --> 00:12:56,678
이런 종류의 사례에 기반한 표기법으로 써라.

159
00:12:56,678 --> 00:12:58,636
두 경우가 정확히 무엇인지 알아 내야합니다.

160
00:12:58,636 --> 00:13:00,920
그리고 무슨 일이 일어나고 있는지.

161
00:13:00,920 --> 00:13:03,789
그리고 그런데,이 스타일의 손실 기능

162
00:13:03,789 --> 00:13:06,805
여기서 우리는 0의 최대 값과 다른 어떤 양을 취한다.

163
00:13:06,805 --> 00:13:10,127
종종 힌지 손실의 일종으로 불리우며,

164
00:13:10,127 --> 00:13:13,202
이 이름은 그래프의 모양에서 유래합니다.

165
00:13:13,202 --> 00:13:14,627
당신이 가서 그것을 계획 할 때,

166
00:13:14,627 --> 00:13:18,127
여기에서 x 축은 S_Yi에 해당하며,

167
00:13:19,103 --> 00:13:22,275
그것은 일부 훈련에 대한 진정한 학급의 점수입니다

168
00:13:22,275 --> 00:13:25,353
예를 들어, 이제 y 축은 손실이고,

169
00:13:25,353 --> 00:13:29,520
실제 카테고리의 점수로 볼 수 있습니다.

170
00:13:30,523 --> 00:13:32,338
이 예제가 증가하면 손실

171
00:13:32,338 --> 00:13:34,174
선형 적으로 내려갈 것이다.

172
00:13:34,174 --> 00:13:37,805
우리가이 안전 마진을 넘을 때까지,

173
00:13:37,805 --> 00:13:40,235
그 후에 손실은 0이 될 것입니다.

174
00:13:40,235 --> 00:13:44,402
우리는 이미이 예를 정확하게 분류했기 때문입니다.

175
00:13:45,550 --> 00:13:47,836
그래서, 오, 질문 하죠?

176
00:13:47,836 --> 00:13:50,464
- [Student] 죄송합니다, 표기법에 관해서

177
00:13:50,464 --> 00:13:52,410
S는 무엇입니까?

178
00:13:52,410 --> 00:13:55,170
그게 너의 옳은 점수 야?

179
00:13:55,170 --> 00:13:56,321
- 네, 그럼 질문은

180
00:13:56,321 --> 00:14:00,097
표기법에서 S는 무엇이고 SYI는 무엇입니까?

181
00:14:00,097 --> 00:14:03,906
특히, Ss는 예측 된 점수입니다.

182
00:14:03,906 --> 00:14:07,246
분류 자에서 나오는 클래스의 경우.

183
00:14:07,246 --> 00:14:10,934
그래서 하나가 고양이 클래스이고
두 개가 개 클래스이면 S1

184
00:14:10,934 --> 00:14:13,942
S2는 각각 개 고양이 점수가됩니다.

185
00:14:13,942 --> 00:14:17,434
우리는 이순신이 땅의 범주라고 말했던 것을 기억하십시오.

186
00:14:17,434 --> 00:14:21,056
예제의 진리 레이블은 정수입니다.

187
00:14:21,056 --> 00:14:25,223
그래서 S Sub Y sub i,
이중 첨자를 유감스럽게 생각합니다.

188
00:14:26,057 --> 00:14:28,966
진정한 등급의 점수에 해당하는

189
00:14:28,966 --> 00:14:32,683
훈련 세트의 i 번째 예제

190
00:14:32,683 --> 00:14:33,870
문제?

191
00:14:33,870 --> 00:14:35,921
- [Student] 그렇다면이
컴퓨팅은 정확히 무엇입니까?

192
00:14:35,921 --> 00:14:38,677
- 그래,이 컴퓨팅이 정확히 여기서 뭐지?

193
00:14:38,677 --> 00:14:41,625
그것은 조금 재미있다, 나는 그것이
더 명백하게 될 것이라고 생각한다.

194
00:14:41,625 --> 00:14:44,590
우리가 명백한 모범을 보았을 때 어떤 의미에서

195
00:14:44,590 --> 00:14:48,675
이 손실이 말하는 것은 우리가
행복하면 행복하다는 것입니다.

196
00:14:48,675 --> 00:14:51,984
점수는 다른 모든 점수보다 훨씬 높습니다.

197
00:14:51,984 --> 00:14:54,593
다른 모든 점수보다 높아야합니다.

198
00:14:54,593 --> 00:14:58,364
일부 안전 여유로, 그리고 진정한 점수

199
00:14:58,364 --> 00:15:01,892
다른 점수보다 더 높지는 않습니다.

200
00:15:01,892 --> 00:15:06,534
그러면 우리는 약간의 손실을 입을
것이고 그것은 나쁠 것입니다.

201
00:15:06,534 --> 00:15:08,657
그래서 이것은 조금 더 이해할 수 있습니다.

202
00:15:08,657 --> 00:15:10,600
명백한 예제를 살펴보면

203
00:15:10,600 --> 00:15:13,223
이 작은 세 가지 예제 데이터 세트.

204
00:15:13,223 --> 00:15:16,096
여기서 내가 사례 공간을 제거했다는 것을 기억하십시오.

205
00:15:16,096 --> 00:15:19,513
표기법을 바꾸고 0 표기법으로 다시 전환하면,

206
00:15:19,513 --> 00:15:21,110
그리고 지금 우리가 보면,

207
00:15:21,110 --> 00:15:24,544
이 다중 클래스 SVM 손실 계산에 대해 생각해 보면

208
00:15:24,544 --> 00:15:26,186
이 첫 번째 훈련 예를 들어 보자.

209
00:15:26,186 --> 00:15:28,758
왼쪽에있는 다음 반복 할 것임을 기억하십시오.

210
00:15:28,758 --> 00:15:31,913
모든 잘못된 클래스들, 그래서이 예제에서,

211
00:15:31,913 --> 00:15:34,922
고양이는 올바른 학급입니다. 그래서
우리는 차를 돌아 다닐 예정입니다.

212
00:15:34,922 --> 00:15:39,089
그리고 개구리 수업, 그리고 지금은 차를 위해, 우리는,

213
00:15:41,088 --> 00:15:44,615
우리는 차 점수 5.1에서 고양이
점수를 뺀 것을 볼 것입니다.

214
00:15:44,615 --> 00:15:48,782
3.2 플러스 1, 우리가 고양이와
차를 비교할 때 우리는 기대한다.

215
00:15:49,969 --> 00:15:53,127
차 점수가 더 높기 때문에 약간의 손실을 여기에서 겪는다.

216
00:15:53,127 --> 00:15:55,134
나쁜 고양이 점수보다.

217
00:15:55,134 --> 00:15:59,153
그래서이 한 가지 예를 들어,

218
00:15:59,153 --> 00:16:01,333
우리는 2.9의 손실을 입을 것이고,

219
00:16:01,333 --> 00:16:03,387
가서 고양이 점수를 비교하면

220
00:16:03,387 --> 00:16:06,512
그리고 우리가 본 개구리 점수는 3.2입니다.

221
00:16:06,512 --> 00:16:08,480
개구리는 마이너스 1.7,

222
00:16:08,480 --> 00:16:11,561
그래서 고양이는 개구리보다 더 큰 고양이입니다.

223
00:16:11,561 --> 00:16:14,728
이는이 두 클래스 사이에서

224
00:16:14,728 --> 00:16:16,409
우리는 손실이 전혀 없습니다.

225
00:16:16,409 --> 00:16:20,748
그럼이 훈련 예를위한 다중 클래스 SVM 손실

226
00:16:20,748 --> 00:16:23,094
각각의 쌍에 걸친 손실의 합계가 될 것입니다

227
00:16:23,094 --> 00:16:27,112
클래스는 2.9와 2.9가 될 것이며, 이는 2.9입니다.

228
00:16:27,112 --> 00:16:30,480
2.9는 양적 측정이라고 말하는 것과 같습니다.

229
00:16:30,480 --> 00:16:32,150
우리 분류가 얼마나 엉망이 됐는지의

230
00:16:32,150 --> 00:16:34,567
이 한 가지 훈련 예를 들어 보겠습니다.

231
00:16:35,795 --> 00:16:37,097
그런 다음이 절차를 반복하면

232
00:16:37,097 --> 00:16:41,574
다음 차 이미지에서 다시 진짜 클래스는 차다.

233
00:16:41,574 --> 00:16:44,051
그래서 우리는 다른 모든 범주들에 대해 반복 할 것입니다.

234
00:16:44,051 --> 00:16:47,205
우리가 차와 고양이 점수를 비교할 때,

235
00:16:47,205 --> 00:16:50,094
우리는 차가 고양이보다 하나 이상
크다는 것을 알 수 있습니다.

236
00:16:50,094 --> 00:16:52,072
그래서 우리는 여기서 손실을 입지 않습니다.

237
00:16:52,072 --> 00:16:54,219
자동차와 개구리를 비교하면 다시 볼 수 있습니다.

238
00:16:54,219 --> 00:16:57,557
차 점수가 개구리보다 하나 이상 크고,

239
00:16:57,557 --> 00:16:59,984
그래서 우리는 여기서 다시 손실을
입지 않으며, 우리의 총 손실

240
00:16:59,984 --> 00:17:02,993
이 훈련 예는 0입니다.

241
00:17:02,993 --> 00:17:05,765
그리고 지금 나는 당신이 희망적으로
사진을 찍을 것이라고 생각하지만,

242
00:17:05,765 --> 00:17:09,051
개구리, 지금 개구리를 보시면
개구리를 다시 비교해 보겠습니다.

243
00:17:09,051 --> 00:17:11,766
고양이는 개구리 점수 때문에 많은 손실을 입습니다.

244
00:17:11,766 --> 00:17:14,895
개구리와 자동차를 비교할 때 매우
낮으며 손실이 많이 발생합니다.

245
00:17:14,895 --> 00:17:18,363
점수가 매우 낮기 때문에 우리는

246
00:17:18,364 --> 00:17:19,697
예는 12.9입니다.

247
00:17:21,150 --> 00:17:23,857
전체 데이터 세트에 대한 최종 손실

248
00:17:23,857 --> 00:17:25,163
이 손실의 평균입니다.

249
00:17:25,164 --> 00:17:26,578
다른 예를 통해,

250
00:17:26,578 --> 00:17:29,277
그래서 당신이 그것들을 요약 할
때 그것은 대략 5.3에 온다.

251
00:17:29,277 --> 00:17:31,530
그러면 그것은 일종의 것입니다.
이것은 우리의 양적 측정입니다.

252
00:17:31,530 --> 00:17:34,929
우리 분류기가이 데이터 세트에서
5.3이라는 것을 알았습니다.

253
00:17:34,929 --> 00:17:36,732
질문 있니?

254
00:17:36,732 --> 00:17:39,152
- [학생] 당신은 플러스를 어떻게 선택합니까?

255
00:17:39,152 --> 00:17:41,920
- 그래, 문제는 플러스를 어떻게 선택 하느냐는거야?

256
00:17:41,920 --> 00:17:43,574
그것은 실제로 정말로 좋은 질문입니다.

257
00:17:43,574 --> 00:17:46,662
그것은 임의적 인 선택의 종류처럼 여기에 보인다.

258
00:17:46,662 --> 00:17:48,778
그것은 손실 함수에 나타나는 유일한 상수이다.

259
00:17:48,778 --> 00:17:50,950
그게 당신의 미적 감각을 불쾌하게하는 것 같아요.

260
00:17:50,950 --> 00:17:52,532
어쩌면 조금.

261
00:17:52,532 --> 00:17:53,850
그러나 이것이 다소

262
00:17:53,850 --> 00:17:57,869
우리가 실제로 신경 쓰지 않기 때문에

263
00:17:57,869 --> 00:18:00,305
점수의 절대 값에 대해

264
00:18:00,305 --> 00:18:02,795
이 손실 함수에서 우리는

265
00:18:02,795 --> 00:18:05,389
점수 사이의 상대적인 차이에 대해

266
00:18:05,389 --> 00:18:06,718
우리는 오직 정확한 점수

267
00:18:06,718 --> 00:18:08,944
잘못된 점수보다 훨씬 큽니다.

268
00:18:08,944 --> 00:18:11,540
그래서 사실 당신이 당신의 전체 W

269
00:18:11,540 --> 00:18:14,742
위 또는 아래, 모든 종류의 점수를 재조정합니다.

270
00:18:14,742 --> 00:18:17,539
그에 상응하여 세부 사항을 통해 일하는 경우

271
00:18:17,539 --> 00:18:20,808
코스 노트에 자세한 내용이 나와 있습니다.

272
00:18:20,808 --> 00:18:24,362
온라인으로, 당신은 하나의 선택이 실제로
중요하지 않다는 것을 알게됩니다.

273
00:18:24,362 --> 00:18:27,657
한 가지 종류의이 무료 매개 변수가

274
00:18:27,657 --> 00:18:29,312
이 규모로 취소됩니다.

275
00:18:29,312 --> 00:18:32,625
W의 규모의 전체 설정과 같습니다.

276
00:18:32,625 --> 00:18:34,656
그리고 코스 노트에서 조금 더 자세하게 확인하십시오.

277
00:18:34,656 --> 00:18:35,489
그걸로.

278
00:18:37,953 --> 00:18:40,374
그래서 나는 생각하는 것이 유용하다고 생각한다.

279
00:18:40,374 --> 00:18:42,519
이해하려고하는 몇 가지 질문에 대해

280
00:18:42,519 --> 00:18:45,374
직감적으로이 손실이 무엇을하는지.

281
00:18:45,374 --> 00:18:48,715
그래서 첫 번째 질문은 손실에 어떤
일이 일어날 것인가하는 것입니다.

282
00:18:48,715 --> 00:18:53,349
우리가 차 이미지의 점수를 조금만 바꾸면?

283
00:18:53,349 --> 00:18:54,182
어떤 아이디어?

284
00:18:56,479 --> 00:18:59,856
모두들 질문하기가 너무 무서워요?

285
00:18:59,856 --> 00:19:00,689
대답?

286
00:19:00,689 --> 00:19:04,272
[희미하게 말하는 학생]

287
00:19:06,983 --> 00:19:10,013
- 그래, 대답은 우리가 점수를 흔들면

288
00:19:10,013 --> 00:19:13,333
이 차 이미지에 대해 조금은 손실이 변하지 않을 것입니다.

289
00:19:13,333 --> 00:19:15,626
따라서 SVM 손실은 기억하고있는 유일한 것입니다.

290
00:19:15,626 --> 00:19:19,073
약 1보다 큰 점수를 얻고 있습니다.

291
00:19:19,073 --> 00:19:21,918
잘못된 점수보다 더 많이,하지만이 경우,

292
00:19:21,918 --> 00:19:25,506
차 점수는 이미 다른 사람보다는 확실히 더 크다,

293
00:19:25,506 --> 00:19:28,048
그래서이 클래스의 점수가이 예제에서 바뀌면

294
00:19:28,048 --> 00:19:30,665
조금 바뀌었다.

295
00:19:30,665 --> 00:19:33,270
여전히 유지되며 손실은 변하지 않을 것입니다.

296
00:19:33,270 --> 00:19:35,437
우리는 여전히 손실이 0이 될 것입니다.

297
00:19:36,870 --> 00:19:39,317
다음 질문은 최소 및 최대 손실 가능성

298
00:19:39,317 --> 00:19:40,150
SVM에 대한?

299
00:19:43,265 --> 00:19:44,445
[희미하게 말하는 학생]

300
00:19:44,445 --> 00:19:45,764
오, 나는 중얼 거리는 소리가 들린다.

301
00:19:45,764 --> 00:19:49,374
따라서 최소 손실은 0입니다.

302
00:19:49,374 --> 00:19:52,018
정확한 점수가 많으면 모든 수업에서

303
00:19:52,018 --> 00:19:55,533
더 크면 모든 수업에서 손실이 0 점이됩니다.

304
00:19:55,533 --> 00:19:57,357
그리고 그것은 0이 될 것이고,

305
00:19:57,357 --> 00:20:00,652
그리고 우리가 가진이 경첩 손실
음모로 돌아가서 생각해 보면,

306
00:20:00,652 --> 00:20:03,495
그러면 정확한 점수가

307
00:20:03,495 --> 00:20:06,192
매우 부정적인면, 우리는

308
00:20:06,192 --> 00:20:07,981
잠재적으로 무한한 손실.

309
00:20:07,981 --> 00:20:11,538
그래서 min은 0이고 max는 무한대입니다.

310
00:20:11,538 --> 00:20:14,427
또 다른 질문으로, 이러한 것들을 초기화 할 때 일종의

311
00:20:14,427 --> 00:20:16,120
처음부터 교육을 시작하십시오.

312
00:20:16,120 --> 00:20:18,015
보통 당신은 일종의 W를 초기화합니다.

313
00:20:18,015 --> 00:20:21,242
결과적으로 당신의 점수가 작은 무작위 값으로

314
00:20:21,242 --> 00:20:23,942
작고 균일 한 무작위 값인 경향이있다.

315
00:20:23,942 --> 00:20:25,535
훈련 시작시.

316
00:20:25,535 --> 00:20:27,926
그리고 질문은 만약 당신의 모든 S,

317
00:20:27,926 --> 00:20:30,064
모든 점수가 거의 0 인 경우

318
00:20:30,064 --> 00:20:31,449
대략 동일하고,

319
00:20:31,449 --> 00:20:32,815
그러면 어떤 종류의 손실이 예상됩니까?

320
00:20:32,815 --> 00:20:35,741
멀티 클래스 SVM을 사용할 때?

321
00:20:35,741 --> 00:20:37,502
- [학생] 수업 수에서 1을 뺍니다.

322
00:20:37,502 --> 00:20:42,448
네, 대답은 수업 수에서 1을 뺀 것입니다.

323
00:20:42,448 --> 00:20:45,871
우리가 반복한다면

324
00:20:45,871 --> 00:20:48,225
모든 잘못된 클래스들, 그래서 우리는 반복하고있다.

325
00:20:48,225 --> 00:20:51,489
C 클래스에서 하나의 클래스를 뺀 클래스

326
00:20:51,489 --> 00:20:53,738
두 개의 S는 거의 같을 것이다.

327
00:20:53,738 --> 00:20:55,096
그래서 우리는 1의 손실을 얻을 것이다.

328
00:20:55,096 --> 00:20:57,536
마진 때문에 우리는 C 빼기를 얻습니다.

329
00:20:57,536 --> 00:21:00,359
그래서 이것은 실제로 유용합니다. 왜냐하면 여러분,

330
00:21:00,359 --> 00:21:01,964
이것은 유용한 디버깅 전략이다.

331
00:21:01,964 --> 00:21:03,243
당신이 이런 것들을 사용할 때,

332
00:21:03,243 --> 00:21:04,734
훈련을 시작할 때,

333
00:21:04,734 --> 00:21:08,082
당신은 당신의 상실을 기대하는 것에 대해 생각해야합니다.

334
00:21:08,082 --> 00:21:10,931
그리고 훈련이 시작될 때 실제로 본 손실이

335
00:21:10,931 --> 00:21:13,784
첫 번째 반복에서 C에서 1을 뺀 것과 같습니다.

336
00:21:13,784 --> 00:21:14,999
이 경우,

337
00:21:14,999 --> 00:21:16,353
그건 아마 버그가 있다는 것을 의미하고 체크를해야합니다.

338
00:21:16,353 --> 00:21:18,655
귀하의 코드, 그래서 이것은 실제로 유용한 일종의

339
00:21:18,655 --> 00:21:20,905
실제로 확인하는 것.

340
00:21:21,886 --> 00:21:25,252
또 다른 질문은, 만약에 일어나면, 그래서 나는 우리가

341
00:21:25,252 --> 00:21:30,010
잘못된 클래스에 대한 SVM,

342
00:21:30,010 --> 00:21:31,487
올바른 클래스를 넘었다.

343
00:21:31,487 --> 00:21:34,323
우리가 모든 것을 다 통과한다면?

344
00:21:34,323 --> 00:21:36,034
- [학생] 손실이 1 증가합니다.

345
00:21:36,034 --> 00:21:39,326
그래, 대답은 손실이 하나씩 증가한다는 것이다.

346
00:21:39,326 --> 00:21:41,929
우리가 실제로이 일을하는 이유는

347
00:21:41,929 --> 00:21:45,052
일반적으로 제로의 손실은 일종의, 좋은이있다

348
00:21:45,052 --> 00:21:47,356
당신이 전혀 잃지 않는다는 해석,

349
00:21:47,356 --> 00:21:51,363
그래서 좋네요, 그래서 나는 당신의 대답을 생각합니다.

350
00:21:51,363 --> 00:21:52,755
정말 변하지 않을거야.

351
00:21:52,755 --> 00:21:54,512
당신은 결국 같은 분류자를 찾는 것입니다.

352
00:21:54,512 --> 00:21:56,484
실제로 모든 범주를 반복하면

353
00:21:56,484 --> 00:21:59,979
하지만 규칙에 따라 올바른 클래스를 생략하면

354
00:21:59,979 --> 00:22:02,729
우리의 최소 손실은 0이됩니다.

355
00:22:04,931 --> 00:22:06,341
그래서 또 다른 질문은, 만약 우리가 평균

356
00:22:06,341 --> 00:22:08,008
여기에 합계 대신에?

357
00:22:09,943 --> 00:22:11,233
- [학생] 변경되지 않습니다.

358
00:22:11,233 --> 00:22:13,076
- 네, 대답은 변하지 않습니다.

359
00:22:13,076 --> 00:22:15,788
따라서 클래스 수는 미리 고정 될 것입니다.

360
00:22:15,788 --> 00:22:18,075
우리가 우리의 데이터 세트를 선택할
때, 그것은 단지 재 스케일링입니다.

361
00:22:18,075 --> 00:22:20,500
상수에 의한 전체 손실 함수,

362
00:22:20,500 --> 00:22:22,334
그래서 그것은별로 중요하지 않습니다.

363
00:22:22,334 --> 00:22:23,991
다른 모든 규모의 것들과

364
00:22:23,991 --> 00:22:25,754
왜냐하면 우리는 실제로 진정한 가치에
관심을 갖지 않기 때문입니다.

365
00:22:25,754 --> 00:22:28,331
또는 손실의 진정한 가치

366
00:22:28,331 --> 00:22:29,664
그 문제에 대한.

367
00:22:30,541 --> 00:22:32,710
이제 다른 예가 있습니다.

368
00:22:32,710 --> 00:22:35,935
이 손실 공식 및 우리는 실제로 사각 항을 더했습니다.

369
00:22:35,935 --> 00:22:37,934
이 최대의 상단에?

370
00:22:37,934 --> 00:22:40,066
이것이 결국 같은 문제가 될까?

371
00:22:40,066 --> 00:22:42,968
아니면 다른 분류 알고리즘이 될까요?

372
00:22:42,968 --> 00:22:44,178
- [학생] 다르다.

373
00:22:44,178 --> 00:22:45,213
- 그래, 이건 다를거야.

374
00:22:45,213 --> 00:22:47,296
그래서 여기서 아이디어는 우리가
일종의 변화라고 생각합니다.

375
00:22:47,296 --> 00:22:49,145
선과 악의 절충

376
00:22:49,145 --> 00:22:50,903
일종의 비선형 방식으로,

377
00:22:50,903 --> 00:22:52,433
그래서 이것은 결국 실제로 컴퓨팅을 끝낼 것입니다.

378
00:22:52,433 --> 00:22:54,264
다른 손실 함수.

379
00:22:54,264 --> 00:22:57,296
제곱 된 경첩 손실이라는이 아이디어는 실제로 사용됩니다.

380
00:22:57,296 --> 00:22:59,915
때로는 실제로, 그래서 또 다른 속임수입니다

381
00:22:59,915 --> 00:23:01,597
너가 위로 만들 때 너의 부대 안에있을 것이다

382
00:23:01,597 --> 00:23:05,066
자신의 문제에 대한 자신의 손실 함수.

383
00:23:05,066 --> 00:23:07,831
이제 끝낼 것입니다, 오, 질문 있니?

384
00:23:07,831 --> 00:23:09,126
- [학생] 왜 제곱 된 손실을 사용합니까?

385
00:23:09,126 --> 00:23:11,596
제곱이 아닌 손실 대신에?

386
00:23:11,596 --> 00:23:14,018
- 네, 그럼 질문은 왜 당신이

387
00:23:14,018 --> 00:23:16,760
비 제곱 손실 대신 제곱 손실을 사용합니까?

388
00:23:16,760 --> 00:23:18,519
그리고 손실 함수의 전체 요점

389
00:23:18,519 --> 00:23:22,281
실수가 얼마나 다른지를 정량화하는 것입니다.

390
00:23:22,281 --> 00:23:25,143
그리고 분류자가 여러 종류의 실수를 저지르고 있다면,

391
00:23:25,143 --> 00:23:26,995
우리는 어떻게 서로 다른 장단점을 달랠 수 있을까요?

392
00:23:26,995 --> 00:23:28,940
서로 다른 유형의 실수 사이에서 분류 자

393
00:23:28,940 --> 00:23:30,141
만들겠습니까?

394
00:23:30,141 --> 00:23:32,265
그래서 제곱 된 손실을 사용한다면,

395
00:23:32,265 --> 00:23:36,371
그 종류의 말은 매우 나쁜 것들이

396
00:23:36,371 --> 00:23:37,834
이제 제곱이 나빠질거야.

397
00:23:37,834 --> 00:23:39,192
그래서 그것은 정말로, 정말로 나쁘고,

398
00:23:39,192 --> 00:23:40,711
우리는 아무것도 원하지 않는 것처럼

399
00:23:40,711 --> 00:23:43,623
그것은 완전히 파국적으로 잘못 분류 된 것입니다.

400
00:23:43,623 --> 00:23:47,321
반면에이 경첩 손실을 사용하는 경우,

401
00:23:47,321 --> 00:23:49,556
우리는 조금 틀린 것 사이에서 실제로 신경 쓰지 않는다.

402
00:23:49,556 --> 00:23:52,553
그리고 많은 잘못되고, 틀린 종류의,

403
00:23:52,553 --> 00:23:55,340
예를 많이 잘못 생각하면 그것을 증가시킵니다.

404
00:23:55,340 --> 00:23:57,051
조금만 잘못하면

405
00:23:57,051 --> 00:23:59,608
그것은 예제와 같은 장점입니다.

406
00:23:59,608 --> 00:24:02,603
그것은 조금 잘못되어 다시 증가시키는 것입니다.

407
00:24:02,603 --> 00:24:04,375
조금 더 옳다.

408
00:24:04,375 --> 00:24:06,268
그래서 약간 물결 모양의 물결 모양입니다.

409
00:24:06,268 --> 00:24:08,785
그러나 선형 대 정사각형을 사용하는이 아이디어

410
00:24:08,785 --> 00:24:11,198
우리가 얼마나 신경을 쓰는지를 수치화하는 방법입니다.

411
00:24:11,198 --> 00:24:13,597
서로 다른 범주의 오류에 대해

412
00:24:13,597 --> 00:24:15,375
그리고 이것은 확실히 당신이 생각해야 할 것입니다.

413
00:24:15,375 --> 00:24:17,938
실제로 이러한 것들을 실제로 적용 할 때,

414
00:24:17,938 --> 00:24:20,245
손실 함수가 방법이기 때문에

415
00:24:20,245 --> 00:24:22,770
알고리즘에 어떤 유형의 오류

416
00:24:22,770 --> 00:24:24,104
당신이 신경을 쓰고 어떤 종류의 오류

417
00:24:24,104 --> 00:24:26,267
그것은 반대해야합니다.

418
00:24:26,267 --> 00:24:27,907
실제로 실제로는 매우 중요합니다.

419
00:24:27,907 --> 00:24:30,407
귀하의 응용 프로그램에 따라.

420
00:24:32,315 --> 00:24:35,017
여기 벡터화 된 코드 종류의 작은 조각이 있습니다.

421
00:24:35,017 --> 00:24:37,566
numpy로 끝내면 결국 구현됩니다.

422
00:24:37,566 --> 00:24:40,962
첫 번째 임무를 위해 이와 비슷한 것,

423
00:24:40,962 --> 00:24:43,593
그러나 이런 종류의 감각은 당신에게이 합계

424
00:24:43,593 --> 00:24:44,952
실제로는 꽤 쉽다.

425
00:24:44,952 --> 00:24:46,798
numpy로 구현하려면 몇 줄 밖에 걸리지 않습니다.

426
00:24:46,798 --> 00:24:48,768
벡터화 된 코드

427
00:24:48,768 --> 00:24:50,926
그리고 실제로 하나의 멋진 트릭을 볼 수 있습니다.

428
00:24:50,926 --> 00:24:55,904
실제로 여기에 들어가서 여백을
제로화 할 수 있다는 것입니다.

429
00:24:55,904 --> 00:24:57,856
올바른 클래스에 해당하는

430
00:24:57,856 --> 00:25:00,750
그리고 그것은 바로 그때에 쉽게,

431
00:25:00,750 --> 00:25:04,345
그것은 건너 뛰는 멋진 벡터화 된 트릭의 일종이며,

432
00:25:04,345 --> 00:25:06,069
하나의 클래스를 제외한 모든 클래스를 반복합니다.

433
00:25:06,069 --> 00:25:07,891
당신은 건너 뛰고 싶은 사람을 제로로 해줍니다.

434
00:25:07,891 --> 00:25:09,854
그리고 어쨌든 합계를 계산하면 좋은 트릭입니다.

435
00:25:09,854 --> 00:25:13,315
당신은 과제에 대한 사용을 고려할 수 있습니다.

436
00:25:13,315 --> 00:25:17,106
자, 이제이 손실 함수에 대한 또 다른 질문입니다.

437
00:25:17,106 --> 00:25:19,439
당신이 W를 찾을만큼 충분히 운이 좋다면

438
00:25:19,439 --> 00:25:21,629
그 손실은 0입니다. 당신은 전혀 손실되지 않습니다.

439
00:25:21,629 --> 00:25:22,745
너는 완전히 이기고있다.

440
00:25:22,745 --> 00:25:24,139
이 손실 함수는 그것을 분쇄하고 있습니다.

441
00:25:24,139 --> 00:25:27,819
하지만 질문이 있습니다.이 독특한 W입니다.

442
00:25:27,819 --> 00:25:29,057
또는 다른 W가 있었나요

443
00:25:29,057 --> 00:25:32,390
그것은 또한 손실을 0으로 만들 수 있었습니까?

444
00:25:33,251 --> 00:25:34,271
- [학생] 다른 W가 있습니다.

445
00:25:34,271 --> 00:25:37,444
- 답변, 예, 그래서 다른 W가 있습니다.

446
00:25:37,444 --> 00:25:39,998
그리고 특히, 우리가 조금 이야기했기 때문에

447
00:25:39,998 --> 00:25:43,976
전체 문제를 위 또는 아래로 조정하는 것에 관한 것

448
00:25:43,976 --> 00:25:46,709
W에 따라 실제로 W를 취할 수 있습니다.

449
00:25:46,709 --> 00:25:50,876
두 배로 곱해졌고 W가 두 배로
증가했습니다 (지금 쿼드 U입니까?

450
00:25:51,984 --> 00:25:52,978
나도 몰라.)

451
00:25:52,978 --> 00:25:54,110
[웃음]

452
00:25:54,110 --> 00:25:56,601
이것은 또한 손실을 0으로 만들 것입니다.

453
00:25:56,601 --> 00:25:58,568
그래서 이것의 구체적인 예로서,

454
00:25:58,568 --> 00:26:00,066
좋아하는 예제로 돌아갈 수 있습니다.

455
00:26:00,066 --> 00:26:01,184
어쩌면 숫자를 통해 일할 수도 있습니다.

456
00:26:01,184 --> 00:26:02,351
나중에 조금,

457
00:26:02,351 --> 00:26:05,309
그러나 당신이 W를 가져 가고 우리가 W를 두 배로한다면,

458
00:26:05,309 --> 00:26:09,129
올바른 점수와 잘못된 점수 사이의 마진

459
00:26:09,129 --> 00:26:10,583
또한 두 배가됩니다.

460
00:26:10,583 --> 00:26:12,190
그래서이 모든 마진이

461
00:26:12,190 --> 00:26:14,521
이미 하나보다 컸고 우리는 그들을 두배로 늘 렸습니다.

462
00:26:14,521 --> 00:26:16,274
그들은 여전히 하나보다 커질 것입니다.

463
00:26:16,274 --> 00:26:18,857
그래서 당신은 여전히 손실이 없을 것입니다.

464
00:26:20,180 --> 00:26:22,182
그리고 이것은 재미있는 일종의,

465
00:26:22,182 --> 00:26:24,572
왜냐하면 우리의 손실 함수

466
00:26:24,572 --> 00:26:27,428
우리가 우리가 원하는 W를 우리
분류 자에게 말하는 방식입니다.

467
00:26:27,428 --> 00:26:29,045
그리고 우리가 신경 쓰는 W,

468
00:26:29,045 --> 00:26:30,333
이것은 조금 이상하다.

469
00:26:30,333 --> 00:26:31,795
지금이 불일치가 있습니다.

470
00:26:31,795 --> 00:26:35,197
분류 자의 선택 방법

471
00:26:35,197 --> 00:26:36,862
이들 서로 다른 버전의 W 사이

472
00:26:36,862 --> 00:26:39,112
모두 제로 손실을 달성합니까?

473
00:26:40,099 --> 00:26:42,210
그리고 그것은 우리가 여기서 한 일이

474
00:26:42,210 --> 00:26:45,303
데이터 측면에서 손실 만 기록됩니다.

475
00:26:45,303 --> 00:26:48,136
우리는 분류 자에게만 말했습니다.

476
00:26:48,982 --> 00:26:50,448
W를 찾아야한다고

477
00:26:50,448 --> 00:26:52,239
교육 자료에 맞는

478
00:26:52,239 --> 00:26:54,233
하지만 실제로 실제로, 우리는 실제로 신경 쓰지 않습니다.

479
00:26:54,233 --> 00:26:56,465
훈련 자료를 맞추는 것에 관한 것,

480
00:26:56,465 --> 00:26:57,760
기계 학습의 요점

481
00:26:57,760 --> 00:27:01,553
우리가 훈련 데이터를 사용하여
일부 분류자를 찾는 것입니다.

482
00:27:01,553 --> 00:27:04,222
그런 다음 테스트 데이터에 적용 할 것입니다.

483
00:27:04,222 --> 00:27:06,858
따라서 우리는 교육 데이터 성능에
대해 실제로 신경 쓰지 않습니다.

484
00:27:06,858 --> 00:27:08,503
우리는 정말 성능에 신경을 썼다.

485
00:27:08,503 --> 00:27:10,928
테스트 데이터에 대한이 분류 기준.

486
00:27:10,928 --> 00:27:12,785
결과적으로 유일한 경우

487
00:27:12,785 --> 00:27:14,671
우리는 분류 자에게 할 일을 알려줍니다.

488
00:27:14,671 --> 00:27:16,176
훈련 자료에 적합하다.

489
00:27:16,176 --> 00:27:18,053
우리는 스스로를 인도 할 수있다.

490
00:27:18,053 --> 00:27:20,307
때로는이 이상한 상황에 빠지기도합니다.

491
00:27:20,307 --> 00:27:24,079
분류기는 직관적이지 않은 동작을 할 수 있습니다.

492
00:27:24,079 --> 00:27:27,786
그래서 이런 종류의 구체적이고 표준적인 예는,

493
00:27:27,786 --> 00:27:29,985
그건 그렇고, 이것은 더 이상 선형 분류가 아니며,

494
00:27:29,985 --> 00:27:31,426
이것은 좀 더 일반적인 것입니다.

495
00:27:31,426 --> 00:27:32,918
기계 학습 개념,

496
00:27:32,918 --> 00:27:35,815
우리가이 파란 점의 데이터 세트를 가지고 있다고 가정하면,

497
00:27:35,815 --> 00:27:38,668
우리는 훈련 데이터에 약간의 곡선을 맞출 것입니다.

498
00:27:38,668 --> 00:27:39,827
파란 점들,

499
00:27:39,827 --> 00:27:42,783
그런 다음 우리가 분류 자에게 말했던 유일한 것

500
00:27:42,783 --> 00:27:44,532
훈련 자료를 시험하고 맞추는 것,

501
00:27:44,532 --> 00:27:46,451
들어가서 아주 휘어지는 커브를 가질 수 있습니다.

502
00:27:46,451 --> 00:27:47,812
완벽하게 분류하려고 시도하다.

503
00:27:47,812 --> 00:27:49,647
모든 교육 데이터 포인트.

504
00:27:49,647 --> 00:27:52,235
그러나 우리가 실제로 걱정하지 않기 때문에 이것은 나쁘다.

505
00:27:52,235 --> 00:27:53,519
이 성능에 대해,

506
00:27:53,519 --> 00:27:56,329
우리는 테스트 데이터의 성능에 신경을 썼다.

507
00:27:56,329 --> 00:27:58,379
이제 새로운 데이터가 있으면

508
00:27:58,379 --> 00:28:00,710
그 종류의 추세는 같은 경향을 따르고 있습니다.

509
00:28:00,710 --> 00:28:02,267
이 아주 휘황 찬란 선

510
00:28:02,267 --> 00:28:03,872
완전히 틀리게 될 것입니다.

511
00:28:03,872 --> 00:28:05,601
그리고 사실 우리가 선호했을만한 것은

512
00:28:05,601 --> 00:28:07,806
할 분류 기준은 아마도 예측할 수 있습니다.

513
00:28:07,806 --> 00:28:09,334
이 똑바로 녹색 선,

514
00:28:09,334 --> 00:28:11,683
이 복잡한 위그 라인보다

515
00:28:11,683 --> 00:28:15,171
모든 교육 데이터에 완벽하게 맞춰야합니다.

516
00:28:15,171 --> 00:28:17,821
그리고 이것은 근본적인 근본적인 문제입니다.

517
00:28:17,821 --> 00:28:19,232
기계 학습에서,

518
00:28:19,232 --> 00:28:20,662
우리가 일반적으로 해결하는 방법,

519
00:28:20,662 --> 00:28:22,816
이 정례화의 개념입니다.

520
00:28:22,816 --> 00:28:25,159
여기에 추가 용어를 추가 할 것입니다.

521
00:28:25,159 --> 00:28:26,443
손실 함수.

522
00:28:26,443 --> 00:28:27,832
데이터 손실 외에도,

523
00:28:27,832 --> 00:28:30,255
그것은 우리 분류 자에게 적합하다고 말해 줄 것입니다.

524
00:28:30,255 --> 00:28:32,448
교육 데이터는 일반적으로 추가 할 것입니다.

525
00:28:32,448 --> 00:28:34,309
손실 함수에 대한 또 다른 용어

526
00:28:34,309 --> 00:28:36,057
정규화 용어 (regularization term)

527
00:28:36,057 --> 00:28:40,691
모델이 어떻게 든 더 단순한 W를 선택하도록 장려한다.

528
00:28:40,691 --> 00:28:42,492
단순한 개념

529
00:28:42,492 --> 00:28:45,992
종류는 작업과 모델에 따라 다릅니다.

530
00:28:47,925 --> 00:28:49,639
Occam의 면도날에 대한 아이디어가 있습니다.

531
00:28:49,639 --> 00:28:52,868
이것은 과학적 발견에서의이 근본적인 생각이다.

532
00:28:52,868 --> 00:28:55,574
더 넓게, 그것은 당신이 많은 다른

533
00:28:55,574 --> 00:28:57,713
경쟁 가설은 설명 할 수있다.

534
00:28:57,713 --> 00:28:59,158
너의 관찰,

535
00:28:59,158 --> 00:29:01,039
당신은 일반적으로 더 간단한 것을 선호해야합니다,

536
00:29:01,039 --> 00:29:03,399
그것이 그 가능성이 더 높은 설명이기 때문입니다.

537
00:29:03,399 --> 00:29:06,801
미래의 새로운 관측에 일반화하기.

538
00:29:06,801 --> 00:29:08,714
그리고 우리가이 직감을 조작하는 방법

539
00:29:08,714 --> 00:29:10,977
기계 학습에서 일반적으로 몇 가지 명시 적으로

540
00:29:10,977 --> 00:29:12,538
정규화 벌금

541
00:29:12,538 --> 00:29:15,121
그것은 종종 R로 기록됩니다.

542
00:29:16,312 --> 00:29:18,687
그러면 표준 손실 함수

543
00:29:18,687 --> 00:29:20,409
일반적으로이 두 가지 용어가 있습니다.

544
00:29:20,409 --> 00:29:22,417
데이터 손실 및 정규화 손실,

545
00:29:22,417 --> 00:29:25,130
여기에 하이퍼 파라미터가 있습니다, 람다,

546
00:29:25,130 --> 00:29:27,200
그 둘 사이에 거래됩니다.

547
00:29:27,200 --> 00:29:28,952
그리고 우리는 하이퍼 파라미터에 대해 이야기했습니다.

548
00:29:28,952 --> 00:29:31,054
마지막 강연에서 상호 유효성 검사,

549
00:29:31,054 --> 00:29:33,820
그래서이 정규화 하이퍼 파라미터 람다

550
00:29:33,820 --> 00:29:35,797
더 중요한 것 중 하나가 될 것이다.

551
00:29:35,797 --> 00:29:37,280
훈련 할 때 조정할 필요가있다.

552
00:29:37,280 --> 00:29:39,363
실제로 이러한 모델.

553
00:29:40,229 --> 00:29:41,062
문제?

554
00:29:42,097 --> 00:29:44,679
- [학생] 그 람다 R 용어는 무엇입니까?

555
00:29:44,679 --> 00:29:48,846
희미하게 말하기와 관련이 있습니다.

556
00:29:50,685 --> 00:29:51,941
- 그래, 문제는,

557
00:29:51,941 --> 00:29:53,850
이 람다 R W 용어 사이의 연결은 무엇인가?

558
00:29:53,850 --> 00:29:55,405
실제로이 위그 라인을 강요합니다.

559
00:29:55,405 --> 00:29:58,072
똑바로 푸른 선이 되려면?

560
00:29:59,912 --> 00:30:01,319
나는 이것에 관한 파생을 겪고 싶지 않았다.

561
00:30:01,319 --> 00:30:03,550
나는 그것이 우리를 너무 멀게
이끌 것이라고 생각했기 때문에,

562
00:30:03,550 --> 00:30:04,768
그러나 당신은 상상할 수 있습니다.

563
00:30:04,768 --> 00:30:06,310
어쩌면 당신은 회귀 문제를 겪고 있습니다.

564
00:30:06,310 --> 00:30:09,830
다른 다항식 기초 함수의 관점에서,

565
00:30:09,830 --> 00:30:13,921
이 회귀 패널티를 추가하는 경우,

566
00:30:13,921 --> 00:30:15,775
어쩌면 모델은 다항식에 접근 할 수 있습니다.

567
00:30:15,775 --> 00:30:18,316
매우 높은 수준이지만,이 회귀 기간을 통해

568
00:30:18,316 --> 00:30:20,715
당신은 모델이 다항식을 선호하도록 장려 할 수 있습니다.

569
00:30:20,715 --> 00:30:23,649
낮은 학위의, 그들은 적절하게 데이터에 맞는 경우,

570
00:30:23,649 --> 00:30:25,569
또는 데이터가 비교적 잘 맞는지 여부.

571
00:30:25,569 --> 00:30:29,021
그래서 이것을 할 수있는 두 가지
방법이 있다고 상상할 수 있습니다.

572
00:30:29,021 --> 00:30:30,681
모델 클래스를 제한 할 수 있습니다.

573
00:30:30,681 --> 00:30:32,897
더 강력한,

574
00:30:32,897 --> 00:30:36,337
더 복잡한 모델을 만들거나이 소프트
페널티를 추가 할 수 있습니다.

575
00:30:36,337 --> 00:30:40,846
모델이 여전히 더 복잡한 모델에 액세스 할 수있는 경우,

576
00:30:40,846 --> 00:30:43,112
이 경우에는 다차원 다항식 일 수도 있지만,

577
00:30:43,112 --> 00:30:44,664
하지만이 부드러운 제약 조건을 추가하면됩니다.

578
00:30:44,664 --> 00:30:47,921
이 복잡한 모델을 사용하기를 원한다면,

579
00:30:47,921 --> 00:30:49,648
이 형벌을 극복해야합니다.

580
00:30:49,648 --> 00:30:51,316
그들의 복잡성을 사용합니다.

581
00:30:51,316 --> 00:30:52,949
여기는 연결입니다.

582
00:30:52,949 --> 00:30:55,282
그건 꽤 선형 분류되지 않습니다,

583
00:30:55,282 --> 00:30:57,858
이것은 많은 사람들이 염두에두고있는 그림입니다.

584
00:30:57,858 --> 00:31:01,691
적어도 정규화에 대해 생각할 때.

585
00:31:02,731 --> 00:31:04,643
그래서 실제로는 여러 가지 유형이 있습니다.

586
00:31:04,643 --> 00:31:06,917
실제로 사용되는 정규화.

587
00:31:06,917 --> 00:31:09,874
가장 보편적 인 것은 아마 L2 정규화 일 것이다.

588
00:31:09,874 --> 00:31:11,022
또는 체중 감량.

589
00:31:11,022 --> 00:31:13,905
그러나 당신이 볼 수있는 다른 것들이 많이 있습니다.

590
00:31:13,905 --> 00:31:17,973
이 L2 정규화는 단지 유클리드 표준입니다.

591
00:31:17,973 --> 00:31:19,470
이 가중 벡터 (W)

592
00:31:19,470 --> 00:31:21,892
때로는 제곱 된 규범입니다.

593
00:31:21,892 --> 00:31:24,021
또는 때때로 제곱 된 표준의 절반

594
00:31:24,021 --> 00:31:25,480
파생 상품을 없애기 때문에

595
00:31:25,480 --> 00:31:26,738
조금 더 좋네.

596
00:31:26,738 --> 00:31:28,832
그러나 L2 정규화의 아이디어

597
00:31:28,832 --> 00:31:30,702
당신은 단지 유클리드 표준을 처벌하고 있습니까?

598
00:31:30,702 --> 00:31:32,650
이 가중치 벡터의

599
00:31:32,650 --> 00:31:35,593
또한 때로는 L1 정규화를 볼 수도 있습니다.

600
00:31:35,593 --> 00:31:38,957
여기서 우리는 가중치 벡터의 L1 표준에 불이익을 당하고,

601
00:31:38,957 --> 00:31:42,525
L1 정규화에는 좋은 속성이 있습니다.

602
00:31:42,525 --> 00:31:46,401
이 행렬에서 격려하는 희소성처럼.

603
00:31:46,401 --> 00:31:47,692
다른 것들을 볼 수도 있습니다.

604
00:31:47,692 --> 00:31:49,815
이 탄성 망 정규화일까요?

605
00:31:49,815 --> 00:31:52,744
L1과 L2의 조합입니다.

606
00:31:52,744 --> 00:31:55,837
때로는 최대 표준 정규화를 볼 수 있습니다.

607
00:31:55,837 --> 00:32:00,004
L1 또는 L2 표준이 아닌 최대 표준을 페널티.

608
00:32:01,119 --> 00:32:03,025
그러나 이러한 정례화

609
00:32:03,025 --> 00:32:05,792
당신이 깊은 학습에서 보는 것뿐만 아니라,

610
00:32:05,792 --> 00:32:07,374
그러나 기계 학습의 많은 분야에 걸쳐

611
00:32:07,374 --> 00:32:10,997
심지어는 더 광범위하게 최적화 할 수도 있습니다.

612
00:32:10,997 --> 00:32:13,091
이후의 강의에서는

613
00:32:13,091 --> 00:32:15,726
특정 유형의 정규화 유형

614
00:32:15,726 --> 00:32:17,138
깊은 학습.

615
00:32:17,138 --> 00:32:19,602
예를 들어 드롭 아웃을 위해, 우리는
몇 가지 강의를 보게 될 것입니다.

616
00:32:19,602 --> 00:32:22,882
또는 배치 정규화, 확률 적 깊이,

617
00:32:22,882 --> 00:32:25,157
최근 몇 년 동안 이러한 일들이 미쳐 버렸습니다.

618
00:32:25,157 --> 00:32:26,761
그러나 정규화의 전체 아이디어

619
00:32:26,761 --> 00:32:29,329
당신이 당신의 모델에 대해하는 일이라면,

620
00:32:29,329 --> 00:32:32,557
그런 종류의 모델은 모델의 복잡성을 어떻게 든 처벌합니다.

621
00:32:32,557 --> 00:32:37,061
명시 적으로 훈련 데이터에 맞추려고하지 않고

622
00:32:37,061 --> 00:32:38,306
문제?

623
00:32:38,306 --> 00:32:41,889
[희미하게 말하는 학생]

624
00:32:44,858 --> 00:32:46,104
그래, 문제는,

625
00:32:46,104 --> 00:32:48,604
L2 정규화는 복잡성을 어떻게 측정합니까?

626
00:32:48,604 --> 00:32:50,186
모델의?

627
00:32:50,186 --> 00:32:52,035
고맙게도 우리는 바로 여기에 그 예가 있습니다.

628
00:32:52,035 --> 00:32:54,202
어쩌면 우리는 걸을 수 있습니다.

629
00:32:55,437 --> 00:32:57,779
그래서 여기서 우리는 아마도
훈련 예를 가질 것입니다, x,

630
00:32:57,779 --> 00:33:00,058
우리가 고려하고있는 두 가지 W가 있습니다.

631
00:33:00,058 --> 00:33:02,673
그래서 x는이 4 개의 벡터입니다.

632
00:33:02,673 --> 00:33:06,324
우리는이 두 가지 가능성을 고려하고 있습니다.

633
00:33:06,324 --> 00:33:07,157
W.

634
00:33:07,157 --> 00:33:08,978
하나는 처음에는 하나, 하나는 하나입니다.

635
00:33:08,978 --> 00:33:10,367
그리고 세 개의 0,

636
00:33:10,367 --> 00:33:12,530
다른 하나는이 0.25의 퍼짐을 가로 지르게됩니다

637
00:33:12,530 --> 00:33:14,191
네 가지 항목.

638
00:33:14,191 --> 00:33:15,898
그리고 지금, 선형 분류를 할 때,

639
00:33:15,898 --> 00:33:17,299
우리는 실제로 점 제품을 복용하고 있습니다.

640
00:33:17,299 --> 00:33:19,702
우리 x와 우리 W 사이.

641
00:33:19,702 --> 00:33:22,533
그래서 선형 분류의 관점에서,

642
00:33:22,533 --> 00:33:24,747
이 두 개의 W는 동일합니다.

643
00:33:24,747 --> 00:33:26,319
그들은 같은 결과를주기 때문에

644
00:33:26,319 --> 00:33:28,302
x로 도트를 찍을 때.

645
00:33:28,302 --> 00:33:29,635
그러나 이제 문제는,

646
00:33:29,635 --> 00:33:31,300
이 두 가지 예를 보면,

647
00:33:31,300 --> 00:33:34,383
어느 것이 L2 회귀를 선호합니까?

648
00:33:36,052 --> 00:33:39,300
그래, L2 회귀는 W2보다 더 좋아질거야.

649
00:33:39,300 --> 00:33:41,030
왜냐하면 그것은 더 작은 규범을 가지고 있기 때문입니다.

650
00:33:41,030 --> 00:33:44,854
그래서 대답은 L2 회귀

651
00:33:44,854 --> 00:33:47,017
분류기의 복잡성을 측정합니다.

652
00:33:47,017 --> 00:33:49,440
이 비교적 거친 방식으로,

653
00:33:49,440 --> 00:33:51,644
그 아이디어는,

654
00:33:51,644 --> 00:33:54,557
선형 분류의 W를 기억하십시오.

655
00:33:54,557 --> 00:33:56,915
얼마나 많이 해석 했는가?

656
00:33:56,915 --> 00:33:59,551
이 벡터의 값 x

657
00:33:59,551 --> 00:34:01,920
이 출력 클래스에 해당합니까?

658
00:34:01,920 --> 00:34:04,202
그래서 L2 정규화가 말하고 있습니다.

659
00:34:04,202 --> 00:34:06,245
그 영향력을 퍼뜨리는 것이 더 좋아

660
00:34:06,245 --> 00:34:09,080
x의 모든 다른 값에 걸쳐.

661
00:34:09,080 --> 00:34:11,038
어쩌면 이것이 더 강력 할 수도 있습니다.

662
00:34:11,039 --> 00:34:14,585
당신이 변화하는 xs를 생각해 내면,

663
00:34:14,585 --> 00:34:16,687
그러면 우리의 결정이 확산됩니다.

664
00:34:16,687 --> 00:34:18,359
전체 x 벡터에 의존하며,

665
00:34:18,359 --> 00:34:20,205
특정 요소에만 의존하기보다

666
00:34:20,205 --> 00:34:22,059
x 벡터의.

667
00:34:22,060 --> 00:34:23,946
그런데, L1 정규화

668
00:34:23,946 --> 00:34:26,839
이 반대 해석이있다.

669
00:34:26,839 --> 00:34:29,357
그래서 우리가 L1 정규화를 사용한다면,

670
00:34:29,357 --> 00:34:32,946
그러면 실제로 W2보다 W1을 선호 할 것입니다.

671
00:34:32,946 --> 00:34:35,595
L1 정규화가이 다른 개념을 가지고 있기 때문에

672
00:34:35,595 --> 00:34:39,762
복잡성에 대해서는 모델이 덜 복잡하고,

673
00:34:41,569 --> 00:34:44,867
어쩌면 우리는 0의 수로 모델의
복잡성을 측정 할 것입니다.

674
00:34:44,867 --> 00:34:46,080
상기 웨이트 벡터에서,

675
00:34:46,080 --> 00:34:49,332
그래서 우리는 어떻게 복잡성을 측정 할 것인가에 대한 질문

676
00:34:49,332 --> 00:34:51,917
L2는 어떻게 복잡성을 측정합니까?

677
00:34:51,917 --> 00:34:54,196
그들은 일종의 문제에 의존합니다.

678
00:34:54,196 --> 00:34:57,126
그리고 특정 설정에 대해 생각해야합니다.

679
00:34:57,126 --> 00:34:58,921
특정 모델 및 데이터의 경우,

680
00:34:58,921 --> 00:35:01,754
그 복잡성을 어떻게 측정해야한다고 생각하니?

681
00:35:01,754 --> 00:35:02,837
이 일에?

682
00:35:03,921 --> 00:35:04,788
문제?

683
00:35:04,788 --> 00:35:07,040
- [Student] 그러면 L1이 W1을 선호하는 이유는 무엇입니까?

684
00:35:07,040 --> 00:35:09,129
그들은 같은 사람과 합쳐지지 않습니까?

685
00:35:09,129 --> 00:35:10,385
- 네, 맞습니다.

686
00:35:10,385 --> 00:35:12,330
따라서이 경우 L1은 실제로 동일합니다.

687
00:35:12,330 --> 00:35:13,830
이 둘 사이.

688
00:35:15,193 --> 00:35:18,018
그러나 이것과 비슷한 예를 만들 수 있습니다.

689
00:35:18,018 --> 00:35:21,546
여기서 W1은 L1 정규화에 의해 선호 될 것이다.

690
00:35:21,546 --> 00:35:24,643
나는 L1 뒤의 일반적인 직관력을 추측한다.

691
00:35:24,643 --> 00:35:26,908
일반적으로 스파 스 솔루션을 선호한다는 것입니다.

692
00:35:26,908 --> 00:35:30,558
W의 모든 항목을 0으로 만듭니다.

693
00:35:30,558 --> 00:35:31,954
커플의 경우를 제외하고는

694
00:35:31,954 --> 00:35:35,016
여기서 0에서 벗어나는 것이 허용됩니다.

695
00:35:35,016 --> 00:35:37,945
L1의 복잡성을 측정하는 방법

696
00:35:37,945 --> 00:35:40,191
어쩌면 0이 아닌 항목의 수,

697
00:35:40,191 --> 00:35:43,677
그리고 나서 L2에 대해서, 그것은 W를 퍼뜨리는 것들

698
00:35:43,677 --> 00:35:45,682
모든 값에 걸쳐 덜 복잡한 있습니다.

699
00:35:45,682 --> 00:35:48,920
따라서 귀하의 데이터에 따라 다르며
귀하의 문제에 달려 있습니다.

700
00:35:48,920 --> 00:35:51,593
오 그런데, 만일 당신이 하드 코어 베이지안이라면,

701
00:35:51,593 --> 00:35:54,584
다음 L2 정규화를 사용 하여이 좋은 해석이 있습니다

702
00:35:54,584 --> 00:35:57,206
이전의 Gaussian에 의한 MAP 추론

703
00:35:57,206 --> 00:35:58,897
매개 변수 벡터에.

704
00:35:58,897 --> 00:36:00,430
나는 그것에 관한 숙제 문제가 있다고 생각한다.

705
00:36:00,430 --> 00:36:02,496
229 년에 우리는 그것에 대해 이야기하지 않을 것입니다.

706
00:36:02,496 --> 00:36:05,343
나머지 분기 동안.

707
00:36:05,343 --> 00:36:08,105
그것은 나의 길고 깊은 잠수의 종류 다.

708
00:36:08,105 --> 00:36:11,400
다중 클래스 SVM 손실로.

709
00:36:11,400 --> 00:36:12,783
문제?

710
00:36:12,783 --> 00:36:14,399
- [Student] 그래, 아직도 혼란 스럽네.

711
00:36:14,399 --> 00:36:17,646
내가해야 할 일들에 대해

712
00:36:17,646 --> 00:36:20,979
선형 대 다항식 일 때,

713
00:36:24,590 --> 00:36:27,673
이 손실 함수의 사용

714
00:36:29,007 --> 00:36:32,111
당신이하고있는 사실을 바꾸지 않을 것입니다.

715
00:36:32,111 --> 00:36:33,982
당신은 선형 분류자를보고 있습니다, 맞습니까?

716
00:36:33,982 --> 00:36:36,905
- 그래, 문제는 그게,

717
00:36:36,905 --> 00:36:37,825
정규화 추가

718
00:36:37,825 --> 00:36:39,798
가설 수업을 변경하지 않을 것입니다.

719
00:36:39,798 --> 00:36:44,036
이것은 선형 분류 자로부터 우리를
멀어지게 만들지 않을 것입니다.

720
00:36:44,036 --> 00:36:45,857
아이디어는 아마도이 예제입니다.

721
00:36:45,857 --> 00:36:47,491
이 다항식 회귀 분석

722
00:36:47,491 --> 00:36:50,038
확실히 선형 회귀가 아닙니다.

723
00:36:50,038 --> 00:36:52,174
그것은 선형 회귀로 볼 수 있습니다.

724
00:36:52,174 --> 00:36:56,826
입력의 다항식 확장 위에,

725
00:36:56,826 --> 00:36:59,712
그리고이 경우에,이 회귀는

726
00:36:59,712 --> 00:37:03,232
당신은 많은 다항식을 사용할 수 없다는 것을

727
00:37:03,232 --> 00:37:05,802
아마도 계수가 있어야합니다.

728
00:37:05,802 --> 00:37:07,385
맞아, 네가 상상할 수 있듯이,

729
00:37:07,385 --> 00:37:09,290
당신이 다항식 회귀를 할 때,

730
00:37:09,290 --> 00:37:11,762
당신은 x의 f로서 다항식을 쓸 수 있습니다.

731
00:37:11,762 --> 00:37:16,187
같음 A 제로 더하기 A 하나 x 더하기 두 x 제곱

732
00:37:16,187 --> 00:37:17,963
플러스 3 x 무엇이든,

733
00:37:17,963 --> 00:37:20,343
이 경우 매개 변수, W,

734
00:37:20,343 --> 00:37:23,093
이것들은 As 일 것입니다. 그 경우,

735
00:37:24,211 --> 00:37:26,154
W에게 벌칙을 주면 그것을 강요 할 수있다.

736
00:37:26,154 --> 00:37:28,190
낮은 차수의 다항식쪽으로.

737
00:37:28,190 --> 00:37:30,139
다항식 회귀의 경우를 제외하고,

738
00:37:30,139 --> 00:37:31,491
당신은 실제로 매개 변수화하고 싶지 않다.

739
00:37:31,491 --> 00:37:33,526
As의 측면에서 보면 다른 매개 변수가 있습니다.

740
00:37:33,526 --> 00:37:34,725
당신이 사용하고 싶은,

741
00:37:34,725 --> 00:37:36,364
그러나 그것은 일반적인 생각입니다,

742
00:37:36,364 --> 00:37:38,871
당신이 모델의 매개 변수를 처벌하는 것과 같습니다.

743
00:37:38,871 --> 00:37:41,868
더 간단한 가설을 향해 그것을 강요하는 것

744
00:37:41,868 --> 00:37:44,285
당신의 가설 수업 내에서.

745
00:37:45,229 --> 00:37:46,956
그리고 아마 우리는 이것을 오프라인으로 가져갈 수 있습니다.

746
00:37:46,956 --> 00:37:50,340
그래도 조금 혼란 스럽다면.

747
00:37:50,340 --> 00:37:54,589
그래서 우리는이 다중 클래스 SVM 손실을 보았습니다.

748
00:37:54,589 --> 00:37:56,349
그리고 옆의 쪽지로서 단지 길 옆에서,

749
00:37:56,349 --> 00:38:00,516
이것은 하나의 확장 또는 SVM 손실의 일반화입니다.

750
00:38:01,924 --> 00:38:03,097
여러 클래스에,

751
00:38:03,097 --> 00:38:04,347
실제로 몇 가지 다른 공식이 있습니다.

752
00:38:04,347 --> 00:38:06,596
문학에서 주위를 볼 수있는,

753
00:38:06,596 --> 00:38:09,848
하지만 내 직감이란 모두가 일하는
경향이 있다는 것을 의미합니다.

754
00:38:09,848 --> 00:38:11,755
실제로 마찬가지로,

755
00:38:11,755 --> 00:38:13,813
적어도 깊은 학습의 맥락에서.

756
00:38:13,813 --> 00:38:16,449
그래서 우리는이 특별한 공식을 고수 할 것입니다.

757
00:38:16,449 --> 00:38:19,949
이 클래스의 다중 클래스 SVM 손실

758
00:38:21,061 --> 00:38:23,145
물론 다양한 손실 함수가 있습니다.

759
00:38:23,145 --> 00:38:25,158
당신은 상상할 수도 있습니다.

760
00:38:25,158 --> 00:38:27,270
그리고 또 다른 인기있는 선택,

761
00:38:27,270 --> 00:38:30,603
다중 클래스 SVM 손실 외에도,

762
00:38:31,761 --> 00:38:33,758
깊은 학습에서 또 다른 인기있는 선택

763
00:38:33,758 --> 00:38:36,269
이 다항식 로지스틱 회귀 분석은,

764
00:38:36,269 --> 00:38:37,769
또는 소프트 맥스 손실.

765
00:38:39,405 --> 00:38:41,298
그리고 이것은 실제로 좀 더 일반적 일 것입니다.

766
00:38:41,298 --> 00:38:43,222
깊은 학습의 맥락에서,

767
00:38:43,222 --> 00:38:48,127
그러나 나는 무엇인가의 이유로이
순간을 발표하기로 결정했다.

768
00:38:48,127 --> 00:38:51,742
멀티 클래스 SVM 손실이라는 맥락에서 기억하십시오.

769
00:38:51,742 --> 00:38:53,651
우리는 실제로 해석이 없었습니다.

770
00:38:53,651 --> 00:38:55,096
그 점수는.

771
00:38:55,096 --> 00:38:56,900
우리가 어떤 분류를 할 때,

772
00:38:56,900 --> 00:39:00,524
우리의 모델 F는이 10 개의 숫자를 뱉어냅니다.

773
00:39:00,524 --> 00:39:02,670
수업에 대한 우리 점수 인

774
00:39:02,670 --> 00:39:04,681
다중 클래스 SVM의 경우,

775
00:39:04,681 --> 00:39:07,787
우리는 실제로 그 점수에 많은 해석을하지 않았습니다.

776
00:39:07,787 --> 00:39:09,726
우리는 단지 우리가 진정한 점수를 원한다고 말했다.

777
00:39:09,726 --> 00:39:11,112
올바른 클래스의 점수

778
00:39:11,112 --> 00:39:13,497
잘못된 클래스보다 커야합니다.

779
00:39:13,497 --> 00:39:17,712
그 이상으로 우리는 그 점수가 무엇을
의미하는지 실제로 말하지 않습니다.

780
00:39:17,712 --> 00:39:21,712
그러나 이제는 다항 로지스틱 회귀 분석에서

781
00:39:23,258 --> 00:39:25,625
우리는 실제로 이러한 점수를 부여 할 것입니다.

782
00:39:25,625 --> 00:39:27,668
몇 가지 추가적인 의미가 있습니다.

783
00:39:27,668 --> 00:39:29,715
그리고 특히 우리는 그 점수를 사용할 것입니다.

784
00:39:29,715 --> 00:39:32,264
확률 분포 계산

785
00:39:32,264 --> 00:39:33,907
우리 수업 이상.

786
00:39:33,907 --> 00:39:37,324
그래서 우리는 소위 softmax 함수를 사용합니다.

787
00:39:37,324 --> 00:39:39,775
우리가 우리의 모든 점수를받는 곳에,

788
00:39:39,775 --> 00:39:43,192
우리는 그것들이 이제는
긍정적이게되도록 그들을 배가시킵니다.

789
00:39:43,192 --> 00:39:46,540
그런 다음 이들 지수의 합으로 다시 정규화합니다.

790
00:39:46,540 --> 00:39:49,140
그래서 지금 우리가 점수를 보낸 후

791
00:39:49,140 --> 00:39:50,636
이 softmax 기능을 통해,

792
00:39:50,636 --> 00:39:53,053
이제 우리는이 확률 분포로 끝나고,

793
00:39:53,053 --> 00:39:55,792
지금 우리는 우리 수업보다 확률이 높습니다.

794
00:39:55,792 --> 00:39:58,193
각각의 확률은 0과 1 사이이며,

795
00:39:58,193 --> 00:40:01,370
모든 클래스에 걸친 확률의 합

796
00:40:01,370 --> 00:40:02,287
하나에 합계.

797
00:40:03,954 --> 00:40:07,169
그리고 이제 해석은 우리가 원하는 것입니다.

798
00:40:07,169 --> 00:40:10,113
이 계산 된 확률 분포가 있습니다.

799
00:40:10,113 --> 00:40:12,020
그것은 우리의 점수에 의해 암시 된,

800
00:40:12,020 --> 00:40:14,650
우리는 이것을 목표와 비교하기를 원합니다.

801
00:40:14,650 --> 00:40:17,138
또는 진정한 확률 분포.

802
00:40:17,138 --> 00:40:19,166
우리가 그 고양이가 고양이라는 것을 알면,

803
00:40:19,166 --> 00:40:22,083
목표 확률 분포

804
00:40:22,083 --> 00:40:24,735
고양이에 모든 확률 질량을 넣을 것이고,

805
00:40:24,735 --> 00:40:26,904
그래서 우리는 고양이가 1과 같을 확률을 가질 것입니다.

806
00:40:26,904 --> 00:40:29,754
다른 모든 클래스에 대해서는 확률이 0입니다.

807
00:40:29,754 --> 00:40:31,612
이제 우리가하고 싶은 일은 격려입니다.

808
00:40:31,612 --> 00:40:33,528
우리의 계산 된 확률 분포

809
00:40:33,528 --> 00:40:35,574
이 softmax 함수에서 나온 것입니다.

810
00:40:35,574 --> 00:40:38,376
이 목표 확률 분포와 일치시킨다.

811
00:40:38,376 --> 00:40:40,671
올바른 클래스의 모든 질량을가집니다.

812
00:40:40,671 --> 00:40:42,175
그리고 우리가 이렇게하는 방법,

813
00:40:42,175 --> 00:40:45,181
내 말은, 당신은 여러 방면에서이
방정식을 할 수 있다는 것입니다.

814
00:40:45,181 --> 00:40:46,795
당신은 KL 발산으로 이것을 할 수 있습니다.

815
00:40:46,795 --> 00:40:48,283
표적 사이

816
00:40:48,283 --> 00:40:51,102
상기 계산 된 확률 분포,

817
00:40:51,102 --> 00:40:53,221
최대 우도 추정치로이를 수행 할 수 있습니다.

818
00:40:53,221 --> 00:40:54,466
그러나 하루가 끝날 때,

819
00:40:54,466 --> 00:40:56,474
우리가 정말로 원하는 것은 확률

820
00:40:56,474 --> 00:41:00,839
진정한 계급은 높고 하나에 가깝습니다.

821
00:41:00,839 --> 00:41:04,015
그래서 우리의 손실은 이제 음의 로그가 될 것입니다.

822
00:41:04,015 --> 00:41:06,389
진실한 계급의 확율의.

823
00:41:06,389 --> 00:41:08,151
이것은 우리가 이것을 넣기 때문에 혼란 스럽습니다.

824
00:41:08,151 --> 00:41:09,707
여러 다른 것들을 통해,

825
00:41:09,707 --> 00:41:11,745
그러나 우리는 확률을 원했다는 것을 기억하십시오.

826
00:41:11,745 --> 00:41:13,524
하나에 가까워지면,

827
00:41:13,524 --> 00:41:17,071
그래서 이제 로그는 단조로운 함수입니다.

828
00:41:17,071 --> 00:41:18,414
수학적으로 밝혀졌습니다.

829
00:41:18,414 --> 00:41:20,840
로그를 최대화하는 것이 더 쉽다.

830
00:41:20,840 --> 00:41:23,277
원시 확률을 최대화하는 것보다,

831
00:41:23,277 --> 00:41:25,604
그래서 우리는 일지를 고수합니다.

832
00:41:25,604 --> 00:41:26,984
그리고 이제 로그는 단조롭지 만,

833
00:41:26,984 --> 00:41:30,244
그래서 우리가 정확한 클래스의 로그 P를 최대화한다면,

834
00:41:30,244 --> 00:41:32,599
그건 우리가 그걸 원한다는 뜻이고,

835
00:41:32,599 --> 00:41:36,024
그러나 손실 함수는 좋지 않음을 측정한다.

836
00:41:36,024 --> 00:41:37,454
그래서 우리는 마이너스 1을 넣을 필요가있다.

837
00:41:37,454 --> 00:41:40,051
그것이 올바른 방향으로 나아갈 수 있도록.

838
00:41:40,051 --> 00:41:42,314
SVM에 대한 우리의 손실 함수

839
00:41:42,314 --> 00:41:44,648
확률의 마이너스 로그가 될 것입니다.

840
00:41:44,648 --> 00:41:46,148
진정한 계급의

841
00:41:48,909 --> 00:41:51,322
네, 여기 그것이 요약입니다.

842
00:41:51,322 --> 00:41:53,782
우리가 점수를 받고, 우리가 softmax를 돌며,

843
00:41:53,782 --> 00:41:56,075
이제 우리의 손실은 확률의 로그를 뺀 것이다.

844
00:41:56,075 --> 00:41:57,575
진정한 계급의

845
00:42:01,697 --> 00:42:03,743
좋아, 그럼이게 어떻게 생겼는지 봐.

846
00:42:03,743 --> 00:42:05,043
구체적인 예를 들면,

847
00:42:05,043 --> 00:42:07,749
그 다음 우리는 우리 아주 좋아하는 아름다운 고양이

848
00:42:07,749 --> 00:42:10,634
세 가지 예를 통해 우리는이 세 가지 점수를 얻었습니다.

849
00:42:10,634 --> 00:42:14,486
우리 선형 분류기에서 나오는

850
00:42:14,486 --> 00:42:16,296
이 점수는 정확하게 그들이했던 방식입니다.

851
00:42:16,296 --> 00:42:18,712
SVM 손실의 맥락에서

852
00:42:18,712 --> 00:42:20,826
하지만 이제는이 점수를받는 것보다

853
00:42:20,826 --> 00:42:22,817
그들을 우리의 손실 함수에 직접적으로 넣는 것,

854
00:42:22,817 --> 00:42:25,422
우리는 그것들을 모두 가져 와서 그들을 압도 할 것입니다.

855
00:42:25,422 --> 00:42:26,990
그래서 그들은 모두 긍정적입니다,

856
00:42:26,990 --> 00:42:29,095
그리고 우리는 그들을 정상화시켜 확실하게 할 것입니다.

857
00:42:29,095 --> 00:42:31,025
그들 모두가 하나가된다.

858
00:42:31,025 --> 00:42:33,788
그리고 이제 우리의 손실은 마이너스 로그가 될 것입니다.

859
00:42:33,788 --> 00:42:35,788
진실한 학급 점수의.

860
00:42:36,643 --> 00:42:38,893
그래서 그것은 softmax 손실입니다,

861
00:42:40,156 --> 00:42:43,823
다항 로지스틱 회귀 (multinomial
logistic regression)라고도합니다.

862
00:42:45,496 --> 00:42:47,253
이제 몇 가지 질문을했습니다.

863
00:42:47,253 --> 00:42:50,750
다중 클래스 SVM 손실에 대한
직감을 얻으려고 시도 할 때,

864
00:42:50,750 --> 00:42:53,778
동일한 질문에 대해 생각하는 것이 유용합니다.

865
00:42:53,778 --> 00:42:57,360
softmax 손실과 대비됩니다.

866
00:42:57,360 --> 00:42:58,614
그럼 질문은,

867
00:42:58,614 --> 00:43:02,697
Softmax 손실의 최소 및 최대 값은 얼마입니까?

868
00:43:04,984 --> 00:43:06,785
좋아, 아마 그렇게 확신 할 수는 없어.

869
00:43:06,785 --> 00:43:08,303
너무 많은 로그와 합계가 있습니다.

870
00:43:08,303 --> 00:43:09,720
여기 들어가.

871
00:43:11,298 --> 00:43:13,759
그래서 대답은 분 손실이 0이라는 것입니다.

872
00:43:13,759 --> 00:43:15,430
최대 손실은 무한대입니다.

873
00:43:15,430 --> 00:43:18,263
그리고 당신이 이것을 볼 수있는 방법,

874
00:43:19,422 --> 00:43:21,165
우리가 원하는 확률 분포

875
00:43:21,165 --> 00:43:24,467
올바른 클래스에 하나, 잘못된 클래스에 0,

876
00:43:24,467 --> 00:43:25,842
우리가하는 방식은,

877
00:43:25,842 --> 00:43:27,199
그래서 그 경우라면,

878
00:43:27,199 --> 00:43:31,366
로그 안에있는이 것은 결국 하나가 될 것이고,

879
00:43:33,662 --> 00:43:36,750
실제 클래스의 로그 확률이기 때문에,

880
00:43:36,750 --> 00:43:40,917
하나의 로그가 0 일 때, 하나의
로그를 뺀 것이 여전히 0입니다.

881
00:43:41,893 --> 00:43:44,001
그래서 우리가 그 일을 완전히 올바르게한다면,

882
00:43:44,001 --> 00:43:46,515
우리의 손실은 0이 될 것입니다.

883
00:43:46,515 --> 00:43:50,249
그러나 그런데, 그 일을 완전히 옳게하기 위해서,

884
00:43:50,249 --> 00:43:53,582
우리 점수는 어떻게 생겼을까요?

885
00:43:55,963 --> 00:43:57,252
불평, 불평.

886
00:43:57,252 --> 00:44:00,135
그래서 점수는 실제로 상당히 극단적으로되어야 할 것입니다.

887
00:44:00,135 --> 00:44:01,572
무한으로 향한 것처럼.

888
00:44:01,572 --> 00:44:04,384
그래서 우리는 실제로이 지수를 가지고 있기 때문에,

889
00:44:04,384 --> 00:44:06,098
이 정규화, 유일한 방법

890
00:44:06,098 --> 00:44:09,029
우리는 실제로 하나의 확률 분포를 얻을 수있다.

891
00:44:09,029 --> 00:44:11,970
그리고 제로는 실제로 무한 점수를 넣고 있습니다.

892
00:44:11,970 --> 00:44:16,006
올바른 클래스의 경우 및 무한대 점수를 뺀 경우

893
00:44:16,006 --> 00:44:17,509
잘못된 모든 클래스에 대해.

894
00:44:17,509 --> 00:44:20,652
그리고 컴퓨터는 무한 성으로 잘하지 못합니다.

895
00:44:20,652 --> 00:44:22,239
그래서 실제로, 당신은 결코 손실이 없어 질 것입니다.

896
00:44:22,239 --> 00:44:24,108
유한 정밀도를 가진이 물건에.

897
00:44:24,108 --> 00:44:25,755
그러나 당신은 여전히이 해석을 가지고 있습니다.

898
00:44:25,755 --> 00:44:29,223
여기서 0은 이론상의 최소 손실입니다.

899
00:44:29,223 --> 00:44:31,607
그리고 최대 손실은 제한되지 않습니다.

900
00:44:31,607 --> 00:44:35,180
우리가 확률 질량이 0 일 때

901
00:44:35,180 --> 00:44:39,483
올바른 클래스에서 마이너스 로그를 얻습니다.

902
00:44:39,483 --> 00:44:42,643
0의 로그는 마이너스 무한대이며,

903
00:44:42,643 --> 00:44:46,283
그래서 0의 마이너스 로그는 플러스 무한대가됩니다.

904
00:44:46,283 --> 00:44:47,334
그래서 그것은 정말로 나쁘다.

905
00:44:47,334 --> 00:44:49,071
그러나 다시, 당신은 결코 여기에 결코 도착하지 않을 것입니다.

906
00:44:49,071 --> 00:44:53,748
이 확률을 실제로 얻을 수있는 유일한 방법이기 때문에

907
00:44:53,748 --> 00:44:58,563
0이 되려면 올바른 클래스 점수에 대한 e가 0이고,

908
00:44:58,563 --> 00:45:00,093
그 올바른 클래스 점수가

909
00:45:00,093 --> 00:45:01,630
음의 무한대입니다.

910
00:45:01,630 --> 00:45:04,034
다시 한번 말하지만, 당신은 실제로 이러한
최소한의 것을 얻지 못할 것입니다.

911
00:45:04,034 --> 00:45:07,117
유한 정밀도의 최대 값.

912
00:45:08,863 --> 00:45:11,032
그럼 우리가이 디버깅을했는지 기억해.

913
00:45:11,032 --> 00:45:14,340
다중 클래스 SVM의 컨텍스트에서 온 전성
체크 (sanity check) 질문,

914
00:45:14,340 --> 00:45:16,401
우리는 softmax에 대해서도 같은 질문을 할 수 있습니다.

915
00:45:16,401 --> 00:45:19,138
모든 S가 작고 약 0이면,

916
00:45:19,138 --> 00:45:21,287
그러면 여기서 손실이 무엇입니까?

917
00:45:21,287 --> 00:45:22,292
그래, 대답?

918
00:45:22,292 --> 00:45:24,363
- [학생] C에서 1을 제외한 로그.

919
00:45:24,363 --> 00:45:27,045
- C에서 1의 로그를 뺀거야?

920
00:45:27,045 --> 00:45:28,795
내 생각 엔 그래,

921
00:45:30,026 --> 00:45:33,352
그래서 C를 넘어서는 하나의 로그를 뺀 것입니다.

922
00:45:33,352 --> 00:45:34,693
로그가 그 일을 뒤집을 수 있기 때문에

923
00:45:34,693 --> 00:45:36,526
그러면 C의 로그 일뿐입니다.

924
00:45:36,526 --> 00:45:38,079
네, 그래서 C의 로그 일뿐입니다.

925
00:45:38,079 --> 00:45:39,909
그리고 다시, 이것은 훌륭한 디버깅 일입니다.

926
00:45:39,909 --> 00:45:41,911
이 softmax 손실로 모델을 훈련하는 경우,

927
00:45:41,911 --> 00:45:43,977
첫 번째 반복을 확인해야합니다.

928
00:45:43,977 --> 00:45:47,894
로그 C가 아니면 뭔가 잘못되었습니다.

929
00:45:50,051 --> 00:45:53,257
그래서 우리는이 두 손실 함수를
비교하고 대조 할 수 있습니다.

930
00:45:53,257 --> 00:45:54,600
약간.

931
00:45:54,600 --> 00:45:56,111
선형 분류의 관점에서,

932
00:45:56,111 --> 00:45:57,532
이 설정은 동일하게 보입니다.

933
00:45:57,532 --> 00:45:59,246
곱해진 W 행렬이 있습니다.

934
00:45:59,246 --> 00:46:02,072
이 유령의 유령을 산출하기위한 우리의 의견에 반하여,

935
00:46:02,072 --> 00:46:04,046
이제 두 손실 함수의 차이점

936
00:46:04,046 --> 00:46:06,434
우리가 점수를 해석하는 방법은

937
00:46:06,434 --> 00:46:09,327
나중에 불량을 정량적으로 측정 할 수 있습니다.

938
00:46:09,327 --> 00:46:11,562
그래서 SVM을 위해 우리는
들어가서 마진을 살펴볼 것입니다.

939
00:46:11,562 --> 00:46:14,987
올바른 수업의 점수 사이에

940
00:46:14,987 --> 00:46:17,138
잘못된 수업의 점수,

941
00:46:17,138 --> 00:46:20,256
이 소프트 맥스 또는 크로스 엔트로피 손실에 대해서는,

942
00:46:20,256 --> 00:46:22,703
우리는 가야하고 확률 분포를 계산할 것입니다.

943
00:46:22,703 --> 00:46:24,980
그런 다음 마이너스 로그 확률을 살펴보십시오.

944
00:46:24,980 --> 00:46:26,663
올바른 클래스의.

945
00:46:26,663 --> 00:46:28,996
그래서 때때로 당신이 보면,

946
00:46:30,198 --> 00:46:33,216
관점에서, 나는 그 점을 건너 뛸 것이다.

947
00:46:33,216 --> 00:46:34,917
[웃음]

948
00:46:34,917 --> 00:46:36,358
흥미로운 또 다른 질문입니다.

949
00:46:36,358 --> 00:46:40,854
이 두 손실 함수를 대조하면 생각할 때,

950
00:46:40,854 --> 00:46:44,241
이 예제 포인트가 있다고 가정 해 보겠습니다.

951
00:46:44,241 --> 00:46:46,058
당신이 그것의 점수를 바꾸면,

952
00:46:46,058 --> 00:46:49,975
우리는이 점에 대해 세 가지 점수가 있다고 가정합니다.

953
00:46:52,859 --> 00:46:54,042
바닥에있는 부분을 무시하십시오.

954
00:46:54,042 --> 00:46:56,558
그러나이 예제로 돌아 가면

955
00:46:56,558 --> 00:46:59,814
여기서 다중 - 클래스 SVM 손실에서,

956
00:46:59,814 --> 00:47:04,144
우리가 차를 가지고 있었을 때,
차 점수가 훨씬 좋았습니다.

957
00:47:04,144 --> 00:47:06,293
모든 잘못된 수업보다

958
00:47:06,293 --> 00:47:08,546
그 차 이미지의 점수를 흔들어 쓴다.

959
00:47:08,546 --> 00:47:11,246
다중 클래스 SVM 손실을 전혀 변경하지 않았습니다.

960
00:47:11,246 --> 00:47:13,141
유일한 이유는 SVM 손실

961
00:47:13,141 --> 00:47:15,282
그 정확한 점수를 얻는 것에 관심이있었습니다.

962
00:47:15,282 --> 00:47:18,359
잘못된 점수보다 큰 여백보다 커야합니다.

963
00:47:18,359 --> 00:47:20,392
하지만 이제 softmax 손실은 실제로 상당히 다릅니다.

964
00:47:20,392 --> 00:47:21,726
이 점에서.

965
00:47:21,726 --> 00:47:24,174
softmax 손실은 실제로 항상 운전하기를 원합니다.

966
00:47:24,174 --> 00:47:26,438
그 확률 덩어리가 하나가 될 때까지.

967
00:47:26,438 --> 00:47:29,771
따라서 점수가 매우 높더라도

968
00:47:31,143 --> 00:47:32,606
올바른 수업에, 그리고 매우 낮은 점수

969
00:47:32,606 --> 00:47:34,298
모든 잘못된 수업에

970
00:47:34,298 --> 00:47:36,852
softmax 당신이 더 많은
확률 질량을 쌓기를 원할 것입니다

971
00:47:36,852 --> 00:47:40,044
올바른 수업을 듣고 점수를 계속 누르십시오.

972
00:47:40,044 --> 00:47:42,350
무한대쪽으로 올라가는 올바른 클래스의

973
00:47:42,350 --> 00:47:44,152
잘못된 수업의 점수

974
00:47:44,152 --> 00:47:46,138
음의 무한대쪽으로 내려 갔다.

975
00:47:46,138 --> 00:47:47,435
그래서 그것은 흥미로운 차이입니다.

976
00:47:47,435 --> 00:47:49,968
실제로이 두 손실 함수 사이.

977
00:47:49,968 --> 00:47:53,530
그 SVM, 막대 위에이 데이터 포인트를 가져올거야.

978
00:47:53,530 --> 00:47:55,920
정확하게 분류하고 그냥 포기하면

979
00:47:55,920 --> 00:47:57,739
그 데이터 포인트에 대해서는 더 이상 신경 쓰지 않습니다.

980
00:47:57,739 --> 00:48:00,296
softmax는 항상 지속적으로 개선하려고 노력하지만

981
00:48:00,296 --> 00:48:01,968
모든 단일 데이터 요소가 더 좋아지고 나아질 것입니다.

982
00:48:01,968 --> 00:48:03,838
그리고 더 좋고 더 낫다.

983
00:48:03,838 --> 00:48:05,405
그래서 그것은 흥미로운 차이입니다.

984
00:48:05,405 --> 00:48:07,378
이 두 기능 사이.

985
00:48:07,378 --> 00:48:09,974
실제로, 나는 그것이 큰 차이를 만들어
내지 않는 경향이 있다고 생각한다.

986
00:48:09,974 --> 00:48:12,240
당신이 선택하는 것은 그들이 수행하는 경향이 있습니다.

987
00:48:12,240 --> 00:48:14,040
꽤 유사하게,

988
00:48:14,040 --> 00:48:15,966
적어도 많은 깊은 학습 응용 프로그램.

989
00:48:15,966 --> 00:48:19,137
그러나 이러한 차이를 유지하는 것이 매우 유용합니다.

990
00:48:19,137 --> 00:48:19,970
마음에.

991
00:48:23,054 --> 00:48:26,176
네, 여기에서 우리가 어디로 왔는지 다시 정리하려면,

992
00:48:26,176 --> 00:48:29,585
xs와 ys의 데이터 세트를 가지고 있다는 것입니다.

993
00:48:29,585 --> 00:48:33,018
우리는 우리의 선형 분류기를 사용하여
몇 가지 점수 함수를 얻습니다.

994
00:48:33,018 --> 00:48:36,595
우리의 점수 S를 계산하기 위해서, 우리의 입력에서 x,

995
00:48:36,595 --> 00:48:38,311
그리고 나서 우리는 손실 함수를 사용할 것입니다,

996
00:48:38,311 --> 00:48:41,134
어쩌면 softmax 또는 SVM
또는 일부 다른 손실 함수

997
00:48:41,134 --> 00:48:45,997
양적으로 얼마나 나쁜지를 예측하는 것이었다.

998
00:48:45,997 --> 00:48:48,954
이 땅에 비하면 진정한 목표, y.

999
00:48:48,954 --> 00:48:52,410
그리고 나서 우리는 종종이 손실 함수를 증가시킬 것입니다.

1000
00:48:52,410 --> 00:48:53,849
정규화 용어로,

1001
00:48:53,849 --> 00:48:56,174
훈련 자료를 맞추는 사이에

1002
00:48:56,174 --> 00:48:59,059
더 단순한 모델을 선호합니다.

1003
00:48:59,059 --> 00:49:01,490
그래서 이것은 꽤 일반적인 개요입니다.

1004
00:49:01,490 --> 00:49:03,966
감독 학습이라고 불리는 많은 것을

1005
00:49:03,966 --> 00:49:07,065
우리가 앞으로 나아갈 때 깊은 학습에서 볼 수있는 것은,

1006
00:49:07,065 --> 00:49:10,888
일반적으로 당신은 어떤 함수
f를 지정하기를 원할 것입니다.

1007
00:49:10,888 --> 00:49:12,664
구조가 매우 복잡 할 수 있습니다.

1008
00:49:12,664 --> 00:49:14,489
결정하는 몇 가지 손실 함수를 지정하십시오.

1009
00:49:14,489 --> 00:49:18,028
당신의 알고리즘이 얼마나 잘하고 있는지,

1010
00:49:18,028 --> 00:49:19,645
파라미터의 임의의 값이 주어지면,

1011
00:49:19,645 --> 00:49:21,053
일부 정규화 용어

1012
00:49:21,053 --> 00:49:24,260
모델 복잡성을 처벌하는 방법

1013
00:49:24,260 --> 00:49:26,141
그런 다음이 것들을 하나로 결합합니다.

1014
00:49:26,141 --> 00:49:27,624
W를 찾으려고 노력해.

1015
00:49:27,624 --> 00:49:30,866
이 최종 손실 함수를 최소화합니다.

1016
00:49:30,866 --> 00:49:32,120
그러나 그때 질문은,

1017
00:49:32,120 --> 00:49:33,636
우리가 실제로 그 일을 어떻게 수행할까요?

1018
00:49:33,636 --> 00:49:37,132
손실을 최소화하는이 W를 실제로 어떻게 찾을 수 있습니까?

1019
00:49:37,132 --> 00:49:40,461
그리고 그것은 우리를 최적화 주제로 이끌고 있습니다.

1020
00:49:40,461 --> 00:49:43,033
그래서 우리가 최적화를 할 때,

1021
00:49:43,033 --> 00:49:45,495
나는 보통 걷는 관점에서 생각한다.

1022
00:49:45,495 --> 00:49:47,482
큰 계곡 주위에.

1023
00:49:47,482 --> 00:49:51,951
그래서이 큰 계곡을 걷고 있다는 생각이 들었습니다.

1024
00:49:51,951 --> 00:49:54,183
다른 산과 계곡과 시내와 함께

1025
00:49:54,183 --> 00:49:56,903
물건들, 그리고이 풍경의 모든 지점

1026
00:49:56,903 --> 00:50:00,729
파라미터 W의 일부 설정에 대응한다.

1027
00:50:00,729 --> 00:50:03,054
그리고이 계곡을 걸어 다니는이 작은 녀석입니다.

1028
00:50:03,054 --> 00:50:04,517
그리고 당신은 찾으려고 노력하고 있습니다.

1029
00:50:04,517 --> 00:50:06,216
이들 각 점의 높이,

1030
00:50:06,216 --> 00:50:10,728
미안하지만, W의 설정에 의해 초래 된 손실과 같습니다.

1031
00:50:10,728 --> 00:50:12,879
그리고이 작은 남자로서의 당신 직업

1032
00:50:12,879 --> 00:50:14,181
이 풍경을 돌아 다니며,

1033
00:50:14,181 --> 00:50:18,000
당신은 어떻게 든이 계곡의 바닥을 찾을 필요가 있습니다.

1034
00:50:18,000 --> 00:50:20,517
그리고 이것은 일반적으로 어려운 문제입니다.

1035
00:50:20,517 --> 00:50:22,851
너는 아마도 내가 정말로 영리하다고 생각할지도 모른다.

1036
00:50:22,851 --> 00:50:25,223
분석 속성에 대해 정말 열심히 생각할 수 있습니다.

1037
00:50:25,223 --> 00:50:27,379
내 손실 기능, 내 모든 정규화,

1038
00:50:27,379 --> 00:50:30,246
어쩌면 나는 미니 마이저를 적어 두거나,

1039
00:50:30,246 --> 00:50:33,089
그리고 그것은 마술처럼 순간 이동에 해당합니다

1040
00:50:33,089 --> 00:50:35,509
이 골짜기의 바닥까지.

1041
00:50:35,509 --> 00:50:38,740
그러나 실제로, 일단 당신의 예측 함수, f,

1042
00:50:38,740 --> 00:50:40,725
당신의 손실 함수와 당신의 정규식,

1043
00:50:40,725 --> 00:50:42,442
일단 이러한 것들이 크고 복잡 해지면

1044
00:50:42,442 --> 00:50:44,609
신경 네트워크를 사용하여,

1045
00:50:46,190 --> 00:50:48,055
적어 두려는 데별로 희망이 없습니다.

1046
00:50:48,055 --> 00:50:49,698
노골적인 분석 솔루션

1047
00:50:49,698 --> 00:50:52,017
그것은 당신을 미니 마에 직접 데려갑니다.

1048
00:50:52,017 --> 00:50:53,271
그래서 실제로

1049
00:50:53,271 --> 00:50:55,485
우리는 다양한 유형의 반복적 인
방법을 사용하는 경향이 있습니다

1050
00:50:55,485 --> 00:50:57,390
우리는 몇 가지 해결책으로 시작한다.

1051
00:50:57,390 --> 00:51:00,524
시간이 지남에 따라 서서히 개선하십시오.

1052
00:51:00,524 --> 00:51:03,357
그래서 아주 첫 번째, 어리석은 일

1053
00:51:04,527 --> 00:51:06,985
당신이 상상할 수있는 것은 무작위 검색입니다.

1054
00:51:06,985 --> 00:51:09,024
그것은 단지 W의 무리를 취할 것입니다,

1055
00:51:09,024 --> 00:51:12,180
무작위로 샘플링하여 손실 함수에 던져 넣습니다.

1056
00:51:12,180 --> 00:51:14,963
그들이 얼마나 잘하는지보십시오.

1057
00:51:14,963 --> 00:51:17,416
그래서 스포일러 경고, 이것은 정말 나쁜 알고리즘입니다,

1058
00:51:17,416 --> 00:51:18,828
당신은 아마 이것을 사용하면 안됩니다.

1059
00:51:18,828 --> 00:51:23,323
그러나 적어도 그것은 당신이 시도하는
것을 상상할 수도있는 한 가지입니다.

1060
00:51:23,323 --> 00:51:25,180
그리고 우리는 실제로 이것을 할 수 있습니다.

1061
00:51:25,180 --> 00:51:27,856
선형 분류기를 실제로 훈련 할 수 있습니다.

1062
00:51:27,856 --> 00:51:30,813
무작위 검색을 통해 CIFAR-10

1063
00:51:30,813 --> 00:51:34,152
그리고 이것을 위해 10 개의 클래스가 있습니다.

1064
00:51:34,152 --> 00:51:35,997
그래서 무작위 확률은 10 %입니다.

1065
00:51:35,997 --> 00:51:39,768
우리가 몇 가지 무작위 시도를했다면,

1066
00:51:39,768 --> 00:51:42,212
우리는 단지 투명한 바보 같은 운을 통하여 단지 발견했다.

1067
00:51:42,212 --> 00:51:45,645
어쩌면 15 %의 정확도를 가진 W의 일부 설정.

1068
00:51:45,645 --> 00:51:48,019
그래서 무작위보다는 낫다.

1069
00:51:48,019 --> 00:51:50,238
그러나 미술 수준은 아마 95 %

1070
00:51:50,238 --> 00:51:53,831
그래서 여기에 약간의 격차가 있습니다.

1071
00:51:53,831 --> 00:51:56,748
다시 한번 말하지만 실제로 이것을 사용하지 마십시오.

1072
00:51:56,748 --> 00:51:58,138
하지만 당신은 이것이 뭔가 있다고 상상할 수 있습니다.

1073
00:51:58,138 --> 00:52:00,677
당신은 잠재적으로 할 수 있습니다.

1074
00:52:00,677 --> 00:52:02,467
따라서 실제로는 더 나은 전략 일 것입니다.

1075
00:52:02,467 --> 00:52:04,664
실제로 일부 로컬 지오메트리를 사용하고 있습니다.

1076
00:52:04,664 --> 00:52:06,168
이 풍경.

1077
00:52:06,168 --> 00:52:07,614
그래서 당신이 걷고있는이 작은 녀석이라면

1078
00:52:07,614 --> 00:52:09,910
이 풍경 주변,

1079
00:52:09,910 --> 00:52:12,178
어쩌면 당신은 직접 경로를 볼 수 없습니다.

1080
00:52:12,178 --> 00:52:13,802
골짜기의 바닥에 이르기까지,

1081
00:52:13,802 --> 00:52:16,031
그러나 당신이 할 수있는 것은 당신의 발로 느끼는 것입니다.

1082
00:52:16,031 --> 00:52:19,531
로컬 지오메트리가 무엇인지 파악하고,

1083
00:52:20,697 --> 00:52:21,927
내가 여기 서 있으면,

1084
00:52:21,927 --> 00:52:24,145
어떤 방법으로 내리막 길을 조금 걸릴까요?

1085
00:52:24,145 --> 00:52:25,595
그래서 당신은 당신의 발로 느낄 수 있습니다.

1086
00:52:25,595 --> 00:52:28,136
지상의 기울기가 어디인지 느껴보십시오.

1087
00:52:28,136 --> 00:52:30,861
이 방향으로 조금 나를 쓰러 뜨 렸어?

1088
00:52:30,861 --> 00:52:32,649
그리고 그 방향으로 나아갈 수 있습니다.

1089
00:52:32,649 --> 00:52:34,037
그리고 너는 조금 내려갈 것이다.

1090
00:52:34,037 --> 00:52:36,260
당신의 발로 다시 어떤 느낌이 내려 졌는지 알아 내려고,

1091
00:52:36,260 --> 00:52:37,704
그런 다음 반복해서 반복하십시오.

1092
00:52:37,704 --> 00:52:39,529
당신이 바닥에 끝나기를 희망합니다.

1093
00:52:39,529 --> 00:52:41,526
결국 계곡의.

1094
00:52:41,526 --> 00:52:45,296
이것은 또한 상대적으로 간단한 알고리즘처럼 보입니다.

1095
00:52:45,296 --> 00:52:47,236
하지만 실제로 이것은 정말 잘 작동하는 경향이 있습니다.

1096
00:52:47,236 --> 00:52:50,195
실제로 모든 세부 사항을 올바르게 얻는다면.

1097
00:52:50,195 --> 00:52:52,209
이것이 일반적으로 우리가 따라야 할 전략입니다.

1098
00:52:52,209 --> 00:52:53,963
이 거대한 신경 네트워크를 훈련 할 때

1099
00:52:53,963 --> 00:52:57,028
선형 분류 자 및 다른 것들.

1100
00:52:57,028 --> 00:52:58,769
그래서, 그것은 물결 모양의 작은 손이었습니다.

1101
00:52:58,769 --> 00:52:59,952
그래서 사면은 무엇입니까?

1102
00:52:59,952 --> 00:53:02,337
미적분 클래스를 기억한다면,

1103
00:53:02,337 --> 00:53:03,842
적어도 하나의 차원에서,

1104
00:53:03,842 --> 00:53:07,673
기울기는이 함수의 미분 값입니다.

1105
00:53:07,673 --> 00:53:09,971
그래서 우리가 1 차원 함수 f를 가지면,

1106
00:53:09,971 --> 00:53:12,969
스칼라 x를 취한 다음 높이를 출력합니다.

1107
00:53:12,969 --> 00:53:16,460
곡선의 일부를 구하면 기울기를 계산할 수 있습니다.

1108
00:53:16,460 --> 00:53:19,717
또는 파생 상품을 언제든지 상상할 수 있습니다.

1109
00:53:19,717 --> 00:53:23,467
우리가 작은 걸음을 내딛으면 어떤 방향 으로든,

1110
00:53:26,298 --> 00:53:28,057
작은 걸음 걸음, 그리고 그 차이를 비교해 보라.

1111
00:53:28,057 --> 00:53:29,798
해당 단계의 함수 값에서

1112
00:53:29,798 --> 00:53:31,679
단계 크기를 0으로 드래그하고,

1113
00:53:31,679 --> 00:53:33,237
그 함수의 기울기를 우리에게 줄 것이다.

1114
00:53:33,237 --> 00:53:34,895
그 시점에서.

1115
00:53:34,895 --> 00:53:36,094
그리고 이것은 아주 자연스럽게 일반화됩니다.

1116
00:53:36,094 --> 00:53:38,333
다중 변수 기능도 제공합니다.

1117
00:53:38,333 --> 00:53:41,377
그래서 실제로 x는 스칼라가 아닙니다.

1118
00:53:41,377 --> 00:53:42,612
그러나 전체 벡터,

1119
00:53:42,612 --> 00:53:46,445
기억하기 때문에, x는 전체 벡터 일 수 있습니다.

1120
00:53:47,532 --> 00:53:49,063
그래서 우리는이 개념을 일반화 할 필요가있다.

1121
00:53:49,063 --> 00:53:51,941
다중 변수에 이르기까지.

1122
00:53:51,941 --> 00:53:54,658
그리고 우리가 파생 상품을 사용하는 일반화

1123
00:53:54,658 --> 00:53:57,896
다중 변수 설정에서 그라데이션,

1124
00:53:57,896 --> 00:54:01,168
그래디언트는 부분 미분의 벡터입니다.

1125
00:54:01,168 --> 00:54:04,409
그래디언트는 x와 동일한 모양을 갖습니다.

1126
00:54:04,409 --> 00:54:07,473
그라디언트의 각 요소는 우리에게 알려줍니다.

1127
00:54:07,473 --> 00:54:09,595
함수 f의 기울기는 얼마인가?

1128
00:54:09,595 --> 00:54:12,391
우리가 그 좌표 방향으로 움직인다면.

1129
00:54:12,391 --> 00:54:13,899
그리고 그라디언트가 밝혀졌습니다.

1130
00:54:13,899 --> 00:54:16,816
이 아주 좋은 재산을 가지려면,

1131
00:54:18,373 --> 00:54:21,036
그래디언트는 이제 부분 미분의 벡터입니다.

1132
00:54:21,036 --> 00:54:23,228
그러나 그것은 가장 큰 증가의 방향을 가리킨다.

1133
00:54:23,228 --> 00:54:25,657
따라서,

1134
00:54:25,657 --> 00:54:27,545
음의 그래디언트 방향을 보면,

1135
00:54:27,545 --> 00:54:29,770
그것은 당신에게 가장 큰 감소의 방향을 제시합니다.

1136
00:54:29,770 --> 00:54:31,612
함수의.

1137
00:54:31,612 --> 00:54:34,210
그리고 더 일반적으로, 당신이 알고 싶다면,

1138
00:54:34,210 --> 00:54:37,418
어떤 방향 으로든 내 풍경의 경사는 무엇입니까?

1139
00:54:37,418 --> 00:54:39,357
그러면 그라데이션의 내적과 같습니다.

1140
00:54:39,357 --> 00:54:42,693
그 방향을 설명하는 단위 벡터와 함께.

1141
00:54:42,693 --> 00:54:44,549
그래서이 그라데이션은 매우 중요합니다.

1142
00:54:44,549 --> 00:54:47,719
이 선형 일차 근사를 제공하기 때문에

1143
00:54:47,719 --> 00:54:50,198
귀하의 현재 지점에서 귀하의 기능에.

1144
00:54:50,198 --> 00:54:51,382
그래서 실제로, 많은 깊은 학습

1145
00:54:51,382 --> 00:54:53,661
함수의 그라데이션 계산하기

1146
00:54:53,661 --> 00:54:56,290
그런 그라디언트를 사용하여 반복적으로 업데이트

1147
00:54:56,290 --> 00:54:58,123
매개 변수 벡터.

1148
00:54:59,204 --> 00:55:02,195
당신이 상상할 수있는 순진한 방법 하나

1149
00:55:02,195 --> 00:55:04,812
실제로 컴퓨터에서이 그라디언트를 평가하면

1150
00:55:04,812 --> 00:55:06,955
유한 차이의 방법을 사용하고 있습니다.

1151
00:55:06,955 --> 00:55:09,488
그래디언트의 한계 정의로 돌아갑니다.

1152
00:55:09,488 --> 00:55:12,621
그래서 왼쪽에서 우리는 우리의 현재 W

1153
00:55:12,621 --> 00:55:14,012
이 매개 변수 벡터입니다.

1154
00:55:14,012 --> 00:55:17,432
어쩌면 우리에게 어쩌면 1.25의
현재 손실을 줄 수 있습니다.

1155
00:55:17,432 --> 00:55:21,127
우리의 목표는 기울기를 계산하는 것입니다, dW,

1156
00:55:21,127 --> 00:55:23,922
이것은 W와 같은 모양의 벡터가 될 것입니다.

1157
00:55:23,922 --> 00:55:26,021
그 그라디언트의 각 슬롯은 우리에게 알려줄 것입니다.

1158
00:55:26,021 --> 00:55:29,050
얼마나 많은 손실이 발생할 것인가?

1159
00:55:29,050 --> 00:55:31,734
그 좌표 방향으로 미미한 양.

1160
00:55:31,734 --> 00:55:33,241
그래서 당신이 상상할 수있는 한가지

1161
00:55:33,241 --> 00:55:35,741
이 유한 차분을 계산하는 것입니다.

1162
00:55:35,741 --> 00:55:38,336
우리가 W를 가지고 있다면, 우리는

1163
00:55:38,336 --> 00:55:41,902
W의 첫 번째 원소, 작은 값, h,

1164
00:55:41,902 --> 00:55:44,210
손실 함수를 사용하여 손실을 다시 계산하십시오.

1165
00:55:44,210 --> 00:55:45,842
우리의 분류 자와 모든 것.

1166
00:55:45,842 --> 00:55:48,112
그리고 아마도이 상황에서 우리가 조금 움직이면

1167
00:55:48,112 --> 00:55:50,792
첫 번째 차원에서 손실이 줄어들 것입니다.

1168
00:55:50,792 --> 00:55:53,792
1.2534에서 1.25322로 조금 증가했습니다.

1169
00:55:55,945 --> 00:55:57,574
그런 다음이 한계 정의를 사용할 수 있습니다.

1170
00:55:57,574 --> 00:56:01,363
이 유한 차분 근사값을 생각해 내야한다.

1171
00:56:01,363 --> 00:56:04,378
이 첫 번째 차원의 그래디언트로

1172
00:56:04,378 --> 00:56:06,194
이제이 절차를 반복하는 것을 상상할 수 있습니다.

1173
00:56:06,194 --> 00:56:07,795
두 번째 차원에서,

1174
00:56:07,795 --> 00:56:09,371
이제 우리는 첫 번째 차원을 취하고,

1175
00:56:09,371 --> 00:56:11,029
원래 값으로 다시 설정하십시오.

1176
00:56:11,029 --> 00:56:13,728
작은 단계로 두 번째 방향을 증가시킵니다.

1177
00:56:13,728 --> 00:56:15,294
그리고 다시, 우리는 손실을 계산합니다.

1178
00:56:15,294 --> 00:56:17,593
이 유한 차분 근사법을 사용하십시오.

1179
00:56:17,593 --> 00:56:19,444
그라데이션에 대한 근사치를 계산하는

1180
00:56:19,444 --> 00:56:21,165
두 번째 슬롯에서.

1181
00:56:21,165 --> 00:56:22,843
그리고 이제 세 번째로 이것을 반복하십시오.

1182
00:56:22,843 --> 00:56:25,135
그리고 계속해서.

1183
00:56:25,135 --> 00:56:27,683
그래서 이것은 실제로 끔찍한 생각입니다.

1184
00:56:27,683 --> 00:56:29,150
왜냐하면 그것은 매우 느리기 때문입니다.

1185
00:56:29,150 --> 00:56:31,980
그래서 여러분은이 함수 f를 계산하면,

1186
00:56:31,980 --> 00:56:34,230
그것이 크다면 실제로는 매우 느릴지도 모른다.

1187
00:56:34,230 --> 00:56:35,920
길쌈 신경 네트워크.

1188
00:56:35,920 --> 00:56:38,369
그리고이 매개 변수 벡터 W는,

1189
00:56:38,369 --> 00:56:40,693
아마 여기에있는 것처럼 10 개의 항목이 없을 것입니다.

1190
00:56:40,693 --> 00:56:42,266
수천만 명이 될 수도있다.

1191
00:56:42,266 --> 00:56:44,344
또는이 수백만 달러 중 일부는 수백만 달러,

1192
00:56:44,344 --> 00:56:46,446
복잡한 심층 학습 모델.

1193
00:56:46,446 --> 00:56:48,482
따라서 실제적으로, 당신은 결코
계산을 원하지 않을 것입니다.

1194
00:56:48,482 --> 00:56:50,381
당신의 유한 차이에 대한 당신의 그라디언트,

1195
00:56:50,381 --> 00:56:52,864
당신이 수억 명을 기다려야하기 때문에

1196
00:56:52,864 --> 00:56:54,749
잠재적으로 기능 평가

1197
00:56:54,749 --> 00:56:56,908
하나의 그래디언트를 얻는다면 그것은 매우 느려질 것입니다.

1198
00:56:56,908 --> 00:56:58,075
슈퍼 나쁜.

1199
00:56:59,351 --> 00:57:02,524
그러나 고맙게도 우리는 그렇게 할 필요가 없습니다.

1200
00:57:02,524 --> 00:57:03,676
바라건대 당신은 미적분 과정을 택했을 것입니다.

1201
00:57:03,676 --> 00:57:05,315
네 인생의 어느 시점에서,

1202
00:57:05,315 --> 00:57:08,146
그래서 당신은이 사람들 덕분에,

1203
00:57:08,146 --> 00:57:11,206
우리는 손실에 대한 표현을 적어 둘 수 있습니다.

1204
00:57:11,206 --> 00:57:13,658
미적분학의 마법 망치를 사용하십시오.

1205
00:57:13,658 --> 00:57:15,584
표현을 적어 두는 것

1206
00:57:15,584 --> 00:57:17,372
이 기울기가 무엇을 위해 있어야합니다.

1207
00:57:17,372 --> 00:57:18,708
그리고 이것은 훨씬 더 효율적 일 것입니다.

1208
00:57:18,708 --> 00:57:20,245
분석적으로 계산하는 것보다

1209
00:57:20,245 --> 00:57:21,658
유한 한 차이를 통해.

1210
00:57:21,658 --> 00:57:22,929
하나, 정확 할거야.

1211
00:57:22,929 --> 00:57:25,433
둘째, 계산이 필요하기 때문에 훨씬 빠릅니다.

1212
00:57:25,433 --> 00:57:27,350
이 단일 표현.

1213
00:57:28,945 --> 00:57:31,405
그래서 이것이 어떻게 생겼는지는 지금입니다.

1214
00:57:31,405 --> 00:57:33,513
우리가 현재의 W의 그림으로 돌아 가면,

1215
00:57:33,513 --> 00:57:36,848
W의 모든 차원을 반복하는 것이 아니라,

1216
00:57:36,848 --> 00:57:38,311
우리가 미리 알아낼거야.

1217
00:57:38,311 --> 00:57:40,653
그래디언트의 분석 식은 무엇입니까?

1218
00:57:40,653 --> 00:57:44,279
그런 다음 그것을 적어서 W에서 직접 이동하십시오.

1219
00:57:44,279 --> 00:57:47,337
하나의 단계에서 dW 또는
그래디언트를 계산할 수 있습니다.

1220
00:57:47,337 --> 00:57:50,875
그리고 그것은 실제로 더 나아질 것입니다.

1221
00:57:50,875 --> 00:57:53,846
요약하자면이 수치 그라디언트

1222
00:57:53,846 --> 00:57:56,738
간단하고 의미있는 것입니다.

1223
00:57:56,738 --> 00:57:58,745
그러나 실제로는 실제로 사용하지 않을 것입니다.

1224
00:57:58,745 --> 00:58:01,794
실제로는 분석 그라디언트를 항상 사용합니다.

1225
00:58:01,794 --> 00:58:03,039
그것을 사용하십시오.

1226
00:58:03,039 --> 00:58:05,301
실제로 이러한 그래디언트 계산을 수행 할 때

1227
00:58:05,301 --> 00:58:06,951
그러나 흥미로운 점은

1228
00:58:06,951 --> 00:58:09,360
이 숫자 그라디언트는 실제로 매우 유용합니다.

1229
00:58:09,360 --> 00:58:10,610
디버깅 도구.

1230
00:58:12,572 --> 00:58:13,841
몇 가지 코드를 작성했다고 가정 해 보겠습니다.

1231
00:58:13,841 --> 00:58:16,169
당신은 손실을 계산하는 코드를 작성했습니다.

1232
00:58:16,169 --> 00:58:17,770
손실의 기울기,

1233
00:58:17,770 --> 00:58:19,562
그렇다면 어떻게 디버깅합니까?

1234
00:58:19,562 --> 00:58:21,909
이 분석식이

1235
00:58:21,909 --> 00:58:24,085
코드에서 파생하고 적어 둔

1236
00:58:24,085 --> 00:58:25,684
실제로 맞습니까?

1237
00:58:25,684 --> 00:58:28,443
따라서 이러한 것들을위한 일반적인 디버깅 전략

1238
00:58:28,443 --> 00:58:31,159
방법으로 숫자 그라디언트를 사용하는 것입니다.

1239
00:58:31,159 --> 00:58:32,823
확실한 단위 테스트의 일종으로

1240
00:58:32,823 --> 00:58:35,141
분석 기울기가 맞는지 확인하십시오.

1241
00:58:35,141 --> 00:58:38,320
다시 말하지만, 이것은 매우 느리고 정확하지 않기 때문에,

1242
00:58:38,320 --> 00:58:41,435
이 숫자 그라디언트 검사를 수행 할 때,

1243
00:58:41,435 --> 00:58:43,739
호출 될 때 매개 변수의 크기를 줄이는 경향이 있습니다.

1244
00:58:43,739 --> 00:58:45,184
문제가 실제로 실행되도록

1245
00:58:45,184 --> 00:58:46,755
합리적인 시간에

1246
00:58:46,755 --> 00:58:49,376
하지만 이것은 유용한 유용한 디버깅 전략이됩니다.

1247
00:58:49,376 --> 00:58:51,721
자신의 그라디언트 계산을 작성할 때.

1248
00:58:51,721 --> 00:58:54,112
그래서 실제로 이것은 실제로 실제로 많이 사용됩니다.

1249
00:58:54,112 --> 00:58:58,610
그리고 당신은 당신의 과제에도 이것을 할 것입니다.

1250
00:58:58,610 --> 00:59:01,834
그래서 일단 그라디언트를 계산하는 방법을 알게되면,

1251
00:59:01,834 --> 00:59:04,547
그러면 우리를이 초간단 단순 알고리즘으로 이끈다.

1252
00:59:04,547 --> 00:59:06,990
그것은 세 줄과 같지만 마음 속에있는 것으로 밝혀졌습니다.

1253
00:59:06,990 --> 00:59:09,480
우리가 이처럼 매우 큰 것을 훈련하는 방법에 대해서,

1254
00:59:09,480 --> 00:59:11,607
가장 복잡한 심층 학습 알고리즘,

1255
00:59:11,607 --> 00:59:13,152
그것은 구배 강하입니다.

1256
00:59:13,152 --> 00:59:16,991
그래디언트 강하가 먼저 W를 초기화합니다.

1257
00:59:16,991 --> 00:59:19,544
어떤 무작위로, 그렇다면 사실,

1258
00:59:19,544 --> 00:59:21,555
우리는 우리의 손실과 그라디언트를 계산할 것입니다.

1259
00:59:21,555 --> 00:59:24,521
우리는 우리의 가중치를 업데이트 할 것입니다.

1260
00:59:24,521 --> 00:59:27,547
그레디언트 방향의 반대 방향에서,

1261
00:59:27,547 --> 00:59:28,722
그라디언트를 기억하십시오.

1262
00:59:28,722 --> 00:59:30,710
가장 큰 증가의 방향을 가리키고 있었다.

1263
00:59:30,710 --> 00:59:32,305
함수의 - 그래디언트 빼기

1264
00:59:32,305 --> 00:59:34,047
가장 큰 감소 방향의 포인트,

1265
00:59:34,047 --> 00:59:36,727
그래서 우리는 방향으로 조금 나아갈 것입니다.

1266
00:59:36,727 --> 00:59:39,262
빼기 그라디언트를 사용하고 영원히 이것을 반복하십시오.

1267
00:59:39,262 --> 00:59:40,774
결국 네트워크가 수렴 할 것입니다.

1268
00:59:40,774 --> 00:59:43,255
희망적으로 당신은 매우 행복 할 것입니다.

1269
00:59:43,255 --> 00:59:45,596
그러나이 단계 크기는 실제로 하이퍼 매개 변수입니다.

1270
00:59:45,596 --> 00:59:48,219
이것은 그라디언트를 계산할 때마다,

1271
00:59:48,219 --> 00:59:50,842
우리는 그 방향으로 얼마나 멀리 나아갈 것입니다.

1272
00:59:50,842 --> 00:59:53,602
그리고이 단계 크기는 학습 속도라고도하며,

1273
00:59:53,602 --> 00:59:55,445
아마 가장 중요한 것 중 하나 일 것입니다.

1274
00:59:55,445 --> 00:59:57,378
설정해야하는 하이퍼 매개 변수

1275
00:59:57,378 --> 01:00:00,033
실제로 이러한 것들을 실제로 훈련 할 때.

1276
01:00:00,033 --> 01:00:02,117
사실 나는이 일을 훈련 할 때 나를 위해,

1277
01:00:02,117 --> 01:00:04,200
이 단계 크기를 알아 내려고

1278
01:00:04,200 --> 01:00:06,340
또는이 학습 률은 첫 번째 하이퍼 파라미터입니다.

1279
01:00:06,340 --> 01:00:07,502
나는 항상 확인한다.

1280
01:00:07,502 --> 01:00:10,949
모형 크기 또는 정규화 강도와 같은 것

1281
01:00:10,949 --> 01:00:12,434
나중에 조금 떠날 때까지,

1282
01:00:12,434 --> 01:00:15,408
학습 속도 또는 단계 크기를 올바르게 얻는 방법

1283
01:00:15,408 --> 01:00:19,361
처음에 설정하려고하는 첫 번째 것입니다.

1284
01:00:19,361 --> 01:00:23,015
그래서 그림처럼 보이는 것입니다.

1285
01:00:23,015 --> 01:00:25,212
여기에 2 차원의 간단한 예제가 있습니다.

1286
01:00:25,212 --> 01:00:28,004
그래서 여기에 우리는 아마도이 사발을 가지고있을 것입니다.

1287
01:00:28,004 --> 01:00:30,051
우리의 손실 함수를 보여주고있다.

1288
01:00:30,051 --> 01:00:33,635
중앙의이 빨간 지역

1289
01:00:33,635 --> 01:00:36,598
우리가 가고 싶은 낮은 손실의이 지역

1290
01:00:36,598 --> 01:00:38,852
에지쪽으로 청색 및 녹색 영역

1291
01:00:38,852 --> 01:00:41,187
우리가 피하고자하는 손실이 더 큽니다.

1292
01:00:41,187 --> 01:00:43,204
이제 우리는 우리의 W를 시작할 것입니다.

1293
01:00:43,204 --> 01:00:44,750
우주의 어떤 임의의 지점에서,

1294
01:00:44,750 --> 01:00:47,536
음의 그래디언트 방향을 계산하고,

1295
01:00:47,536 --> 01:00:49,680
바라건대 우리를 방향으로 안내 할 것입니다.

1296
01:00:49,680 --> 01:00:51,387
결국 미니 마의.

1297
01:00:51,387 --> 01:00:53,171
그리고 우리가 이것을 반복해서 반복한다면,

1298
01:00:53,171 --> 01:00:56,407
우리는 최후에 정확한 미니 마를 얻을 수 있기를 바랍니다.

1299
01:00:56,407 --> 01:01:00,283
실제로 이것이 어떻게 생겼는지는,

1300
01:01:00,283 --> 01:01:03,140
오,이 쥐 문제가 다시 발생했습니다.

1301
01:01:03,140 --> 01:01:04,358
그래서 이것이 실제로 어떻게 생겼는지

1302
01:01:04,358 --> 01:01:09,250
우리가이 일을 반복해서 반복한다면,

1303
01:01:09,250 --> 01:01:12,061
그러면 우리는 어느 시점부터 시작합니다.

1304
01:01:12,061 --> 01:01:15,266
결국에는 매번 작은 그래디언트 단계를 거치면서,

1305
01:01:15,266 --> 01:01:19,221
매개 변수가 중심을 향해 원호 모양으로 표시되고,

1306
01:01:19,221 --> 01:01:20,626
이 미니 마의 영역,

1307
01:01:20,626 --> 01:01:22,236
그리고 그것은 당신이 정말로 원하는 것입니다.

1308
01:01:22,236 --> 01:01:24,241
낮은 손실을 원하기 때문입니다.

1309
01:01:24,241 --> 01:01:26,812
그리고 그런데, 티저의 비트로서,

1310
01:01:26,812 --> 01:01:28,139
우리는 이전 슬라이드에서 보았습니다.

1311
01:01:28,139 --> 01:01:30,905
아주 간단한 그래디언트 디센트의이 예제,

1312
01:01:30,905 --> 01:01:32,705
모든 단계에서 우리는 방향으로 나아가고 있습니다.

1313
01:01:32,705 --> 01:01:33,998
그라디언트의

1314
01:01:33,998 --> 01:01:35,997
그러나 실제로, 다음 강의에서,

1315
01:01:35,997 --> 01:01:38,679
우리는 약간 더 매끈한 단계가 있음을 볼 것입니다,

1316
01:01:38,679 --> 01:01:40,885
그들이이 업데이트 규칙이라고 부르는 것,

1317
01:01:40,885 --> 01:01:43,598
약간 애호가를 취할 수있는 곳

1318
01:01:43,598 --> 01:01:46,037
여러 시간 단계에 걸쳐 그라디언트 통합

1319
01:01:46,037 --> 01:01:48,206
그리고 그런 것들은 조금 더 잘 작동하는 경향이 있습니다.

1320
01:01:48,206 --> 01:01:50,836
실제로는 훨씬 더 일반적으로 사용됩니다.

1321
01:01:50,836 --> 01:01:52,513
이 바닐라 그라데이션 하강보다

1322
01:01:52,513 --> 01:01:54,610
실제로 이러한 것들을 훈련 할 때.

1323
01:01:54,610 --> 01:01:55,877
그리고 약간의 미리보기로서,

1324
01:01:55,877 --> 01:01:59,101
우리는이 약간의 애호가 방법 중 일부를 볼 수 있습니다.

1325
01:01:59,101 --> 01:02:01,054
동일한 문제를 최적화합니다.

1326
01:02:01,054 --> 01:02:04,701
다시 검정은이 같은 그라디언트 계산이 될 것입니다.

1327
01:02:04,701 --> 01:02:07,530
그리고 이것들은 그들이 어떤 색인지 잊어 버렸습니다.

1328
01:02:07,530 --> 01:02:08,871
그러나이 두 개의 다른 곡선

1329
01:02:08,871 --> 01:02:11,580
약간 더 까다로운 업데이트 규칙을 사용하고 있습니다.

1330
01:02:11,580 --> 01:02:13,960
그래디언트 정보를 사용하는 방법을 정확하게 결정하는

1331
01:02:13,960 --> 01:02:15,929
우리의 다음 단계를 만들기 위해.

1332
01:02:15,929 --> 01:02:20,451
따라서 이들 중 하나는 기세가있는 그라데이션 강하입니다.

1333
01:02:20,451 --> 01:02:22,835
다른 하나는이 Adam Optimizer입니다.

1334
01:02:22,835 --> 01:02:24,273
자세한 내용은

1335
01:02:24,273 --> 01:02:25,389
나중에 코스에서.

1336
01:02:25,389 --> 01:02:28,300
그러나 아이디어는 우리가이 아주 기본적인
알고리즘을 가지고 있다는 것입니다.

1337
01:02:28,300 --> 01:02:29,795
그라디언트 디센트라는,

1338
01:02:29,795 --> 01:02:31,544
매 단계마다 그라디언트를 사용합니다.

1339
01:02:31,544 --> 01:02:33,424
다음으로 나아갈 위치를 결정하기 위해,

1340
01:02:33,424 --> 01:02:35,874
우리에게 알려주는 다른 업데이트 규칙이 있습니다.

1341
01:02:35,874 --> 01:02:38,559
그 기울기 정보를 얼마나 정확하게 사용하는지.

1342
01:02:38,559 --> 01:02:40,391
그러나 그것은 모두 동일한 기본 알고리즘입니다.

1343
01:02:40,391 --> 01:02:44,058
매 단계마다 내리막 길을 가려고했다.

1344
01:02:50,022 --> 01:02:51,852
하지만 실제로는 조금 더 주름살이 있습니다.

1345
01:02:51,852 --> 01:02:53,212
우리가 얘기해야 할 것.

1346
01:02:53,212 --> 01:02:56,818
그래서 우리가 손실 함수를 정의했음을 기억하십시오.

1347
01:02:56,818 --> 01:02:59,356
우리는 얼마나 나쁜지를 계산하는 손실을 정의했다.

1348
01:02:59,356 --> 01:03:02,237
어떤 단일 훈련 예에서 우리
분류 자 (classifier)

1349
01:03:02,237 --> 01:03:04,536
데이터 세트에 대한 우리의 모든 손실이

1350
01:03:04,536 --> 01:03:06,077
평균 손실이 될거야.

1351
01:03:06,077 --> 01:03:08,314
전체 교육 세트에서

1352
01:03:08,314 --> 01:03:12,572
그러나 실제로이 N은 매우 커질 수 있습니다.

1353
01:03:12,572 --> 01:03:15,274
예를 들어 이미지 넷 데이터 세트를 사용하는 경우,

1354
01:03:15,274 --> 01:03:17,010
우리가 첫 번째 강의에서 이야기 한 내용,

1355
01:03:17,010 --> 01:03:19,199
N은 130 만 명,

1356
01:03:19,199 --> 01:03:21,403
그래서 실제로이 손실을 계산합니다.

1357
01:03:21,403 --> 01:03:23,208
실제로는 매우 비쌀 수있다.

1358
01:03:23,208 --> 01:03:26,081
아마도 수백만 가지 평가 계산이 필요합니다.

1359
01:03:26,081 --> 01:03:28,206
이 함수의

1360
01:03:28,206 --> 01:03:29,821
그래서 그것은 정말로 느릴 수 있습니다.

1361
01:03:29,821 --> 01:03:32,094
실제로 그라디언트는 선형 연산자이므로,

1362
01:03:32,094 --> 01:03:34,109
그라디언트를 실제로 계산하려고 할 때

1363
01:03:34,109 --> 01:03:37,034
이 표현에서 우리는 손실의 기울기

1364
01:03:37,034 --> 01:03:39,504
이제는 손실 그라디언트의 합계입니다.

1365
01:03:39,504 --> 01:03:41,461
각 개별 용어에 대해

1366
01:03:41,461 --> 01:03:43,734
이제 그라디언트를 다시 계산하려면,

1367
01:03:43,734 --> 01:03:45,248
그것은 우리에게 반복을 요구합니다.

1368
01:03:45,248 --> 01:03:46,930
전체 교육 데이터 세트에 대해

1369
01:03:46,930 --> 01:03:48,469
이 모든 N 개의 예제.

1370
01:03:48,469 --> 01:03:50,390
그래서 만약 우리 N이 백만 달러라면,

1371
01:03:50,390 --> 01:03:51,960
이것은 슈퍼 슈퍼 천천히,

1372
01:03:51,960 --> 01:03:54,078
우리는 아주 오랜 시간을 기다려야 할 것입니다.

1373
01:03:54,078 --> 01:03:56,978
우리가 W를 개별적으로 업데이트하기 전에.

1374
01:03:56,978 --> 01:03:58,577
그래서 실제로, 우리는

1375
01:03:58,577 --> 01:04:00,728
확률 적 구배 강하라고 불리는 것,

1376
01:04:00,728 --> 01:04:04,061
손실 및 기울기 계산보다는

1377
01:04:04,061 --> 01:04:05,697
전체 훈련 세트에서,

1378
01:04:05,697 --> 01:04:08,938
대신 매 반복마다 샘플 세트

1379
01:04:08,938 --> 01:04:12,540
minibatch라고하는 훈련 예를들 수 있습니다.

1380
01:04:12,540 --> 01:04:14,213
보통 이것은 관습에 따라 2의 힘입니다.

1381
01:04:14,213 --> 01:04:17,705
32, 64, 128은 일반적인 숫자입니다.

1382
01:04:17,705 --> 01:04:19,887
그런 다음이 작은 미니 바를 사용합니다.

1383
01:04:19,887 --> 01:04:22,483
전체 합계의 추정치를 계산하기 위해,

1384
01:04:22,483 --> 01:04:25,047
그리고 진정한 그라데이션의 추정치.

1385
01:04:25,047 --> 01:04:27,703
그리고 이것은 당신이 이것을 볼
수 있기 때문에 확률 적입니다.

1386
01:04:27,703 --> 01:04:31,845
몬테 카를로의 기대치에 대한 추정치 일 수도 있습니다.

1387
01:04:31,845 --> 01:04:33,345
진정한 가치의

1388
01:04:34,716 --> 01:04:37,318
그래서 이것은 우리의 알고리즘을
약간 더 매끈하게 만듭니다.

1389
01:04:37,318 --> 01:04:38,945
하지만 여전히 4 줄 밖에 없습니다.

1390
01:04:38,945 --> 01:04:43,112
이제 데이터가 무작위로 추출됩니다.

1391
01:04:44,291 --> 01:04:46,682
minibatch에서 손실 및 그라디언트를 평가하고,

1392
01:04:46,682 --> 01:04:48,690
이제 매개 변수를 업데이트하십시오.

1393
01:04:48,690 --> 01:04:51,113
이 손실 추정에 기초하여,

1394
01:04:51,113 --> 01:04:53,661
이 기울기의 추정치.

1395
01:04:53,661 --> 01:04:56,769
그리고 다시 약간 더 매끄러운 업데이트 규칙을 보겠습니다.

1396
01:04:56,769 --> 01:04:59,811
여러 가지 그라디언트를 통합하는 방법을 정확히 설명합니다.

1397
01:04:59,811 --> 01:05:02,780
시간이지 나면서,하지만 이것은 기본적인 훈련 알고리즘입니다

1398
01:05:02,780 --> 01:05:04,948
우리가 거의 모든 심층 신경 네트워크에 사용하는

1399
01:05:04,948 --> 01:05:05,948
실제로.

1400
01:05:06,875 --> 01:05:10,235
그래서 우리는 또 다른 대화 형 웹 데모를 가지고 있습니다.

1401
01:05:10,235 --> 01:05:12,625
선형 분류기로 실제로 놀고,

1402
01:05:12,625 --> 01:05:14,970
확률 적 구배 강하를 통해 이러한 것들을 훈련하고,

1403
01:05:14,970 --> 01:05:17,782
그러나 웹 데모가 얼마나 비참한지를 생각해 보면,

1404
01:05:17,782 --> 01:05:20,122
나는 실제로 링크를 열지 않을 것이다.

1405
01:05:20,122 --> 01:05:23,269
대신, 나는이 비디오를 재생할 것입니다.

1406
01:05:23,269 --> 01:05:25,339
[웃음]

1407
01:05:25,339 --> 01:05:26,594
하지만 이걸 확인해 보시길 바랍니다.

1408
01:05:26,594 --> 01:05:27,749
온라인으로 게임을 즐기고,

1409
01:05:27,749 --> 01:05:29,256
실제로 어떤 직감을 구축하는 데 도움이되기 때문에

1410
01:05:29,256 --> 01:05:31,036
선형 분류기에 대해 배우고 훈련하기

1411
01:05:31,036 --> 01:05:32,735
그라데이션 강하를 통해.

1412
01:05:32,735 --> 01:05:34,919
여기 왼쪽에서 볼 수 있습니다.

1413
01:05:34,919 --> 01:05:37,485
우리가 분류하고있는이 문제가 있습니다.

1414
01:05:37,485 --> 01:05:40,146
3 개의 다른 종류,

1415
01:05:40,146 --> 01:05:42,551
우리는이 녹색, 파란색 및 빨간색 점을 가지고 있습니다.

1416
01:05:42,551 --> 01:05:45,753
그것은이 세 가지 수업에서 얻은 우리의 훈련 견본입니다.

1417
01:05:45,753 --> 01:05:48,351
그리고 이제 우리는 결정 경계를 이끌어 냈습니다.

1418
01:05:48,351 --> 01:05:52,068
컬러 백그라운드 영역 인 이러한 클래스의 경우,

1419
01:05:52,068 --> 01:05:54,270
이러한 방향뿐만 아니라,

1420
01:05:54,270 --> 01:05:57,487
학급 점수 인상 방향을 알려줍니다.

1421
01:05:57,487 --> 01:05:59,007
이 세 가지 클래스 각각에 대해.

1422
01:05:59,007 --> 01:06:03,108
그리고 지금 보시다시피, 실제로 가서 놀면

1423
01:06:03,108 --> 01:06:04,814
이 물건을 온라인으로

1424
01:06:04,814 --> 01:06:07,579
당신은 우리가 가서 W를 조정할 수
있다는 것을 볼 수 있습니다.

1425
01:06:07,579 --> 01:06:09,442
Ws 값 변경

1426
01:06:09,442 --> 01:06:12,176
이러한 결정 경계가 회전하게됩니다.

1427
01:06:12,176 --> 01:06:14,189
편향을 변경하면 결정 경계

1428
01:06:14,189 --> 01:06:17,476
회전하지 않고 대신 좌우로 움직입니다.

1429
01:06:17,476 --> 01:06:18,662
또는 위 아래로.

1430
01:06:18,662 --> 01:06:19,989
그러면 실제로 단계를 밟을 수 있습니다.

1431
01:06:19,989 --> 01:06:21,855
이 손실을 업데이트하려고 시도하는

1432
01:06:21,855 --> 01:06:23,984
또는이 슬라이더로 스텝 크기를 변경할 수 있습니다.

1433
01:06:23,984 --> 01:06:26,009
이 버튼을 눌러 실제 작동시킬 수 있습니다.

1434
01:06:26,009 --> 01:06:27,296
이제 큰 단계 크기로,

1435
01:06:27,296 --> 01:06:29,076
우리는 현재 그라디언트 디센트를 실행 중입니다.

1436
01:06:29,076 --> 01:06:30,624
이러한 결정의 경계는

1437
01:06:30,624 --> 01:06:32,874
데이터를 맞추려고합니다.

1438
01:06:34,553 --> 01:06:36,432
이제는 괜찮아요.

1439
01:06:36,432 --> 01:06:39,890
실제 손실 함수를 실시간으로 변경할 수 있습니다.

1440
01:06:39,890 --> 01:06:41,845
이 서로 다른 SVM 공식들 사이

1441
01:06:41,845 --> 01:06:43,567
및 다른 softmax.

1442
01:06:43,567 --> 01:06:44,754
그리고 당신은 볼 수 있습니다.

1443
01:06:44,754 --> 01:06:47,695
손실 함수의 이들 상이한 공식들 사이에서,

1444
01:06:47,695 --> 01:06:50,219
그것은 일반적으로 똑같은 일을합니다.

1445
01:06:50,219 --> 01:06:52,340
우리의 의사 결정 영역은 대부분 같은 장소에 있습니다.

1446
01:06:52,340 --> 01:06:54,945
그러나 그들이 어떻게 서로에 관해서는 결국

1447
01:06:54,945 --> 01:06:56,263
정확하게 상충되는 점은 무엇입니까?

1448
01:06:56,263 --> 01:06:59,139
이 다른 것들을 범주화하는 것

1449
01:06:59,139 --> 01:07:00,743
조금 바뀐다.

1450
01:07:00,743 --> 01:07:02,137
그래서 나는 정말로 당신이 온라인에 가기를 권장합니다.

1451
01:07:02,137 --> 01:07:03,877
이 일로 약간의 직감을 얻으려고 노력한다.

1452
01:07:03,877 --> 01:07:05,699
실제로 어떻게 생겼는지

1453
01:07:05,699 --> 01:07:07,428
이러한 선형 분류자를 훈련 시키려고

1454
01:07:07,428 --> 01:07:09,178
그라데이션 강하를 통해.

1455
01:07:12,343 --> 01:07:16,245
이제는 제쳐두고, 저는 다른 생각에
대해 이야기하고 싶습니다.

1456
01:07:16,245 --> 01:07:18,102
그것은 이미지 기능의 것입니다.

1457
01:07:18,102 --> 01:07:20,668
지금까지 선형 분류기에 대해 이야기했습니다.

1458
01:07:20,668 --> 01:07:23,032
그냥 우리의 원시 이미지 픽셀을 복용하는 것입니다

1459
01:07:23,032 --> 01:07:25,184
원시 픽셀 자체를 공급하는 단계

1460
01:07:25,184 --> 01:07:27,434
우리의 선형 분류 자로.

1461
01:07:28,464 --> 01:07:31,075
그러나 마지막 강연에서 우리가 이야기 한 것처럼,

1462
01:07:31,075 --> 01:07:33,343
이것은 아마도 그렇게 대단한 것이 아니며,

1463
01:07:33,343 --> 01:07:36,233
멀티 - 모달과 같은 것들 때문에.

1464
01:07:36,233 --> 01:07:39,368
따라서 실제로는 원본 픽셀 값을 실제로 공급합니다.

1465
01:07:39,368 --> 01:07:42,789
선형 분류기로 변환하는 것이 잘
작동하지 않는 경향이 있습니다.

1466
01:07:42,789 --> 01:07:45,742
그래서 그것은 지배 이전에 실제로 일반적이었습니다.

1467
01:07:45,742 --> 01:07:47,145
깊은 신경 네트워크,

1468
01:07:47,145 --> 01:07:49,592
대신에이 2 단계 접근법을 사용하는 것이 었습니다.

1469
01:07:49,592 --> 01:07:51,105
먼저, 당신은 당신의 이미지를 찍을 것입니다.

1470
01:07:51,105 --> 01:07:53,916
다양한 특징 표현을 계산할 수 있습니다.

1471
01:07:53,916 --> 01:07:55,886
어쩌면 계산중인 이미지의

1472
01:07:55,886 --> 01:07:59,219
외관과 관련된 여러 종류의 양

1473
01:07:59,219 --> 01:08:00,179
이미지의

1474
01:08:00,179 --> 01:08:02,117
이들 서로 다른 특징 벡터들을 연결

1475
01:08:02,117 --> 01:08:05,137
이미지의 일부 기능 표현을 제공하려면,

1476
01:08:05,137 --> 01:08:07,019
이제 이미지의이 특징 표현

1477
01:08:07,019 --> 01:08:08,836
선형 분류기에 공급 될 것이고,

1478
01:08:08,836 --> 01:08:10,683
원시 픽셀 자체를 먹이기보다는

1479
01:08:10,683 --> 01:08:12,902
분류기에 넣습니다.

1480
01:08:12,902 --> 01:08:15,671
그리고 여기서의 동기는,

1481
01:08:15,671 --> 01:08:17,700
그래서 왼쪽에 훈련 데이터 세트가 있다고 상상해보십시오.

1482
01:08:17,701 --> 01:08:20,193
이 빨간 점들과 중간에 빨간 점들

1483
01:08:20,193 --> 01:08:22,244
그리고 그 주변의 푸른 점.

1484
01:08:22,244 --> 01:08:23,693
그리고 이러한 종류의 데이터 세트의 경우,

1485
01:08:23,693 --> 01:08:26,440
선형 결정 경계를 그릴 수있는 방법이 없습니다.

1486
01:08:26,441 --> 01:08:29,157
빨간색 점과 파란색 점을 구분합니다.

1487
01:08:29,157 --> 01:08:32,155
그리고 우리는 마지막 강의에서 이것에
대한 더 많은 예를 보았습니다.

1488
01:08:32,156 --> 01:08:34,459
그러나 영리한 피처 변환을 사용한다면,

1489
01:08:34,459 --> 01:08:36,660
이 경우에는 극좌표로 변환,

1490
01:08:36,660 --> 01:08:39,079
이제 우리가 피쳐 변환을 한 후에,

1491
01:08:39,079 --> 01:08:42,361
이 복잡한 데이터 세트는 실제로

1492
01:08:42,361 --> 01:08:43,677
선형으로 분리 가능한,

1493
01:08:43,677 --> 01:08:45,160
실제로 정확하게 분류 될 수있다.

1494
01:08:45,160 --> 01:08:46,858
선형 분류기에 의해

1495
01:08:46,858 --> 01:08:48,435
그리고 여기있는 모든 트릭은 이제 알아내는 것입니다.

1496
01:08:48,436 --> 01:08:51,034
올바른 피쳐 변환은 무엇입니까?

1497
01:08:51,034 --> 01:08:53,197
그것은 올바른 양의 계산입니다.

1498
01:08:53,197 --> 01:08:55,129
당신이 걱정하는 문제에 대해서.

1499
01:08:55,129 --> 01:08:58,017
따라서 이미지의 경우 픽셀을 변환 할 수 있습니다.

1500
01:08:58,017 --> 01:08:59,751
극좌표로, 이해가 안 돼,

1501
01:08:59,751 --> 01:09:01,505
그러나 실제로 당신은 적어 내려고 노력할 수 있습니다.

1502
01:09:01,505 --> 01:09:03,084
이미지의 특징 표현

1503
01:09:03,085 --> 01:09:04,749
그게 말이 되겠지,

1504
01:09:04,749 --> 01:09:06,458
실제로 너를 도울지도 모른다.

1505
01:09:06,458 --> 01:09:08,385
원시 픽셀을 넣는 것보다 효과적 일 수 있습니다.

1506
01:09:08,385 --> 01:09:10,391
분류기에 넣습니다.

1507
01:09:10,392 --> 01:09:13,157
그래서 이런 종류의 피쳐 표현의 한 예

1508
01:09:13,157 --> 01:09:16,343
그것은 매우 간단합니다. 색상 히스토그램의 아이디어입니다.

1509
01:09:16,343 --> 01:09:18,526
그래서 당신은 아마 각 픽셀을 가져갈 것입니다,

1510
01:09:18,526 --> 01:09:21,188
당신은이 색조 스펙트럼을 취할 것입니다.

1511
01:09:21,188 --> 01:09:23,985
버킷으로 나눈 다음 모든 픽셀에 대해

1512
01:09:23,986 --> 01:09:26,425
당신은 그 색 버킷 중 하나에 그것을 매핑 할 것입니다.

1513
01:09:26,425 --> 01:09:28,535
얼마나 많은 픽셀을 카운트하는지

1514
01:09:28,536 --> 01:09:31,162
이 각각의 버킷으로 떨어지십시오.

1515
01:09:31,162 --> 01:09:34,638
따라서 이미지에 어떤 색상이 있는지
전 세계적으로 알 수 있습니다.

1516
01:09:34,639 --> 01:09:36,278
어쩌면이 개구리의 예가

1517
01:09:36,278 --> 01:09:37,500
이 특징 벡터는 우리에게

1518
01:09:37,500 --> 01:09:39,076
녹색 물건이 많이 있어요.

1519
01:09:39,076 --> 01:09:40,938
어쩌면 자주색이나 붉은 색이 아닌 것도있을 것입니다.

1520
01:09:40,938 --> 01:09:43,043
그리고 이것은 여러분이 볼 수있는 단순한 특징 벡터입니다

1521
01:09:43,043 --> 01:09:44,043
실제로.

1522
01:09:45,108 --> 01:09:47,720
우리가 본 또 다른 공통 특징 벡터

1523
01:09:47,720 --> 01:09:49,430
신경 네트워크의 부상 전에,

1524
01:09:49,431 --> 01:09:50,983
또는 신경 네트워크의 지배 이전에

1525
01:09:50,983 --> 01:09:53,219
지향 그라디언트의 히스토그램이었습니다.

1526
01:09:53,220 --> 01:09:54,952
첫 번째 강의에서 기억하십시오.

1527
01:09:54,952 --> 01:09:57,829
Hubel과 Wiesel은 이러한
지향성 가장자리를 발견했습니다.

1528
01:09:57,829 --> 01:10:00,046
인간 시각 시스템에서 정말로 중요합니다.

1529
01:10:00,046 --> 01:10:02,209
지향성 그라디언트의 히스토그램

1530
01:10:02,209 --> 01:10:04,690
피쳐 표현은 캡쳐를 시도한다.

1531
01:10:04,690 --> 01:10:07,680
똑같은 직감과 현지 오리엔테이션 측정

1532
01:10:07,680 --> 01:10:09,974
이미지 가장자리.

1533
01:10:09,974 --> 01:10:11,280
그래서이 일이 무엇을 할 것인지,

1534
01:10:11,280 --> 01:10:13,286
우리의 이미지를 받아 그것을 나눕니다.

1535
01:10:13,286 --> 01:10:16,354
이 작은 8x8 픽셀 영역으로

1536
01:10:16,354 --> 01:10:19,142
그리고 나서, 각각의 8 × 8 픽셀 영역 내에서,

1537
01:10:19,142 --> 01:10:22,268
지배적 인 에지 방향을 계산합니다.

1538
01:10:22,268 --> 01:10:24,921
각 픽셀의 에지 방향

1539
01:10:24,921 --> 01:10:27,776
여러 버킷에 넣은 다음 각 영역 내에서

1540
01:10:27,776 --> 01:10:31,857
이들 서로 다른 에지 방향에
대해 히스토그램을 계산하십시오.

1541
01:10:31,857 --> 01:10:33,417
이제 전체 기능 벡터

1542
01:10:33,417 --> 01:10:35,797
이러한 서로 다른 버킷 히스토그램이됩니다.

1543
01:10:35,797 --> 01:10:37,382
가장자리 방위의

1544
01:10:37,382 --> 01:10:39,121
8 개 지역마다 8 개 지역에 걸쳐

1545
01:10:39,121 --> 01:10:40,204
이미지에서.

1546
01:10:41,660 --> 01:10:43,450
그래서 이것은 어떤면에서는 이중적인 것입니다.

1547
01:10:43,450 --> 01:10:47,029
이전에 본 색상 히스토그램 분류기로

1548
01:10:47,029 --> 01:10:49,704
그래서 색 막대 그래프는 전 세계적으로 어떤 색

1549
01:10:49,704 --> 01:10:51,082
이미지에 존재한다.

1550
01:10:51,082 --> 01:10:53,751
이것은 전반적으로 어떤 유형의 에지 정보

1551
01:10:53,751 --> 01:10:55,305
이미지에 존재합니다.

1552
01:10:55,305 --> 01:10:57,991
그리고 심지어 이미지의 다른 부분에 국한되어,

1553
01:10:57,991 --> 01:11:01,191
다른 영역에 어떤 유형의 모서리가 있는지.

1554
01:11:01,191 --> 01:11:03,546
어쩌면 왼쪽에있는이 개구리의 경우,

1555
01:11:03,546 --> 01:11:04,938
당신은 그가 잎에 앉아있는 것을 볼 수 있습니다,

1556
01:11:04,938 --> 01:11:07,561
이 나뭇잎들은이 지배적 인 대각선 모서리를 가지며,

1557
01:11:07,561 --> 01:11:10,480
방향 그라디언트의 히스토그램을 시각화하면

1558
01:11:10,480 --> 01:11:12,833
기능을 사용하면이 지역에서

1559
01:11:12,833 --> 01:11:14,667
우리는 대각선 가장자리가 많습니다.

1560
01:11:14,667 --> 01:11:16,227
방향성 그라디언트의 히스토그램

1561
01:11:16,227 --> 01:11:19,340
특징 표현의 캡쳐.

1562
01:11:19,340 --> 01:11:21,509
그래서 이것은 매우 일반적인 특징 표현이었습니다.

1563
01:11:21,509 --> 01:11:23,535
물체 인식에 많이 사용되었습니다.

1564
01:11:23,535 --> 01:11:25,702
사실 너무 오래 전에.

1565
01:11:26,573 --> 01:11:29,789
거기에서 볼 수있는 또 다른 특징 표현

1566
01:11:29,789 --> 01:11:32,810
단어의 가방이 아이디어입니다.

1567
01:11:32,810 --> 01:11:34,202
그래서 이것은 영감을 얻고 있습니다.

1568
01:11:34,202 --> 01:11:36,355
자연 언어 처리에서.

1569
01:11:36,355 --> 01:11:38,220
그래서 단락이 있다면,

1570
01:11:38,220 --> 01:11:40,799
그런 다음 단락을 나타낼 수있는 방법

1571
01:11:40,799 --> 01:11:43,398
특징 벡터에 의해 발생 횟수를 세고있다.

1572
01:11:43,398 --> 01:11:45,732
그 단락의 다른 단어들.

1573
01:11:45,732 --> 01:11:47,666
그래서 우리는 그 직감을 받아 적용하고 싶습니다.

1574
01:11:47,666 --> 01:11:49,664
어떤 식 으로든 이미지에.

1575
01:11:49,664 --> 01:11:51,708
그러나 문제는 실제로는 단순한 것이 아니라,

1576
01:11:51,708 --> 01:11:54,288
이미지에 대한 단어의 직접적인 유추,

1577
01:11:54,288 --> 01:11:56,632
그래서 우리는 우리 자신의 어휘를 정의 할 필요가있다.

1578
01:11:56,632 --> 01:11:57,965
시각적 단어의

1579
01:11:58,880 --> 01:12:01,106
그래서 우리는이 2 단계 접근법을 취합니다.

1580
01:12:01,106 --> 01:12:04,318
먼저 우리는 많은 이미지를 얻을 것입니다.

1581
01:12:04,318 --> 01:12:06,455
작은 무작위 농작물을 한꺼번에 채취하다.

1582
01:12:06,455 --> 01:12:07,995
그 이미지들로부터

1583
01:12:07,995 --> 01:12:09,723
K와 같은 것을 사용하는 것을 의미합니다.

1584
01:12:09,723 --> 01:12:12,820
이 다른 클러스터 센터를 생각해 내야한다.

1585
01:12:12,820 --> 01:12:15,189
어쩌면 다른 유형을 나타내는 것입니다.

1586
01:12:15,189 --> 01:12:17,139
이미지의 시각적 단어.

1587
01:12:17,139 --> 01:12:19,341
이 예제를 여기 오른쪽에서 보면,

1588
01:12:19,341 --> 01:12:21,160
이것은 클러스터링의 실제 예입니다.

1589
01:12:21,160 --> 01:12:23,335
이미지에서 실제로 다른 이미지 패치,

1590
01:12:23,335 --> 01:12:25,627
이 클러스터링 단계 후에,

1591
01:12:25,627 --> 01:12:28,450
우리의 시각적 인 단어는이 다른 색을 포착합니다.

1592
01:12:28,450 --> 01:12:30,552
빨간색과 파란색과 노란색처럼,

1593
01:12:30,552 --> 01:12:32,553
이러한 다양한 유형의 지향 에지

1594
01:12:32,553 --> 01:12:34,556
다른 방향으로,

1595
01:12:34,556 --> 01:12:36,482
우리가보기 시작한 것이 흥미 롭습니다.

1596
01:12:36,482 --> 01:12:38,909
이러한 지향 에지는 데이터에서 나옵니다.

1597
01:12:38,909 --> 01:12:40,480
데이터 중심 방식으로

1598
01:12:40,480 --> 01:12:43,097
그리고 이제 우리가 이러한 일련의
시각적 단어들을 얻게되면,

1599
01:12:43,097 --> 01:12:44,291
또한 코드북이라고 불리는,

1600
01:12:44,291 --> 01:12:47,249
그러면 우리는 우리의 이미지를 인 코드 할 수 있습니다.

1601
01:12:47,249 --> 01:12:48,862
이러한 시각적 단어들 각각에 대해,

1602
01:12:48,862 --> 01:12:52,468
이 시각적 단어는 이미지에서 얼마나 발생합니까?

1603
01:12:52,468 --> 01:12:54,082
그리고 이제 이것은 우리에게 다시금,

1604
01:12:54,082 --> 01:12:55,463
약간 다른 정보

1605
01:12:55,463 --> 01:12:59,427
이 이미지의 시각적 모양은 무엇입니까?

1606
01:12:59,427 --> 01:13:02,124
실제로 이것은 일종의 특징 표현입니다.

1607
01:13:02,124 --> 01:13:04,638
황비홍 (Fei-Fei)은 대학원생이었을 때 일했고,

1608
01:13:04,638 --> 01:13:07,555
그래서 이것은 당신이 실제로 보았던 무언가입니다.

1609
01:13:07,555 --> 01:13:08,972
그리 오래 전 아니에요.

1610
01:13:10,783 --> 01:13:13,033
그래서 약간의 티저로,

1611
01:13:14,951 --> 01:13:16,837
이 모든 것을 다시 묶어서,

1612
01:13:16,837 --> 01:13:19,743
이 이미지 분류 파이프 라인

1613
01:13:19,743 --> 01:13:20,886
같이 보일지도 모른다.

1614
01:13:20,886 --> 01:13:22,725
어쩌면 5 년에서 10 년 전쯤에

1615
01:13:22,725 --> 01:13:24,421
당신이 당신의 이미지를 찍을 것이고,

1616
01:13:24,421 --> 01:13:26,677
그런 다음 이러한 다양한 특징 표현을 계산할 수 있습니다.

1617
01:13:26,677 --> 01:13:28,809
너의 심상의, 낱말의 부대 같이 것,

1618
01:13:28,809 --> 01:13:31,173
또는 방향 그라디언트의 히스토그램,

1619
01:13:31,173 --> 01:13:33,381
모든 기능을 함께 연결하고,

1620
01:13:33,381 --> 01:13:35,519
이러한 피쳐 추출기로 피드

1621
01:13:35,519 --> 01:13:38,590
일부 선형 분류기로

1622
01:13:38,590 --> 01:13:39,461
나는 조금 단순화하고있다.

1623
01:13:39,461 --> 01:13:42,018
파이프 라인은 그보다 조금 더 복잡했습니다.

1624
01:13:42,018 --> 01:13:44,576
그러나 이것은 일반적인 직감입니다.

1625
01:13:44,576 --> 01:13:48,158
그리고 그 아이디어는 당신이 추출한 후였습니다.

1626
01:13:48,158 --> 01:13:50,196
이러한 기능들,이 피쳐 추출기

1627
01:13:50,196 --> 01:13:52,326
업데이트되지 않는 고정 된 블록이 될 것입니다.

1628
01:13:52,326 --> 01:13:53,563
훈련 도중.

1629
01:13:53,563 --> 01:13:54,396
그리고 훈련 도중,

1630
01:13:54,396 --> 01:13:55,933
선형 분류 자만 업데이트하면됩니다.

1631
01:13:55,933 --> 01:13:57,907
기능 상단에서 작업하는 경우

1632
01:13:57,907 --> 01:13:59,972
그리고 실제로, 저는 일단 우리가 움직이면

1633
01:13:59,972 --> 01:14:01,774
길쌈 신경망 (convolutional neural networks)

1634
01:14:01,774 --> 01:14:03,140
이 깊은 신경 네트워크,

1635
01:14:03,140 --> 01:14:06,486
실제로 다른 것을 보지 못합니다.

1636
01:14:06,486 --> 01:14:08,651
유일한 차이점은 쓰기보다는

1637
01:14:08,651 --> 01:14:10,322
미리 기능,

1638
01:14:10,322 --> 01:14:12,687
우리는 데이터에서 직접 기능을 배우려고합니다.

1639
01:14:12,687 --> 01:14:15,916
그래서 우리는 원시 픽셀을 가져다가 먹일 것입니다.

1640
01:14:15,916 --> 01:14:17,530
이것을 컨볼 루션 네트워크에 연결함으로써,

1641
01:14:17,530 --> 01:14:19,687
이것은 여러 다른 레이어를 통해 컴퓨팅을 끝낼 것입니다.

1642
01:14:19,687 --> 01:14:21,488
어떤 유형의 피쳐 표현

1643
01:14:21,488 --> 01:14:23,459
데이터에 의해 주도되고 실제로 훈련하게됩니다.

1644
01:14:23,459 --> 01:14:26,120
이 전체 네트워크에 대한이 전체 가중치,

1645
01:14:26,120 --> 01:14:27,954
선형 분류기의 가중치가 아닌

1646
01:14:27,954 --> 01:14:28,787
위에.

1647
01:14:30,329 --> 01:14:32,970
그럼, 다음에 우리는이 아이디어에 뛰어들 것입니다.

1648
01:14:32,970 --> 01:14:36,131
좀 더 자세히 살펴보면, 우리는 몇
가지 신경망을 소개 할 것이며,

1649
01:14:36,131 --> -00:00:00,800
backpropagation에
대해서도 이야기를 시작하십시오.

