1
00:00:09,679 --> 00:00:13,891
12시가 지났으니
진행하도록 하겠습니다.

2
00:00:13,891 --> 00:00:17,822
이번 8강에서는
Deep learning software에 대해 알아볼 것입니다.

3
00:00:17,822 --> 00:00:21,283
매년 아주 많은 변화가 생기는
정말 재미있는 주제입니다.

4
00:00:21,283 --> 00:00:25,621
다만 매년 너무 많이 바뀌어서
매번 강의준비가 좀 빡세긴 합니다.

5
00:00:25,621 --> 00:00:30,024
우선 몇가지 공지사항을 전달하겠습니다.

6
00:00:30,024 --> 00:00:34,563
우선 지난 화요일에 프로젝트 기획서
제출기한이 마감되었습니다.

7
00:00:34,563 --> 00:00:42,766
다들 좋은 아이디어를 가지고 기한내에 잘 제출했길 바랍니다.

8
00:00:42,766 --> 00:00:50,217
현재 프로젝트 분야에 맞는 적절한 TA를 배치중에 있습니다.

9
00:00:50,217 --> 00:00:54,264
추후에 더 많은 정보가 공지될 것입니다.

10
00:00:54,264 --> 00:00:56,563
그리고 현재 과제1을 채점중에 있습니다.

11
00:00:56,563 --> 00:01:00,942
가능한 빨리 점수를 알려드리도록 하겠습니다.

12
00:01:00,942 --> 00:01:08,680
그리고 과제2의 제출기한도 얼마 남지 않았습니다.
다음 주 까지 라는 것을 꼭 명심하시기 바랍니다.

13
00:01:08,680 --> 00:01:16,231
과제2를 진행할 때 반드시 Google Cloud를 잘 종료시켜서
각자 크래딧을 아끼도록 하세요

14
00:01:16,231 --> 00:01:24,812
그리고 다시한번 말씀드리고 싶은 것은 과제2에서 GPU를
사용한만한 것은 마지막 문제뿐이 없다는 것입니다.

15
00:01:24,812 --> 00:01:32,250
나머지 문제들은 python과 numpy만 사용하기 때문에
GPU를 쓸 일이 없습니다.

16
00:01:32,250 --> 00:01:36,701
GPU는 정말 필요할 때만 사용하셔서 크래딧을 아껴 쓰시기 바랍니다.

17
00:01:36,701 --> 00:01:39,973
마지막으로 이제 곧 중간고사가 예정되어 있습니다.

18
00:01:39,973 --> 00:01:45,683
벌써 중간고사 기간이라는 것이 믿어지지 않지만
중간고사는 5월 9일 화요일 수업시간에 진행됩니다.

19
00:01:45,683 --> 00:01:47,901
중간고사는 이론중심으로 출제될 것입니다.

20
00:01:47,901 --> 00:01:57,071
지금까지 수업을 잘 이해하고 있는지를 확인하기 위한
몇가지 이론적인 문제들을 출제할 것입니다.

21
00:01:57,071 --> 00:02:02,506
몇 가지 예상문제를 올려놓도록 하겠습니다.

22
00:02:02,506 --> 00:02:03,695
질문 있나요?

23
00:02:03,695 --> 00:02:05,310
[학생이 질문]

24
00:02:05,310 --> 00:02:10,675
오픈북이냐고 질문하셨는데요
오픈북은 아닙니다.

25
00:02:10,675 --> 00:02:15,671
지금까지 항상 클로즈북으로 진행해 왔습니다.

26
00:02:15,671 --> 00:02:21,735
여러분이 수업에서 다룬 것들의 직관들을 얼마나
이해했는지를 확인하는 것 위주로 출제될 것입니다.

27
00:02:23,618 --> 00:02:27,577
자 그럼 지난시간의 내용을 간단하게 복습해 보도록 하겠습니다.

28
00:02:27,577 --> 00:02:29,737
지난 시간에 다양한 딥러닝 최적화 알고리즘을 배웠습니다.

29
00:02:29,737 --> 00:02:34,975
 SGD, Momentum, Nesterov, RMSProp 그리고 Adam
에 대해서 배웠습니다.

30
00:02:34,975 --> 00:02:45,492
이 방법들을 모두 기본적인 SGD를 조금씩 변형시킨 방법이었죠
구현 자체는 간단하나 네트워크의 수렴속도는 더 빨랐습니다.

31
00:02:45,492 --> 00:02:48,529
그리고 regularization에 대해서도 배웠습니다.
특히나 Dropout에 관한 것이었죠

32
00:02:48,529 --> 00:02:56,975
Dropout은 forward pass에서 임의의 부분은 0으로 만들고
test time에서는 그 noise를 marginalize out 했습니다.

33
00:02:56,975 --> 00:03:02,805
그리고 딥러닝에서 사용하는 다양한 regularization들의
일반적인 패턴에 대해서는 배웠습니다.

34
00:03:02,805 --> 00:03:08,415
Train time에 noise를 추가하고, Test time에는
marginalize out 합니다.

35
00:03:08,415 --> 00:03:15,376
그리고 Transfer learning도 배웠습니다. Pre-train
model을 다운받아서 fine-tune 하는 방법이죠

36
00:03:15,376 --> 00:03:21,314
Transfer learning을 이용하면 데이터셋이 많지 않더라도
딥러닝 문제를 다룰 수 있는 방법이었습니다.

37
00:03:22,781 --> 00:03:29,615
이번 시간에는 화제를 조금 돌려서 소프트웨어/하드웨어가 동작하는
방식에 대해서 조금 다배워보도록 하겠습니다.

38
00:03:29,615 --> 00:03:36,276
그리고 실제 여러분이 학습 시 사용하는 소프트웨어들을
조금 더 심도깊게 다뤄보도록 하겠습니다.

39
00:03:36,276 --> 00:03:43,967
정리하자면 CPU와 GPU에 대해 조금 배울 것이고, 요즘 사람들이
가장 많이 사용하는 립러닝 프레임워크들에 대해서도 배울 것입니다.

40
00:03:45,471 --> 00:03:52,961
자 우선 이런 얘기는 수도 없이 많이 들으셨을 것입니다.
컴퓨터에 CPU와 GPU가 있다는 것 말입니다.

41
00:03:52,961 --> 00:04:02,655
딥러닝에서 GPU를 사용하는데, 사실 지금까지 왜 GPU가 CPU가 좋은지
에 대해서 명확하게 언급하지 않고 넘어갔습니다.

42
00:04:02,655 --> 00:04:06,472
컴퓨터를 조립해본 적이 있나요?
손 한번 들어보시겠습니까

43
00:04:06,472 --> 00:04:10,965
한 1/3에서 절반정도는 계시는군요

44
00:04:10,965 --> 00:04:15,174
이 사진은 제 컴퓨터입니다. 제가 직접 조립했죠

45
00:04:15,174 --> 00:04:22,261
컴퓨터 내부에서 많은 부품들이 있습니다. 아마 여러분들은 각각이
무엇인지 아실 거라고 믿습니다.

46
00:04:22,261 --> 00:04:25,594
CPU는 Central Processing Unit이죠

47
00:04:25,594 --> 00:04:31,391
이 조그마한 칩은 쿨러 믿에 숨어있습니다.

48
00:04:31,391 --> 00:04:39,555
CPU는 엄청 작습니다. 공간을 크게 잡아먹지 않죠

49
00:04:39,555 --> 00:04:46,221
자 여기 GPU들을 보면 공간을 엄청 차지하고 있습니다.
괴물같은 녀석들입니다.

50
00:04:46,221 --> 00:04:50,296
GPU는 자기만의 쿨러가 있고 파워도 엄청 먹고 엄청 큽니다.

51
00:04:50,296 --> 00:04:59,139
GPU가 얼마나 많은 파워를 사용하는지 그리고 얼마나 큰지는
케이스에서 얼마나 많은 부분을 차지하는지만 봐도 알 수 있습니다.

52
00:04:59,139 --> 00:05:04,516
GPU란 것이 무엇이길래 딥러닝에서 이렇게 중요한 걸까요

53
00:05:04,516 --> 00:05:08,937
GPU는 graphics card 혹은
Graphics Processing Unit 이라고 합니다.

54
00:05:08,937 --> 00:05:16,166
사실 GPU는 computer graphics를 랜더링하기 위해서
만들어졌습니다. 게임같은 것들을 위해서죠

55
00:05:16,166 --> 00:05:23,247
자 다시한번 손을 들어보죠, 여기에 집에서 컴퓨터게임을
하시는 분들이 계십니까?

56
00:05:23,247 --> 00:05:25,693
절반정도 있군요 좋습니다.

57
00:05:25,693 --> 00:05:32,196
게임을 하시거나 컴퓨터를 직접 조립해 보신 분들이라면
다음과 같은 질문에 대한 나름대로의 의견을 가지고 계실 것입니다.

58
00:05:32,196 --> 00:05:34,095
[웃음]

59
00:05:34,095 --> 00:05:37,666
이는 Computer Science에서 가장 큰 논쟁 중 하나입니다.

60
00:05:37,666 --> 00:05:42,620
Intel vs AMD 라던가, 그래픽카드의 경우
NVIDIA vs AMD 같은 논쟁들이죠

61
00:05:42,620 --> 00:05:45,394
텍스트 에디터의 경우 Vim vs Emacs도 있겠군요

62
00:05:45,394 --> 00:05:51,945
대부분의 게이머들은 이 논쟁에 대해서
자기만의 의견을 가지고 있습니다.

63
00:05:51,945 --> 00:05:59,116
딥러닝의 경우는 대부분 한쪽만 선택합니다.
NVIDIA죠

64
00:05:59,116 --> 00:06:05,117
만약 AMD 카드를 가지고있다면 아마 딥러닝에 사용하는데
문제가 많을 것입니다.

65
00:06:05,117 --> 00:06:08,812
지난 수년간 NVIDIA는 딥러닝에 많은 공을 들여왔습니다.

66
00:06:08,812 --> 00:06:11,997
NVIDIA의 전략은 세간의 주목을 받았습니다.

67
00:06:11,997 --> 00:06:19,354
Nvidia의 많은 엔지니어들은 딥러닝에 적합한 하드웨어를
만들기 위해서 많은 노력을 했습니다.

68
00:06:19,354 --> 00:06:27,718
그래서 딥러닝과 관련해서는 NVIDIA GPU가
 거의 독점적으로 언급됩니다.

69
00:06:27,718 --> 00:06:35,268
아마 미래에는 또 다른 후발주자가 생겨날 지 모르겠지만
적어도 지금은 NVIDIA가 독점적입니다.

70
00:06:35,268 --> 00:06:41,705
자 그럼 CPU와 GPU의 차이는 무엇일까요? 제가 아주 간단하게
표로 한번 정리해 봤습니다.

71
00:06:41,705 --> 00:06:52,079
위에 두개는 Intel의 최신 상업용 CPU들이고 밑에 두개는
NVIDIA의 최신 상업용 GPU들입니다.

72
00:06:52,079 --> 00:06:55,975
여기에는 몇 가지 주요 트렌드가 있습니다.

73
00:06:55,975 --> 00:07:03,284
GPU와 CPU모두 임의의 명령어를 수행할 수 있는
범용 컴퓨팅 머신입니다.

74
00:07:03,284 --> 00:07:05,987
하지만 이 둘은 질적으로 아주 다릅니다.

75
00:07:05,987 --> 00:07:16,714
CPU의 경우 core의 수가 적습니다. 오늘날의 상업용 테스크탑 CPU의
경우에 코어가 4개에서 6개 조금 많으면 10개 정도입니다.

76
00:07:16,714 --> 00:07:24,893
그리고 hyperthreading 기술와 더불어 CPU는 8 ~ 20개의
스레드를 동시에 실행시킬 수 있습니다.

77
00:07:24,893 --> 00:07:29,700
따라서 CPU는 한번에 20가지의 일(스레드)을
할 수 있는 것입니다.

78
00:07:29,700 --> 00:07:34,527
이 숫자(20)가 그닥 커보이진 않습니다. 하지만
CPU의 멀티스레드는 아주 강력합니다.

79
00:07:34,527 --> 00:07:37,223
아주 많은 일을 할 수 있으면 엄청 빠릅니다.

80
00:07:37,223 --> 00:07:43,011
모든 CPU 명령어들은 정말 많은 일을 할 수 있죠
그리고 아주 독립적으로 수행합니다.

81
00:07:43,011 --> 00:07:51,909
하지만 GPU의 경우는 상황이 조금 다릅니다.
고성능의 상업 GPU의 경우 수천개의 코어가 있습니다.

82
00:07:51,909 --> 00:08:00,412
NVIDIA Titan XP와 같은 최상위 모델의 경우 3840개의
코어가 있습니다. 정말 엄청난 수 입니다.

83
00:08:02,223 --> 00:08:06,357
동일 가격의 CPU를 보면 코어가 10개 뿐이 없죠

84
00:08:06,357 --> 00:08:12,207
하지만 GPU의 단점을 보자면 각각의 코어가
더 느린 clock speed에서 동작한다는 점입니다.

85
00:08:12,207 --> 00:08:14,439
그리고 두번째는 그 코어들이 그렇게 많은 일을 할 수 없다는 것입니다.

86
00:08:14,439 --> 00:08:19,680
사실 CPU와 GPU코어는 1:1로 비교할 수 없습니다.

87
00:08:19,680 --> 00:08:22,510
GPU코어들은 독립적으로 동작하지 않습니다.

88
00:08:22,510 --> 00:08:29,297
코어마다 독립적인 테스크가 있는 것이 아니라 많은 코어들이
하나의 테스크를 병렬적으로 수행하는 것이죠

89
00:08:29,297 --> 00:08:32,405
따라서 코어의 수만 가지고 직접적으로 비교할 수는 없습니다.

90
00:08:32,405 --> 00:08:41,370
하지만 GPU의 코어의 수가 많다는 것은 어떤 테스크가 있을 때
이 일을 병렬로 수행하기 아주 적합하다는 것은 알 수 있습니다.

91
00:08:41,370 --> 00:08:44,742
하지만 그 테스크는 전부 같은 테스크여야 할 것입니다.

92
00:08:44,742 --> 00:08:49,387
그리고 또 한가지 말씀드릴 것은 메모리와 관련된 것입니다.

93
00:08:49,387 --> 00:08:58,523
CPU에도 캐시가 있습니다. 하지만 비교적 작습니다. CPU는 대부분의
메모리를 RAM에서 끌어다 씁니다.

94
00:08:58,523 --> 00:09:06,589
RAM은 일반적으로  8, 12, 16, 32 GB 바이트 정도이죠

95
00:09:06,589 --> 00:09:10,646
반면 GPU는 칩 안에 RAM이 내장되어 있습니다.

96
00:09:12,055 --> 00:09:22,675
실제 RAM와 GPU간의 통신은 상당한 보틀넥을 초래합니다.
그렇게 떄문에 GPU는 보통 칩에 RAM이 내장되어 있습니다.

97
00:09:23,955 --> 00:09:33,481
Titan XP의 경우 내장 메모리가 12GB정도 됩니다.

98
00:09:33,481 --> 00:09:41,790
GPU는 12GB의 메모리와 GPU 코어 사이의 캐싱을 하기 위한
일종의 다계층 캐싱 시스템을 가지고 있습니다.

99
00:09:41,790 --> 00:09:46,908
이는 CPU의 캐싱 계층구조와 아주 유사합니다.

100
00:09:47,985 --> 00:09:52,583
CPU는 범용처리에 적합합니다.
CPU는 아주 다양한 일을 할 수 있죠

101
00:09:52,583 --> 00:09:57,089
그리고 GPU는 병렬처리에 더 특화되어 있습니다.

102
00:09:57,089 --> 00:10:04,106
GPU에서 정말 잘 동작하고 아주 적합한 알고리즘은 바로
행렬곱(Matrix multiplication) 연산입니다.

103
00:10:04,106 --> 00:10:14,348
왼쪽 행렬은 행이 엄청 많은 행렬입니다.
오른쪽은 열이 많죠

104
00:10:14,348 --> 00:10:25,009
오른쪽 행렬은 왼쪽 두 행렬의 내적입니다. 그리고 이 때의
내적 연산은 모두 서로 독립적입니다.

105
00:10:25,009 --> 00:10:33,653
오른쪽의 결과행렬을 살펴보면 각각의 원소가 전부 독립적입니다.
따라서 모두 병렬로 수행될 수 있습니다.

106
00:10:33,653 --> 00:10:38,289
그리고 각 원소들은 모두 같은 일을 수행합니다.
두개의 벡터를 내적하는 것이죠.

107
00:10:38,289 --> 00:10:44,177
다만 서로 입력 데이터만 조금씩 다를 뿐입니다.

108
00:10:44,177 --> 00:10:55,166
GPU는 결과 행렬의 각 요소들을 병렬로 계산할 수 있으며
이러한 특성 때문에 GPU는 엄청나게 빠릅니다.

109
00:10:55,166 --> 00:11:04,940
이런 연산들은 GPU가 정말 잘 하는 것들입니다.
CPU였다면 각 원소를 하나씩만 계산할 것입니다.

110
00:11:06,337 --> 00:11:13,829
물론 CPU라고 그렇게 단순하게 동작하지는 않습니다. CPU는
여러개의 코어가 있고 Vectorized instructions이 존재합니다.

111
00:11:13,829 --> 00:11:19,568
그렇다고 해도 아주 massive한 병렬화 문제에 대해서는
GPU의 처리량이 압도적입니다.

112
00:11:19,568 --> 00:11:25,404
가령 행렬의 크기가 엄청 큰 경우가 될 수 있겠습니다. 그리고
비슷한 맥락으로 Convolution의 경우도 같습니다.

113
00:11:25,404 --> 00:11:36,359
convolution에는 입력(텐서)이 있고 가중치가 있죠 cov 출력은
마찬가지로 입력과 가중치간의 내적입니다.

114
00:11:36,359 --> 00:11:43,354
GPU의 경우라면 이 연산을 각 코어에 분배시켜서
아주 빠르게 연산할 수 있도록 해줍니다.

115
00:11:43,354 --> 00:11:49,510
이런 종류의 연산들이 일반적으로 CPU에 비해 GPU에서
연산 속도의 이점을 볼 수 있는 것들입니다.

116
00:11:51,695 --> 00:11:55,498
여러분들도 직접 GPU에서 실행되는 코드를 작성할 수 있습니다.

117
00:11:55,498 --> 00:12:03,614
NVIDIA에서 CUDA를 지원하는데 그 코드를 보면 c언어
스럽게 생겼습니다. 하지만 GPU에서 실행되는 코드입니다.

118
00:12:03,614 --> 00:12:05,484
하지만 CUDA코드를 작성하는 것은 상당히 까다롭습니다.

119
00:12:05,484 --> 00:12:12,002
GPU의 성능을 전부 짜낼 수 있는 코드를 작성하는 것은
상당히 힘든 일입니다.

120
00:12:12,002 --> 00:12:19,163
아주 세심하게 메모리구조를 관리해야 합니다. 가령 cache misses나
branch mispredictions 같은 것들을 전부 고려해야 하죠

121
00:12:19,163 --> 00:12:22,930
따라서 여러분 스스로 아주 효율적인 CUDA code를
작성하는 것은 상당히 어렵습니다.

122
00:12:22,930 --> 00:12:32,537
때문에 NVIDIA는 GPU에 고도로 최적화시킨 기본연산
라이브러리를 배포해 왔습니다.

123
00:12:32,537 --> 00:12:40,610
가령 cuBLAS는 다양한 행렬곱을 비롯한 연산들을 제공합니다.
이는 아주 고도로 최적화되어 있습니다.

124
00:12:40,610 --> 00:12:46,438
이는 GPU에서 아주 잘 동작하고 하드웨어 사용의
이론적 최대치까지 끌어올려 놓은 라이브러리입니다.

125
00:12:46,438 --> 00:12:54,499
유사하게 cuDNN이라는 라이브러리도 있습니다. 이는 convolution,
forward/backward pass, batch norm, rnn 등

126
00:12:54,499 --> 00:12:57,454
등 딥러닝에 필요한 거의 모든 기본적인 연산들을
제공하고 있습니다.

127
00:12:57,454 --> 00:13:03,842
NVIDIA는 자사의 하드웨어에 아주 효율적으로 동작하는
라이브러리를 바이어리로 배포하고 있습니다.

128
00:13:03,842 --> 00:13:09,624
따라서 실제로는 딥러닝을 위해 CUDA 코드를 직접 작성하는
일은 없을 것입니다.

129
00:13:09,624 --> 00:13:14,173
이미 다른 사람들이 작성한 코드를 불러와서 쓰기만 하면 됩니다.

130
00:13:14,173 --> 00:13:19,573
이미 NVIDIA에서 극도로 최적화된 소스를 가져와서 쓰기만 하면 됩니다.

131
00:13:19,573 --> 00:13:23,693
그리고 또 하나의 언어가 있는데 OpenCL입니다.
OpenCL이 조금 더 범용적입니다.

132
00:13:23,693 --> 00:13:29,185
NVIDIA GPU에서만 동작하는 것이 아니라 AMD 에서도
그리고 CPU에서도 동작합니다.

133
00:13:29,185 --> 00:13:43,938
하지만 OpenCL은 아직 딥러닝에 극도로 최적화된 연산이나 라이브러리가
개발되지는 않았습니다. 그래서 CUDA보다는 성능이 떨어집니다.

134
00:13:43,938 --> 00:13:51,839
미래에는 더 많은 open standard가 생기고 이종 플랫폼 간의
다양한 방법들이 생겨날지도 모르겠지만

135
00:13:51,839 --> 00:13:55,488
현재로썬 NVIDIA가 딥러닝의 선두주자입니다.

136
00:13:55,488 --> 00:14:01,686
GPU 프로그래밍을 직접 해보면 익힐 수 있는 다양한 리소스들이 있습니다.
아마 아주 흥미로울 것입니다.

137
00:14:01,686 --> 00:14:05,900
아주 massive한 병령처리 아키텍쳐를 다루는 것이기 때문에
코드를 작성하는 패러다임도 조금 다를 것입니다.

138
00:14:05,900 --> 00:14:08,023
하지만 이 이상은 우리 강의의 주제를 벗어나기 때문에
여기까지만 하겠습니다.

139
00:14:08,023 --> 00:14:12,263
다시한번 만씀드리지만 GPU로의 딥러닝을 위해서
굳이 스스로 CUDA code를 작성할 필요는 없습니다.

140
00:14:12,263 --> 00:14:16,600
사실 저의 경우도 연구 프로젝트를 진행하면서
CUDA  code를 직접 짜본 적은 없습니다.

141
00:14:16,600 --> 00:14:22,219
하지만 여러분이 직접 코드를 짜진 않더라도 어떻게 동작하는지
잘 알아두는 것은 상당히 유욜할 수 있습니다.

142
00:14:23,488 --> 00:14:29,168
실제 CPU와 GPU의 성능을 한번 살펴보고 싶으시다면,
제가 작년 여름이 벤치마크 한 것이 있습니다.

143
00:14:29,168 --> 00:14:36,065
그때 당시 성능이 꽤 괜찮았던 Intel CPU와
성능이 가장 좋았던 NVIDIA GPU를 비교한 것입니다.

144
00:14:38,747 --> 00:14:48,954
더 자세한 내용은 제 Github에서 찾아볼 수 있습니다.
VGG16/19와 다양한 ResNets을 이용했는데요

145
00:14:49,830 --> 00:14:57,114
그리고 그 결과 GPU에서 보통 65에서 75배의 speed up을 볼 수
있었습니다. 완전히 같은 연산량으로 비교한 것이죠

146
00:14:57,114 --> 00:15:00,984
GPU의 경우는 Pascal TitanX입니다.

147
00:15:00,984 --> 00:15:08,604
CPU의 경우 최상위 성능 까지는 아니였지만
Intel E5 Processor였습니다.

148
00:15:08,604 --> 00:15:15,550
하지만 여러분들이 이와 같은 딥러닝 벤치마크를 살펴볼 때
정말 조심해야 할 점이 있습니다.

149
00:15:15,550 --> 00:15:20,103
왜냐하면 비교군 간에 불공평한 비교를 하기 십상이기 떄문이죠

150
00:15:20,103 --> 00:15:26,339
따라서 여러분들이 벤치마크를 살펴볼 때 어떤 부분이 불공평한지를
주도면밀하게 살펴볼 필요가 있습니다.

151
00:15:26,339 --> 00:15:35,855
제가 이 벤치마크에 대해 말씀드려 보자면 이 결과는 사실
CPU에게는 조금 불리합니다.

152
00:15:35,855 --> 00:15:38,721
제가 CPU에서는 성능을 최대화 시킬 수 있을만큼
공을 들이기 않았기 때문입니다.

153
00:15:38,721 --> 00:15:42,483
저는 아마 BLAS 라이브러리가 CPU에서 더 잘 동작하도록
수정해 볼 수도 있었을 것입니다.

154
00:15:42,483 --> 00:15:44,540
그렇게 되면 이 표의 결과 보다는 조금 더 좋았을 것입니다.

155
00:15:44,540 --> 00:15:51,964
하지만 이또한 뛰어난 성능이긴 합니다. Torch를 설치하고
CPU와 GPU로 실행시켜본 것입니다.

156
00:15:51,964 --> 00:15:57,872
이 성능도 나쁘진 않지만 CPU의 성능을 극대화 시킨 것은 아닙니다.

157
00:15:57,872 --> 00:16:02,422
CPU의 성능을 높힐 수 있는 여지는 충분했습니다.

158
00:16:02,422 --> 00:16:15,543
그리고 또 하나의 벤치마크가 있는데 convolution 연산에
cuDNN을 사용한 것과 일반적인 CUDA를 사용한 것입니다.

159
00:16:15,543 --> 00:16:17,623
오픈소스 커뮤니티에 게제된 결과입니다.

160
00:16:17,623 --> 00:16:24,653
여기에서 볼 수 있는 것은 동일한 하드웨어와 동일한 딥러닝
프레임워크를 사용한 것이고 다른 점은 오직

161
00:16:24,653 --> 00:16:37,442
cuDNN/CUDA 입니다. CUDA은 조금 덜 최적화된 것이죠.
cuDNN이 3X의 속도향상이 있습니다.

162
00:16:37,442 --> 00:16:45,202
일반적으로 여러분이 GPU 코드를 직접 작성해야 한다면
cuDNN을 반드시 사용해야 할 것입니다.

163
00:16:45,202 --> 00:16:51,602
cuDNN 라이브러리를 쓰고 안 쓰고의 차이가 거의
3배가 날테니 말입니다.

164
00:16:51,602 --> 00:17:02,882
그리고 실제로 GPU로 학습을 할 때 생기는 문제 중 하나는 바로 Model과
Model의 가중치는 전부 GPU RAM에 상주하고 있는 반면에

165
00:17:02,882 --> 00:17:07,243
실제 Train data(Big data)는 SSD와 같은 하드드라이브에
있다는 것입니다.

166
00:17:07,243 --> 00:17:13,204
때문에 Train time에 디스크에서 데이터를 읽어드리는 작업을
세심하게 신경쓰지 않으면 보틀넥이 발생할 수 있습니다.

167
00:17:14,321 --> 00:17:23,002
GPU는 forward/backward 가 아주 빠른 것은 사실이지만, 디스크
에서 데이터를 읽어드리는 것이 보틀넥이 되는 것입니다.

168
00:17:23,002 --> 00:17:25,699
이는 상당히 좋지 않은 상황이고 느려지게 될 것입니다.

169
00:17:25,700 --> 00:17:31,459
해결책 중 하나는 바로 데이터셋이 작은 경우에는 전체를
RAM에 올려 놓는 것입니다.

170
00:17:31,459 --> 00:17:36,479
데이터셋이 작지 않더라도, 서버에 RAM 용량이 크다면
가능할 수도 있을 것입니다.

171
00:17:36,479 --> 00:17:42,917
혹은 HDD대신에 SSD를 사용하는 방법이 있습니다.
데이터를 읽는 속도를 개선시킬 수 있겠죠

172
00:17:42,917 --> 00:17:52,152
또 한가지 방법은 바로 CPU의 다중스레드를 이용해서 데이터를
RAM에 미리 올려 놓는 것(pre-fetching) 입니다.

173
00:17:52,152 --> 00:17:57,724
그리고 buffer에서 GPU로 데이터를 전송시키게 되면
성능향상을 기대할 수 있을 것입니다.

174
00:17:57,724 --> 00:18:08,804
물론 이런 설정 자체가 조금 까다롭긴 하지만 GPU는 빠른데 데이터
전송 자체가 충분히 빠르지 못하면 보틀넥이 생길수 밖에 없습니다.

175
00:18:08,804 --> 00:18:11,657
이 부분은 항상 인지하고 있어야 합니다.

176
00:18:11,657 --> 00:18:17,432
지금까지는 딥러닝에 관련한 GPU와 CPU에 대한
간략한 소개였습니다.

177
00:18:17,432 --> 00:18:21,616
자 이제 조금 주제를 틀어서
소프트웨어와 관련된 이야기를 해볼까 합니다.

178
00:18:21,616 --> 00:18:25,006
실제로 사람들이 사용하는 딥러닝 프레임워크와 관련된 것입니다.

179
00:18:25,006 --> 00:18:28,819
다음으로 넘어가기에 앞서 혹시 CPU와 GPU에 관한
질문 있으십니까?

180
00:18:28,819 --> 00:18:30,519
네 질문있나요?

181
00:18:30,519 --> 00:18:34,686
[학생이 질문]

182
00:18:40,961 --> 00:18:45,854
질문은 바로 데이터읽기 에서의 병목을 해결하기 위해서
프로그래밍으로 우리가 할 수 있는 일이 무엇인지 입니다.

183
00:18:45,854 --> 00:18:50,833
소프트웨어적으로 할 수 있는 일은
CPU에서 미리 불러오는 것입니다.(pre-fetching)

184
00:18:50,833 --> 00:18:55,054
여러분은 다음과 같이 과정들이 순차적으로 진행되는
것을 원하지 않을 수 있습니다.

185
00:18:55,054 --> 00:18:58,791
디스크에서 데이터를 읽습니다. 그리고 미니배치의 데이터가
다 읽힐 때 까지 기다립니다.

186
00:18:58,791 --> 00:19:02,458
그리고 미니배치를 GPU에 전송합니다.
그리고 GPU에서 forward/backward를 수행하고

187
00:19:02,458 --> 00:19:05,442
그다음 또 다른 미니배치를 읽는 것이죠
이 단계를 전부 순차적으로 진행합니다.

188
00:19:06,714 --> 00:19:15,469
하지만 그러지 말고 CPU의 멀티스레딩을 통해 디스크에서
데이터를 불러오는 백그라운드 작업을 하고 있다면

189
00:19:15,469 --> 00:19:17,076
이 과정을 interleave하게 진행할 수 있을 것입니다.

190
00:19:17,076 --> 00:19:21,506
GPU이 계산을 수행하는 동안 CPU의 백그라운드 스레드는
디스크에서 데이터를 불러옵니다.

191
00:19:21,506 --> 00:19:28,534
그리고 CPU의 메인 스레드에서 synchronization를 관리하게 되면
이 모든 작업이 병렬로 수행될 수 있을 것입니다.

192
00:19:28,534 --> 00:19:38,016
그리고 고맙게도 여러분이 딥러닝 프레임워크를 사용한다면 이미 다
구현되어 있습니다. 일일이 구현하기 까다롭기 때문이죠

193
00:19:38,016 --> 00:19:41,738
딥러닝 프레임워크의 동향을 급변하고 있습니다.

194
00:19:41,738 --> 00:19:47,915
작년까지만 해도 Caffe, Torch, Tensorflow에 대해서만
언급했었습니다.

195
00:19:47,915 --> 00:20:00,232
1년 전까지만 해도 Tensorflow가 가장 최신의 프레임워크라서
사람들에게 널리 쓰이지 않았었습니다.

196
00:20:00,232 --> 00:20:06,310
하지만 지금은 아주 유명해져서 많은 사람들이 선택하는
메인 프레임워크가 되었습니다. 아주 큰 변화입니다.

197
00:20:07,342 --> 00:20:12,282
그리고 작년부터 정말 많은 프레임워크가 생겨나고 있습니다.

198
00:20:12,282 --> 00:20:18,052
특히나 Caffe2와 PyTorch가 생겼습니다.Facebook에서 밀고있는
프레임워크로 아주 흥미롭습니다.

199
00:20:18,052 --> 00:20:20,409
그 밖에도 정말 많은 프레임워크들이 있습니다.

200
00:20:20,409 --> 00:20:24,089
Baidu에는 Paddle이 있고 Microsoft는 CNTK이 있죠

201
00:20:24,089 --> 00:20:33,449
Amazon은 주로 MXNet을 사용하는데 이 밖에도 정말 많은
프레임워크가 있습니다. 다 써볼 시간도 부족하죠

202
00:20:33,449 --> 00:20:43,572
이 그림에서 주목할만한 재미있는 점은 바로 딥러닝 프레임워크의
초창기 세대는 학계(academia)에서 구축되었다는 것입니다.

203
00:20:43,572 --> 00:20:49,388
원래 Caffe은 Berkeley에서 Torch는 NYU에서 개발되었고
이후 Facebook과 공동연구를 시작한 것이죠

204
00:20:49,388 --> 00:20:52,077
Theana은 대부분 Montreal에서 개발했습니다.

205
00:20:52,077 --> 00:20:56,491
하지만 다음세대의 딥러닝 프레임워크들은
기업(industry)에서 태동했습니다.

206
00:20:56,491 --> 00:21:00,659
Caffe2와 PyTorch는 Facebook에서 나왔습니다.
TensorFlow은 Google에서 나왔죠

207
00:21:00,659 --> 00:21:08,925
최근 몇 년간의 흥미로운 변화는 바로
academia에서 industry로의 이동이라고 할 수 있습니다.

208
00:21:08,925 --> 00:21:13,187
이제는 industry가 이러한 강력한 프레임워크를 제공하고 있습니다.

209
00:21:14,147 --> 00:21:24,850
오늘은 대부분 PyTorch와 TensorFlow에 대해서 말씀드릴 것입니다.
아마 여러분은 앞으로 이 두 프레임워크를 자주 다루게 될 것입니다.

210
00:21:24,850 --> 00:21:32,192
Caffe와 Caffe2도 간단히 설명해 드리겠습니다.
하지만 깊게 말씀드리진 않을 것입니다.

211
00:21:32,192 --> 00:21:36,705
우선 더 깊게 들어가기 전에 제가 어떤 것을 주로 사용했는지
미리 말씀드려야 할 것 같습니다.

212
00:21:36,705 --> 00:21:43,501
저 같은 경우에는 지난 몇 년간 Torch를 이용했습니다.
아주 많이 사용했고 참 좋아하는 녀석입니다.

213
00:21:43,501 --> 00:21:48,568
그리고 얼마전에 PyTorch로 갈아탔습니다.

214
00:21:48,568 --> 00:21:52,306
때문에 저 같은 경우 Tensorflow 같은 다른 플랫폼의
경험은 조금 부족합니다.

215
00:21:52,306 --> 00:21:58,382
그렇지만 저는 여러분들에게 최대한 잘 설명해
드리기 위해 노력 할 것입니다.

216
00:21:58,382 --> 00:22:06,807
자 그럼 지난 강의를 상기해보면 Computational graphs
에 대해서 수도없이 들었을 것입니다.

217
00:22:06,807 --> 00:22:13,176
여러분이 딥러닝과 관련된 작업을 한다면
computational graph를 빼고는 아무 것도 할 수 없죠

218
00:22:13,176 --> 00:22:18,778
여기 선형분류기의 예를 들자면 데이터 X가 있고 가중지 W가 있습니다.
그리고 이를 행렬곱으로 표현하죠

219
00:22:18,778 --> 00:22:22,832
그리고 hinge loss 같은 손실함수를 계산할 것입니다.

220
00:22:22,832 --> 00:22:28,909
regularization term도 있겠죠. 이런 모든 것들을 전부
그래프 구조 안에 표현할 수 있습니다.

221
00:22:28,909 --> 00:22:36,167
Neural net의 경우에 이 그래프구조는 상당히 복잡합니다.
다양한 레이어와 활성함수가 존재할 것입니다.

222
00:22:36,167 --> 00:22:39,687
그리고 아주 많은 가중치들이 그래프 전역에 퍼져 있을 것입니다.

223
00:22:39,687 --> 00:22:47,328
Neural turing machines의 그래프는 훨씬 더 복잡합니다.
너무 크고 복잡해서 그릴 수 조차 없습니다.

224
00:22:48,349 --> 00:22:58,727
따라서 우리가 굳이 코드를 손으로 작성하지 않고 딥러닝 프레임워크
를 이용하는데는 크게 세 가지 이유가 있습니다.

225
00:22:58,727 --> 00:23:08,610
우선 첫째는 딥러닝 프레임워크를 이용하게 되면 이처럼 엄청 복잡한
그래프를 우리가 직접 만들지 않아도 된다는 것입니다.

226
00:23:08,610 --> 00:23:13,956
두번째는, 딥러닝에서는 항상 그래디언트를 계산해야 하죠

227
00:23:14,812 --> 00:23:18,900
Loss를 계산하고 Loss에 부합하는 가중치의 그래디언트를
계산해야 합니다.

228
00:23:18,900 --> 00:23:26,115
그래디언트가 자동으로 계산되면 정말 좋을 것입니다.
이를 계산하는 코드를 직접 짜고싶은 사람은 없겠죠

229
00:23:26,115 --> 00:23:36,539
딥러닝 프레임워크를 사용하면 forward pass만 잘 구현해 놓으면
back propagation은 알아서 구성됩니다.

230
00:23:36,539 --> 00:23:42,000
그리고 마지막 세번째로는 여러분은 GPU을 효율적으로
사용하고 싶을 것입니다.

231
00:23:42,000 --> 00:23:48,389
하지만 cuBLAS, cuDNN, CUDA 그리고 memory등을 여러분이
직접 세심하게 다루고 싶지는 않을 것입니다.

232
00:23:48,389 --> 00:23:52,439
이런 모든 것들을 다루는 것은 정말 까다롭고 힘들 일이죠

233
00:23:52,439 --> 00:23:59,450
이러한 이유 때문에 보통 밑바닥 부터 구현하지 않고
프레임워크를 이용하는 것입니다.

234
00:23:59,450 --> 00:24:05,231
computational graph의 예를 한번 봅시다.
이런 식으로 엄청 간단한 예제를 만들어 볼 수 있겠죠

235
00:24:05,231 --> 00:24:13,071
입력으로 X,Y,Z가 있습니다. X와 Y를 곱하면 a가 됩니다.
a와 Z를 더하면 b가 되죠

236
00:24:13,071 --> 00:24:18,630
그리고 b를 sum out 연산을 이용해 다 더하면 scaler 값
하나가 나옵니다. C 이죠

237
00:24:18,630 --> 00:24:31,631
여러분들은 아마 지금까지의 코드를 Numpy로 충분히
구현할 수 있다고 생각하실 지도 모릅니다.

238
00:24:31,631 --> 00:24:41,923
Numpy를 이용해서 random data를 입력으로 해서 적절하게 곱하고
더하고 하면 아주 쉽게 구현할 수 있겠죠

239
00:24:41,923 --> 00:24:48,355
하지만 X,Y,Z에 대한 그래디언트를 구하는 경우라면
어떨까요?

240
00:24:48,355 --> 00:24:52,725
여러분이 Numpy로 작성한 경우라면 backward도
여러분이 스스로 작성해야 합니다.

241
00:24:52,725 --> 00:25:02,859
아마도 여러분들이 과제에서 많이 해보셨겠지만 이 복잡한 수식을
구현하는 것은 아주 까다롭고 고통스러운 일입니다.

242
00:25:02,859 --> 00:25:05,675
그리고 Numpy의 또 하나의 문제점은 바로 GPU에서
돌아가지 않는다는 것입니다.

243
00:25:05,675 --> 00:25:14,920
Numpy는 CPU에서만 동작하죠. 그렇게 때문에 Numpy만 쓰는 사람들은
GPU의 참맛을 경험해 보지 못합니다.

244
00:25:14,920 --> 00:25:19,527
다시 말씀드리자면 그때 그때 마다 그래디언트를 여러분
스스로 계산하는 것은 상당히 힘든 일입니다.

245
00:25:19,527 --> 00:25:29,047
때문에 딥러닝 프레임워크의 목표는 바로 여러분이 forward
pass 코드를 Numpy 스럽게 작성을 해 놓으면

246
00:25:29,047 --> 00:25:33,069
GPU에서도 동작하고 그래디언트도 알아서 계산해
주는 것입니다.

247
00:25:33,069 --> 00:25:36,397
이는 프레임워크가 존재하는 가장 큰 목적입니다.

248
00:25:36,397 --> 00:25:44,314
Tensorflow의 예를 한번 볼까요? 두 코드의 computational
graph는 동일합니다. forward pass를 한번 봅시다.

249
00:25:44,314 --> 00:25:52,687
Tensorflow 의 코드를 보면 Numpy의 forward pass 코드와
아주 유사합니다. 곱셈/덧셈 연산을 한번 보시죠

250
00:25:52,687 --> 00:25:57,623
하지만 Tensorflow에는 여러분을 위해 그래디언트를 계산해 주는
마법의 코드가 있습니다.

251
00:25:57,623 --> 00:26:02,235
직접 backward pass를 작성 할 필요가 없는 것입니다.
아주 유용한 기능입니다.

252
00:26:02,235 --> 00:26:08,926
그리고 Tensorflow가 좋은 점 중 하나는 명령어 한 줄이면
CPU/GPU에서 동작하도록 전환 시킬 수 있다는 것입니다.

253
00:26:08,926 --> 00:26:16,668
forward pass에서 이런 식으로 선언을 해주면
난 CPU로 돌리고 싶어! 라고 하는것 과 같죠

254
00:26:16,668 --> 00:26:24,866
그리고 단지 'C'를 'G'로 바꾸기만 하면 이 코드는 GPU에서
동작하게 됩니다.

255
00:26:24,866 --> 00:26:31,388
코드 몇 줄만 추가해 주면 두 가지 문제를 해결할 수 있는 것입니다.
우선 GPU를 사용할 수 있게 해주는 것이고

256
00:26:31,388 --> 00:26:35,685
모든 그레디언트를 계산할 수도 있습니다.
아주 큰 장점입니다.

257
00:26:35,685 --> 00:26:38,459
PyTorch도 거의 비슷합니다.

258
00:26:38,459 --> 00:26:42,509
PyTorch의 예로 다시한번 살펴보자면 여기에 번수를 선언하고

259
00:26:42,509 --> 00:26:49,262
forward pass를 진행합니다. 여기에서도 Numpy에서의 코드와
아주 유사하다는 것을 알 수 있습니다.

260
00:26:49,262 --> 00:26:56,251
PyTorch에서도 한줄이면 그레디언트를 계산할 수 있습니다.

261
00:26:56,251 --> 00:27:06,781
PyTorch에서 GPU를 사용하려면 CUDA data type으로 선언하기만
하면 됩니다. 그러면 GPU에서 연산이 수행되죠

262
00:27:06,781 --> 00:27:13,878
지금까지 세가지 예제를 살펴보았습니다.
Numpy, Tensorflow, PyTorch 입니다.

263
00:27:13,878 --> 00:27:20,564
TensorFlow와 PyTorch모두 Numpy 스럽게 생겼습니다.

264
00:27:20,564 --> 00:27:24,349
Numpy가 사용하기 쉽기 때문에 Numpy처럼 생긴 것은
정말 좋은 특성입니다.

265
00:27:24,349 --> 00:27:29,192
Numpy스럽게 생겼을 뿐만 아니라 그레디언트도 알아서 계산해주고
GPU에서도 자동으로 돌아갑니다.

266
00:27:30,186 --> 00:27:37,502
간략한 소개가 끝났으니 이제 TensorFlow에 대해서 좀더
심도깊게 알아보도록 하겠습니다.

267
00:27:37,502 --> 00:27:50,662
앞으로는 이 한가지 예시를 가지고 계속 말씀드릴 것입니다.
두개의 fc layer + ReLU를 학습시키는 네트워크 입니다.

268
00:27:50,662 --> 00:27:55,289
그리고 손실함수로는 L2 Euclidean 를 사용합니다.

269
00:27:55,289 --> 00:28:08,966
이 네트워크의 성능을 기대하긴 어렵겠지만 딥 러닝을 프레임워크를
설명하는데는 아주 재격입니다. 슬라이드 크기에도 딱 맞죠

270
00:28:08,966 --> 00:28:15,900
그리고 이 코드는 numpy와 tensorflow를
이미 import한 것입니다.

271
00:28:15,900 --> 00:28:21,163
Tensorflow의 경우에는 코드가 크게 두가지 스테이지로 나뉩니다.

272
00:28:21,163 --> 00:28:28,363
상단에 빨간색 박스를 보시죠. 우선은 computational graph
를 정의하는 코드를 작성할 것입니다.

273
00:28:28,363 --> 00:28:32,360
그래프를 정의하고 나면 그래프를 실행시켜야겠죠

274
00:28:32,360 --> 00:28:36,851
우선 연산을 수행하려면 데이터를 넣어줘야 합니다.

275
00:28:36,851 --> 00:28:40,961
Tensorflow를 사용할 때 아주 일반적인 순서가 되겠습니다.

276
00:28:40,961 --> 00:28:46,615
우선 그래프를 구성하는 코드를 한번 작성해 놓으면 여러분은
이 그래프를 언제든지 실행시킬 수 있습니다.

277
00:28:48,099 --> 00:28:52,763
그래프를 구성하는 부분을 좀 더 자세히 살펴봅시다.

278
00:28:52,763 --> 00:29:00,709
우선 맨 처음에 X,Y,w1,w2를 정의합니다. 이들은 모두
tf.placeholder 객체입니다.

279
00:29:01,637 --> 00:29:05,193
이들은 그래프의 입력노드가 됩니다.

280
00:29:05,193 --> 00:29:15,379
computational graph의 입구 역할을 하는 것이죠. 가령 데이터를
넣어주고자 하면 이쪽으로 넣어주면 됩니다.

281
00:29:15,379 --> 00:29:21,944
하지만 이 부분에서 실제적인 메모리할당이 일어나진 않습니다.
단지 그래프만 구성하는 것입니다.

282
00:29:23,272 --> 00:29:28,665
앞서 선언한 입력 변수를 이용해 봅시다.

283
00:29:28,665 --> 00:29:37,135
이 변수들을 기반으로  다양한 Tensorflow 연산을 수행합니다.
여러분이 만들고 싶은 연산을 구성할 수 있는 것이죠

284
00:29:37,135 --> 00:29:46,109
이 예제의 경우 X와 w1의 행렬곱 연산을 수행합니다. 그리고
tf.maximum을 이용해 ReLU를 구현할 수 있습니다.

285
00:29:46,109 --> 00:29:49,240
그리고 행렬곱 연산을 한번 더 수행해서 그래프의 최종
출력값을 계산할 수 있습니다.

286
00:29:49,240 --> 00:29:58,175
그리고 예측값과 정답값 Y의 유클리디안 거리를 계산하기
위해서도 Tensor operation 을 사용합니다.

287
00:29:58,175 --> 00:30:05,824
한가지 주목할 점은 이 코드는 현재 아무런 연산도 수행하지 않는다는
점입니다. 아직까지는 계산할 데이터가 존재하지 않습니다.

288
00:30:05,824 --> 00:30:15,001
지금은 단지 그래프 구조만 만들어 놓아서 실제 데이터가
들어왔을때 어떻게 연산을 진행해야 하는지만 구성합니다.

289
00:30:15,001 --> 00:30:18,648
그래프를 구성하는 것 외에 아무 일도일어나지 않습니다.

290
00:30:18,648 --> 00:30:33,135
그리고 이 빨간색 라인을 통해 로스를 계산하고 w1과 w2의 그레디언트를
계산해주는 마법적일 일을 할 수 있습니다.

291
00:30:33,135 --> 00:30:37,981
이를 통해 여러분이 과제에서 했던  backporp 코드를
직접 짜는 수고를 덜 수 있습니다.

292
00:30:37,981 --> 00:30:40,439
이 때도 사실은 실제 계산이 이루어지지는 않습니다.

293
00:30:40,439 --> 00:30:51,108
여기에서도 단지 그래프에 그레디언트를 계산해 주는
추가적인 연산을 더해준 것과 같습니다.

294
00:30:51,108 --> 00:31:01,421
자 지금까지는 computational graph를 구성했습니다. Loss와
그레디언트를 계산하기 위한 커다란 그래프를 만든 것이죠

295
00:31:01,421 --> 00:31:06,843
자 이제 Tensorflow session으로 들어가 봅시다. 이를 통해 실제
그래프를 실행시키고 데이터를 넣어줄 수 있습니다.

296
00:31:06,843 --> 00:31:13,859
session에 진입하려면 그래프에 들어갈 실제 값을
만들어 줘야 합니다.

297
00:31:13,859 --> 00:31:19,459
대부분의 경우 TensorFlow는 Numpy arrays
이용할 수 있습니다.

298
00:31:19,459 --> 00:31:30,226
이 경우에도 Numpy를 이용해서 x,w1,w2,y의 값을
할당해 줬습니다.

299
00:31:30,226 --> 00:31:32,743
그리고 바로 이 부분이 실제로 그래프를 실행시키는 부분입니다.

300
00:31:32,743 --> 00:31:38,120
session.run 을 이용해서 그래프의 일부를 실행시킬
수 있습니다.

301
00:31:38,120 --> 00:31:43,899
첫 번째 인자가 loss라는 것은, 우리가 그래프의 출력으로 어떤
부분을 원하는 지를 말해주는 부분입니다.

302
00:31:43,899 --> 00:31:50,950
예제에서는 우리가 현재 Loss와 grad1과 grad2를 계산하기를
원한다고 말해주고 있는 것입니다.

303
00:31:50,950 --> 00:31:57,140
그리고 feed_dict를 통해 실제 값을 전달해 주는 것이죠

304
00:31:57,140 --> 00:32:06,541
이 한 줄만 실행시키면 그래프가 실행되고
loss와 grad1과 grad2가 계산됩니다.

305
00:32:06,541 --> 00:32:12,003
그리고 출력 값들도 Numpy array입니다.

306
00:32:12,003 --> 00:32:19,859
output이란 변수를 나눠보게 되면  loss와 gradient가
Numpy array의 형태로 반환됨을 알 수 있습니다.

307
00:32:19,859 --> 00:32:23,697
이 값을 가지고 여러분이 원하는 작업을 하시면 됩니다.

308
00:32:23,697 --> 00:32:29,599
이 경우에는 그래프에서 단 한 번의 forward/backward pass
를 수행한 경우 입니다.

309
00:32:29,599 --> 00:32:33,167
네트워크를 학습시키기 위해서는 단 몇 줄이면 충분합니다.

310
00:32:33,167 --> 00:32:45,511
그래프를 여러번 실행시키기 위해서는 for loop를 사용하면 됩니다.
반복적으로 session.run을 호출해 loss와 grad를 계산합니다.

311
00:32:45,511 --> 00:32:52,291
이 예제에서는 가중치를 업데이트하기 위해 그레디언트를 계산해서
"수동으로(manual)"  Gradient diescent를 하고 있습니다.

312
00:32:52,291 --> 00:33:00,749
이 코드의 loss를 계산해 보면 loss가 아주 잘 내려가며 이는
네트워크의 학습이 이루어지는 것입니다. 아주 잘 동작하는 것이죠

313
00:33:00,749 --> 00:33:06,113
이 예제는 TensorFlow에서 네트워크가 어떤 식으로 학습이 되는지를
아주 명확하게 알려줍니다.

314
00:33:06,113 --> 00:33:08,046
하지만 여기에는 문제가 하나 있습니다.

315
00:33:08,046 --> 00:33:15,086
forward pass에서 그래프가 실행될 때마다
가중치를 넣어줘야 합니다.

316
00:33:15,086 --> 00:33:18,835
현재 Numpy로 된 가중치를 가지고 있고 이 값을
그래프에 넣어줘야 하는 것이죠

317
00:33:18,835 --> 00:33:26,339
그래프가 한번 실행되고 나면 그레디언트를 반환해 줬습니다.
그레디언트는 가중치와 동일한 크기의 행렬이었죠

318
00:33:26,339 --> 00:33:32,665
이것이 의미하는 바는 여러분이 그래프를 실행할 때 마다 우선
Numpy array로 된 가중치행렬을 Tensorflow로 복사합니다.

319
00:33:32,665 --> 00:33:36,419
그리고 gradient를 계산해서는 gradient행렬을
Tensorflow에서 Numpy array로 반환해 주는 것이죠

320
00:33:36,419 --> 00:33:39,849
만약 코드가 CPU에서 실행이 되고 있다면
큰 문제가 되지는 않을 것입니다.

321
00:33:39,849 --> 00:33:47,235
하지만 지난번에 GPU의 보틀넥에 대해서 말씀드린 적이 있었죠.
 CPU/GPU 메모리산의 데이터교환을 상당히 비용이 큽니다.

322
00:33:47,235 --> 00:33:59,256
때문에 여러분의 네트워크가 엄청 크고 가중치가 엄청 많다면 이런
GPU/CPU간의 데이터 교환은 이는 엄청 느리고 비용이 클 것입니다.

323
00:33:59,256 --> 00:34:01,689
이것은 안좋은 일입니다. 바꿀 필요가 있습니다.

324
00:34:01,689 --> 00:34:06,027
TensorFlow에는 이에 대한 해결책이 있습니다.

325
00:34:06,027 --> 00:34:17,969
굳이 매번 placeholder를 써서 가중치를 넣어 줄 필요는 없습니다.
대신에 단지 variables로 선언하는 것이죠

326
00:34:17,969 --> 00:34:27,346
variable은 computational graph안에 서식하는 변수입니다.
그래프가 실행될 때 마다 지속적으로 그래프 안에 상주합니다.

327
00:34:27,347 --> 00:34:33,094
따라서 w1와 w2를 placeholder 대신에 variables로
선언해 줍니다.

328
00:34:33,094 --> 00:34:39,219
하지만 이제는 그들이 그래프 안에 살기 때문에 우리는
Tensorflow에게 어떻게 초기화시킬 것인지를 알려줘야 합니다.

329
00:34:39,219 --> 00:34:44,606
왜냐하면 기존에 그래프 밖에 있을때는 그래프에 넣어주기 전에
Numpy를 이용해서 초기화 시켜주면 그만이었습니다.

330
00:34:44,606 --> 00:34:50,569
하지만 지금은 그래프 안에 있기 때문에 그 변수들을 초기화시킬
권한은 TensorFlow에 있는 것입니다.

331
00:34:50,569 --> 00:34:53,149
따라서 우리는 tr.randomnormal 이 필요합니다.

332
00:34:53,149 --> 00:35:00,627
사실 저 명령어 자체가 변수들을 초기화시켜 주는 것은 아니고
Tensorflow에게 어떻게 이 값들이 초기화되야 하는지는 알려줍니다.

333
00:35:00,627 --> 00:35:03,215
이 부분에서 다소 헷갈릴 수 있습니다.

334
00:35:04,869 --> 00:35:11,862
자 그리고 이전 예제에서는  computational graph의
외부에서 가중치를 업데이트했습니다.

335
00:35:11,862 --> 00:35:17,219
이전 예제에서는 그레디언트를 계산하고 Numpy array로
가중치를 업데이트 했죠

336
00:35:17,219 --> 00:35:20,264
그리고 계산된 가중치를 다음 스텝에 다시 넣어줬습니다.

337
00:35:20,264 --> 00:35:29,402
하지만 가중치를 그래프 안에서 업데이트 하기 위해서는 그
연산 자체가 그래프에 포함되어야 합니다.

338
00:35:29,402 --> 00:35:37,020
이를 위해 assign 함수를 이용합니다. 이를 통해 변수가
그래프 내에서 업데이트가 일어나도록 하는 것입니다.

339
00:35:37,020 --> 00:35:41,487
이 업데이트된 값들은 항상 그래프 내부에 존재하게 됩니다.

340
00:35:41,487 --> 00:35:45,976
자 이제 실제 학습을 진행하기에 앞서
(tf.global_variables_initializer() 를 통해)

341
00:35:45,976 --> 00:35:53,825
그래프 내부의 변수들을 초기화시켜 주는 명령어가 필요합니다.

342
00:35:53,825 --> 00:35:58,574
초기화가 끝나면 이제 그래프를 돌릴 차례입니다.

343
00:35:58,574 --> 00:36:05,091
이 코드를 보시면 이제는 데이터와 레이블만 넣어주면 되고
가중치는 그래프 내부에 항상 상주하게 됩니다.

344
00:36:05,091 --> 00:36:09,517
여기에서는 Tensorflow에게 loss를 계산해 달라고 하는 것이죠

345
00:36:09,517 --> 00:36:13,001
여러분은 이 코드로 네트워크를 학습시킬 수 있다고
생각할 수 있습니다.

346
00:36:13,001 --> 00:36:19,964
하지만 사실 이 코드에는 버그가 있습니다. 이 코드를 돌려서
Loss를 그려보면 전혀 학습이 안됩니다.

347
00:36:19,964 --> 00:36:23,401
도대체 왜 이런 것일까요?

348
00:36:23,401 --> 00:36:29,957
assign 코드도 작성했고 그래프를 돌려서 Loss와 그레디언트도
계산을 잘 했는데 Loss가 변하질 않습니다. 왜 그럴까요?

349
00:36:29,957 --> 00:36:31,460
아시는 분 계신가요?

350
00:36:31,460 --> 00:36:34,595
[학생이 대답]

351
00:36:34,595 --> 00:36:44,979
대답은 가중치가 매번 초기화되기 때문이라는 것인데요
그럴듯한 답변이지만 사실 그 문제는 아닙니다.

352
00:36:44,979 --> 00:36:48,057
[학생이 대답]

353
00:36:48,057 --> 00:36:56,318
대답은 바로 우리가 Tensorflow에게 w1과 w2를 업데이트하라고
명시적으로 말해 줘야 한다는 것입니다.

354
00:36:56,318 --> 00:36:58,835
그래서 우리는이 큰 계산 그래프 데이터를 구축했습니다.

355
00:36:58,835 --> 00:37:01,699
구조를 메모리에 저장하고 실행을 호출 할 때,

356
00:37:01,699 --> 00:37:04,894
우리는 TensorFlow에게 우리가
손실을 계산하기를 원한다고 말했습니다.

357
00:37:04,894 --> 00:37:09,155
그리고 그래프 내부의 dependence를 고려 해 보면

358
00:37:09,155 --> 00:37:13,715
Loss를 계산하기 위해서 굳이 업데이트를 할 필요는
없다는 것을 알게 될 것입니다.

359
00:37:13,715 --> 00:37:21,496
Tensorflow는 아주 스마트하기 때문에 output 에 필요한
연산만 수행합니다.

360
00:37:21,496 --> 00:37:26,656
이는 Tensorflow의 장점이기도 합니다. 필요한 부분만
실행하기 때문이지요

361
00:37:26,656 --> 00:37:32,739
하지만 간혹 이 특성이 우리를 헷갈리게 하고 
예상하지 못했던 결과를 초래할 수도 있습니다.

362
00:37:32,739 --> 00:37:39,141
이 경우에는 우리가 Tensorflow에게 업데이트를 수행하라고
명시적으로 말해줘야 합니다.

363
00:37:39,141 --> 00:37:49,531
해결법 중 하나는 new_w1과 new_w2 를 출력으로 추가하면 됩니다.

364
00:37:49,531 --> 00:37:57,366
하지만 new_w1과 new_w2의 사이즈가 큰 tensor라면은 
상황이 안좋아집니다.

365
00:37:58,891 --> 00:38:05,138
Tensorflow에게 출력을 요청하는 것은 매 반복마다 
CPU/GPU간의 데이터 전송이 요구되기 때문입니다.

366
00:38:05,138 --> 00:38:07,316
이는 좋지 않죠

367
00:38:07,316 --> 00:38:11,742
이 대신에 할 수 있는 트릭이 하나 있습니다. 
더미 노드(dummy node)하나를 그래프에 추가하는 것이죠

368
00:38:11,742 --> 00:38:20,307
이 fake data dependencies를 이용하면 
 new_w1과 new_w2를 업데이트 할 수 있습니다.

369
00:38:20,307 --> 00:38:25,803
그리고 그래프를 실행시키면 loss와 더미노드를
계산하게 됩니다.

370
00:38:25,803 --> 00:38:38,468
이 더미노드는 아무 것도 반환하지 않지만 dependence를 만들었기
때문에 이를 통해 가중치가 업데이트 될 수 있는 것입니다.

371
00:38:38,468 --> 00:38:39,551
질문있나요?

372
00:38:40,788 --> 00:38:44,955
[학생이 질문]

373
00:38:45,854 --> 00:38:51,370
질문은 바로 X와 Y도 그래프에 넣어주지 않은 이유는 무엇인지 입니다.
이들은 여전히 Numpy array로 되어있죠

374
00:38:51,370 --> 00:38:57,151
이 예제에서는 우리가 매 반복시마다 동일한 X와 Y를
재사용 하고 있습니다.

375
00:38:57,151 --> 00:39:10,122
이 경우에는 넣어줘도 되겠죠 하지만 좀 더 현실적인 경우에는 X와 Y는
미니배치입니다. 매번 바뀌기 때문에 매번 넣어줘야 합니다.

376
00:39:10,122 --> 00:39:14,330
이 예제에서는 그래프 안에 넣어주는 것이 가능할 지 모르겠으나
실제로 대부분의 경우에는 그렇지 않습니다.

377
00:39:14,330 --> 00:39:17,913
대부분의 경우에는 X,Y가 그래프 안에 있으면 안좋습니다.

378
00:39:19,388 --> 00:39:21,290
다른 질문 있나요?

379
00:39:21,290 --> 00:39:25,457
[학생이 질문]

380
00:39:37,046 --> 00:39:44,305
현재 우리는 TensorFlow에게 loss와 update를 출력하길 원한다고
말했습니다.

381
00:39:44,305 --> 00:39:51,801
사실 Updates는 어떤 값이 아니죠. 
따라서 Updates는 None을 반환합니다.

382
00:39:51,801 --> 00:39:57,416
하지만 dependence로 인해 updates를 통해 
assign operations을 수행할 수 있는 것이죠

383
00:39:57,416 --> 00:40:02,358
그리고 assign operations은 그래프 내부에 있기 때문에
전부 GPU 메모리 내에 상주하는 녀석들입니다.

384
00:40:02,358 --> 00:40:10,190
덕분에 GPU 내부에서 업데이트를 진행하면서 이 결과를
다시 CPU로 가져오지 않아도 되도록 해 주는 것입니다.

385
00:40:11,723 --> 00:40:15,112
[학생이 질문]

386
00:40:15,112 --> 00:40:18,195
질문은 tf.group은 왜 none을 반환하는지 입니다.

387
00:40:18,195 --> 00:40:25,923
여기에는 TensorFlow의 트릭이 숨어있습니다. 
tf.group자체는 어떤 복잡한 TensorFlow 객체를 반환합니다.

388
00:40:25,923 --> 00:40:32,658
그래프를 구성하기 위해 필요한 일종의 TensorFlow 노드
객체를 반환하는 것이죠

389
00:40:32,658 --> 00:40:43,333
그리고 session.run를 통해서 그래프를 실행하면
그때 바로 none을 반환하는 것입니다.

390
00:40:43,333 --> 00:40:45,482
따라서 TensorFlow로 작업 할 때마다

391
00:40:45,482 --> 00:40:53,487
그래프를 구성하는 동안 반환되는 값들은
Tensorflow 노드와 관련된 어떤 객체들이고

392
00:40:53,487 --> 00:40:55,466
실제 그래프를 돌려야 실제 원하는 값들이 나오는 것입니다.

393
00:40:55,466 --> 00:40:59,967
여기에서는 update를 돌리면 출력으로 none이 나오는 것입니다.
이해가 좀 되셨나요?

394
00:40:59,967 --> 00:41:04,134
[학생이 질문]

395
00:41:18,796 --> 00:41:22,334
질문은 바로 loss는 왜 값이 있고
updata는 none인지 입니다.

396
00:41:22,334 --> 00:41:24,068
이는 바로 update가 동작하는 방식에 있습니다.

397
00:41:24,068 --> 00:41:30,176
loss의 경우에는 우리가 계산하는 실제 값이 있습니다.

398
00:41:30,176 --> 00:41:35,753
반면 update는 특수한 데이터 타입입니다. 실행을 해도
값을 반환하지 않습니다. none을 반환하죠

399
00:41:35,753 --> 00:41:38,703
이는 일종의 TensorFlow의 마법이라고 할 수 있습니다.

400
00:41:38,703 --> 00:41:40,602
아직 헷갈리면 수업이 끝나고 조금 더 말해보죠

401
00:41:40,602 --> 00:41:42,678
[학생이 질문]

402
00:41:42,678 --> 00:41:46,186
네 맞습니다. update의 마법은 
group이라는 함수에서 비롯되는 것입니다.

403
00:41:46,186 --> 00:41:52,492
assign operations를 위해서 tf.group을 이용하는 것은
사실 정상적인 방법은 아닌 것 같아 보입니다.

404
00:41:52,492 --> 00:42:00,004
별로 좋은 방법은 아닌 것 같습니다. 하지만 고맙게도 
Tensorflow는 이런 일들을 해주는 편리한 명령어들을 제공해 줍니다.

405
00:42:00,004 --> 00:42:01,706
이는 바로 optimizer입니다.

406
00:42:01,706 --> 00:42:06,047
여기 보시면 tf.train.GradientDescentOptimizer
라는 것을 사용하고 있습니다.

407
00:42:06,047 --> 00:42:08,458
learning rate도 정해줍니다.

408
00:42:08,458 --> 00:42:12,784
여기에 Adam이나 RMSprop같은
다양한 oprimization 알고리즘이 있는 것입니다.

409
00:42:12,784 --> 00:42:16,284
자 이제 optimizer.minimize(loss) 
를 호출합니다.

410
00:42:17,311 --> 00:42:21,204
이는 아주 마법같은 일을 합니다.

411
00:42:21,204 --> 00:42:30,586
이는 w1과 w2가 학습 가능하다는 것을 인식합니다.
optimizer.minimize의 내부를 살펴보면

412
00:42:30,586 --> 00:42:35,184
그래프에 w1와 w2의 그레디언트를 계산하는 노드도 추가하고

413
00:42:35,184 --> 00:42:42,219
그리고 w1와 w2의 update operation도 추가합니다.  그리고 
assigns을 위한 grouping operation도 있죠

414
00:42:42,219 --> 00:42:44,206
이 같은 많은 일들이 내부적으로 수행됩니다.

415
00:42:44,206 --> 00:42:53,518
이 함수를 들여다보면 tf.group도 들어가 있는 등
이전에 봤던 예제와 유사한 일을 하는 것입니다.

416
00:42:53,518 --> 00:43:00,004
자 이제 반복문을 살펴보면 매 반복마다 
loss와 update를 계산해 달라고 하고 있죠

417
00:43:00,004 --> 00:43:07,450
매번 그래프에게 update를 계산하라고 할 때 마다 
update가 일어납니다.

418
00:43:07,450 --> 00:43:08,593
질문 있나요?

419
00:43:08,593 --> 00:43:10,959
[학생이 질문]

420
00:43:10,959 --> 00:43:14,249
질문은 tf.GlobalVariablesInitializer
가 무엇인지 입니다.

421
00:43:14,249 --> 00:43:20,502
이는 w1과 w2를 초기화시켜 줍니다. 이 변수들을 
그래프 내부에 존재하기 때문이죠

422
00:43:20,502 --> 00:43:37,733
앞서 tf.variable을 만들때 tf.randomnormal이 들어갔죠
tf.GlobalVariablesInitializer이 실제 초기화를 합니다.

423
00:43:37,733 --> 00:43:40,794
[학생이 질문]

424
00:43:40,794 --> 00:43:42,271
잘 못들었습니다?

425
00:43:42,271 --> 00:43:45,233
[학생이 질문]

426
00:43:45,233 --> 00:43:51,385
placeholder는 그래프 밖에서 데이터를 넣어주는 변수이고
variable은 그래프 내부에 있는 변수입니다.

427
00:43:51,385 --> 00:44:00,384
저도 자세한 것은 모르지만
코드를 살펴보거나 관련 문서를 참고하시기 바랍니다.

428
00:44:00,384 --> 00:44:06,130
자 지금까지는 TensorFlow로 네트워크를 학습시키는
전체 예제를 한번 살펴보았고

429
00:44:06,130 --> 00:44:09,328
이를 더 편리하게 하는 몇 가지 요소들도 살펴보았습니다.

430
00:44:09,328 --> 00:44:16,954
이전 까지의 예제에서는 loss를 계산하는 연산을 우리가 직접
만들었습니다.

431
00:44:16,954 --> 00:44:20,739
하지만 TensorFlow에서 제공하는 기본적인
tensor operations을 이용하는 방법도 있습니다.

432
00:44:20,739 --> 00:44:26,734
Tensorflow는 일반적인 Neural network 모델에 들어가는
편리한 함수들을 제공합니다.

433
00:44:26,734 --> 00:44:30,040
tf.losses.mean_squared_error를
사용할 수 있습니다.

434
00:44:30,040 --> 00:44:36,273
이 tensor operations를 사용하면
L2 loss를 직접 구현하지 않아도 됩니다.

435
00:44:36,273 --> 00:44:46,667
그리고 귀찮은 것이 하나 더 있습니다. 입력과 가중치를 정의하고
행렬 곱연산으로 둘을 묶는 그런 일들입니다.

436
00:44:46,667 --> 00:44:54,291
이 예제에서는 bias도 넣지 않았습니다. bias를
넣을라 치면 bias를 또 초기화시켜 줘야 하고

437
00:44:54,291 --> 00:44:58,494
적절한 shape으로 맞춰도 줘야 하고 또 출력 값에
맞춰서 broadcasting이 필요할 수도 있을 것입니다.

438
00:44:58,494 --> 00:45:01,966
bias 하나를 추가하는데도
엄청나게 많은 코드가 필요한 것이죠

439
00:45:01,966 --> 00:45:03,664
이를 일일이 다 작성하는 것은 불편합니다.

440
00:45:03,664 --> 00:45:09,653
여러분이 딥러닝에 들어가는 기본적인 레이어들 가령
convolutions이나 batch norm을 구현하려 치면

441
00:45:09,653 --> 00:45:17,403
입/출력도 선언해 줘야하고 이를 모두 묶어서 
computational graph를 만들어줘야 하는데

442
00:45:17,403 --> 00:45:22,954
이의 가중치를 초기화시켜주고 shape를 잘 맞춰주는 일은
정말로 성가신 일입니다.

443
00:45:22,954 --> 00:45:30,615
이에 TensorFlow를 warpping한 
 higher level libraries이 존재합니다.

444
00:45:30,615 --> 00:45:35,965
TensorFlow에서 제공해주는 것중 하나는 
tf.layers입니다.

445
00:45:35,965 --> 00:45:44,060
이 예제를 보시면 X와 Y만 placeholders로 선언해 줍니다.

446
00:45:44,060 --> 00:45:53,036
그리고 밑에 줄을 보시면 h = tf.layers라고 선언합니다.  
그리고 inputs = x, units = H 를 넣어줍니다.

447
00:45:53,036 --> 00:45:55,171
이 또한 아주 마법스러운 것입니다.

448
00:45:55,171 --> 00:46:07,411
내부적으로 w1과 b2을 variables로 만들어주고 그래프 내부에
적절한 shape으로 선언해 줍니다. 다만 우리에게는 보이지 않습니다.

449
00:46:07,411 --> 00:46:12,931
그리고 xavier initialize 객체를 사용하여
어떻게 초기화시킬 것인지를 말해줍니다.

450
00:46:12,931 --> 00:46:17,200
기존에는 tf.randomnormal을 사용해서
일일이 초기화시켜 줬었죠

451
00:46:17,200 --> 00:46:22,266
하지만 지금은 이 모든 것들을 알아서 해줍니다. 
그리고 h가 출력으로 나옵니다.

452
00:46:22,266 --> 00:46:27,515
다음 레이어를 보면 이전 레이어 출력인 h를 받아서
똑같은 일을 해줍니다.

453
00:46:28,487 --> 00:46:36,910
그리고 여기 activation=tf.nn.relu 를 볼 수 있는데 이를
통해 레이어에 relu를 추가해 줄 수 있습니다.

454
00:46:36,910 --> 00:46:41,370
이처럼 이들은 모두 우리를 대신에서 대부분의 아키텍쳐와
관련된 세부사항들을 다뤄줍니다.

455
00:46:41,370 --> 00:46:42,784
질문 있나요?

456
00:46:42,784 --> 00:46:46,446
[학생이 질문]

457
00:46:46,446 --> 00:46:51,168
질문은 xavier initializer이 특정 분포를
기본값으로 정해 놓고 있는지 입니다.

458
00:46:51,168 --> 00:46:55,850
아마도 기본값이 있긴 할테지만 정확히는 모르겠네요 
관련 문서를 한번 찾아보시기 바랍니다.

459
00:46:55,850 --> 00:46:58,010
관련 문서를 찾아보는 것은 좋은 전략이 될 수 있습니다.

460
00:46:58,010 --> 00:47:04,111
가중치 초기화를 더 잘해서 모델이 이전보다 더 빨리
수렴할 수도 있다면 말이죠

461
00:47:04,111 --> 00:47:11,911
우리는 단지 tf.layers만 두번 호출했을 뿐인데 모델을 
구출할 수 있었습니다. 세부사항을 다 기술할 필요가 없죠

462
00:47:11,911 --> 00:47:14,273
이는 정말 편리합니다.

463
00:47:14,273 --> 00:47:18,682
하지만 우리가 쓸 수 있는 라이브러리가 
tf.contrib.layer만 있는 것은 아닙니다.

464
00:47:18,682 --> 00:47:23,349
TensorFlow를 기반으로 한 아주 다양한
higher level libraries가 있습니다.

465
00:47:23,349 --> 00:47:26,841
다양한 라이브러리가 존재하는 이유는 computational graph가
 상대적으로 low level이기 때문입니다.

466
00:47:26,841 --> 00:47:30,315
computational graph는 상대적으로 low level입니다.

467
00:47:30,315 --> 00:47:36,426
반면 우리가 Neural network를 다룰때면 레이어와 가중치에 대한
개념을 다룹니다.

468
00:47:36,426 --> 00:47:41,866
따라서 자연스럽게 computational graph 에 대한 조금은 더 
higher level인 추상화 모델을 생각해 볼 수 있을 것입니다.

469
00:47:41,866 --> 00:47:48,503
다양한 라이브러리와 패키지들은 여러분이 조금 더  
높은 추상화레벨에서 작업할 수 있도록 돕는 것입니다.

470
00:47:48,503 --> 00:47:52,460
아주 유명한 패키지 하나를 더 소개시켜 드리겠습니다
Keras입니다.

471
00:47:52,460 --> 00:48:02,806
Keras은 아주 훌륭한 APUI로 TensorFlow를 backend로 
해서 computational graph를 알아서 만들어줍니다.

472
00:48:02,806 --> 00:48:07,704
Keras는 Theano backend도 지원합니다.

473
00:48:07,704 --> 00:48:10,958
이 예제에서 레이어의 시퀀스로 모델을 구성하는
것을 보실 수 있습니다.

474
00:48:10,958 --> 00:48:17,910
그리고 optimizer 객체를 만들고 model.compile을 하면 
그래프가 알아서 만들어지는 마법이 이루어지는 것이죠

475
00:48:17,910 --> 00:48:22,797
그리고 model.fit을 하게되면 전체 학습과정이
알아서 진행됩니다.

476
00:48:22,797 --> 00:48:28,523
저는 더 자세한 내용은 잘 모르지만  Keras는 아주 유명하고
여러분이 TensorFlow관련된 일을 한다면 써볼만합니다.

477
00:48:29,797 --> 00:48:31,270
질문있나요?

478
00:48:31,270 --> 00:48:35,437
[학생이 질문]

479
00:48:41,717 --> 00:48:45,525
질문은 바로 여기에는 왜 CPU/GPU에 대한
명시가 없댜는 것입니다.

480
00:48:45,525 --> 00:48:48,409
제가 깔끔한 코드를 위해서 생략한 것입니다.

481
00:48:48,409 --> 00:48:54,607
하지만 keras tutorial 을 살펴보시면 아시겠지만 
CPU/GPU 전환은 아주 쉽습니다. global flag을 사용하거나

482
00:48:54,607 --> 00:49:01,635
특정 데이터 타입을 사용하거나 with문을 이용할 수도 있는데 
어쩄든 한 줄만 추가하면 될것입니다. 아주 쉽죠

483
00:49:01,635 --> 00:49:06,149
다만 현재 상황에 따라 선언하는 방식은 상이할 수 있습니다.

484
00:49:06,149 --> 00:49:14,186
지금 보시는 것은 실제로 여러분들이 앞으로 접하게 될
higher level TensorFlow wrappers들입니다.

485
00:49:14,186 --> 00:49:21,276
심지어 Google 사람들 조차도 어떤 것이 
더 좋은지 잘 모르는 것 같습니다.

486
00:49:22,230 --> 00:49:26,829
Keras와 TFLearn은 third party libraries 입니다.
Google이 아닌 다른 사람들이 만든 라이브러리들입니다.

487
00:49:26,829 --> 00:49:32,563
그리고 여기 tf.layers, TF-Slim, tf.contrib.learn
이 있습니다.

488
00:49:32,563 --> 00:49:39,727
TensorFlow와 함께 제공되는 라이브러리 입니다. 
세가지 모두 다 조금씩 다른 방식으로 wrapping되어 있죠

489
00:49:39,727 --> 00:49:46,291
Google에서 만들었지만 같이 제공하지는 않은 라이브러리도
있습니다. Pretty Tensor라는 것이고 비슷한 맥락입니다.

490
00:49:46,291 --> 00:49:48,599
하지만 DeepMind에게는 이 모든것들이 만족스럽지 않았나 봅니다.

491
00:49:48,599 --> 00:49:54,530
몇 주 전에 Sonnet이라는 자체 Tensorflow wapper를 만들었죠

492
00:49:54,530 --> 00:50:00,715
더이상 나가면 여러분들이 혼란스러운 것 같아 그만하겠습니다. 
결론은 여러분들에게 다양한 선택지가 있다는 것입니다.

493
00:50:00,715 --> 00:50:07,423
그것 들이 상호간에 호환이 잘 안될 수는 있겠지만, 
어찌됐든 여러분에게든 여러 옵션이 있는 것이니 좋은 것이죠

494
00:50:07,423 --> 00:50:09,123


495
00:50:09,123 --> 00:50:11,112
TF-Slim과 Keras에 몇 가지 예가 있습니다.

496
00:50:11,112 --> 00:50:14,072
재교육 된 모델이 매우 중요하다는 것을 기억하라.

497
00:50:14,072 --> 00:50:15,874
당신이 자신의 것을 훈련 할 때.

498
00:50:15,874 --> 00:50:17,931
Tensorboard에 대한 아이디어도 있습니다.

499
00:50:17,931 --> 00:50:19,634
너를로드 할 수있는 곳,

500
00:50:19,634 --> 00:50:21,072
나는 세부 사항에 들어가고 싶지 않다,

501
00:50:21,072 --> 00:50:22,834
그러나 Tensorboard를 사용하면
계기의 종류를 추가 할 수 있습니다.

502
00:50:22,834 --> 00:50:24,733
귀하의 코드에 다음 손실과 물건을 줄거리

503
00:50:24,733 --> 00:50:27,747
당신이 훈련 과정을 통과 할 때.

504
00:50:27,747 --> 00:50:29,535
TensorFlow는 분산 형으로도 실행 해 보겠습니다.

505
00:50:29,535 --> 00:50:31,372
계산 그래프를 나눌 수있는 곳

506
00:50:31,372 --> 00:50:32,760
다른 컴퓨터에서 실행됩니다.

507
00:50:32,760 --> 00:50:35,222
그건 아주 멋지지만 아마 아무도 아닌 것 같아.

508
00:50:35,222 --> 00:50:37,613
Google 외부에서 실제로 큰 성공을 거두고 있습니다.

509
00:50:37,613 --> 00:50:40,733
요즘에는 배포 된 자료를 실행하고 싶다면

510
00:50:40,733 --> 00:50:44,193
아마 TensorFlow가 그 도시의 주요 게임입니다.

511
00:50:44,193 --> 00:50:46,834
부수적 인 부분은 TensorFlow의 많은 디자인

512
00:50:46,834 --> 00:50:49,992
이 초기 프레임 워크에서 영감을받은 것입니다.

513
00:50:49,992 --> 00:50:51,533
몬트리올에서 Theano라고 불렀습니다.

514
00:50:51,533 --> 00:50:54,008
나는 세부 사항을 여기에서 지나고 싶지 않다,

515
00:50:54,008 --> 00:50:55,933
당신이이 슬라이드를 직접 살펴 본다면,

516
00:50:55,933 --> 00:50:58,127
Theano 코드가 끝나는 것을 볼 수 있습니다.

517
00:50:58,127 --> 00:50:59,979
TensorFlow와 매우 유사합니다.

518
00:50:59,979 --> 00:51:01,420
변수를 정의 할 때,

519
00:51:01,420 --> 00:51:03,512
우리는 몇 가지 진행 경로를 수행,
우리는 몇 가지 그라디언트를 계산,

520
00:51:03,512 --> 00:51:05,978
우리는 함수를 컴파일 한 다음 함수를 실행합니다.

521
00:51:05,978 --> 00:51:08,034
반복해서 네트워크를 훈련 시키십시오.

522
00:51:08,034 --> 00:51:10,290
TensorFlow와 비슷합니다.

523
00:51:10,290 --> 00:51:13,010
그래서 우리는 아직 많이해야 할 일이 있습니다.

524
00:51:13,010 --> 00:51:14,462
그래서 저는 PyTorch로 이동할 것입니다.

525
00:51:14,462 --> 00:51:16,671
마지막에 질문을 할 수도 있습니다.

526
00:51:16,671 --> 00:51:20,770
따라서 페이스 북의 PyTorch는

527
00:51:20,770 --> 00:51:22,868
TensorFlow는 3 개의 명시 적

528
00:51:22,868 --> 00:51:26,397
PyTorch 내부의 다른 추상화 계층.

529
00:51:26,397 --> 00:51:29,415
그래서 PyTorch는이 텐서 객체를 가지고 있습니다.

530
00:51:29,415 --> 00:51:30,619
너 프디 배열.

531
00:51:30,619 --> 00:51:33,603
그것은 단지 명령형 배열 일 뿐이며 아무것도 모릅니다.

532
00:51:33,603 --> 00:51:36,770
심층적 인 학습은 있지만 GPU로 실행할 수 있습니다.

533
00:51:36,770 --> 00:51:38,676
우리는이 변수 객체를 가지고 있습니다.

534
00:51:38,676 --> 00:51:42,274
계산 그래프를 구성하는 전산 그래프,

535
00:51:42,274 --> 00:51:44,093
그래디언트를 계산할 수 있습니다.

536
00:51:44,093 --> 00:51:46,312
그리고 우리는 신경망 인 모듈 객체를 가지고 있습니다.

537
00:51:46,312 --> 00:51:48,573
이 모듈들을 함께 구성 할 수있는 레이어

538
00:51:48,573 --> 00:51:50,766
큰 네트워크를 구축 할 수 있습니다.

539
00:51:50,766 --> 00:51:52,973
그래서 여러분이 거친 균등 물에 대해 생각하고 싶다면

540
00:51:52,973 --> 00:51:55,971
PyTorch와 TensorFlow 사이에

541
00:51:55,971 --> 00:51:58,759
PyTorch 텐서가 같은 역할을 수행함

542
00:51:58,759 --> 00:52:01,457
TensorFlow의 Numpy 배열로.

543
00:52:01,457 --> 00:52:04,602
PyTorch 변수는 TensorFlow 텐서와 유사합니다

544
00:52:04,602 --> 00:52:07,549
변수 또는 자리 표시 자, 모든 종류의 노드

545
00:52:07,549 --> 00:52:08,803
전산 그래프에서.

546
00:52:08,803 --> 00:52:11,970
그리고 이제 PyTorch 모듈은 동급입니다.

547
00:52:11,970 --> 00:52:16,288
tf.slim 또는 tf.layers에서
이러한 상위 수준의 작업

548
00:52:16,288 --> 00:52:18,448
또는 소네트 또는 다른 상위 레벨 프레임 워크.

549
00:52:18,448 --> 00:52:21,102
PyTorch에 대해 알아야 할 사항이 하나 있습니다.

550
00:52:21,102 --> 00:52:24,072
이 높은 수준의 추상화와 함께 제공되기 때문입니다.

551
00:52:24,072 --> 00:52:26,696
정말 멋진 고차원 추상화 같아.

552
00:52:26,696 --> 00:52:28,947
자체적으로 모듈이라고 불리는데, 선택의 여지가 적습니다.

553
00:52:28,947 --> 00:52:29,780
뒤얽힌.

554
00:52:29,780 --> 00:52:32,534
nnmodules을 고수하면 잘 갈 수 있습니다.

555
00:52:32,534 --> 00:52:35,809
어느 상위 레벨 래퍼에 대해 걱정할 필요가 없습니다.

556
00:52:35,809 --> 00:52:36,642
사용.

557
00:52:37,777 --> 00:52:41,944
그래서 내가 말했듯이, PyTorch의
텐서는 마치 Numpy 배열과 같습니다.

558
00:52:43,660 --> 00:52:46,181
여기 오른쪽에서 우리는 전체 2
계층 네트워크를 수행했습니다.

559
00:52:46,181 --> 00:52:47,787
전적으로 PyTorch 텐서를 사용합니다.

560
00:52:47,787 --> 00:52:50,279
한 가지주의 할 점은 우리가 여기
Numpy를 수입하지 않는다는 것입니다.

561
00:52:50,279 --> 00:52:51,379
더 이상.

562
00:52:51,379 --> 00:52:53,910
우리는 PyTorch 텐서를 사용하여
이러한 모든 작업을 수행하고 있습니다.

563
00:52:53,910 --> 00:52:58,624
그리고이 코드는 2 층 네트 코드와 똑같습니다.

564
00:52:58,624 --> 00:53:01,245
너 프룻이 첫 숙제를 할 때 썼다.

565
00:53:01,245 --> 00:53:05,774
임의의 데이터를 설정 했으므로 몇 가지 연산을 사용합니다.

566
00:53:05,774 --> 00:53:07,127
전달 경로를 계산합니다.

567
00:53:07,127 --> 00:53:09,332
그리고 나서 우리는 백 워드 패스를
명시 적으로보고 있습니다.

568
00:53:09,332 --> 00:53:10,165
나 자신.

569
00:53:10,165 --> 00:53:12,794
네트워크를 통해 일종의 백 호핑 (backhopping)

570
00:53:12,794 --> 00:53:15,980
당신이 숙제 중 하나에서했던 것처럼, 작업을 통해.

571
00:53:15,980 --> 00:53:18,241
이제 우리는 가중치의 수동 업데이트를 수행하고 있습니다.

572
00:53:18,241 --> 00:53:22,672
학습 속도를 사용하고 계산 된 그라디언트를 사용합니다.

573
00:53:22,672 --> 00:53:24,681
그러나 PyTorch 텐서

574
00:53:24,681 --> 00:53:27,785
Numpy 배열은 GPU에서 실행된다는 것입니다.

575
00:53:27,785 --> 00:53:30,651
이 코드를 실행하려면

576
00:53:30,651 --> 00:53:33,034
GPU는 다른 데이터 유형을 사용합니다.

577
00:53:33,034 --> 00:53:35,092
torch.FloatTensor를 사용하는 대신,

578
00:53:35,092 --> 00:53:39,259
당신은 토치를합니다. 쿠다. 플로트
텐서, 모든 텐서 스를 던져라.

579
00:53:40,152 --> 00:53:42,174
이 새로운 데이터 유형에 모든 것이 마술처럼 작동합니다.

580
00:53:42,174 --> 00:53:43,709
GPU에서.

581
00:53:43,709 --> 00:53:47,637
PyTorch tensors는 Numpy와
GPU를 더한 것으로 생각해야합니다.

582
00:53:47,637 --> 00:53:49,401
그게 정확히 무엇인지, 어떤 것도 구체적이지 않습니다.

583
00:53:49,401 --> 00:53:50,818
깊은 학습.

584
00:53:52,638 --> 00:53:55,278
따라서 PyTorch에서 다음 추상화 계층은 변수입니다.

585
00:53:55,278 --> 00:53:58,440
그래서 우리가 일단 텐서 (tensors)에서
변수 (variables)로 이동하면

586
00:53:58,440 --> 00:54:00,702
이제 우리는 전산 그래프를 구축하고있다.

587
00:54:00,702 --> 00:54:02,194
우리는 자동으로 그라디언트를 취할 수 있습니다.

588
00:54:02,194 --> 00:54:03,460
그리고 그 모든게 좋아.

589
00:54:03,460 --> 00:54:07,627
그래서 여기에서 X가 변수이면 x.data는 텐서입니다.

590
00:54:08,890 --> 00:54:12,164
x.grad는 그라디언트를 포함하는 또 다른 변수입니다.

591
00:54:12,164 --> 00:54:14,007
그 텐서에 관한 손실의

592
00:54:14,007 --> 00:54:15,913
따라서 x.grad.data는 실제 텐서입니다.

593
00:54:15,913 --> 00:54:17,246
그 그라디언트.

594
00:54:18,972 --> 00:54:22,387
그리고 PyTorch 텐서와 변수는
똑같은 API를 가지고 있습니다.

595
00:54:22,387 --> 00:54:25,606
그래서 PyTorch 텐서에서 작업 한 코드는

596
00:54:25,606 --> 00:54:28,457
그 (것)들에게 대신에 가변을
만들고 동일한 부호를 달리십시오,

597
00:54:28,457 --> 00:54:30,292
지금은 계산 그래프를 작성하는 것을 제외하고는

598
00:54:30,292 --> 00:54:34,459
단지 이러한 명령 적 작업을 수행하는 것이 아닙니다.

599
00:54:35,943 --> 00:54:38,553
여기서 우리는이 변수들을 만들 때

600
00:54:38,553 --> 00:54:41,234
변수 생성자에 대한 각 호출은 PyTorch를 래핑합니다.

601
00:54:41,234 --> 00:54:43,652
다음에 텐서를 붙일 지 어떨지를 나타내는 플래그도 붙인다

602
00:54:43,652 --> 00:54:47,461
이 변수에 대한 그라디언트를 계산하려고합니다.

603
00:54:47,461 --> 00:54:49,412
그리고 앞으로 패스에서 그것은 정확하게 보입니다.

604
00:54:49,412 --> 00:54:52,012
그것은 이전에 텐서가있는 경우의 변수에서 수행되었습니다.

605
00:54:52,012 --> 00:54:54,073
그들은 같은 API를 가지고 있기 때문에.

606
00:54:54,073 --> 00:54:55,667
이제 우리는 예측을 계산하고 있습니다.

607
00:54:55,667 --> 00:54:58,431
우리는이 명령의 종류로 우리의 손실을 계산하고있다.

608
00:54:58,431 --> 00:54:59,683
종류의 길.

609
00:54:59,683 --> 00:55:03,492
그리고 나서 우리는 손실이라고 부릅니다.
뒤로 그리고 지금이 모든 그라데이션들

610
00:55:03,492 --> 00:55:05,251
우리를 위해 나와라.

611
00:55:05,251 --> 00:55:06,790
그런 다음 그라데이션 업데이트 단계를 만들 수 있습니다.

612
00:55:06,790 --> 00:55:09,214
현재 존재하는 그라데이션을 사용하는 가중치

613
00:55:09,214 --> 00:55:11,528
w1.grad.data에서

614
00:55:11,528 --> 00:55:16,227
그래서 이것은 Numpy 사건과 아주 유사하게 보입니다.

615
00:55:16,227 --> 00:55:18,137
모든 그라디언트가 무료로 제공된다는 점을 제외하고는

616
00:55:18,137 --> 00:55:20,596
그것이 주목할만한 점은

617
00:55:20,596 --> 00:55:23,353
PyTorch와 TensorFlow는
TensorFlow의 경우입니다.

618
00:55:23,353 --> 00:55:25,289
우리는이 명백한 그래프를 만들고있었습니다.

619
00:55:25,289 --> 00:55:27,132
그런 다음 그래프를 여러 번 실행합니다.

620
00:55:27,132 --> 00:55:30,308
여기 PyTorch에서는 새로운
그래프를 작성하고 있습니다.

621
00:55:30,308 --> 00:55:32,152
우리가 앞으로 나아갈 때마다.

622
00:55:32,152 --> 00:55:33,887
그리고 이것은 코드를 좀 더 깔끔하게 보입니다.

623
00:55:33,887 --> 00:55:35,284
그리고 다른 의미도 있습니다.

624
00:55:35,284 --> 00:55:37,058
조금 들어가.

625
00:55:37,058 --> 00:55:40,630
그래서 PyTorch에서 여러분은 새로운
autograd 함수를 정의 할 수 있습니다.

626
00:55:40,630 --> 00:55:42,933
텐서 (tensors)의 관점에서 앞뒤를 정의함으로써.

627
00:55:42,933 --> 00:55:45,954
이것은 결국 모듈 레이어처럼 보이게됩니다.

628
00:55:45,954 --> 00:55:48,303
숙제 2를 위해 작성한 코드.

629
00:55:48,303 --> 00:55:50,404
다음을 사용하여 앞뒤로 구현할 수있는 위치

630
00:55:50,404 --> 00:55:52,676
텐서 (tensor) 연산을 수행 한 다음,

631
00:55:52,676 --> 00:55:54,433
전산 그래프.

632
00:55:54,433 --> 00:55:56,297
그래서 우리는 우리 자신의 relu를 정의하고 있습니다.

633
00:55:56,297 --> 00:56:00,654
그리고 나서 우리는 실제로 들어가서 우리
자신의 relu를 사용할 수 있습니다.

634
00:56:00,654 --> 00:56:02,754
연산을 수행하고 이제 계산 그래프에 집어 넣습니다.

635
00:56:02,754 --> 00:56:05,214
이런 식으로 우리 자신의 작업을 정의하십시오.

636
00:56:05,214 --> 00:56:06,857
하지만 대부분의 시간은 아마 필요 없을 것입니다.

637
00:56:06,857 --> 00:56:09,097
자신 만의 autograd 작업을 정의 할 수 있습니다.

638
00:56:09,097 --> 00:56:10,814
대부분의 경우 필요한 작업은

639
00:56:10,814 --> 00:56:14,246
대부분 이미 구현되어 있습니다.

640
00:56:14,246 --> 00:56:16,293
그래서 TensorFlow에서 우리는 보았습니다,

641
00:56:16,293 --> 00:56:19,054
우리가 Keras 또는 TF와
같은 것으로 이동할 수 있다면.

642
00:56:19,054 --> 00:56:21,253
이것은 우리에게 더 높은 수준의 API를 제공합니다.

643
00:56:21,253 --> 00:56:23,349
이 원시 계산 그래프보다.

644
00:56:23,349 --> 00:56:25,433
PyTorch에서 이에 상응하는 것은 nn 패키지입니다.

645
00:56:25,433 --> 00:56:29,448
작업을위한 이러한 높은 수준의 래퍼를 제공하는 곳

646
00:56:29,448 --> 00:56:30,948
이걸로.

647
00:56:31,882 --> 00:56:33,454
그러나 TensorFlow와는
달리 그 중 하나만 있습니다.

648
00:56:33,454 --> 00:56:35,837
그리고 그것은 꽤 잘 작동합니다,
그렇다면 그것을 사용하십시오.

649
00:56:35,837 --> 00:56:37,772
PyTorch를 사용합니다.

650
00:56:37,772 --> 00:56:39,374
그래서 여기, 이것은 마치 Keras처럼
보이는 것을 끝내고 있습니다.

651
00:56:39,374 --> 00:56:42,196
우리는 모델을 일련의 레이어 시퀀스로 정의합니다.

652
00:56:42,196 --> 00:56:44,436
우리의 직선 및 relu 작업.

653
00:56:44,436 --> 00:56:47,574
그리고 우리는 nn 패키지에 정의
된 손실 함수를 사용합니다.

654
00:56:47,574 --> 00:56:49,816
즉 평균 제곱 오류 손실입니다.

655
00:56:49,816 --> 00:56:51,593
이제 루프 반복마다

656
00:56:51,593 --> 00:56:53,716
우리는 모델을 통해 데이터를 전달하여

657
00:56:53,716 --> 00:56:55,214
우리의 예측.

658
00:56:55,214 --> 00:56:57,511
손실 함수를 통해 예측을 진행할 수 있습니다.

659
00:56:57,511 --> 00:56:59,054
우리의 규모 또는 손실을 얻기 위해,

660
00:56:59,054 --> 00:57:01,177
우리는 손실을 전화 할 수 있습니다. 뒤로,
우리의 모든 그라디언트를 얻으십시오.

661
00:57:01,177 --> 00:57:04,021
자유로운을 위해 그리고 모형의 매개 변수에 반복하십시오

662
00:57:04,021 --> 00:57:06,143
업데이트하기 위해 명시적인 그래디언트
강하 단계를 수행하십시오.

663
00:57:06,143 --> 00:57:07,273
모델.

664
00:57:07,273 --> 00:57:09,054
그리고 다시 우리는 이것이 우리가 일종의

665
00:57:09,054 --> 00:57:12,749
우리가 전방 통과를 할 때마다 새로운 전산 그래프.

666
00:57:12,749 --> 00:57:14,714
그리고 우리가 TensorFlow에서 본 것처럼,

667
00:57:14,714 --> 00:57:17,017
PyTorch는 이러한 최적화 작업을 제공합니다.

668
00:57:17,017 --> 00:57:19,655
그 종류의 추상적인 업데이트 논리

669
00:57:19,655 --> 00:57:21,758
Adam과 같은 고급 업데이트 규칙 구현

670
00:57:21,758 --> 00:57:23,000
그리고 이것 저것.

671
00:57:23,000 --> 00:57:25,038
여기서 우리는 최적화 객체를 생성하고 있습니다.

672
00:57:25,038 --> 00:57:27,034
우리가 그것을 통해 최적화하기를 원한다는 것을

673
00:57:27,034 --> 00:57:28,771
모델의 매개 변수.

674
00:57:28,771 --> 00:57:31,115
하이퍼 매개 변수 아래에서 약간의 학습 속도를 제공합니다.

675
00:57:31,115 --> 00:57:33,438
그리고 지금 우리가 그라디언트를 계산 한 후에

676
00:57:33,438 --> 00:57:35,356
optimizer.step을 호출하면 업데이트됩니다.

677
00:57:35,356 --> 00:57:39,810
우리를위한 모델의 모든 매개 변수는 바로 여기에 있습니다.

678
00:57:39,810 --> 00:57:41,951
PyTorch에서 또 다른 공통점이 있습니다.

679
00:57:41,951 --> 00:57:44,714
많은 것은 자신의 nn 모듈을 정의합니다.

680
00:57:44,714 --> 00:57:47,268
따라서 일반적으로 자신의 수업을 작성할 것입니다.

681
00:57:47,268 --> 00:57:49,961
전체 모델을 단일 모델로 정의합니다.

682
00:57:49,961 --> 00:57:51,801
새로운 nn 모듈 클래스.

683
00:57:51,801 --> 00:57:54,979
그리고 모듈은 단지 신경 네트워크 계층의 일종입니다.

684
00:57:54,979 --> 00:57:57,678
다른 모듈을 포함 할 수있는

685
00:57:57,678 --> 00:58:01,043
또는 훈련 가능한 무게 또는 다른 다른 종류의 상태.

686
00:58:01,043 --> 00:58:04,142
그래서이 경우 우리는 2 층 네트
예제를 다시 할 수 있습니다.

687
00:58:04,142 --> 00:58:07,051
우리 자신의 nn 모듈 클래스를 정의함으로써.

688
00:58:07,051 --> 00:58:09,925
이제 클래스의 초기화 프로그램에 있습니다.

689
00:58:09,925 --> 00:58:11,672
이 linear1과 linear2를 할당합니다.

690
00:58:11,672 --> 00:58:13,853
우리는이 새로운 모듈 객체를 구성하고 있습니다.

691
00:58:13,853 --> 00:58:17,257
그들을 우리 자신의 수업 시간 내에 보관하십시오.

692
00:58:17,257 --> 00:58:20,335
그리고 앞으로 패스에서 우리는 우리 자신의

693
00:58:20,335 --> 00:58:22,832
내부 모듈 및 임의의 자동 기록 작업

694
00:58:22,832 --> 00:58:26,466
변수를 사용하여 네트워크의 출력을 계산합니다.

695
00:58:26,466 --> 00:58:29,782
그래서 여기에 우리는이 앞쪽에있는 방법 안에서,

696
00:58:29,782 --> 00:58:31,594
입력은 변수의 역할을하며,

697
00:58:31,594 --> 00:58:34,213
변수를 우리 자신에게 넘깁니다. 선형 1

698
00:58:34,213 --> 00:58:35,817
첫 번째 레이어.

699
00:58:35,817 --> 00:58:38,129
우리는 autograd op clamp를
사용하여 relu를 완료합니다.

700
00:58:38,129 --> 00:58:40,233
우리는 그 출력을 두 번째 선형으로 전달합니다.

701
00:58:40,233 --> 00:58:42,233
그리고 나서 그것은 우리에게 결과를줍니다.

702
00:58:42,233 --> 00:58:44,732
이제이 코드를 훈련시키는 나머지 부분은

703
00:58:44,732 --> 00:58:46,633
거의 같은 것처럼 보입니다.

704
00:58:46,633 --> 00:58:48,455
최적화 도구를 만들고 루프를 반복하는 곳

705
00:58:48,455 --> 00:58:50,916
모델에 대한 반복 피드 데이터에서,

706
00:58:50,916 --> 00:58:52,777
손실로 그라디언트를 계산합니다.

707
00:58:52,777 --> 00:58:54,676
호출 최적화 기. 단계.

708
00:58:54,676 --> 00:58:57,924
그래서 이것은 상대적으로 특징적입니다.

709
00:58:57,924 --> 00:59:00,233
많은 PyTorch 유형에서 볼 수있는 것 중

710
00:59:00,233 --> 00:59:01,817
교육 시나리오.

711
00:59:01,817 --> 00:59:02,964
자신 만의 클래스를 정의 할 때,

712
00:59:02,964 --> 00:59:04,932
다른 모듈을 포함하는 자신 만의 모델 정의하기

713
00:59:04,932 --> 00:59:07,103
그리고 그 외의 것들은 명시적인 훈련을받습니다.

714
00:59:07,103 --> 00:59:11,166
이것을 실행하고 업데이트하는 루프.

715
00:59:11,166 --> 00:59:13,353
당신이 가지고있는 삶의 질 좋은 종류의 한 종류

716
00:59:13,353 --> 00:59:16,057
PyTorch에서 dataloader입니다.

717
00:59:16,057 --> 00:59:18,873
따라서 데이터 로더는 건물 미니
바를 처리 할 수 있습니다.

718
00:59:18,873 --> 00:59:21,273
우리가 말한 멀티 스레딩 중 일부를 처리 할 수 있습니다.

719
00:59:21,273 --> 00:59:23,876
당신이 실제로 여러 스레드를 사용할 수있는 곳

720
00:59:23,876 --> 00:59:25,934
당신을 위해 많은 배치를 만드는 백그라운드에서

721
00:59:25,934 --> 00:59:27,273
디스크에서 스트리밍.

722
00:59:27,273 --> 00:59:30,777
그래서 여기서 dataloader는 데이터 셋을 랩핑하고

723
00:59:30,777 --> 00:59:33,221
당신을위한 이러한 추상화 중 일부.

724
00:59:33,221 --> 00:59:35,562
실제로 자신의 데이터를 실행할 때,

725
00:59:35,562 --> 00:59:38,138
일반적으로 자체 데이터 집합 클래스를 작성합니다.

726
00:59:38,138 --> 00:59:40,208
특정 유형의 데이터를 읽는 방법을 알고 있습니다.

727
00:59:40,208 --> 00:59:42,251
원하는 모든 소스를 끄고 랩핑하십시오.

728
00:59:42,251 --> 00:59:44,458
데이터 로더와 기차.

729
00:59:44,458 --> 00:59:47,631
그래서, 여기서 우리는 이제 우리가
반복하는 것을 볼 수 있습니다.

730
00:59:47,631 --> 00:59:50,444
dataloader 객체와 모든 반복마다

731
00:59:50,444 --> 00:59:52,233
이것은 데이터의 minibatches을 낳고있다.

732
00:59:52,233 --> 00:59:55,527
그리고 내부적으로 데이터 셔플을 처리하고 있습니다.

733
00:59:55,527 --> 00:59:57,576
및 다중 스레드 데이터 로딩과 이러한 모든 종류의 것들

734
00:59:57,576 --> 00:59:58,409
너를 위해서.

735
00:59:58,409 --> 01:00:00,654
따라서 이것은 완전히 PyTorch 예제입니다.

736
01:00:00,654 --> 01:00:02,494
PyTorch 교육 코드가 많이 보입니다.

737
01:00:02,494 --> 01:00:04,161
이 같은.

738
01:00:05,583 --> 01:00:07,587
PyTorch는 사전 훈련 된 모델을 제공합니다.

739
01:00:07,587 --> 01:00:09,469
그리고 이것은 아마 가장 매끄러운
pretrained 모델입니다

740
01:00:09,469 --> 01:00:11,521
내가 본 경험.

741
01:00:11,521 --> 01:00:14,268
torchvision.models.alexnet
pretained = true라고 말하면됩니다.

742
01:00:14,268 --> 01:00:16,951
그 배경에서 내려갈거야, pretrained 다운로드

743
01:00:16,951 --> 01:00:18,759
당신이 이미 그것을 가지고 있지 않다면 당신을위한 무게,

744
01:00:18,759 --> 01:00:21,052
그리고 바로 거기에 있습니다, 당신은 잘 가게됩니다.

745
01:00:21,052 --> 01:00:24,242
따라서 사용하기가 쉽습니다.

746
01:00:24,242 --> 01:00:27,094
PyTorch는 또한 Visdom이라는 패키지도 있습니다.

747
01:00:27,094 --> 01:00:30,253
이 손실 통계 중 일부를 시각화 할 수 있습니다.

748
01:00:30,253 --> 01:00:33,600
Tensorboard와 다소 비슷합니다.

749
01:00:33,600 --> 01:00:35,168
그래서 친절 하네, 나는 실제로 얻지 못했다.

750
01:00:35,168 --> 01:00:36,934
내 자신과 놀 수있는 기회. 그래서
나는 정말로 할 수 없다.

751
01:00:36,934 --> 01:00:38,569
그것이 얼마나 유용한 지 이야기하십시오.

752
01:00:38,569 --> 01:00:40,927
하지만 Tensorboard의 주요 차이점 중 하나는

753
01:00:40,927 --> 01:00:43,769
Visdom은 실제로 Tensorboard를
사용하면 시각화 할 수 있습니다.

754
01:00:43,769 --> 01:00:45,907
전산 그래프의 구조.

755
01:00:45,907 --> 01:00:47,984
정말 멋진 디버깅 전략입니다.

756
01:00:47,984 --> 01:00:50,989
그리고 Visdom은 아직 그
기능을 가지고 있지 않습니다.

757
01:00:50,989 --> 01:00:53,011
하지만 난 정말 이걸 사용하지 않았어.
그래서 나는 정말로 할 수 없어.

758
01:00:53,011 --> 01:00:54,761
그 유틸리티에 말하십시오.

759
01:00:56,350 --> 01:00:58,627
제쳐두고, PyTorch는 일종의 진화입니다.

760
01:00:58,627 --> 01:01:01,747
이전 프레임 워크의 최신 업데이트 버전

761
01:01:01,747 --> 01:01:04,086
내가 마지막으로 많이 일했던 Torch

762
01:01:04,086 --> 01:01:05,491
2 년.

763
01:01:05,491 --> 01:01:07,577
그리고 저는 여기에 세부 사항을
설명하기를 원하지 않습니다.

764
01:01:07,577 --> 01:01:10,569
하지만 PyTorch는 많은면에서 훨씬 뛰어납니다.

765
01:01:10,569 --> 01:01:13,280
늙은 루아 토치보다,하지만 그들은
실제로 많은 것을 공유합니다.

766
01:01:13,280 --> 01:01:15,657
같은 백엔드의 C 코드

767
01:01:15,657 --> 01:01:18,100
및 tensors 및 이것 저것에 GPU 작업.

768
01:01:18,100 --> 01:01:19,554
따라서이 Torch 예제를 살펴보면,

769
01:01:19,554 --> 01:01:21,906
일부는 PyTorch와 비슷한 종류의 것으로 보입니다.

770
01:01:21,906 --> 01:01:23,369
일부는 약간 다릅니다.

771
01:01:23,369 --> 01:01:25,957
어쩌면 당신은 오프라인에서 단계를 밟을 수 있습니다.

772
01:01:25,957 --> 01:01:28,361
그러나 사이에 높은 수준의 차이의 종류

773
01:01:28,361 --> 01:01:31,229
Torch와 PyTorch는 Torch가
실제로 Lua에 있음을 나타냅니다.

774
01:01:31,229 --> 01:01:33,011
이 다른 것들과 달리 파이썬이 아닙니다.

775
01:01:33,011 --> 01:01:37,748
루아를 배우는 것은 어떤 사람들에게는 약간의 휴식입니다.

776
01:01:37,748 --> 01:01:40,009
토치에는 자동 굴림 기능이 없습니다.

777
01:01:40,009 --> 01:01:41,710
성화는 또한 더 오래 되었기 때문에 더 안정적입니다.

778
01:01:41,710 --> 01:01:43,491
버그에 덜 민감하고, 예제 코드가 더 많습니다.

779
01:01:43,491 --> 01:01:44,324
토치 용.

780
01:01:45,230 --> 01:01:47,214
그들은 거의 같은 속도이며, 그것은
정말로 걱정거리가 아닙니다.

781
01:01:47,214 --> 01:01:49,827
하지만 PyTorch에서는 Python을 사용합니다.

782
01:01:49,827 --> 01:01:52,270
당신은 훨씬 더 간단하게 해주는
autograd를 가지고 있습니다.

783
01:01:52,270 --> 01:01:54,531
복잡한 모델을 작성하는 것.

784
01:01:54,531 --> 01:01:56,422
루아 토치에서 당신은 많은 자신의 글을 쓰게됩니다.

785
01:01:56,422 --> 01:01:59,670
때로는 소품 코드가 돌아 다니기 때문에 조금 성가시다.

786
01:01:59,670 --> 01:02:01,650
그러나 PyTorch는 최신
버전이며 기존 코드가 적습니다.

787
01:02:01,650 --> 01:02:03,689
여전히 변경 될 수 있습니다.

788
01:02:03,689 --> 01:02:06,051
그래서 좀 더 모험이 될 것입니다.

789
01:02:06,051 --> 01:02:08,145
그러나 적어도 나를 위해, 나는 조금은 좋아한다.

790
01:02:08,145 --> 01:02:10,162
나는 내 자신에 대한 많은 이유를 실제로 보지 못한다.

791
01:02:10,162 --> 01:02:13,228
이 시점에서 더 이상 TorTch를
PyTorch 이상 사용하십시오.

792
01:02:13,228 --> 01:02:15,848
그래서 저는 PyTorch를 독점적으로 사용하고 있습니다.

793
01:02:15,848 --> 01:02:17,765
요즘 내 모든 작업.

794
01:02:18,606 --> 01:02:20,557
우리는이 아이디어에 대해 조금 이야기했습니다.

795
01:02:20,557 --> 01:02:22,531
정적 그래프와 동적 그래프 비교

796
01:02:22,531 --> 01:02:24,350
그리고 이것은 주요 특징 중 하나입니다.

797
01:02:24,350 --> 01:02:26,291
PyTorch와 TensorFlow 사이.

798
01:02:26,291 --> 01:02:29,416
우리는 Tensor에서 보았습니다.이 두 단계가 있습니다.

799
01:02:29,416 --> 01:02:31,667
첫 번째로

800
01:02:31,667 --> 01:02:34,371
전산 그래프, 그럼 당신은 전산 그래프를 실행합니다.

801
01:02:34,371 --> 01:02:37,246
반복해서 여러 번 반복하여

802
01:02:37,246 --> 01:02:38,145
그래프.

803
01:02:38,145 --> 01:02:40,209
정적 계산 그래프라고합니다.

804
01:02:40,209 --> 01:02:42,403
그들 중 하나만.

805
01:02:42,403 --> 01:02:44,940
그리고 우리는 PyTorch가 우리가 실제로
어디에 있는지 전혀 다른 것을 보았습니다.

806
01:02:44,940 --> 01:02:46,829
이 새로운 전산 그래프를 구축하고,

807
01:02:46,829 --> 01:02:48,771
모든 앞으로 패스 에서이 새로운 신선한.

808
01:02:48,771 --> 01:02:52,259
이를 동적 계산 그래프라고합니다.

809
01:02:52,259 --> 01:02:54,075
피드 종류의 종류와 함께 단순한 종류의 경우

810
01:02:54,075 --> 01:02:57,053
신경 네트워크, 그것은 실제로 큰 차이를 만들지 않습니다,

811
01:02:57,053 --> 01:02:58,467
코드는 비슷하게 끝납니다.

812
01:02:58,467 --> 01:03:00,225
비슷하게 작동합니다.

813
01:03:00,225 --> 01:03:02,079
그러나 나는 몇 가지 의미에 대해 조금 이야기하고 싶다.

814
01:03:02,079 --> 01:03:04,227
정적 대 동적.

815
01:03:04,227 --> 01:03:07,102
그리고이 둘의 절충안은 무엇입니까?

816
01:03:07,102 --> 01:03:08,947
정적 그래프를 사용한 멋진 아이디어

817
01:03:08,947 --> 01:03:11,331
우리가 일종의 건물이기 때문에

818
01:03:11,331 --> 01:03:15,286
전산 그래프를 한 번 사용한 다음 여러 번 재사용하면,

819
01:03:15,286 --> 01:03:17,355
프레임 워크에 들어갈 기회가있을 수 있습니다.

820
01:03:17,355 --> 01:03:19,571
그 그래프에 대한 최적화를 수행합니다.

821
01:03:19,571 --> 01:03:22,821
그리고 몇 가지 작업을 통합하고, 일부 작업을 재정렬하고,

822
01:03:22,821 --> 01:03:24,520
가장 효율적인 작동 방법 알아보기

823
01:03:24,520 --> 01:03:26,809
그 그래프는 정말 효율적 일 수 있습니다.

824
01:03:26,809 --> 01:03:28,725
그리고 우리는 그 그래프를 재사용 할 것이기 때문에

825
01:03:28,725 --> 01:03:31,790
여러 번, 아마 그 최적화 과정

826
01:03:31,790 --> 01:03:33,039
앞면이 비싸다.

827
01:03:33,039 --> 01:03:34,947
그러나 우리는 그 비용을 스피드 업으로 상각 할 수있다.

828
01:03:34,947 --> 01:03:37,230
우리가 여러 번 그래프를 실행할 때 우리가 얻은 것.

829
01:03:37,230 --> 01:03:40,162
그래서 구체적인 예로서,

830
01:03:40,162 --> 01:03:41,814
어쩌면 회선이있는 그래프를 작성하면

831
01:03:41,814 --> 01:03:44,085
그리고 후퇴 작전 종류의 하나씩,

832
01:03:44,085 --> 01:03:48,250
멋진 그래프 최적화 도구가

833
01:03:48,250 --> 01:03:51,865
실제로 사용자 정의 코드를 내보내는 것처럼
들어가서 실제로 출력 할 수 있습니다.

834
01:03:51,865 --> 01:03:54,530
운영을 융합시킨, 그 회선을 융합 한

835
01:03:54,530 --> 01:03:56,371
그리고 relu 그래서 지금
그것은 같은 것을 계산하고있다.

836
01:03:56,371 --> 01:04:00,525
당신이 쓴 코드처럼, 지금은 될 수 있을지도 몰라.

837
01:04:00,525 --> 01:04:03,445
보다 효율적으로 실행됩니다.

838
01:04:03,445 --> 01:04:07,909
그래서 나는 실제로 국가가 정확히
무엇인지 확신하지 못한다.

839
01:04:07,909 --> 01:04:10,419
TensorFlow 그래프 최적화 중 지금은,

840
01:04:10,419 --> 01:04:14,469
그러나 적어도 원칙적으로, 이것은 하나의 장소입니다.

841
01:04:14,469 --> 01:04:17,747
정적 그래프 정말, 당신은 잠재력을 가질 수

842
01:04:17,747 --> 01:04:20,131
정적 그래프에서이 최적화를 수행

843
01:04:20,131 --> 01:04:24,298
어쩌면 동적 그래프에서는 다루기가 쉽지 않을 것입니다.

844
01:04:25,504 --> 01:04:26,937
정적 대 동적에 대한 또 다른 종류의 미묘한 점

845
01:04:26,937 --> 01:04:28,931
이 직렬화에 대한 아이디어입니다.

846
01:04:28,931 --> 01:04:32,347
따라서 정적 그래프를 사용하면

847
01:04:32,347 --> 01:04:34,026
그래프를 작성하는이 코드

848
01:04:34,026 --> 01:04:35,641
그래프를 만든 후에는

849
01:04:35,641 --> 01:04:37,667
당신은이 데이터 구조를 나타내는 메모리에

850
01:04:37,667 --> 01:04:39,571
네트워크의 전체 구조.

851
01:04:39,571 --> 01:04:41,226
이제 그 데이터 구조를 사용할 수 있습니다.

852
01:04:41,226 --> 01:04:42,428
디스크에 직렬화하면됩니다.

853
01:04:42,428 --> 01:04:44,528
이제는 네트워크의 전체 구조를 갖게되었습니다.

854
01:04:44,528 --> 01:04:45,996
일부 파일에 저장되었습니다.

855
01:04:45,996 --> 01:04:48,711
그리고 나서 나중에 그걸 로딩 할 수 있습니다.

856
01:04:48,711 --> 01:04:51,627
그런 다음 액세스하지 않고 계산 그래프를 실행하십시오.

857
01:04:51,627 --> 01:04:53,630
원래 코드를 작성한 것입니다.

858
01:04:53,630 --> 01:04:55,450
그래서 이것은 배치 시나리오에서 좋을 것입니다.

859
01:04:55,450 --> 01:04:57,606
너는 너를 훈련시키고 싶을지도
모른다라고 생각할지도 모른다.

860
01:04:57,606 --> 01:05:00,424
네트워크가 파이썬에서 작동하기 쉽기 때문에,

861
01:05:00,424 --> 01:05:01,788
하지만 그 네트워크를 직렬화 한 후에

862
01:05:01,788 --> 01:05:04,170
이제 C ++로 배포 할 수 있습니다.

863
01:05:04,170 --> 01:05:06,409
원본을 사용할 필요가없는 환경

864
01:05:06,409 --> 01:05:07,759
그래프를 작성한 코드.

865
01:05:07,759 --> 01:05:10,909
그래서 이것은 정적 그래프의 좋은 장점입니다.

866
01:05:10,909 --> 01:05:12,510
동적 그래프에서는 인터리빙하기 때문에

867
01:05:12,510 --> 01:05:15,793
이러한 그래프 작성 및 그래프 실행 프로세스,

868
01:05:15,793 --> 01:05:17,822
항상 원본 코드가 필요합니다.

869
01:05:17,822 --> 01:05:22,012
당신이 미래에 그 모델을 재사용하고 싶다면.

870
01:05:22,012 --> 01:05:24,390
반면에 동적 그래프의 몇 가지 장점

871
01:05:24,390 --> 01:05:26,921
그게 일종의 만드는거야, 그냥 코드를 만든다.

872
01:05:26,921 --> 01:05:29,163
많은 경우에 훨씬 깨끗하고 훨씬 쉽습니다.

873
01:05:29,163 --> 01:05:31,264
예를 들어, 우리가

874
01:05:31,264 --> 01:05:34,501
조건부 연산 값에 따라

875
01:05:34,501 --> 01:05:37,541
어떤 변수 Z의 경우, 우리는 다른 연산을하고 싶다.

876
01:05:37,541 --> 01:05:38,624
Y를 계산한다.

877
01:05:39,723 --> 01:05:42,123
Z가 양수이면 하나의 가중치 행렬을 사용하고,

878
01:05:42,123 --> 01:05:45,070
Z가 음수이면 다른 가중치 행렬을 사용하려고합니다.

879
01:05:45,070 --> 01:05:47,981
그리고 우리는이 두 가지 대안
사이에서 전환하기를 원합니다.

880
01:05:47,981 --> 01:05:50,720
우리가 동적 그래프를 사용하기 때문에 PyTorch에서,

881
01:05:50,720 --> 01:05:52,011
그것은 매우 간단합니다.

882
01:05:52,011 --> 01:05:54,101
귀하의 코드 종류가 예상했던 것과 똑같습니다.

883
01:05:54,101 --> 01:05:56,400
너가 니피에서 뭘 할거야.

884
01:05:56,400 --> 01:05:58,877
일반 파이썬 제어 흐름을 사용할 수 있습니다.

885
01:05:58,877 --> 01:06:00,795
이 일을 처리합니다.

886
01:06:00,795 --> 01:06:03,264
그리고 우리는 매번 그래프를 작성하기 때문에,

887
01:06:03,264 --> 01:06:05,563
이 작업을 수행 할 때마다

888
01:06:05,563 --> 01:06:08,021
어쩌면 다른 그래프를 만들지.

889
01:06:08,021 --> 01:06:10,864
각 순회 패스에서,하지만 우리가하는 모든 그래프에 대해

890
01:06:10,864 --> 01:06:13,104
우리가 그것을 전파 할 수있게된다.

891
01:06:13,104 --> 01:06:14,337
잘 됐네.

892
01:06:14,337 --> 01:06:15,941
코드는 매우 깨끗하고 작업하기 쉽습니다.

893
01:06:15,941 --> 01:06:18,843
이제는 TensorFlow에서
상황이 조금 더 나아졌습니다.

894
01:06:18,843 --> 01:06:23,201
우리는 그래프를 한 번 빌드하기 때문에 복잡합니다.

895
01:06:23,201 --> 01:06:25,219
이 제어 흐름 연산자 종류가 필요합니다.

896
01:06:25,219 --> 01:06:28,400
TensorFlow 그래프의 명시 적 연산자입니다.

897
01:06:28,400 --> 01:06:31,301
그리고 이제는 우리가 이것을 볼 수 있습니다.

898
01:06:31,301 --> 01:06:34,319
TensorFlow 버전과 같은 종류의 tf.cond 호출

899
01:06:34,319 --> 01:06:36,818
if 문을 사용하지만 이제는 구운 것입니다.

900
01:06:36,818 --> 01:06:38,838
일종의 계산 그래프를 사용하는 것보다

901
01:06:38,838 --> 01:06:40,741
파이썬 제어 흐름.

902
01:06:40,741 --> 01:06:43,473
그리고 문제는 그래프 만 작성하기 때문입니다.

903
01:06:43,473 --> 01:06:46,123
일단, 제어 흐름의 모든 잠재적 인 경로는

904
01:06:46,123 --> 01:06:48,729
우리의 프로그램은 구워야 할 필요가있을 수 있습니다.

905
01:06:48,729 --> 01:06:51,200
우리가 전에 그래프를 만들 때

906
01:06:51,200 --> 01:06:52,523
그것을 실행하십시오.

907
01:06:52,523 --> 01:06:54,353
그래서 모든 종류의 제어 흐름 연산자

908
01:06:54,353 --> 01:06:58,394
파이썬 제어 흐름이 아니길 바랄뿐입니다.

909
01:06:58,394 --> 01:07:00,409
통신 수, 당신은 어떤 종류의 마술을 사용할 필요가 있고,

910
01:07:00,409 --> 01:07:03,360
제어 흐름을 수행하는 특수한 텐서 흐름 연산.

911
01:07:03,360 --> 01:07:05,527
이 경우이 tf.cond.

912
01:07:06,713 --> 01:07:09,400
비슷한 상황이 발생하면

913
01:07:09,400 --> 01:07:10,763
루프가있다.

914
01:07:10,763 --> 01:07:12,684
그래서 우리가 어떤 종류의 재발을
계산하기를 원한다고 가정 해보자.

915
01:07:12,684 --> 01:07:16,607
관계 여기서 Y T는 Y T가 1을 뺀 것과 같습니다.

916
01:07:16,607 --> 01:07:19,839
더하기 X T는 가중치 행렬 W를 곱해서

917
01:07:19,839 --> 01:07:23,077
우리가 이것을 할 때마다, 우리가 이것을 계산할 때마다,

918
01:07:23,077 --> 01:07:26,436
우리는 다른 크기의 데이터 시퀀스를 가질 수 있습니다.

919
01:07:26,436 --> 01:07:28,265
그리고 우리의 데이터 순서의 길이에 관계없이,

920
01:07:28,265 --> 01:07:30,217
이 반복 관계를 계산하고 싶습니다.

921
01:07:30,217 --> 01:07:33,371
입력 시퀀스의 크기에 관계없이

922
01:07:33,371 --> 01:07:35,796
그래서 PyTorch에서 이것은 매우 쉽습니다.

923
01:07:35,796 --> 01:07:39,489
우리는 Python에서 일반 for
루프를 사용할 수 있습니다.

924
01:07:39,489 --> 01:07:41,534
우리가 원하는 횟수만큼 반복하면됩니다.

925
01:07:41,534 --> 01:07:44,436
unroll 그리고 지금은 입력 데이터의 크기에 따라,

926
01:07:44,436 --> 01:07:47,095
우리의 계산 그래프는 다른 크기로 끝날 것입니다.

927
01:07:47,095 --> 01:07:49,737
하지만 괜찮아요, 우리는 다시 전파 할 수 있습니다.

928
01:07:49,737 --> 01:07:51,694
각 하나, 한 번에 하나씩.

929
01:07:51,694 --> 01:07:55,782
이제는 TensorFlow에서 조금 더 못 생겼습니다.

930
01:07:55,782 --> 01:07:58,494
그리고 다시, 우리는 그래프를 구성해야하기 때문에

931
01:07:58,494 --> 01:08:02,398
모두 한 번에 앞에,이 제어 흐름 루핑 구조

932
01:08:02,398 --> 01:08:06,364
다시 TensorFlow 그래프의
명시 적 노드 여야합니다.

933
01:08:06,364 --> 01:08:08,084
그래서 당신의 함수 프로그래밍을 기억하기 바란다.

934
01:08:08,084 --> 01:08:10,432
이러한 종류의 연산자를 사용해야하기 때문에

935
01:08:10,432 --> 01:08:13,517
TensorFlow에서 루핑 구문을 구현합니다.

936
01:08:13,517 --> 01:08:16,292
이 경우,이 특정한 반복 관계

937
01:08:16,292 --> 01:08:18,857
foldl 조작을 사용하여 패스 할 수 있습니다.

938
01:08:18,857 --> 01:08:23,024
foldl의 관점에서이 특정 루프를 구현합니다.

939
01:08:24,100 --> 01:08:26,200
그러나 이것이 기본적으로 의미하는 것은
당신이이 감각을 가지고 있다는 것입니다.

940
01:08:26,201 --> 01:08:28,734
TensorFlow는 거의 자체 전체를 구축하고 있습니다.

941
01:08:28,734 --> 01:08:31,214
프로그래밍 언어,의 언어 사용

942
01:08:31,214 --> 01:08:33,212
전산 그래프.

943
01:08:33,212 --> 01:08:34,821
그리고 모든 종류의 제어 흐름 연산자,

944
01:08:34,821 --> 01:08:37,215
또는 모든 종류의 데이터 구조를 롤백해야합니다.

945
01:08:37,215 --> 01:08:40,014
계산 그래프에 넣으세요.

946
01:08:40,014 --> 01:08:42,595
모든 필수적인 패러다임을 명령 적으로 사용하라.

947
01:08:42,595 --> 01:08:44,216
파이썬.

948
01:08:44,216 --> 01:08:46,195
전체 집합으로 다시 학습해야 할 필요가 있습니다.

949
01:08:46,196 --> 01:08:47,956
제어 흐름 연산자.

950
01:08:47,956 --> 01:08:49,991
그리고 어떤 종류의 제어 흐름을 원한다면

951
01:08:49,991 --> 01:08:52,804
TensorFlow를 사용하여 계산 그래프 내부에서

952
01:08:52,804 --> 01:08:56,252
그래서 적어도 저에게는 혼란 스럽습니다.

953
01:08:56,252 --> 01:08:58,238
내 머리를 때로는 감싸기가 힘듭니다.

954
01:08:58,238 --> 01:09:01,259
필자는 PyTorch 동적 그래프를
사용하는 것과 비슷합니다.

955
01:09:01,259 --> 01:09:03,555
당신은 당신이 좋아하는 명령형
프로그래밍을 사용할 수 있습니다.

956
01:09:03,555 --> 01:09:06,722
모든 구문이 잘 작동합니다.

957
01:09:07,737 --> 01:09:12,051
그런데 실제로는 아주 새로운 라이브러리가 있습니다.

958
01:09:12,051 --> 01:09:15,732
이 중 하나 인 TensorFlow Fold라고 불리는

959
01:09:15,732 --> 01:09:17,662
TensorFlow의 맨 위에있는 레이어를 사용하면

960
01:09:17,662 --> 01:09:21,579
동적 그래프, 자신 만의 코드 작성 가능

961
01:09:22,416 --> 01:09:24,986
TensorFlow Fold를
사용하여 동적 인 것처럼 보입니다.

962
01:09:24,986 --> 01:09:27,977
그래프 조작 후 TensorFlow
Fold가 마법을 수행합니다.

963
01:09:27,977 --> 01:09:30,617
당신을 위해 그리고 어떻게 든 그것을

964
01:09:30,617 --> 01:09:32,277
정적 TensorFlow 그래프.

965
01:09:32,277 --> 01:09:35,225
이것은 제시되고있는 초 신종 종이입니다

966
01:09:35,225 --> 01:09:37,357
이번 주에는 프랑스에서 ICLR에서 열렸습니다.

967
01:09:37,358 --> 01:09:39,737
그래서 나는 다이빙을하고 놀 기회를 얻지 못했습니다.

968
01:09:39,737 --> 01:09:41,694
아직 이걸로.

969
01:09:41,694 --> 01:09:44,252
그러나 나의 처음 인상은 그것이
약간을 더한다라는 것이었다.

970
01:09:44,252 --> 01:09:46,455
TensorFlow에 대한 동적 그래프의 양

971
01:09:46,455 --> 01:09:48,798
일종의 원주민보다 일하는 것이 조금 더 어색하다.

972
01:09:48,798 --> 01:09:51,952
PyTorch에있는 동적 그래프.

973
01:09:51,952 --> 01:09:54,527
그럼, 동기 부여하는 것이 좋을 거라 생각 했어.

974
01:09:54,527 --> 01:09:57,257
왜 우리는 일반적으로 동적 그래프에 관심이 있습니까?

975
01:09:57,257 --> 01:10:00,257
따라서 하나의 옵션은 반복적 인 네트워크입니다.

976
01:10:01,177 --> 01:10:03,256
그래서 당신은 이미지 캡션과 같은 것을 볼 수 있습니다.

977
01:10:03,256 --> 01:10:05,715
우리는 재발하는 네트워크를 사용합니다.

978
01:10:05,715 --> 01:10:07,612
길이가 다른 시퀀스.

979
01:10:07,612 --> 01:10:10,798
이 경우 생성하려는 문장

980
01:10:10,798 --> 01:10:13,337
캡션은 시퀀스이고 그 시퀀스는 다를 수 있으므로

981
01:10:13,337 --> 01:10:15,636
입력 데이터에 따라 다릅니다.

982
01:10:15,636 --> 01:10:18,414
이제 우리는이 역 동성을 사물에서 볼 수 있습니다.

983
01:10:18,414 --> 01:10:21,694
문장의 크기에 따라,

984
01:10:21,694 --> 01:10:24,136
우리의 전산 그래프는 더 많은 것을 가질 필요가 있습니다.

985
01:10:24,136 --> 01:10:25,716
또는 더 적은 수의 요소.

986
01:10:25,716 --> 01:10:29,920
그래서 이것은 동적 그래프의 공통된 적용의 한 종류입니다.

987
01:10:29,920 --> 01:10:34,115
지난 분기에 CS224N을 가져간 분들의 경우,

988
01:10:34,115 --> 01:10:36,377
재귀 네트워크에 대한이 아이디어를 보았습니다.

989
01:10:36,377 --> 01:10:38,674
때때로 자연 언어 처리에서

990
01:10:38,674 --> 01:10:41,316
예를 들어 파싱 된 트리를 계산할 수 있습니다.

991
01:10:41,316 --> 01:10:43,934
문장을 읽은 다음 신경을 갖기를 원한다.

992
01:10:43,934 --> 01:10:47,337
네트워크 종류의이 파스 트리를 재귀 적으로 작동합니다.

993
01:10:47,337 --> 01:10:49,291
그래서 신경망을 가지고 그런 종류의 작품,

994
01:10:49,291 --> 01:10:51,817
계층의 순차적 인 순서가 아니라,

995
01:10:51,817 --> 01:10:54,516
그러나 대신 그것은 어떤 그래프를 통해 일종의 일을한다.

996
01:10:54,516 --> 01:10:56,856
또는 트리 구조 대신 이제 각 데이터 요소

997
01:10:56,856 --> 01:10:58,732
다른 그래프 또는 트리 구조를 가질 수 있습니다.

998
01:10:58,732 --> 01:11:00,756
그래서 계산 그래프의 구조

999
01:11:00,756 --> 01:11:03,194
종류는 입력 데이터의 구조를 반영합니다.

1000
01:11:03,194 --> 01:11:05,714
데이터 포인트마다 다를 수 있습니다.

1001
01:11:05,714 --> 01:11:07,934
그래서 이런 종류의 일은 복잡해 보이고

1002
01:11:07,934 --> 01:11:10,316
TensorFlow를 사용하여 구현하기에 털이 많습니다.

1003
01:11:10,316 --> 01:11:12,478
그러나 PyTorch에서 당신은 단지 종류의 사용을 할 수 있습니다.

1004
01:11:12,478 --> 01:11:14,054
정상적인 파이썬 제어 흐름과 같이 작동합니다.

1005
01:11:14,054 --> 01:11:14,887
잘 됐네.

1006
01:11:16,574 --> 01:11:19,198
더 많은 연구 응용 프로그램의 또
다른 비트가 정말이 있습니다

1007
01:11:19,198 --> 01:11:21,614
신경 이완 네트워크라고하는 멋진 아이디어.

1008
01:11:21,614 --> 01:11:23,678
시각적 인 질문 응답.

1009
01:11:23,678 --> 01:11:26,718
여기서 우리는 몇 가지 질문을하고 싶습니다.

1010
01:11:26,718 --> 01:11:29,278
이 이미지를 입력 할 수있는 이미지에 대해

1011
01:11:29,278 --> 01:11:31,737
고양이와 개들에 대해서는 몇 가지 질문이 있습니다.

1012
01:11:31,737 --> 01:11:34,756
어떤 색깔이 고양이이고, 그 다음에는 내부적으로

1013
01:11:34,756 --> 01:11:37,614
그 질문을 읽을 수 있고, 그것들은이
다른 것들을 가지고 있습니다.

1014
01:11:37,614 --> 01:11:39,758
수행을위한 특수화 된 신경망 모듈

1015
01:11:39,758 --> 01:11:43,594
색을 묻고 고양이를 찾는 것과 같은 작업.

1016
01:11:43,594 --> 01:11:45,915
그리고 질문의 텍스트에 따라,

1017
01:11:45,915 --> 01:11:48,193
응답을 위해이 사용자 지정 아키텍처를
컴파일 할 수 있습니다.

1018
01:11:48,193 --> 01:11:49,838
질문.

1019
01:11:49,838 --> 01:11:52,294
그리고 지금 우리가 다른 질문을한다면,

1020
01:11:52,294 --> 01:11:55,094
개보다 고양이가 더 많습니까?

1021
01:11:55,094 --> 01:11:58,241
이제 우리는 아마 같은 기본 모듈
세트를 가지고있을 것입니다.

1022
01:11:58,241 --> 01:12:00,534
고양이와 개를 발견하고 세는 것,

1023
01:12:00,534 --> 01:12:03,076
그러나 그들은 다른 순서로 정렬됩니다.

1024
01:12:03,076 --> 01:12:05,177
그래서 우리는이 역 동성을 다른 데이터 포인트

1025
01:12:05,177 --> 01:12:07,716
다른 계산 그래프가 생길 수 있습니다.

1026
01:12:07,716 --> 01:12:09,635
하지만 이것은 좀 더 연구적인 것입니다.

1027
01:12:09,635 --> 01:12:12,574
어쩌면 지금은 그렇게 주요 스트림이 아닐 수도 있습니다.

1028
01:12:12,574 --> 01:12:15,037
그러나 좀 더 큰 요점으로, 나는 생각한다.

1029
01:12:15,037 --> 01:12:17,198
사람들이 좋아하는 멋지고 창의적인 응용 프로그램

1030
01:12:17,198 --> 01:12:19,214
동적 계산 그래프로 할 수있다.

1031
01:12:19,214 --> 01:12:21,577
어쩌면 지금은 그렇게 많지 않을 수도 있습니다.

1032
01:12:21,577 --> 01:12:23,471
단지 그들과 함께 일하는 것은
너무 고통 스럽기 때문입니다.

1033
01:12:23,471 --> 01:12:25,577
그래서 저는 많은 기회가 있다고 생각합니다.

1034
01:12:25,577 --> 01:12:27,396
시원하고 창조적 인 일을하기 때문에

1035
01:12:27,396 --> 01:12:30,596
동적 계산 그래프.

1036
01:12:30,596 --> 01:12:32,297
그리고 멋진 아이디어가 떠오르면,

1037
01:12:32,297 --> 01:12:34,078
우리는 내년에 강의에서 그 특징을 다룰 것입니다.

1038
01:12:34,078 --> 01:12:37,612
그래서 저는 Caffe에 대해 아주
간단히 이야기하고 싶었습니다.

1039
01:12:37,612 --> 01:12:39,854
버클리에서이 프레임 워크입니다.

1040
01:12:39,854 --> 01:12:43,795
어느 Caffe가 다른 Caffe와 다소 다른가요?

1041
01:12:43,795 --> 01:12:45,774
깊은 학습 프레임 워크는 많은 경우에

1042
01:12:45,774 --> 01:12:47,454
당신은 실제로 글쓰기없이 네트워크를 훈련시킬 수 있습니다.

1043
01:12:47,454 --> 01:12:48,815
모든 코드를 직접 작성하십시오.

1044
01:12:48,815 --> 01:12:50,798
이런 기존의 바이너리를 호출하는 것만으로도 충분합니다.

1045
01:12:50,798 --> 01:12:53,214
구성 파일을 설정하고 많은 경우에

1046
01:12:53,214 --> 01:12:56,697
자신의 코드를 작성하지 않고도
데이터를 학습 할 수 있습니다.

1047
01:12:56,697 --> 01:13:00,078
먼저 데이터를 변환하고

1048
01:13:00,078 --> 01:13:03,054
HDF5 또는 LMDB와 같은 형식으로

1049
01:13:03,054 --> 01:13:06,014
Caffe 내부의 일부 스크립트는
다음과 같이 변환 할 수 있습니다.

1050
01:13:06,014 --> 01:13:08,638
이미지 및 텍스트 파일을이 형식으로 변환 할 수 있습니다.

1051
01:13:08,638 --> 01:13:12,537
코드를 작성하는 대신 정의해야합니다.

1052
01:13:12,537 --> 01:13:14,478
계산 그래프의 구조를 정의하려면,

1053
01:13:14,478 --> 01:13:17,414
대신 prototxt라는 텍스트 파일을 편집합니다.

1054
01:13:17,414 --> 01:13:19,934
이는 계산 그래프의 구조를 설정합니다.

1055
01:13:19,934 --> 01:13:22,997
여기 구조는 우리가 어떤 입력으로부터 읽는 것입니다.

1056
01:13:22,997 --> 01:13:26,537
HDF5 파일, 우리는 일부 내부 제품을 수행,

1057
01:13:26,537 --> 01:13:28,974
우리는 약간의 손실과 전체 구조를 계산한다.

1058
01:13:28,974 --> 01:13:30,875
이 텍스트 파일에 그래프 그래프가 설정됩니다.

1059
01:13:30,875 --> 01:13:33,653
한 가지 종류의 단점은 이러한 파일

1060
01:13:33,653 --> 01:13:35,956
매우 큰 네트워크의 경우 정말 추악해질 수 있습니다.

1061
01:13:35,956 --> 01:13:38,455
그래서 152 레이어 ResNet 모델과 같은 것을 위해,

1062
01:13:38,455 --> 01:13:41,535
그건 원래 Caffe에서 훈련 받았어.

1063
01:13:41,535 --> 01:13:44,253
이 prototxt 파일은 거의 7000 줄에 이릅니다.

1064
01:13:44,253 --> 01:13:47,278
그래서 사람들은 이것을 손으로 쓰지 않습니다.

1065
01:13:47,278 --> 01:13:49,887
사람들은 때로는 파이썬 스크립트 작성을 원할 것입니다.

1066
01:13:49,887 --> 01:13:51,817
prototxt 파일을 생성합니다.

1067
01:13:51,817 --> 01:13:53,275
[웃음]

1068
01:13:53,275 --> 01:13:55,154
그렇다면 당신은 자신의 것을 굴리는 영역에 친절합니다.

1069
01:13:55,154 --> 01:13:56,318
전산 그래프 추상화.

1070
01:13:56,318 --> 01:13:58,974
그것은 아마 좋은 생각이 아니지만
이전에 그것을 보았습니다.

1071
01:13:58,974 --> 01:14:02,238
그런 다음, 옵티 마이저 오브젝트를 갖는 것보다는,

1072
01:14:02,238 --> 01:14:05,316
대신에 어떤 솔버가 있습니다. 솔버를 정의합니다.

1073
01:14:05,316 --> 01:14:07,497
다른 prototxt 안에.

1074
01:14:07,497 --> 01:14:09,118
이것은 학습 속도를 정의하며,

1075
01:14:09,118 --> 01:14:11,036
귀하의 최적화 알고리즘 및 이것 저것.

1076
01:14:11,036 --> 01:14:12,334
그리고 나서이 모든 일을하면

1077
01:14:12,334 --> 01:14:14,174
train 명령으로 Caffe
바이너리를 실행할 수 있습니다.

1078
01:14:14,174 --> 01:14:17,278
모든 것이 마술처럼 일어납니다.

1079
01:14:17,278 --> 01:14:19,577
카피 (Cafee)에는 일련의 사전
모델링 된 모델 동물원이 있으며,

1080
01:14:19,577 --> 01:14:21,294
그건 꽤 유용합니다.

1081
01:14:21,294 --> 01:14:23,454
Caffe는 파이썬 인터페이스를
가지고 있지만 슈퍼가 아닙니다.

1082
01:14:23,454 --> 01:14:25,438
잘 기록 된.

1083
01:14:25,438 --> 01:14:27,358
파이썬의 소스 코드를 읽을 필요가 있습니다.

1084
01:14:27,358 --> 01:14:29,017
인터페이스를 사용하여 수행 할 수있는 작업을 확인하고,

1085
01:14:29,017 --> 01:14:30,116
그래서 그것은 짜증나게합니다.

1086
01:14:30,116 --> 01:14:31,455
그러나 그것은 효과가있다.

1087
01:14:31,455 --> 01:14:35,622
Caffe에 관한 나의 일반적인 점은,

1088
01:14:36,596 --> 01:14:38,334
어쩌면 피드 포워드 모델에 좋을 것입니다.

1089
01:14:38,334 --> 01:14:40,174
프로덕션 시나리오에 유용 할 수 있습니다.

1090
01:14:40,174 --> 01:14:42,796
파이썬에 의존하지 않기 때문입니다.

1091
01:14:42,796 --> 01:14:45,075
하지만 요즘 연구를 위해 아마도
Caffe를 보았을 것입니다.

1092
01:14:45,075 --> 01:14:47,358
어쩌면 조금 덜 사용되었을 수도 있습니다.

1093
01:14:47,358 --> 01:14:49,597
비록 그것이 아직도 꽤 일반적으로 사용된다고 생각하지만

1094
01:14:49,597 --> 01:14:51,417
생산을 위해 업계에서 다시.

1095
01:14:51,417 --> 01:14:54,410
나는 Caffe 2에서 하나의 슬라이드,
하나 또는 두 개의 슬라이드를 약속합니다.

1096
01:14:54,410 --> 01:14:58,596
그래서 Caffe 2는 Facebook의
Caffe의 후계자입니다.

1097
01:14:58,596 --> 01:15:02,432
그것은 슈퍼 새로운, 그것은 일주일 전에 릴리스되었습니다.

1098
01:15:02,432 --> 01:15:04,436
[웃음]

1099
01:15:04,436 --> 01:15:06,617
그래서 저는 수퍼를 형성 할 시간이 없었습니다.

1100
01:15:06,617 --> 01:15:09,314
Caffe 2에 대한 교육받은 의견은 아직,

1101
01:15:09,314 --> 01:15:12,318
TensorFlow와 비슷한 정적 그래프를 사용합니다.

1102
01:15:12,318 --> 01:15:15,198
Caffe와 같은 종류의 핵심은 C ++로 작성되었습니다.

1103
01:15:15,198 --> 01:15:17,817
그들은 파이썬 인터페이스를 가지고 있습니다.

1104
01:15:17,817 --> 01:15:19,694
차이점은 이제 더 이상 필요가 없다는 것입니다.

1105
01:15:19,694 --> 01:15:21,518
자신의 파이썬 스크립트를 작성하여
prototxt 파일을 생성하십시오.

1106
01:15:21,518 --> 01:15:25,312
계산 그래프 구조를 정의 할 수 있습니다.

1107
01:15:25,312 --> 01:15:28,170
파이썬에서는 모두 보이는 API를 사용하여 찾고 있습니다.

1108
01:15:28,170 --> 01:15:29,657
TensorFlow와 같은 종류입니다.

1109
01:15:29,657 --> 01:15:31,854
하지만 그 다음에 침을 뱉을 수 있습니다.
이것을 직렬화 할 수 있습니다.

1110
01:15:31,854 --> 01:15:34,596
전산 그래프 구조를 prototxt 파일로 변환합니다.

1111
01:15:34,596 --> 01:15:36,777
그리고 일단 당신의 모델이 훈련되면 이것 저것,

1112
01:15:36,777 --> 01:15:38,676
우리는 정적에 대해 이야기 한이 유익을 얻습니다.

1113
01:15:38,676 --> 01:15:41,257
그래프는 할 수 있습니다. 원본은 필요 없습니다.

1114
01:15:41,257 --> 01:15:43,534
훈련 된 모델을 배치하기위한 교육 코드를 제공합니다.

1115
01:15:43,534 --> 01:15:46,958
그래서 흥미로운 점은 Google을 보았다는 것입니다.

1116
01:15:46,958 --> 01:15:49,417
어쩌면 하나의 중요한 깊은 실행
프레임 워크를 가질 수 있습니다.

1117
01:15:49,417 --> 01:15:52,094
TensorFlow는 페이스 북이이
두 가지를 가지고 있으며,

1118
01:15:52,094 --> 01:15:53,761
PyTorch와 Caffe 2.

1119
01:15:54,596 --> 01:15:57,252
그래서 이들은 서로 다른 철학입니다.

1120
01:15:57,252 --> 01:15:59,751
Google이 규칙으로 하나의 프레임
워크를 구축하려고 시도하고 있습니다.

1121
01:15:59,751 --> 01:16:01,569
가능한 모든 시나리오에서 작동하는 모든 것

1122
01:16:01,569 --> 01:16:02,847
깊은 학습을 위해.

1123
01:16:02,847 --> 01:16:04,609
이것은 모든 노력을 통합하기 때문에 친절합니다.

1124
01:16:04,609 --> 01:16:06,209
하나의 프레임 워크에.

1125
01:16:06,209 --> 01:16:07,852
그것은 당신이 오직 한 가지를 배울 필요가 있음을 의미합니다.

1126
01:16:07,852 --> 01:16:09,464
다양한 시나리오에서 작동합니다.

1127
01:16:09,464 --> 01:16:11,708
분산 시스템, 생산,

1128
01:16:11,708 --> 01:16:13,772
배치, 모바일, 연구, 모든 것.

1129
01:16:13,772 --> 01:16:15,706
이러한 모든 일을 수행하는 하나의
프레임 워크를 배울 필요가 있습니다.

1130
01:16:15,706 --> 01:16:18,151
반면 페이스 북은 조금 다른 접근 방식을 취하고있다.

1131
01:16:18,151 --> 01:16:20,849
PyTorch가 정말로 더 전문화 된 곳에서,

1132
01:16:20,849 --> 01:16:23,591
글쓰기의 관점에서 연구에 더욱 초점을 맞 춥니 다.

1133
01:16:23,591 --> 01:16:26,071
연구 코드를 작성하고 아이디어를
신속하게 반복 할 수 있습니다.

1134
01:16:26,071 --> 01:16:27,948
그것은 PyTorch에서 매우 쉽지만,

1135
01:16:27,948 --> 01:16:30,869
프로덕션에서 실행, 모바일 장치에서 실행,

1136
01:16:30,869 --> 01:16:32,951
PyTorch는 많은 도움을받지 못합니다.

1137
01:16:32,951 --> 01:16:35,210
대신, Caffe 2는 더 많은
것을 목표로 개발되었습니다.

1138
01:16:35,210 --> 01:16:37,710
생산 지향 유스 케이스.

1139
01:16:39,567 --> 01:16:42,929
그래서 내 일반적인 연구, 제 장군, 전반적인 조언

1140
01:16:42,929 --> 01:16:45,409
어떤 프레임 워크가 어떤 문제에 사용되는지

1141
01:16:45,409 --> 01:16:47,350
그 두 종류 다,

1142
01:16:47,350 --> 01:16:50,172
나는 TensorFlow가 단지
대략 안전한 내기라고 생각합니다.

1143
01:16:50,172 --> 01:16:53,510
새로운 프로젝트를 시작하고 싶은 프로젝트는 무엇입니까?

1144
01:16:53,510 --> 01:16:56,168
그것들을 모두 다스리는 하나의
프레임 워크의 일종이기 때문에,

1145
01:16:56,168 --> 01:16:58,849
그것은 거의 모든 상황을 위해 사용될 수 있습니다.

1146
01:16:58,849 --> 01:17:01,166
그러나, 당신은 아마 그것과 함께 쌍을해야합니다

1147
01:17:01,166 --> 01:17:03,510
높은 수준의 래퍼 및 동적 그래프가 필요한 경우,

1148
01:17:03,510 --> 01:17:05,207
어쩌면 운이 좋을 수도 있습니다.

1149
01:17:05,207 --> 01:17:07,152
코드 중 일부가 조금 더 못 생겼습니다.

1150
01:17:07,152 --> 01:17:10,226
내 의견으로는,하지만 어쩌면 그것은
일종의 코스메틱 디테일입니다.

1151
01:17:10,226 --> 01:17:13,190
그다지 중요하지 않습니다.

1152
01:17:13,190 --> 01:17:15,809
필자는 개인적으로 PyTorch가
연구에 정말 유용하다고 생각합니다.

1153
01:17:15,809 --> 01:17:18,675
연구 코드 작성에만 집중한다면,

1154
01:17:18,675 --> 01:17:21,233
PyTorch는 훌륭한 선택이라고 생각합니다.

1155
01:17:21,233 --> 01:17:23,830
그러나 조금 더 새롭고 지역 사회 지원이 적으며,

1156
01:17:23,830 --> 01:17:25,649
코드가 적어서 약간의 모험이 될 수 있습니다.

1157
01:17:25,649 --> 01:17:28,412
잘 밟은 길을 더 원한다면, TensorFlow

1158
01:17:28,412 --> 01:17:29,969
더 나은 선택 일 수 있습니다.

1159
01:17:29,969 --> 01:17:32,365
프로덕션 배포에 관심이있는 경우,

1160
01:17:32,365 --> 01:17:34,710
Caffe, Caffe 2 또는
TensorFlow를보아야합니다.

1161
01:17:34,710 --> 01:17:37,017
모바일 배포에 중점을 두는 경우,

1162
01:17:37,017 --> 01:17:39,312
나는 TensorFlow와 Caffe 2 둘
다 일부가 내장되어 있다고 생각합니다.

1163
01:17:39,312 --> 01:17:41,270
그 지원.

1164
01:17:41,270 --> 01:17:43,325
불행히도, 그런 것은 아닙니다.

1165
01:17:43,325 --> 01:17:45,009
하나의 글로벌 베스트 프레임 워크, 일종의 의존

1166
01:17:45,009 --> 01:17:47,393
네가 실제로하려는 일에

1167
01:17:47,393 --> 01:17:49,212
예상하는 응용 프로그램은 무엇입니까?

1168
01:17:49,212 --> 01:17:52,045
그 것들에 대한 나의 일반적인 충고.

1169
01:17:53,169 --> 01:17:55,691
다음 번에는 몇 가지 사례 연구에 대해 이야기하겠습니다.

1170
01:17:55,691 --> 00:00:00,000
다양한 CNN 아키텍처에 대해

