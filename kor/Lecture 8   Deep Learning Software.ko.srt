1
00:00:09,679 --> 00:00:13,891
12시가 지났으니
진행하도록 하겠습니다.

2
00:00:13,891 --> 00:00:17,822
이번 8강에서는 
Deep learning software에 대해 알아볼 것입니다.

3
00:00:17,822 --> 00:00:21,283
매년 아주 많은 변화가 생기는 
정말 재미있는 주제입니다.

4
00:00:21,283 --> 00:00:25,621
다만 매년 너무 많이 바뀌어서 
매번 강의준비가 좀 빡세긴 합니다.

5
00:00:25,621 --> 00:00:30,024
우선 몇가지 공지사항을 전달하겠습니다.

6
00:00:30,024 --> 00:00:34,563
우선 지난 화요일에 프로젝트 기획서
제출기한이 마감되었습니다.

7
00:00:34,563 --> 00:00:42,766
다들 좋은 아이디어를 가지고 기한내에 잘 제출했길 바랍니다.

8
00:00:42,766 --> 00:00:50,217
현재 프로젝트 분야에 맞는 적절한 TA를 배치중에 있습니다.

9
00:00:50,217 --> 00:00:54,264
추후에 더 많은 정보가 공지될 것입니다.

10
00:00:54,264 --> 00:00:56,563
그리고 현재 과제1을 채점중에 있습니다.

11
00:00:56,563 --> 00:01:00,942
가능한 빨리 점수를 알려드리도록 하겠습니다.

12
00:01:00,942 --> 00:01:08,680
그리고 과제2의 제출기한도 얼마 남지 않았습니다.
다음 주 까지 라는 것을 꼭 명심하시기 바랍니다.

13
00:01:08,680 --> 00:01:16,231
과제2를 진행할 때 반드시 Google Cloud를 잘 종료시켜서 
각자 크래딧을 아끼도록 하세요

14
00:01:16,231 --> 00:01:24,812
그리고 다시한번 말씀드리고 싶은 것은 과제2에서 GPU를
사용한만한 것은 마지막 문제뿐이 없다는 것입니다.

15
00:01:24,812 --> 00:01:32,250
나머지 문제들은 python과 numpy만 사용하기 때문에
GPU를 쓸 일이 없습니다.

16
00:01:32,250 --> 00:01:36,701
GPU는 정말 필요할 때만 사용하셔서 크래딧을 아껴 쓰시기 바랍니다.

17
00:01:36,701 --> 00:01:39,973
마지막으로 이제 곧 중간고사가 예정되어 있습니다.

18
00:01:39,973 --> 00:01:45,683
벌써 중간고사 기간이라는 것이 믿어지지 않지만
중간고사는 5월 9일 화요일 수업시간에 진행됩니다.

19
00:01:45,683 --> 00:01:47,901
중간고사는 이론중심으로 출제될 것입니다.

20
00:01:47,901 --> 00:01:57,071
지금까지 수업을 잘 이해하고 있는지를 확인하기 위한
몇가지 이론적인 문제들을 출제할 것입니다.

21
00:01:57,071 --> 00:02:02,506
몇 가지 예상문제를 올려놓도록 하겠습니다.

22
00:02:02,506 --> 00:02:03,695
질문 있나요?

23
00:02:03,695 --> 00:02:05,310
[학생이 질문]

24
00:02:05,310 --> 00:02:10,675
오픈북이냐고 질문하셨는데요 
오픈북은 아닙니다.

25
00:02:10,675 --> 00:02:15,671
지금까지 항상 클로즈북으로 진행해 왔습니다.

26
00:02:15,671 --> 00:02:21,735
여러분이 수업에서 다룬 것들의 직관들을 얼마나
이해했는지를 확인하는 것 위주로 출제될 것입니다.

27
00:02:23,618 --> 00:02:27,577
자 그럼 지난시간의 내용을 간단하게 복습해 보도록 하겠습니다.

28
00:02:27,577 --> 00:02:29,737
지난 시간에 다양한 딥러닝 최적화 알고리즘을 배웠습니다.

29
00:02:29,737 --> 00:02:34,975
 SGD, Momentum, Nesterov, RMSProp 그리고 Adam
에 대해서 배웠습니다.

30
00:02:34,975 --> 00:02:45,492
이 방법들을 모두 기본적인 SGD를 조금씩 변형시킨 방법이었죠
구현 자체는 간단하나 네트워크의 수렴속도는 더 빨랐습니다.

31
00:02:45,492 --> 00:02:48,529
그리고 regularization에 대해서도 배웠습니다. 
특히나 Dropout에 관한 것이었죠

32
00:02:48,529 --> 00:02:56,975
Dropout은 forward pass에서 임의의 부분은 0으로 만들고
test time에서는 그 noise를 marginalize out 했습니다.

33
00:02:56,975 --> 00:03:02,805
그리고 딥러닝에서 사용하는 다양한 regularization들의
일반적인 패턴에 대해서는 배웠습니다.

34
00:03:02,805 --> 00:03:08,415
Train time에 noise를 추가하고, Test time에는
marginalize out 합니다.

35
00:03:08,415 --> 00:03:15,376
그리고 Transfer learning도 배웠습니다. Pre-train
model을 다운받아서 fine-tune 하는 방법이죠

36
00:03:15,376 --> 00:03:21,314
Transfer learning을 이용하면 데이터셋이 많지 않더라도
딥러닝 문제를 다룰 수 있는 방법이었습니다.

37
00:03:22,781 --> 00:03:29,615
이번 시간에는 화제를 조금 돌려서 소프트웨어/하드웨어가 동작하는
방식에 대해서 조금 다배워보도록 하겠습니다.

38
00:03:29,615 --> 00:03:36,276
그리고 실제 여러분이 학습 시 사용하는 소프트웨어들을
조금 더 심도깊게 다뤄보도록 하겠습니다.

39
00:03:36,276 --> 00:03:43,967
정리하자면 CPU와 GPU에 대해 조금 배울 것이고, 요즘 사람들이
가장 많이 사용하는 립러닝 프레임워크들에 대해서도 배울 것입니다.

40
00:03:45,471 --> 00:03:52,961
자 우선 이런 얘기는 수도 없이 많이 들으셨을 것입니다.
컴퓨터에 CPU와 GPU가 있다는 것 말입니다.

41
00:03:52,961 --> 00:04:02,655
딥러닝에서 GPU를 사용하는데, 사실 지금까지 왜 GPU가 CPU가 좋은지
에 대해서 명확하게 언급하지 않고 넘어갔습니다.

42
00:04:02,655 --> 00:04:06,472
컴퓨터를 조립해본 적이 있나요?
손 한번 들어보시겠습니까

43
00:04:06,472 --> 00:04:10,965
한 1/3에서 절반정도는 계시는군요

44
00:04:10,965 --> 00:04:15,174
이 사진은 제 컴퓨터입니다. 제가 직접 조립했죠

45
00:04:15,174 --> 00:04:22,261
컴퓨터 내부에서 많은 부품들이 있습니다. 아마 여러분들은 각각이
무엇인지 아실 거라고 믿습니다.

46
00:04:22,261 --> 00:04:25,594
CPU는 Central Processing Unit이죠

47
00:04:25,594 --> 00:04:31,391
이 조그마한 칩은 쿨러 믿에 숨어있습니다.

48
00:04:31,391 --> 00:04:39,555
CPU는 엄청 작습니다. 공간을 크게 잡아먹지 않죠

49
00:04:39,555 --> 00:04:46,221
자 여기 GPU들을 보면 공간을 엄청 차지하고 있습니다.
괴물같은 녀석들입니다.

50
00:04:46,221 --> 00:04:50,296
GPU는 자기만의 쿨러가 있고 파워도 엄청 먹고 엄청 큽니다.

51
00:04:50,296 --> 00:04:59,139
GPU가 얼마나 많은 파워를 사용하는지 그리고 얼마나 큰지는 
케이스에서 얼마나 많은 부분을 차지하는지만 봐도 알 수 있습니다.

52
00:04:59,139 --> 00:05:04,516
GPU란 것이 무엇이길래 딥러닝에서 이렇게 중요한 걸까요

53
00:05:04,516 --> 00:05:08,937
GPU는 graphics card 혹은 
Graphics Processing Unit 이라고 합니다.

54
00:05:08,937 --> 00:05:16,166
사실 GPU는 computer graphics를 랜더링하기 위해서 
만들어졌습니다. 게임같은 것들을 위해서죠

55
00:05:16,166 --> 00:05:23,247
자 다시한번 손을 들어보죠, 여기에 집에서 컴퓨터게임을
하시는 분들이 계십니까?

56
00:05:23,247 --> 00:05:25,693
절반정도 있군요 좋습니다.

57
00:05:25,693 --> 00:05:32,196
게임을 하시거나 컴퓨터를 직접 조립해 보신 분들이라면
다음과 같은 질문에 대한 나름대로의 의견을 가지고 계실 것입니다.

58
00:05:32,196 --> 00:05:34,095
[웃음]

59
00:05:34,095 --> 00:05:37,666
이는 Computer Science에서 가장 큰 논쟁 중 하나입니다.

60
00:05:37,666 --> 00:05:42,620
Intel vs AMD 라던가, 그래픽카드의 경우
NVIDIA vs AMD 같은 논쟁들이죠

61
00:05:42,620 --> 00:05:45,394
텍스트 에디터의 경우 Vim vs Emacs도 있겠군요

62
00:05:45,394 --> 00:05:51,945
대부분의 게이머들은 이 논쟁에 대해서
자기만의 의견을 가지고 있습니다.

63
00:05:51,945 --> 00:05:59,116
딥러닝의 경우는 대부분 한쪽만 선택합니다.
NVIDIA죠

64
00:05:59,116 --> 00:06:05,117
만약 AMD 카드를 가지고있다면 아마 딥러닝에 사용하는데
문제가 많을 것입니다.

65
00:06:05,117 --> 00:06:08,812
지난 수년간 NVIDIA는 딥러닝에 많은 공을 들여왔습니다.

66
00:06:08,812 --> 00:06:11,997
NVIDIA의 전략은 세간의 주목을 받았습니다.

67
00:06:11,997 --> 00:06:19,354
Nvidia의 많은 엔지니어들은 딥러닝에 적합한 하드웨어를
만들기 위해서 많은 노력을 했습니다.

68
00:06:19,354 --> 00:06:27,718
그래서 딥러닝과 관련해서는 NVIDIA GPU가
 거의 독점적으로 언급됩니다.

69
00:06:27,718 --> 00:06:35,268
아마 미래에는 또 다른 후발주자가 생겨날 지 모르겠지만
적어도 지금은 NVIDIA가 독점적입니다.

70
00:06:35,268 --> 00:06:41,705
자 그럼 CPU와 GPU의 차이는 무엇일까요? 제가 아주 간단하게
표로 한번 정리해 봤습니다.

71
00:06:41,705 --> 00:06:52,079
위에 두개는 Intel의 최신 상업용 CPU들이고 밑에 두개는
NVIDIA의 최신 상업용 GPU들입니다.

72
00:06:52,079 --> 00:06:55,975
여기에는 몇 가지 주요 트렌드가 있습니다.

73
00:06:55,975 --> 00:07:03,284
GPU와 CPU모두 임의의 명령어를 수행할 수 있는 
범용 컴퓨팅 머신입니다.

74
00:07:03,284 --> 00:07:05,987
하지만 이 둘은 질적으로 아주 다릅니다.

75
00:07:05,987 --> 00:07:16,714
CPU의 경우 core의 수가 적습니다. 오늘날의 상업용 테스크탑 CPU의
경우에 코어가 4개에서 6개 조금 많으면 10개 정도입니다.

76
00:07:16,714 --> 00:07:24,893
그리고 hyperthreading 기술와 더불어 CPU는 8 ~ 20개의
스레드를 동시에 실행시킬 수 있습니다.

77
00:07:24,893 --> 00:07:29,700
따라서 CPU는 한번에 20가지의 일(스레드)을
할 수 있는 것입니다.

78
00:07:29,700 --> 00:07:34,527
이 숫자(20)가 그닥 커보이진 않습니다. 하지만 
CPU의 멀티스레드는 아주 강력합니다.

79
00:07:34,527 --> 00:07:37,223
아주 많은 일을 할 수 있으면 엄청 빠릅니다.

80
00:07:37,223 --> 00:07:43,011
모든 CPU 명령어들은 정말 많은 일을 할 수 있죠
그리고 아주 독립적으로 수행합니다.

81
00:07:43,011 --> 00:07:51,909
하지만 GPU의 경우는 상황이 조금 다릅니다.
고성능의 상업 GPU의 경우 수천개의 코어가 있습니다.

82
00:07:51,909 --> 00:08:00,412
NVIDIA Titan XP와 같은 최상위 모델의 경우 3840개의 
코어가 있습니다. 정말 엄청난 수 입니다.

83
00:08:02,223 --> 00:08:06,357
동일 가격의 CPU를 보면 코어가 10개 뿐이 없죠

84
00:08:06,357 --> 00:08:12,207
하지만 GPU의 단점을 보자면 각각의 코어가 
더 느린 clock speed에서 동작한다는 점입니다.

85
00:08:12,207 --> 00:08:14,439
그리고 두번째는 그 코어들이 그렇게 많은 일을 할 수 없다는 것입니다.

86
00:08:14,439 --> 00:08:19,680
사실 CPU와 GPU코어는 1:1로 비교할 수 없습니다.

87
00:08:19,680 --> 00:08:22,510
GPU코어들은 독립적으로 동작하지 않습니다.

88
00:08:22,510 --> 00:08:29,297
코어마다 독립적인 테스크가 있는 것이 아니라 많은 코어들이
하나의 테스크를 병렬적으로 수행하는 것이죠

89
00:08:29,297 --> 00:08:32,405
따라서 코어의 수만 가지고 직접적으로 비교할 수는 없습니다.

90
00:08:32,405 --> 00:08:41,370
하지만 GPU의 코어의 수가 많다는 것은 어떤 테스크가 있을 때
이 일을 병렬로 수행하기 아주 적합하다는 것은 알 수 있습니다.

91
00:08:41,370 --> 00:08:44,742
하지만 그 테스크는 전부 같은 테스크여야 할 것입니다.

92
00:08:44,742 --> 00:08:49,387
그리고 또 한가지 말씀드릴 것은 메모리와 관련된 것입니다.

93
00:08:49,387 --> 00:08:58,523
CPU에도 캐시가 있습니다. 하지만 비교적 작습니다. CPU는 대부분의 
메모리를 RAM에서 끌어다 씁니다.

94
00:08:58,523 --> 00:09:06,589
RAM은 일반적으로  8, 12, 16, 32 GB 바이트 정도이죠

95
00:09:06,589 --> 00:09:10,646
반면 GPU는 칩 안에 RAM이 내장되어 있습니다.

96
00:09:12,055 --> 00:09:22,675
실제 RAM와 GPU간의 통신은 상당한 보틀넥을 초래합니다.
그렇게 떄문에 GPU는 보통 칩에 RAM이 내장되어 있습니다.

97
00:09:23,955 --> 00:09:33,481
Titan XP의 경우 내장 메모리가 12GB정도 됩니다.

98
00:09:33,481 --> 00:09:41,790
GPU는 12GB의 메모리와 GPU 코어 사이의 캐싱을 하기 위한
일종의 다계층 캐싱 시스템을 가지고 있습니다.

99
00:09:41,790 --> 00:09:46,908
이는 CPU의 캐싱 계층구조와 아주 유사합니다.

100
00:09:47,985 --> 00:09:52,583
CPU는 범용처리에 적합합니다. 
CPU는 아주 다양한 일을 할 수 있죠

101
00:09:52,583 --> 00:09:57,089
그리고 GPU는 병렬처리에 더 특화되어 있습니다.

102
00:09:57,089 --> 00:10:04,106
GPU에서 정말 잘 동작하고 아주 적합한 알고리즘은 바로
행렬곱(Matrix multiplication) 연산입니다.

103
00:10:04,106 --> 00:10:14,348
왼쪽 행렬은 행이 엄청 많은 행렬입니다. 
오른쪽은 열이 많죠

104
00:10:14,348 --> 00:10:25,009
오른쪽 행렬은 왼쪽 두 행렬의 내적입니다. 그리고 이 때의
내적 연산은 모두 서로 독립적입니다.

105
00:10:25,009 --> 00:10:33,653
오른쪽의 결과행렬을 살펴보면 각각의 원소가 전부 독립적입니다. 
따라서 모두 병렬로 수행될 수 있습니다.

106
00:10:33,653 --> 00:10:38,289
그리고 각 원소들은 모두 같은 일을 수행합니다. 
두개의 벡터를 내적하는 것이죠.

107
00:10:38,289 --> 00:10:44,177
다만 서로 입력 데이터만 조금씩 다를 뿐입니다.

108
00:10:44,177 --> 00:10:55,166
GPU는 결과 행렬의 각 요소들을 병렬로 계산할 수 있으며
이러한 특성 때문에 GPU는 엄청나게 빠릅니다.

109
00:10:55,166 --> 00:11:04,940
이런 연산들은 GPU가 정말 잘 하는 것들입니다. 
CPU였다면 각 원소를 하나씩만 계산할 것입니다.

110
00:11:06,337 --> 00:11:13,829
물론 CPU라고 그렇게 단순하게 동작하지는 않습니다. CPU는
여러개의 코어가 있고 Vectorized instructions이 존재합니다.

111
00:11:13,829 --> 00:11:19,568
그렇다고 해도 아주 massive한 병렬화 문제에 대해서는
GPU의 처리량이 압도적입니다.

112
00:11:19,568 --> 00:11:25,404
가령 행렬의 크기가 엄청 큰 경우가 될 수 있겠습니다. 그리고 
비슷한 맥락으로 Convolution의 경우도 같습니다.

113
00:11:25,404 --> 00:11:36,359
convolution에는 입력(텐서)이 있고 가중치가 있죠 cov 출력은 
마찬가지로 입력과 가중치간의 내적입니다.

114
00:11:36,359 --> 00:11:43,354
GPU의 경우라면 이 연산을 각 코어에 분배시켜서
아주 빠르게 연산할 수 있도록 해줍니다.

115
00:11:43,354 --> 00:11:49,510
이런 종류의 연산들이 일반적으로 CPU에 비해 GPU에서
연산 속도의 이점을 볼 수 있는 것들입니다.

116
00:11:51,695 --> 00:11:55,498
여러분들도 직접 GPU에서 실행되는 코드를 작성할 수 있습니다.

117
00:11:55,498 --> 00:12:03,614
NVIDIA에서 CUDA를 지원하는데 그 코드를 보면 c언어
스럽게 생겼습니다. 하지만 GPU에서 실행되는 코드입니다.

118
00:12:03,614 --> 00:12:05,484
하지만 CUDA코드를 작성하는 것은 상당히 까다롭습니다.

119
00:12:05,484 --> 00:12:12,002
GPU의 성능을 전부 짜낼 수 있는 코드를 작성하는 것은
상당히 힘든 일입니다.

120
00:12:12,002 --> 00:12:19,163
아주 세심하게 메모리구조를 관리해야 합니다. 가령 cache misses나
branch mispredictions 같은 것들을 전부 고려해야 하죠

121
00:12:19,163 --> 00:12:22,930
따라서 여러분 스스로 아주 효율적인 CUDA code를
작성하는 것은 상당히 어렵습니다.

122
00:12:22,930 --> 00:12:32,537
때문에 NVIDIA는 GPU에 고도로 최적화시킨 기본연산 
라이브러리를 배포해 왔습니다.

123
00:12:32,537 --> 00:12:40,610
가령 cuBLAS는 다양한 행렬곱을 비롯한 연산들을 제공합니다. 
이는 아주 고도로 최적화되어 있습니다.

124
00:12:40,610 --> 00:12:46,438
이는 GPU에서 아주 잘 동작하고 하드웨어 사용의
이론적 최대치까지 끌어올려 놓은 라이브러리입니다.

125
00:12:46,438 --> 00:12:54,499
유사하게 cuDNN이라는 라이브러리도 있습니다. 이는 convolution,
forward/backward pass, batch norm, rnn 등

126
00:12:54,499 --> 00:12:57,454
등 딥러닝에 필요한 거의 모든 기본적인 연산들을
제공하고 있습니다.

127
00:12:57,454 --> 00:13:03,842
NVIDIA는 자사의 하드웨어에 아주 효율적으로 동작하는 
라이브러리를 바이어리로 배포하고 있습니다.

128
00:13:03,842 --> 00:13:09,624
따라서 실제로는 딥러닝을 위해 CUDA 코드를 직접 작성하는
일은 없을 것입니다.

129
00:13:09,624 --> 00:13:14,173
이미 다른 사람들이 작성한 코드를 불러와서 쓰기만 하면 됩니다.

130
00:13:14,173 --> 00:13:19,573
이미 NVIDIA에서 극도로 최적화된 소스를 가져와서 쓰기만 하면 됩니다.

131
00:13:19,573 --> 00:13:23,693
그리고 또 하나의 언어가 있는데 OpenCL입니다. 
OpenCL이 조금 더 범용적입니다.

132
00:13:23,693 --> 00:13:29,185
NVIDIA GPU에서만 동작하는 것이 아니라 AMD 에서도 
그리고 CPU에서도 동작합니다.

133
00:13:29,185 --> 00:13:43,938
하지만 OpenCL은 아직 딥러닝에 극도로 최적화된 연산이나 라이브러리가
개발되지는 않았습니다. 그래서 CUDA보다는 성능이 떨어집니다.

134
00:13:43,938 --> 00:13:51,839
미래에는 더 많은 open standard가 생기고 이종 플랫폼 간의 
다양한 방법들이 생겨날지도 모르겠지만

135
00:13:51,839 --> 00:13:55,488
현재로썬 NVIDIA가 딥러닝의 선두주자입니다.

136
00:13:55,488 --> 00:14:01,686
GPU 프로그래밍을 직접 해보면 익힐 수 있는 다양한 리소스들이 있습니다.
아마 아주 흥미로울 것입니다.

137
00:14:01,686 --> 00:14:05,900
아주 massive한 병령처리 아키텍쳐를 다루는 것이기 때문에 
코드를 작성하는 패러다임도 조금 다를 것입니다.

138
00:14:05,900 --> 00:14:08,023
하지만 이 이상은 우리 강의의 주제를 벗어나기 때문에
여기까지만 하겠습니다.

139
00:14:08,023 --> 00:14:12,263
다시한번 만씀드리지만 GPU로의 딥러닝을 위해서
굳이 스스로 CUDA code를 작성할 필요는 없습니다.

140
00:14:12,263 --> 00:14:16,600
사실 저의 경우도 연구 프로젝트를 진행하면서 
CUDA  code를 직접 짜본 적은 없습니다.

141
00:14:16,600 --> 00:14:22,219
하지만 여러분이 직접 코드를 짜진 않더라도 어떻게 동작하는지
잘 알아두는 것은 상당히 유욜할 수 있습니다.

142
00:14:23,488 --> 00:14:29,168
실제 CPU와 GPU의 성능을 한번 살펴보고 싶으시다면, 
제가 작년 여름이 벤치마크 한 것이 있습니다.

143
00:14:29,168 --> 00:14:36,065
그때 당시 성능이 꽤 괜찮았던 Intel CPU와 
성능이 가장 좋았던 NVIDIA GPU를 비교한 것입니다.

144
00:14:38,747 --> 00:14:48,954
더 자세한 내용은 제 Github에서 찾아볼 수 있습니다.
VGG16/19와 다양한 ResNets을 이용했는데요

145
00:14:49,830 --> 00:14:57,114
그리고 그 결과 GPU에서 보통 65에서 75배의 speed up을 볼 수
있었습니다. 완전히 같은 연산량으로 비교한 것이죠

146
00:14:57,114 --> 00:15:00,984
GPU의 경우는 Pascal TitanX입니다.

147
00:15:00,984 --> 00:15:08,604
CPU의 경우 최상위 성능 까지는 아니였지만 
Intel E5 Processor였습니다.

148
00:15:08,604 --> 00:15:15,550
하지만 여러분들이 이와 같은 딥러닝 벤치마크를 살펴볼 때 
정말 조심해야 할 점이 있습니다.

149
00:15:15,550 --> 00:15:20,103
왜냐하면 비교군 간에 불공평한 비교를 하기 십상이기 떄문이죠

150
00:15:20,103 --> 00:15:26,339
따라서 여러분들이 벤치마크를 살펴볼 때 어떤 부분이 불공평한지를
주도면밀하게 살펴볼 필요가 있습니다.

151
00:15:26,339 --> 00:15:35,855
제가 이 벤치마크에 대해 말씀드려 보자면 이 결과는 사실 
CPU에게는 조금 불리합니다.

152
00:15:35,855 --> 00:15:38,721
제가 CPU에서는 성능을 최대화 시킬 수 있을만큼
공을 들이기 않았기 때문입니다.

153
00:15:38,721 --> 00:15:42,483
저는 아마 BLAS 라이브러리가 CPU에서 더 잘 동작하도록
수정해 볼 수도 있었을 것입니다.

154
00:15:42,483 --> 00:15:44,540
그렇게 되면 이 표의 결과 보다는 조금 더 좋았을 것입니다.

155
00:15:44,540 --> 00:15:51,964
하지만 이또한 뛰어난 성능이긴 합니다. Torch를 설치하고
CPU와 GPU로 실행시켜본 것입니다.

156
00:15:51,964 --> 00:15:57,872
이 성능도 나쁘진 않지만 CPU의 성능을 극대화 시킨 것은 아닙니다.

157
00:15:57,872 --> 00:16:02,422
CPU의 성능을 높힐 수 있는 여지는 충분했습니다.

158
00:16:02,422 --> 00:16:15,543
그리고 또 하나의 벤치마크가 있는데 convolution 연산에
cuDNN을 사용한 것과 일반적인 CUDA를 사용한 것입니다.

159
00:16:15,543 --> 00:16:17,623
오픈소스 커뮤니티에 게제된 결과입니다.

160
00:16:17,623 --> 00:16:24,653
여기에서 볼 수 있는 것은 동일한 하드웨어와 동일한 딥러닝
프레임워크를 사용한 것이고 다른 점은 오직

161
00:16:24,653 --> 00:16:37,442
cuDNN/CUDA 입니다. CUDA은 조금 덜 최적화된 것이죠.
cuDNN이 3X의 속도향상이 있습니다.

162
00:16:37,442 --> 00:16:45,202
일반적으로 여러분이 GPU 코드를 직접 작성해야 한다면 
cuDNN을 반드시 사용해야 할 것입니다.

163
00:16:45,202 --> 00:16:51,602
cuDNN 라이브러리를 쓰고 안 쓰고의 차이가 거의 
3배가 날테니 말입니다.

164
00:16:51,602 --> 00:17:02,882
그리고 실제로 GPU로 학습을 할 때 생기는 문제 중 하나는 바로 Model과
Model의 가중치는 전부 GPU RAM에 상주하고 있는 반면에

165
00:17:02,882 --> 00:17:07,243
실제 Train data(Big data)는 SSD와 같은 하드드라이브에 
있다는 것입니다.

166
00:17:07,243 --> 00:17:13,204
때문에 Train time에 디스크에서 데이터를 읽어드리는 작업을
세심하게 신경쓰지 않으면 보틀넥이 발생할 수 있습니다.

167
00:17:14,321 --> 00:17:23,002
GPU는 forward/backward 가 아주 빠른 것은 사실이지만, 디스크
에서 데이터를 읽어드리는 것이 보틀넥이 되는 것입니다.

168
00:17:23,002 --> 00:17:25,699
이는 상당히 좋지 않은 상황이고 느려지게 될 것입니다.

169
00:17:25,700 --> 00:17:31,459
해결책 중 하나는 바로 데이터셋이 작은 경우에는 전체를
RAM에 올려 놓는 것입니다.

170
00:17:31,459 --> 00:17:36,479
데이터셋이 작지 않더라도, 서버에 RAM 용량이 크다면 
가능할 수도 있을 것입니다.

171
00:17:36,479 --> 00:17:42,917
혹은 HDD대신에 SSD를 사용하는 방법이 있습니다. 
데이터를 읽는 속도를 개선시킬 수 있겠죠

172
00:17:42,917 --> 00:17:52,152
또 한가지 방법은 바로 CPU의 다중스레드를 이용해서 데이터를
RAM에 미리 올려 놓는 것(pre-fetching) 입니다.

173
00:17:52,152 --> 00:17:57,724
그리고 buffer에서 GPU로 데이터를 전송시키게 되면 
성능향상을 기대할 수 있을 것입니다.

174
00:17:57,724 --> 00:18:08,804
물론 이런 설정 자체가 조금 까다롭긴 하지만 GPU는 빠른데 데이터
전송 자체가 충분히 빠르지 못하면 보틀넥이 생길수 밖에 없습니다.

175
00:18:08,804 --> 00:18:11,657
이 부분은 항상 인지하고 있어야 합니다.

176
00:18:11,657 --> 00:18:17,432
지금까지는 딥러닝에 관련한 GPU와 CPU에 대한
간략한 소개였습니다.

177
00:18:17,432 --> 00:18:21,616
자 이제 조금 주제를 틀어서 
소프트웨어와 관련된 이야기를 해볼까 합니다.

178
00:18:21,616 --> 00:18:25,006
실제로 사람들이 사용하는 딥러닝 프레임워크와 관련된 것입니다.

179
00:18:25,006 --> 00:18:28,819
다음으로 넘어가기에 앞서 혹시 CPU와 GPU에 관한
질문 있으십니까?

180
00:18:28,819 --> 00:18:30,519
네 질문있나요?

181
00:18:30,519 --> 00:18:34,686
[학생이 질문]

182
00:18:40,961 --> 00:18:45,854
질문은 바로 데이터읽기 에서의 병목을 해결하기 위해서
프로그래밍으로 우리가 할 수 있는 일이 무엇인지 입니다.

183
00:18:45,854 --> 00:18:50,833
소프트웨어적으로 할 수 있는 일은 
CPU에서 미리 불러오는 것입니다.(pre-fetching)

184
00:18:50,833 --> 00:18:55,054
여러분은 다음과 같이 과정들이 순차적으로 진행되는 
것을 원하지 않을 수 있습니다.

185
00:18:55,054 --> 00:18:58,791
디스크에서 데이터를 읽습니다. 그리고 미니배치의 데이터가
다 읽힐 때 까지 기다립니다.

186
00:18:58,791 --> 00:19:02,458
그리고 미니배치를 GPU에 전송합니다.
그리고 GPU에서 forward/backward를 수행하고

187
00:19:02,458 --> 00:19:05,442
그다음 또 다른 미니배치를 읽는 것이죠
이 단계를 전부 순차적으로 진행합니다.

188
00:19:06,714 --> 00:19:15,469
하지만 그러지 말고 CPU의 멀티스레딩을 통해 디스크에서
데이터를 불러오는 백그라운드 작업을 하고 있다면

189
00:19:15,469 --> 00:19:17,076
이 과정을 interleave하게 진행할 수 있을 것입니다.

190
00:19:17,076 --> 00:19:21,506
GPU이 계산을 수행하는 동안 CPU의 백그라운드 스레드는 
디스크에서 데이터를 불러옵니다.

191
00:19:21,506 --> 00:19:28,534
그리고 CPU의 메인 스레드에서 synchronization를 관리하게 되면
이 모든 작업이 병렬로 수행될 수 있을 것입니다.

192
00:19:28,534 --> 00:19:38,016
그리고 고맙게도 여러분이 딥러닝 프레임워크를 사용한다면 이미 다 
구현되어 있습니다. 일일이 구현하기 까다롭기 때문이죠

193
00:19:38,016 --> 00:19:41,738
딥러닝 프레임워크의 동향을 급변하고 있습니다.

194
00:19:41,738 --> 00:19:47,915
작년까지만 해도 Caffe, Torch, Tensorflow에 대해서만
언급했었습니다.

195
00:19:47,915 --> 00:20:00,232
1년 전까지만 해도 Tensorflow가 가장 최신의 프레임워크라서
사람들에게 널리 쓰이지 않았었습니다.

196
00:20:00,232 --> 00:20:06,310
하지만 지금은 아주 유명해져서 많은 사람들이 선택하는
메인 프레임워크가 되었습니다. 아주 큰 변화입니다.

197
00:20:07,342 --> 00:20:12,282
그리고 작년부터 정말 많은 프레임워크가 생겨나고 있습니다.

198
00:20:12,282 --> 00:20:18,052
특히나 Caffe2와 PyTorch가 생겼습니다.Facebook에서 밀고있는 
프레임워크로 아주 흥미롭습니다.

199
00:20:18,052 --> 00:20:20,409
그 밖에도 정말 많은 프레임워크들이 있습니다.

200
00:20:20,409 --> 00:20:24,089
Baidu에는 Paddle이 있고 Microsoft는 CNTK이 있죠

201
00:20:24,089 --> 00:20:33,449
Amazon은 주로 MXNet을 사용하는데 이 밖에도 정말 많은
프레임워크가 있습니다. 다 써볼 시간도 부족하죠

202
00:20:33,449 --> 00:20:43,572
이 그림에서 주목할만한 재미있는 점은 바로 딥러닝 프레임워크의
초창기 세대는 학계(academia)에서 구축되었다는 것입니다.

203
00:20:43,572 --> 00:20:49,388
원래 Caffe은 Berkeley에서 Torch는 NYU에서 개발되었고
이후 Facebook과 공동연구를 시작한 것이죠

204
00:20:49,388 --> 00:20:52,077
Theana은 대부분 Montreal에서 개발했습니다.

205
00:20:52,077 --> 00:20:56,491
하지만 다음세대의 딥러닝 프레임워크들은
기업(industry)에서 태동했습니다.

206
00:20:56,491 --> 00:21:00,659
Caffe2와 PyTorch는 Facebook에서 나왔습니다.
TensorFlow은 Google에서 나왔죠

207
00:21:00,659 --> 00:21:08,925
최근 몇 년간의 흥미로운 변화는 바로 
academia에서 industry로의 이동이라고 할 수 있습니다.

208
00:21:08,925 --> 00:21:13,187
이제는 industry가 이러한 강력한 프레임워크를 제공하고 있습니다.

209
00:21:14,147 --> 00:21:24,850
오늘은 대부분 PyTorch와 TensorFlow에 대해서 말씀드릴 것입니다.
아마 여러분은 앞으로 이 두 프레임워크를 자주 다루게 될 것입니다.

210
00:21:24,850 --> 00:21:32,192
Caffe와 Caffe2도 간단히 설명해 드리겠습니다.
하지만 엄청 강조하진 않겠죠

211
00:21:32,192 --> 00:21:34,341
그리고 우리가 더 멀리 움직이기 전에,
나는 내가해야한다고 생각했다.

212
00:21:34,341 --> 00:21:36,705
내 자신의 편견은 좀 더 명백하다.

213
00:21:36,705 --> 00:21:39,058
그래서 저는 주로 토치와 함께 일했습니다.

214
00:21:39,058 --> 00:21:40,315
지난 몇 년간

215
00:21:40,315 --> 00:21:43,501
그리고 저는 그것을 아주 많이 사용했습니다.
나는 그것을 아주 좋아합니다.

216
00:21:43,501 --> 00:21:46,306
그리고 작년에 저는 대부분 PyTorch로 전환했습니다.

217
00:21:46,306 --> 00:21:48,568
나의 주요 연구 틀로서

218
00:21:48,568 --> 00:21:50,487
그래서 나는 약간의 경험이 조금있다.

219
00:21:50,487 --> 00:21:52,306
이들 중 특히 TensorFlow,

220
00:21:52,306 --> 00:21:54,087
그러나 나는 너에게 공정한 것을주기
위해 최선을 다하려고 노력할 것이다.

221
00:21:54,087 --> 00:21:58,382
그림과 이러한 것들의보기 흉하지 않은 개요.

222
00:21:58,382 --> 00:22:02,507
그래서 마지막 몇 강의에서

223
00:22:02,507 --> 00:22:04,725
우리는이 계산상의 아이디어를

224
00:22:04,725 --> 00:22:06,807
일종의 계속.

225
00:22:06,807 --> 00:22:08,217
당신이 깊은 학습을 할 때마다,

226
00:22:08,217 --> 00:22:09,970
당신은 몇 가지 전산 그래프
작성에 대해 생각하고 싶습니다.

227
00:22:09,970 --> 00:22:13,176
그것은 당신이 계산하기를 원하는 기능을 계산합니다.

228
00:22:13,176 --> 00:22:15,090
따라서 선별기의 경우에는

229
00:22:15,090 --> 00:22:18,778
데이터 X와 행렬 곱셈을 사용하는 가중치 W.

230
00:22:18,778 --> 00:22:21,554
당신은 아마도 가지고있는 힌지 손실의 일종을 할 것입니다,

231
00:22:21,554 --> 00:22:22,832
귀하의 손실을 계산하십시오.

232
00:22:22,832 --> 00:22:24,386
정규화 기간이 있습니다.

233
00:22:24,386 --> 00:22:26,397
너는이 모든 다른 것들을 함께 엮어서 상상한다.

234
00:22:26,397 --> 00:22:28,909
작업을 일부 그래프 구조로 변환합니다.

235
00:22:28,909 --> 00:22:31,069
이 그래프 구조가 꽤 아름답다는 것을 기억하십시오.

236
00:22:31,069 --> 00:22:33,191
큰 신경망의 경우 복잡한,

237
00:22:33,191 --> 00:22:34,680
이제는 여러 가지 레이어가 있습니다.

238
00:22:34,680 --> 00:22:36,167
많은 다른 활성화.

239
00:22:36,167 --> 00:22:38,087
많은 다른 가중치들이 예쁘게 둘러 쌓여 있습니다.

240
00:22:38,087 --> 00:22:39,687
복잡한 그래프.

241
00:22:39,687 --> 00:22:41,490
그리고 당신이 신경 튜링 기계와 같은 것들로 이동할 때

242
00:22:41,490 --> 00:22:44,130
그러면이 정말로 미친 계산 그래프를 얻을 수 있습니다.

243
00:22:44,130 --> 00:22:45,911
당신이 정말로 끌기조차 할 수 없다는 이유는

244
00:22:45,911 --> 00:22:47,328
너무 크고 지저분 해.

245
00:22:48,349 --> 00:22:52,127
따라서 심화 학습 프레임 워크의 핵심은 실제로,

246
00:22:52,127 --> 00:22:54,392
실제로 왜 세 가지 주요 이유가 있습니다.

247
00:22:54,392 --> 00:22:56,425
이 깊은 학습 프레임 워크 중 하나를 사용하고 싶다.

248
00:22:56,425 --> 00:22:58,727
자신의 코드를 작성하는 것이 아닙니다.

249
00:22:58,727 --> 00:23:01,414
그래서 첫 번째는 이러한 프레임 워크가

250
00:23:01,414 --> 00:23:03,255
당신은 쉽게이 커다란 털이 많은 것을 만들 수 있습니다.

251
00:23:03,255 --> 00:23:05,950
걱정하지 않고 전산 그래프

252
00:23:05,950 --> 00:23:08,610
그 부기에 관한 정보가 많이 있습니다.

253
00:23:08,610 --> 00:23:10,860
또 다른 주요 아이디어는,

254
00:23:11,716 --> 00:23:13,479
우리가 깊은 학습에서 일할 때마다

255
00:23:13,479 --> 00:23:14,812
항상 그래디언트를 계산해야합니다.

256
00:23:14,812 --> 00:23:16,211
우리는 항상 약간의 손실을 계산하고 있으며,

257
00:23:16,211 --> 00:23:17,629
우리는 항상 컴퓨터 무게의 우리 몸무게입니다.

258
00:23:17,629 --> 00:23:18,900
손실과 관련하여.

259
00:23:18,900 --> 00:23:22,973
그리고 우리는 이것을 자동으로
그라디언트를 계산하고 싶습니다.

260
00:23:22,973 --> 00:23:26,115
당신은 그 코드를 직접 작성하고 싶지 않습니다.

261
00:23:26,115 --> 00:23:28,287
이 프레임 워크가 이러한 모든 역
전파를 처리하기를 원합니다.

262
00:23:28,287 --> 00:23:30,485
당신을위한 세부 사항을 생각해 볼 수 있습니다.

263
00:23:30,485 --> 00:23:32,526
네트워크 정방향 패스 기록하기

264
00:23:32,526 --> 00:23:34,725
그리고 후진 패스를 무료로 나눠줘야 해.

265
00:23:34,725 --> 00:23:36,539
추가 작업없이.

266
00:23:36,539 --> 00:23:38,905
그리고 마지막으로이 모든 것들이
효율적으로 실행되기를 바랍니다.

267
00:23:38,905 --> 00:23:42,000
GPU를 사용하면 이러한 문제에
대해 너무 걱정할 필요가 없습니다.

268
00:23:42,000 --> 00:23:44,973
cuBLAS 및 cuDNN에
대한 저수준 하드웨어 세부 정보

269
00:23:44,973 --> 00:23:48,389
CUDA 및 CPU와 GPU 메모리간에 데이터 이동.

270
00:23:48,389 --> 00:23:51,030
너는 그 모든 지저분한 세부 사항들을 돌보고 싶다.

271
00:23:51,030 --> 00:23:52,439
너를 위해서.

272
00:23:52,439 --> 00:23:54,483
그래서 그것들은 몇 가지 주요한 이유 중 일부입니다

273
00:23:54,483 --> 00:23:56,930
왜 프레임 워크 대신에 프레임 워크를
사용할지 선택할 수 있습니다.

274
00:23:56,930 --> 00:23:59,450
자신의 물건을 처음부터 쓰는 것.

275
00:23:59,450 --> 00:24:02,969
따라서 계산 그래프의 구체적인 예

276
00:24:02,969 --> 00:24:05,231
우리는이 슈퍼 단순한 것을 적어 둘 수 있습니다.

277
00:24:05,231 --> 00:24:08,367
X, Y, Z의 세 가지 입력이있는 곳입니다.

278
00:24:08,367 --> 00:24:09,908
X와 Y를 결합하여 A를 만듭니다.

279
00:24:09,908 --> 00:24:13,071
그런 다음 A와 Z를 결합하여 B를 생성합니다.

280
00:24:13,071 --> 00:24:15,406
그리고 나서 마침내 우리는 어쩌면 합계를 낼 것입니다.

281
00:24:15,406 --> 00:24:18,630
일부 스케일러에 최종 결과를 제공하는 B에 대한 연산

282
00:24:18,630 --> 00:24:21,638
그래서 당신은 아마도 지금 Numpy
코드를 작성했을 것입니다.

283
00:24:21,638 --> 00:24:24,310
적어두기가 쉽다는 것을 깨닫기 위해,

284
00:24:24,310 --> 00:24:27,216
이 계산 그래프를 구현하기 위해,

285
00:24:27,216 --> 00:24:30,798
또는 오히려 Numpy에서이 계산 비트를 구현하려면,

286
00:24:30,798 --> 00:24:31,631
권리?

287
00:24:31,631 --> 00:24:33,724
Numpy에서 원하는대로 적어 놓을 수 있습니다.

288
00:24:33,724 --> 00:24:36,508
임의의 데이터를 생성하고, 두 가지를 곱하고 싶습니다.

289
00:24:36,508 --> 00:24:38,547
당신은 두 가지를 더하고 싶습니다.
당신은 두 가지를 합치기를 원합니다.

290
00:24:38,547 --> 00:24:41,923
그리고 Numpy에서 이것을하는 것은 정말 쉽습니다.

291
00:24:41,923 --> 00:24:44,360
그런데 질문은 우리가 원하는 것을 가정합니다.

292
00:24:44,360 --> 00:24:48,355
X, Y 및 Z에 대한 C의 그래디언트를 계산합니다.

293
00:24:48,355 --> 00:24:51,149
그래서, 만약 당신이 Numpy에서
일한다면, 당신은 쓸 필요가 있습니다.

294
00:24:51,149 --> 00:24:52,725
이 역방향 자신을 전달합니다.

295
00:24:52,725 --> 00:24:54,965
그리고 당신은 이것을 가지고 많은 연습을했습니다.

296
00:24:54,965 --> 00:24:58,127
숙제지만 고통 스러울 수 있습니다.

297
00:24:58,127 --> 00:25:00,306
그리고 일단 당신이 도착하면
조금 짜증나고 지저분 해집니다.

298
00:25:00,306 --> 00:25:02,859
정말 큰 복잡한 일들.

299
00:25:02,859 --> 00:25:04,487
Numpy의 다른 문제는 실행되지 않는다는 것입니다.

300
00:25:04,487 --> 00:25:05,675
GPU에서.

301
00:25:05,675 --> 00:25:08,189
따라서 Numpy는 확실히 CPU 만입니다.

302
00:25:08,189 --> 00:25:10,514
그리고 당신은 결코 경험할 수 없을 것입니다.

303
00:25:10,514 --> 00:25:13,112
또는 이러한 GPU 가속화 된 속도 향상을 활용하십시오.

304
00:25:13,112 --> 00:25:14,920
너끼리 일하면서 붙어있어.

305
00:25:14,920 --> 00:25:17,011
그리고 다시 한번 계산해야 할 고통이 있습니다.

306
00:25:17,011 --> 00:25:19,527
이 모든 상황에서 자신의 그라디언트.

307
00:25:19,527 --> 00:25:22,807
따라서, 가장 깊은 학습 프레임 워크의 목표의 종류

308
00:25:22,807 --> 00:25:26,829
요즘은 정방향 패스에서 코드를 작성하도록하고 있습니다.

309
00:25:26,829 --> 00:25:29,047
Numpy와 매우 흡사합니다.

310
00:25:29,047 --> 00:25:31,170
하지만 GPU에서 실행할 수 있습니다.

311
00:25:31,170 --> 00:25:33,069
그라디언트를 자동으로 계산할 수 있습니다.

312
00:25:33,069 --> 00:25:34,967
그리고 이것들이 대부분의 큰 그림 목표입니다

313
00:25:34,967 --> 00:25:36,397
프레임 워크.

314
00:25:36,397 --> 00:25:38,533
그래서 당신이보기를 상상한다면, 우리가보기를 본다면

315
00:25:38,533 --> 00:25:42,226
Tensor에서 정확한 전산 그래프의 흐름,

316
00:25:42,226 --> 00:25:44,314
우리는 지금이 앞으로의 패스에서,

317
00:25:44,314 --> 00:25:47,241
당신은이 코드를 작성하여 매우 비슷하게 보입니다.

318
00:25:47,241 --> 00:25:49,334
너가하고있는 종류 인 Numpy 전달 통로에

319
00:25:49,334 --> 00:25:52,687
이러한 곱셈과 이러한 덧셈 연산.

320
00:25:52,687 --> 00:25:55,669
하지만 이제 TensorFlow에는

321
00:25:55,669 --> 00:25:57,623
모든 그래디언트를 계산합니다.

322
00:25:57,623 --> 00:25:59,686
이제는 들어 가지 않고 자기 자신의 역행을 쓰지 마라.

323
00:25:59,686 --> 00:26:02,235
그리고 그것은 훨씬 더 편리합니다.

324
00:26:02,235 --> 00:26:04,095
TensorFlow의 또 다른 좋은 점은

325
00:26:04,095 --> 00:26:06,841
한 줄로 모든 계산을 전환 할 수 있습니다.

326
00:26:06,841 --> 00:26:08,926
CPU와 GPU 사이.

327
00:26:08,926 --> 00:26:11,016
그래서 여기에 성명서에 이것을 더하면

328
00:26:11,016 --> 00:26:13,037
이 앞으로 패스를하기 전에,

329
00:26:13,037 --> 00:26:14,866
당신은 프레임 워크에 명시 적으로 말할 수 있습니다.

330
00:26:14,866 --> 00:26:16,668
이 코드를 CPU에서 실행하고 싶습니다.

331
00:26:16,668 --> 00:26:19,537
하지만 이제 우리가 성명서로 그걸 조금 바꾸면

332
00:26:19,537 --> 00:26:21,527
이 경우 한 문자 변경만으로

333
00:26:21,527 --> 00:26:24,866
C를 G로 바꾸면 코드가 GPU에서 실행됩니다.

334
00:26:24,866 --> 00:26:27,868
이제이 작은 코드 스 니펫에서,

335
00:26:27,868 --> 00:26:29,539
우리는이 두 가지 문제를 해결했습니다.

336
00:26:29,539 --> 00:26:31,388
우리는 GPU에서 코드를 실행하고 있습니다.

337
00:26:31,388 --> 00:26:33,127
우리는 프레임 워크가 모든
그라데이션을 계산하도록하고 있습니다.

338
00:26:33,127 --> 00:26:35,685
우리를 위해, 그것은 정말로 멋지다.

339
00:26:35,685 --> 00:26:38,459
그리고 PyTorch 종류는 거의 똑같이 보입니다.

340
00:26:38,459 --> 00:26:40,349
다시 말하지만, PyTorch에서 당신은 적어두고,

341
00:26:40,349 --> 00:26:42,509
일부 변수를 정의하고,

342
00:26:42,509 --> 00:26:45,149
너는 약간의 포워드 패스와 포워드 패스를 다시 가진다.

343
00:26:45,149 --> 00:26:47,640
좋아,이 경우에는 매우 유사하다.

344
00:26:47,640 --> 00:26:49,262
Numpy 코드에.

345
00:26:49,262 --> 00:26:52,146
그런 다음 다시 PyTorch를 사용하여

346
00:26:52,146 --> 00:26:56,251
그라디언트, 단 하나의 선으로 모든 그라디언트.

347
00:26:56,251 --> 00:26:58,084
이제 PyTorch에서 다시 전환하기가 쉽습니다.

348
00:26:58,084 --> 00:27:00,263
GPU로 가져 가면 모든 내용을

349
00:27:00,263 --> 00:27:03,210
계산을 실행하기 전에 CUDA 데이터 유형

350
00:27:03,210 --> 00:27:06,781
이제 모든 것이 투명하게 GPU에서 실행됩니다.

351
00:27:06,781 --> 00:27:09,091
그래서 당신이이 세 가지 예를 보시면,

352
00:27:09,091 --> 00:27:11,321
이 세 가지 코드 조각을 나란히 놓고,

353
00:27:11,321 --> 00:27:13,878
Numpy, TensorFlow 및 PyTorch

354
00:27:13,878 --> 00:27:17,579
당신은 TensorFlow와
PyTorch 코드를 볼 수 있습니다.

355
00:27:17,579 --> 00:27:20,564
앞으로 패스에서 거의 정확히 Numpy처럼 보입니다.

356
00:27:20,564 --> 00:27:23,048
Numpy는 아름다운 API를
가지고 있기 때문에 위대한입니다.

357
00:27:23,048 --> 00:27:24,349
함께 일하기가 정말 쉽습니다.

358
00:27:24,349 --> 00:27:26,109
그러나 그라디언트를 자동으로 계산할 수 있습니다.

359
00:27:26,109 --> 00:27:29,192
우리는 GPU를 자동으로 실행할 수 있습니다.

360
00:27:30,186 --> 00:27:31,873
그래서 그런 종류의 소개 후에,

361
00:27:31,873 --> 00:27:33,654
나는 잠수하고 더 얘기하고 싶었다.

362
00:27:33,654 --> 00:27:35,758
이 일 안에서 벌어지는 일의 종류에 관한 세부 사항

363
00:27:35,758 --> 00:27:37,502
TensorFlow 예제입니다.

364
00:27:37,502 --> 00:27:40,384
그래서 강의 나머지 부분에 걸쳐 실행 예제로,

365
00:27:40,384 --> 00:27:44,028
교육을 2 층 완전 연결에 사용하겠습니다.

366
00:27:44,028 --> 00:27:48,377
랜덤 데이터에 대한 ReLU 네트워크의 실행 예제

367
00:27:48,377 --> 00:27:50,662
여기 나머지 예제에서는

368
00:27:50,662 --> 00:27:53,353
그리고 우리는 L2 Euclidean으로이
것을 훈련 할 것입니다.

369
00:27:53,353 --> 00:27:55,289
무작위 데이터 손실.

370
00:27:55,289 --> 00:27:57,633
그래서 이것은 일종의 어리석은 네트워크입니다.
실제로는 아닙니다.

371
00:27:57,633 --> 00:27:59,836
유용한 것은 없지만 그것은 당신에게 줄 것입니다,

372
00:27:59,836 --> 00:28:01,723
그것은 상대적으로 작고, 자체 포함되어 있으며,

373
00:28:01,723 --> 00:28:04,444
코드는 너무 작지 않고 슬라이드에 맞습니다.

374
00:28:04,444 --> 00:28:06,428
그리고 그것은 당신이 유용한 많은
종류의 것을 보여줄 수있게합니다.

375
00:28:06,428 --> 00:28:08,966
이러한 프레임 워크 내부의 아이디어.

376
00:28:08,966 --> 00:28:10,814
그래서 여기 오른쪽, 오, 그리고 또 다른 쪽지에,

377
00:28:10,814 --> 00:28:13,397
나는 Numpy와 TensorFlow를 추측하고 있습니다.

378
00:28:13,397 --> 00:28:15,900
이 모든 코드 스 니펫에 이미 가져 왔습니다.

379
00:28:15,900 --> 00:28:19,308
따라서 TensorFlow에서는
일반적으로 계산을 나눕니다.

380
00:28:19,308 --> 00:28:21,163
두 가지 주요 단계로

381
00:28:21,163 --> 00:28:23,996
먼저, 다음을 정의하는 코드를 작성합니다.

382
00:28:23,996 --> 00:28:26,852
우리의 계산 그래프, 그리고 이것은 빨간 코드입니다.

383
00:28:26,852 --> 00:28:28,363
상위 절반.

384
00:28:28,363 --> 00:28:30,347
그리고 나서 그래프를 정의한 후,

385
00:28:30,347 --> 00:28:32,360
그래프를 반복해서 실행하려고합니다.

386
00:28:32,360 --> 00:28:34,113
실제로 데이터를 그래프에 공급합니다.

387
00:28:34,113 --> 00:28:36,851
원하는 모든 계산을 수행 할 수 있습니다.

388
00:28:36,851 --> 00:28:38,772
그래서 이것은 정말로입니다, 이것은 일종의 큰 것입니다.

389
00:28:38,772 --> 00:28:40,961
TensorFlow의 공통 패턴.

390
00:28:40,961 --> 00:28:42,907
먼저 그래프를 작성하는 코드 묶음이 생깁니다.

391
00:28:42,907 --> 00:28:45,282
가서 그래프를 실행하고 재사용하십시오.

392
00:28:45,282 --> 00:28:46,615
많은 시간.

393
00:28:48,099 --> 00:28:50,827
건물 코드에 다이빙하면

394
00:28:50,827 --> 00:28:52,763
이 경우의 그래프.

395
00:28:52,763 --> 00:28:56,542
맨 위에서 당신은 우리가이 X, Y,

396
00:28:56,542 --> 00:29:00,709
w1과 w2, 그리고 우리는이
tf.placeholder 객체를 생성하고 있습니다.

397
00:29:01,637 --> 00:29:05,193
그래서 이들은 그래프의 입력 노드가 될 것입니다.

398
00:29:05,193 --> 00:29:08,360
이것들은 일종의 진입 점이 될 것입니다

399
00:29:08,360 --> 00:29:11,101
그래프를 실행할 때 데이터를 피드에 넣을 것입니다.

400
00:29:11,101 --> 00:29:13,379
이 입력 슬롯을 통해

401
00:29:13,379 --> 00:29:15,379
전산 그래프.

402
00:29:15,379 --> 00:29:17,218
그래서 이것은 실제로 어떤 메모리를
할당하는 것과 같지 않습니다.

403
00:29:17,218 --> 00:29:18,051
지금.

404
00:29:19,044 --> 00:29:20,861
우리는 일종의 이러한 입력 슬롯을 설정하고 있습니다.

405
00:29:20,861 --> 00:29:21,944
그래프에.

406
00:29:23,272 --> 00:29:25,560
그런 다음 우리는 지금있는 입력 슬롯을 사용할 것입니다.

407
00:29:25,560 --> 00:29:28,665
이러한 상징적 변수와 같은 종류

408
00:29:28,665 --> 00:29:31,021
다른 TensorFlow 작업을 수행하려고합니다.

409
00:29:31,021 --> 00:29:33,917
설정하기 위해이 상징적 변수들에

410
00:29:33,917 --> 00:29:37,135
우리는 그 변수들에 대해 어떤 계산을하고 싶습니까?

411
00:29:37,135 --> 00:29:39,379
그래서이 경우 우리는 행렬 곱셈을하고 있습니다.

412
00:29:39,379 --> 00:29:43,904
X와 w1 사이에서 우리는 몇 가지 일을하고 있습니다.

413
00:29:43,904 --> 00:29:46,109
ReLU 비선형 성 그리고 나서
우리는 또 다른 것을하고있다.

414
00:29:46,109 --> 00:29:49,240
행렬 곱셈을 사용하여 출력 예측을 계산합니다.

415
00:29:49,240 --> 00:29:50,955
그리고 나서 다시 일종의 기본적인
Tensor를 사용합니다.

416
00:29:50,955 --> 00:29:53,356
유클리드 거리를 계산하는 연산,

417
00:29:53,356 --> 00:29:58,175
우리의 예측과 목표 Y 사이의 L2 손실.

418
00:29:58,175 --> 00:30:00,099
여기에서 지적 할 또 다른 점은

419
00:30:00,099 --> 00:30:03,647
이 코드 라인은 실제로 아무 것도 계산하지 않습니다.

420
00:30:03,647 --> 00:30:05,824
현재 시스템에 데이터가 없습니다.

421
00:30:05,824 --> 00:30:07,571
우리는이 계산 그래프 데이터를 구축하고 있습니다.

422
00:30:07,571 --> 00:30:10,799
TensorFlow에게 어떤 작업을 지시하는지

423
00:30:10,799 --> 00:30:15,001
우리는 실제 데이터를 입력하면 결국 실행되기를 원합니다.

424
00:30:15,001 --> 00:30:16,393
이것은 그래프를 작성하는 것입니다.

425
00:30:16,393 --> 00:30:18,648
이것은 실제로 아무 것도하지 않습니다.

426
00:30:18,648 --> 00:30:21,658
그런 다음 우리는이 마법의 선을 가지고 있습니다.

427
00:30:21,658 --> 00:30:24,739
상징적 인 작전으로 인한 우리의 손실,

428
00:30:24,739 --> 00:30:27,181
그러면 TensorFlow에게 계산하도록 요청할 수 있습니다.

429
00:30:27,181 --> 00:30:31,186
w1 및 w2에 대한 손실의 기울기

430
00:30:31,186 --> 00:30:33,135
이 마술적이고 아름다운 라인.

431
00:30:33,135 --> 00:30:35,619
그리고 이것은 자신의 모든 역주 코드를
작성하는 것을 피할 수 있습니다.

432
00:30:35,619 --> 00:30:37,981
당신이 과제에서해야했던 일.

433
00:30:37,981 --> 00:30:40,439
그러나 여기에는 실제 계산이 없습니다.

434
00:30:40,439 --> 00:30:42,521
이것은 일종의 추가 작업을 추가하는 것입니다.

435
00:30:42,521 --> 00:30:46,009
전산 그래프에

436
00:30:46,009 --> 00:30:47,950
그래프에는 이러한 추가 작업이 있습니다.

437
00:30:47,950 --> 00:30:51,108
당신을 위해 이러한 그라디언트를 계산합니다.

438
00:30:51,108 --> 00:30:53,129
이제이 시점에서 우리는 계산 된

439
00:30:53,129 --> 00:30:56,638
그래프, 우리는이 그래프 데이터 구조에
큰 그래프를 가지고 있습니다.

440
00:30:56,638 --> 00:30:59,039
우리가 수행하고자하는 작업을 알고있는 메모리

441
00:30:59,039 --> 00:31:01,421
그라디언트의 손실을 계산합니다.

442
00:31:01,421 --> 00:31:03,705
이제 실제로 실행하기 위해
TensorFlow 세션에 들어갑니다.

443
00:31:03,705 --> 00:31:06,843
이 그래프에 데이터를 입력하십시오.

444
00:31:06,843 --> 00:31:09,160
그럼 일단 세션에 들어가면

445
00:31:09,160 --> 00:31:11,943
실제로 구체적인 값을 만들어야합니다.

446
00:31:11,943 --> 00:31:13,859
그것은 그래프에 공급 될 것입니다.

447
00:31:13,859 --> 00:31:17,227
따라서 TensorFlow는

448
00:31:17,227 --> 00:31:19,459
대부분의 경우 너비가 많은 배열.

449
00:31:19,459 --> 00:31:23,701
그래서 여기에 우리는 단지 구체적인
실제 값을 만들고 있습니다.

450
00:31:23,701 --> 00:31:28,066
Numpy를 사용하여 X, Y, w1 및 w2에 대해

451
00:31:28,066 --> 00:31:30,226
일부 사전에.

452
00:31:30,226 --> 00:31:32,743
이제 그래프를 실제로 실행하는 곳이 여기 있습니다.

453
00:31:32,743 --> 00:31:36,206
그래서 우리는 세션을 호출하고 있음을 볼 수 있습니다.
실행

454
00:31:36,206 --> 00:31:38,120
실제로 그래프의 일부분을 실행합니다.

455
00:31:38,120 --> 00:31:41,603
첫 번째 인수 손실은 그래프의
어느 부분에 있는지 알려줍니다.

456
00:31:41,603 --> 00:31:43,899
우리는 실제로 산출물로서 원하는가?

457
00:31:43,899 --> 00:31:45,979
그리고 우리는 실제로 그래프를 원합니다.

458
00:31:45,979 --> 00:31:47,597
이 경우 우리는 실제로 우리가

459
00:31:47,597 --> 00:31:50,950
손실 및 grad1 및 grad2를 계산하려고합니다.

460
00:31:50,950 --> 00:31:53,880
이 feed dict 매개 변수를 전달해야합니다.

461
00:31:53,880 --> 00:31:57,140
그래프에 공급되는 실제의 구체적인 값.

462
00:31:57,140 --> 00:32:00,043
그리고 나서,이 한 줄에서,

463
00:32:00,043 --> 00:32:02,888
그것은 그래프를 실행하고 실행 한 다음

464
00:32:02,888 --> 00:32:06,541
손실 grad1 ~ grad2의 값

465
00:32:06,541 --> 00:32:09,097
실제의 구체적인 값을 돌려주는

466
00:32:09,097 --> 00:32:12,003
Numpy 배열에있는 사람들을 위해.

467
00:32:12,003 --> 00:32:14,398
이제 두 번째 줄에서이 출력의 압축을 푼 다음,

468
00:32:14,398 --> 00:32:18,446
당신은 Numpy 배열을 얻습니다, 또는 당신은
손실을 가진 Numpy 배열을 얻습니다

469
00:32:18,446 --> 00:32:19,859
및 그라디언트.

470
00:32:19,859 --> 00:32:21,720
그럼 네가 가서 원하는대로 할 수있어.

471
00:32:21,720 --> 00:32:23,697
이 값들.

472
00:32:23,697 --> 00:32:28,655
그럼, 이것은 단지 하나의 앞으로 및 뒤로 실행했습니다.

473
00:32:28,655 --> 00:32:29,599
우리의 그래프를 지나치고,

474
00:32:29,599 --> 00:32:31,468
우리가 실제로한다면 몇 줄 밖에 걸리지 않습니다.

475
00:32:31,468 --> 00:32:33,167
네트워크를 훈련시키고 싶습니다.

476
00:32:33,167 --> 00:32:36,225
그래서 우리는 이제 그래프를 여러 번 실행하고 있습니다.

477
00:32:36,225 --> 00:32:38,577
루프에서 우리는 4 개의 루프를하고있다.

478
00:32:38,577 --> 00:32:40,739
루프의 각 반복에서,

479
00:32:40,739 --> 00:32:43,635
우리는 session.run을 호출하여
그것을 계산하도록 요청합니다.

480
00:32:43,635 --> 00:32:45,511
손실 및 그라디언트.

481
00:32:45,511 --> 00:32:48,360
이제 수동 그라디언트 단계를 수행하고 있습니다.

482
00:32:48,360 --> 00:32:50,852
계산 된 그래디언트를 사용하여 현재 업데이트

483
00:32:50,852 --> 00:32:52,291
가중치의 값.

484
00:32:52,291 --> 00:32:56,159
따라서 실제로이 코드를 실행하고 손실을 계획한다면,

485
00:32:56,159 --> 00:32:57,770
그러면 손실이 줄어들 것입니다.

486
00:32:57,770 --> 00:33:00,749
네트워크가 훈련 중이며 꽤 잘 작동하고 있습니다.

487
00:33:00,749 --> 00:33:03,449
그래서 이것은 아주 맨손의 뼈와 같은 종류의 예입니다

488
00:33:03,449 --> 00:33:06,113
TensorFlow에서 완전히
연결된 네트워크를 교육합니다.

489
00:33:06,113 --> 00:33:08,046
그러나 여기에는 문제가 있습니다.

490
00:33:08,046 --> 00:33:11,437
여기에서, 앞으로의 패스에서,

491
00:33:11,437 --> 00:33:13,256
이 그래프를 실행할 때마다,

492
00:33:13,256 --> 00:33:15,086
우리는 실제로 무게를 먹고 있습니다.

493
00:33:15,086 --> 00:33:16,659
우리는 Numpy 배열로서 가중치를 가지고 있습니다.

494
00:33:16,659 --> 00:33:18,835
우리는 분명히 그것들을 그래프에 넣고 있습니다.

495
00:33:18,835 --> 00:33:21,395
그래프가 실행을 끝내면

496
00:33:21,395 --> 00:33:23,230
우리에게 이러한 그라디언트를 줄 것입니다.

497
00:33:23,230 --> 00:33:24,979
그리고 그라디언트가 같은 크기임을 기억하십시오.

498
00:33:24,979 --> 00:33:26,339
무게로.

499
00:33:26,339 --> 00:33:28,192
그래서 이것은 우리가 여기서 그래프를 실행할 때마다,

500
00:33:28,192 --> 00:33:30,675
Numpy 배열의 가중치를

501
00:33:30,675 --> 00:33:32,665
그라디언트를 얻는 TensorFlow

502
00:33:32,665 --> 00:33:34,583
그런 다음 TensorFlow에서 그라디언트를 복사합니다.

503
00:33:34,583 --> 00:33:36,419
Numpy 배열로 되돌아 간다.

504
00:33:36,419 --> 00:33:37,886
따라서 CPU에서 실행중인 경우,

505
00:33:37,886 --> 00:33:39,849
어쩌면 엄청난 거래는 아니지만,

506
00:33:39,849 --> 00:33:42,676
그러나 우리는 CPU GPU 병목 현상에
대해 이야기했음을 기억하십시오.

507
00:33:42,676 --> 00:33:44,867
실제로 데이터를 복사하는 것이 얼마나 비쌉니까?

508
00:33:44,867 --> 00:33:47,235
CPU 메모리와 GPU 메모리 사이.

509
00:33:47,235 --> 00:33:49,840
따라서 네트워크가 매우 크고 가중치가

510
00:33:49,840 --> 00:33:51,096
그리고 그라디언트는 매우 컸고,

511
00:33:51,096 --> 00:33:52,975
이런 일을하는 것은 매우 비싸다.

512
00:33:52,975 --> 00:33:55,900
우리가 모든 종류의 데이터를 복사 할 것이므로

513
00:33:55,900 --> 00:33:58,423
매 순간마다 CPU와 GPU 사이를 오가며

514
00:33:58,423 --> 00:33:59,256
시간 단계.

515
00:33:59,256 --> 00:34:00,441
그렇게 나쁜 것이고, 우리는 그렇게하고 싶지 않습니다.

516
00:34:00,441 --> 00:34:01,689
우리는 그것을 고쳐야합니다.

517
00:34:01,689 --> 00:34:06,027
따라서 TensorFlow에는 이에
대한 해결책이 분명 있습니다.

518
00:34:06,027 --> 00:34:08,342
그리고 아이디어는 이제 우리가
우리의 무게를 원한다는 것입니다,

519
00:34:08,342 --> 00:34:11,437
w1과 w2를 사용하여

520
00:34:11,437 --> 00:34:14,456
우리가 네트워크에 그들을 먹일 것으로 기대하는 곳으로

521
00:34:14,456 --> 00:34:17,969
대신 모든 전달 경로에서 변수로 정의합니다.

522
00:34:17,969 --> 00:34:20,476
변수는 내부에 사는 가치입니다.

523
00:34:20,476 --> 00:34:23,175
전산 그래프는 지속될 것입니다.

524
00:34:23,176 --> 00:34:25,601
서로 다른 시간대의 계산 그래프 내부

525
00:34:25,601 --> 00:34:27,347
같은 그래프를 실행할 때.

526
00:34:27,347 --> 00:34:31,300
이제는 w1과 w2를 자리 표시 자로 선언하는 대신,

527
00:34:31,300 --> 00:34:33,094
대신 우리는 변수로 그것들을 구성합니다.

528
00:34:33,094 --> 00:34:35,514
그러나 지금 그들은 그래프 안에 살고 있기 때문에,

529
00:34:35,514 --> 00:34:38,041
우리는 Tensor에게 그들이 어떻게
있어야 하는지를 말해야합니다.

530
00:34:38,041 --> 00:34:39,219
초기화 됐지?

531
00:34:39,219 --> 00:34:40,815
이전의 경우 우리가 먹이를 먹고 있었기 때문에

532
00:34:40,815 --> 00:34:42,697
그래프 외부의 값,

533
00:34:42,697 --> 00:34:44,606
그래서 우리는 Numpy에서 그들을 초기화했다.

534
00:34:44,606 --> 00:34:47,437
그러나 이제는 이러한 것들이 그래프 안에 있기 때문에,

535
00:34:47,437 --> 00:34:50,569
TensorFlow는 초기화를 담당합니다.

536
00:34:50,569 --> 00:34:53,149
그래서 우리는 tf.randomnormal
작업을 통과해야합니다.

537
00:34:53,149 --> 00:34:55,688
실제로 다시 초기화하지는 않습니다.

538
00:34:55,688 --> 00:34:58,220
우리가이 선을 돌릴 때, 이것은 TensorFlow에게

539
00:34:58,220 --> 00:35:00,627
우리가 그것들을 초기화하는 방법.

540
00:35:00,627 --> 00:35:02,048
그래서 약간의 오해가 있습니다.

541
00:35:02,048 --> 00:35:03,215
여기에.

542
00:35:04,869 --> 00:35:07,478
그리고 이전 예제에서 기억하십시오.

543
00:35:07,478 --> 00:35:10,207
우리는 실제로 외부의 가중치를 업데이트하고있었습니다.

544
00:35:10,207 --> 00:35:11,862
계산 그래프의

545
00:35:11,862 --> 00:35:14,554
앞의 예제에서 그라디언트를 계산했습니다.

546
00:35:14,554 --> 00:35:17,219
그런 다음 Numpy 배열로 가중치를
업데이트하는 데 사용합니다.

547
00:35:17,219 --> 00:35:19,431
다음에 업데이트 된 가중치를 공급합니다

548
00:35:19,431 --> 00:35:20,264
시간 단계.

549
00:35:20,264 --> 00:35:22,742
그러나 지금 우리는이 무게가 안으로 살기를 원하기 때문에

550
00:35:22,742 --> 00:35:25,818
그래프,이 가중치를 갱신하는 동작

551
00:35:25,818 --> 00:35:28,004
내부에서도 조작이 필요하다.

552
00:35:28,004 --> 00:35:29,402
전산 그래프

553
00:35:29,402 --> 00:35:34,242
이제 우리는이 assign 함수를 사용했습니다.

554
00:35:34,242 --> 00:35:37,020
계산 그래프 내부의 이러한 변수들

555
00:35:37,020 --> 00:35:39,407
이제 돌연변이 된 값은 여러 번 실행되는 동안 지속됩니다.

556
00:35:39,407 --> 00:35:41,487
같은 그래프의

557
00:35:41,487 --> 00:35:44,195
이제이 그래프를 실행할 때입니다.

558
00:35:44,195 --> 00:35:45,976
우리가 네트워크를 훈련 할 때,

559
00:35:45,976 --> 00:35:48,420
이제 그래프를 한 번 실행해야합니다.

560
00:35:48,420 --> 00:35:50,830
TensorFlow에게 이들을
설정하도록 지시하는 특별한 주문

561
00:35:50,830 --> 00:35:53,825
그래프 안에 살 수있는 변수.

562
00:35:53,825 --> 00:35:55,779
그런 다음 초기화를 완료하면

563
00:35:55,779 --> 00:35:58,574
이제 그래프를 반복해서 실행할 수 있습니다.

564
00:35:58,574 --> 00:36:02,149
그리고 여기에 우리는 이제 데이터와 레이블 만 공급합니다.

565
00:36:02,149 --> 00:36:05,091
X와 Y 그리고 가중치는 그래프 안에 있습니다.

566
00:36:05,091 --> 00:36:07,035
그리고 여기 우리는 네트워크에,

567
00:36:07,035 --> 00:36:09,517
우리는 TensorFlow에
손실을 계산하도록 요청했습니다.

568
00:36:09,517 --> 00:36:13,001
그리고 나서 이것이 네트워크를 훈련시킬
것이라고 생각할 수도 있습니다.

569
00:36:13,001 --> 00:36:15,627
실제로 여기에 버그가 있습니다.

570
00:36:15,627 --> 00:36:17,574
따라서이 코드를 실제로 실행하면,

571
00:36:17,574 --> 00:36:19,964
그리고 당신은 손실을 계획하고 훈련을하지 않습니다.

572
00:36:19,964 --> 00:36:23,401
그렇게 나쁘다, 혼란 스럽다,
무슨 일이 벌어지고있는 것처럼?

573
00:36:23,401 --> 00:36:25,385
우리는이 assign 코드를 작성하고 실행했습니다.

574
00:36:25,385 --> 00:36:26,902
우리가 손실과 그레디언트를 계산 한 것처럼

575
00:36:26,902 --> 00:36:29,957
우리의 손실은 평평합니다. 무슨 일이 벌어지고 있습니까?

576
00:36:29,957 --> 00:36:31,460
어떤 아이디어?

577
00:36:31,460 --> 00:36:34,595
[학생의 말은 마이크가 없어서 가려졌습니다.]

578
00:36:34,595 --> 00:36:38,654
그래, 한 가설은 우연히 우리가

579
00:36:38,654 --> 00:36:41,749
그래프를 호출 할 때마다 w를 다시 초기화합니다.

580
00:36:41,749 --> 00:36:43,854
그것은 좋은 가설입니다. 사실 그것은 문제가 아닙니다.

581
00:36:43,854 --> 00:36:44,979
이 경우에.

582
00:36:44,979 --> 00:36:48,057
[학생의 말은 마이크가 없어서 가려졌습니다.]

583
00:36:48,057 --> 00:36:51,777
그래, 대답은 우리가 실제로 명시
적으로 필요하다는 것입니다.

584
00:36:51,777 --> 00:36:54,339
TensorFlow에게 우리가이 새로운
w1을 실행하고 싶다고 말하십시오.

585
00:36:54,339 --> 00:36:56,318
새로운 w2 작업.

586
00:36:56,318 --> 00:36:58,835
그래서 우리는이 큰 계산 그래프 데이터를 구축했습니다.

587
00:36:58,835 --> 00:37:01,699
구조를 메모리에 저장하고 실행을 호출 할 때,

588
00:37:01,699 --> 00:37:04,894
우리는 TensorFlow에게 우리가
손실을 계산하기를 원한다고 말했습니다.

589
00:37:04,894 --> 00:37:07,361
그리고 만약 당신이 다른 이들 사이의 의존성을 살펴 본다면

590
00:37:07,361 --> 00:37:09,155
그래프 내의 조작,

591
00:37:09,155 --> 00:37:11,277
당신은 손실을 계산하기 위해 그것을 봅니다.

592
00:37:11,277 --> 00:37:13,715
실제로이 업데이트 작업을 수행 할 필요는 없습니다.

593
00:37:13,715 --> 00:37:16,366
따라서 TensorFlow는 똑똑하고 부품 만 계산합니다.

594
00:37:16,366 --> 00:37:19,416
출력 계산에 필요한 그래프의

595
00:37:19,416 --> 00:37:21,496
당신이 계산하도록 요구했다고.

596
00:37:21,496 --> 00:37:24,499
그래서 그것은 단지 좋은 의미입니다.

597
00:37:24,499 --> 00:37:26,656
필요에 따라 많은 일을하고,

598
00:37:26,656 --> 00:37:29,729
하지만 이런 상황에서는 약간 혼란 스러울 수 있습니다.

599
00:37:29,729 --> 00:37:32,739
당신이 예상하지 못한 행동으로 이어질 수 있습니다.

600
00:37:32,739 --> 00:37:34,936
그래서이 경우의 해결책은 우리가
실제로 필요로하는 것입니다.

601
00:37:34,936 --> 00:37:37,656
명시 적으로 TensorFlow에이를 수행하도록 알립니다.

602
00:37:37,656 --> 00:37:39,141
업데이트 작업.

603
00:37:39,141 --> 00:37:41,475
그래서 우리가 할 수있는 한 가지는 제안 된 것입니다.

604
00:37:41,475 --> 00:37:45,603
새로운 w1과 w2를 출력으로 추가 할 수 있습니까?

605
00:37:45,603 --> 00:37:47,761
우리가 생산하고자하는 TensorFlow에게 말하십시오.

606
00:37:47,761 --> 00:37:49,531
이 값들은 출력으로 사용됩니다.

607
00:37:49,531 --> 00:37:53,199
하지만 그것은 문제이기도합니다.

608
00:37:53,199 --> 00:37:57,366
새로운 w1, 새로운 w2 값은
다시 이러한 큰 텐서입니다.

609
00:37:58,891 --> 00:38:01,123
그래서 우리가 TensorFlow에게 말하면
우리는 그것들을 산출물로 원합니다.

610
00:38:01,123 --> 00:38:03,068
이 복사 동작을 다시 얻으려고합니다.

611
00:38:03,068 --> 00:38:05,138
CPU와 GPU 사이의 반복.

612
00:38:05,138 --> 00:38:07,316
그렇게 나쁜 것이고, 우리는 그것을 원하지 않습니다.

613
00:38:07,316 --> 00:38:09,217
그래서 당신이 대신 할 수있는 약간의 트릭이 있습니다.

614
00:38:09,217 --> 00:38:11,742
그래프에 더미 노드를 추가하는 것입니다.

615
00:38:11,742 --> 00:38:14,255
위조 된 데이터 의존성

616
00:38:14,255 --> 00:38:16,986
우리는 단지이 더미 노드가 업데이트한다고 말하면서,

617
00:38:16,986 --> 00:38:20,307
새로운 w1 및 w2의 이러한 데이터 종속성을가집니다.

618
00:38:20,307 --> 00:38:22,410
그리고 이제 실제로 그래프를 실행할 때,

619
00:38:22,410 --> 00:38:25,803
손실과이 더미 노드를 계산하도록합니다.

620
00:38:25,803 --> 00:38:27,840
그리고이 더미 노드는 실제로 반환하지 않습니다.

621
00:38:27,840 --> 00:38:31,169
아무 값도 반환하지 않습니다.하지만이 때문에

622
00:38:31,169 --> 00:38:33,952
의존성은 우리가 보장하는 노드에 넣었다.

623
00:38:33,952 --> 00:38:35,980
업데이트 값을 실행할 때,

624
00:38:35,980 --> 00:38:38,468
우리는 실제로 이러한 업데이트 작업을 실행합니다.

625
00:38:38,468 --> 00:38:39,551
그럼, 질문 하나?

626
00:38:40,788 --> 00:38:44,955
[학생의 말은 마이크가 없어서 가려졌습니다.]

627
00:38:45,854 --> 00:38:48,548
그래프에 X와 Y를 넣지 않은 이유가 있습니까?

628
00:38:48,548 --> 00:38:51,370
그리고 그것은 Numpy로 머물렀다.

629
00:38:51,370 --> 00:38:54,725
그래서이 예에서 우리는 매번
X와 Y를 재사용하고 있습니다.

630
00:38:54,725 --> 00:38:57,151
모든 반복에서 동일한 X와 Y를 재사용하고 있습니다.

631
00:38:57,151 --> 00:38:59,156
당신 말이 맞습니다. 우리는 또한
그것들을 붙일 수있었습니다.

632
00:38:59,156 --> 00:39:02,211
그래프에서, 그러나보다 현실적인 시나리오에서,

633
00:39:02,211 --> 00:39:05,301
X와 Y는 데이터의 미니 바이트가되므로 실제로

634
00:39:05,301 --> 00:39:07,526
매 반복마다 변화하고 우리는 먹이고 싶을 것이다.

635
00:39:07,526 --> 00:39:10,122
반복 할 때마다 다른 값.

636
00:39:10,122 --> 00:39:12,190
그래서이 경우 그들은 그래프에 머물러있을 수 있었고,

637
00:39:12,190 --> 00:39:14,330
그러나 대부분의 경우에 그들은 변화 할 것이다,

638
00:39:14,330 --> 00:39:17,913
그래서 우리는 그들이 그래프에 살기를 원하지 않습니다.

639
00:39:19,388 --> 00:39:21,290
오, 또 하나의 질문?

640
00:39:21,290 --> 00:39:25,457
[학생의 말은 마이크가 없어서 가려졌습니다.]

641
00:39:37,046 --> 00:39:40,927
그래, 우리가 말했을 텐데, 우리는
TensorFlow에 넣었다.

642
00:39:40,927 --> 00:39:44,305
우리가 원하는 출력은 손실과 업데이트입니다.

643
00:39:44,305 --> 00:39:47,388
업데이트는 실제 가치가 아닙니다.

644
00:39:48,666 --> 00:39:51,801
따라서 업데이트가 평가할 때 아무 것도 반환하지 않습니다.

645
00:39:51,801 --> 00:39:54,570
그러나이 의존성 때문에 우리는 업데이트

646
00:39:54,570 --> 00:39:57,416
이러한 할당 작업에 따라 다릅니다.

647
00:39:57,416 --> 00:39:59,356
그러나 이러한 할당 작업은 내부에서 수행됩니다.

648
00:39:59,356 --> 00:40:02,358
계산 그래프와 모든 것은 GPU 메모리 안에 있습니다.

649
00:40:02,358 --> 00:40:04,426
그럼 우리는이 업데이트 작업을하고 있습니다.

650
00:40:04,426 --> 00:40:07,107
전적으로 GPU에 있으며 우리는 더 이상

651
00:40:07,107 --> 00:40:10,190
업데이트 된 값이 다시 그래프 밖으로 나옵니다.

652
00:40:11,723 --> 00:40:15,112
[학생의 말은 마이크가 없어서 가려졌습니다.]

653
00:40:15,112 --> 00:40:18,195
그래서 질문은 tf.group이 반환하지 않는 것입니까?

654
00:40:18,195 --> 00:40:21,824
따라서 TensorFlow의 까다로운 점이 있습니다.

655
00:40:21,824 --> 00:40:25,923
따라서 tf.group은 약간의
TensorFlow 값을 반환합니다.

656
00:40:25,923 --> 00:40:29,371
그것은 내부 TensorFlow
노드와 같은 것을 반환합니다.

657
00:40:29,371 --> 00:40:32,658
우리는 그래프를 계속 작성해야합니다.

658
00:40:32,658 --> 00:40:34,266
그러나 그래프를 실행할 때,

659
00:40:34,266 --> 00:40:37,417
그리고 말할 때, session.run 안에,

660
00:40:37,417 --> 00:40:40,250
우리가 구체적인 값을 계산하기를 원한다고 말했을 때

661
00:40:40,250 --> 00:40:43,333
업데이 트에서, 그럼 아무도를 반환합니다.

662
00:40:43,333 --> 00:40:45,482
따라서 TensorFlow로 작업 할 때마다

663
00:40:45,482 --> 00:40:47,907
당신은 그래프를 만드는 것 사이에
우스운 간접 접근법을가집니다.

664
00:40:47,907 --> 00:40:50,781
그래프를 작성하는 동안의 실제 출력 값

665
00:40:50,781 --> 00:40:53,487
이상한 이상한 물건입니다. 그리고 나서 당신은 실제로

666
00:40:53,487 --> 00:40:55,466
그래프를 실행할 때의 구체적인 값.

667
00:40:55,466 --> 00:40:58,658
업데이트를 실행 한 후에는 결과가 none입니다.

668
00:40:58,658 --> 00:40:59,967
그게 좀 이상 해지나요?

669
00:40:59,967 --> 00:41:04,134
[학생의 말은 마이크가 없어서 가려졌습니다.]

670
00:41:18,796 --> 00:41:20,792
그래서 질문은 손실이 가치가있는 이유입니다

671
00:41:20,792 --> 00:41:22,334
업데이트가없는 이유는 무엇입니까?

672
00:41:22,334 --> 00:41:24,068
이는 업데이트가 작동하는 방식에 불과합니다.

673
00:41:24,068 --> 00:41:25,988
따라서 우리가 계산할 때 손실은 가치입니다.

674
00:41:25,988 --> 00:41:28,597
TensorFlow에게 우리는 텐서
(tensor)를 달리고 싶다고 말하면서,

675
00:41:28,597 --> 00:41:30,176
우리는 구체적인 가치를 얻습니다.

676
00:41:30,176 --> 00:41:33,147
이러한 특수한 다른 데이터 유형의 업데이트

677
00:41:33,147 --> 00:41:35,753
값을 반환하지 않으면 아무 것도 반환하지 않습니다.

678
00:41:35,753 --> 00:41:38,703
그래서 거기에서 벌어지고있는 약간의
TensorFlow 마법입니다.

679
00:41:38,703 --> 00:41:40,602
당신이 혼란 스러울지라도 우리는
오프라인에서 대화 할 수 있습니다.

680
00:41:40,602 --> 00:41:42,678
[학생의 말은 마이크가 없어서 가려졌습니다.]

681
00:41:42,678 --> 00:41:46,186
그래, 그 행동은 그룹 방법에서 오는거야.

682
00:41:46,186 --> 00:41:48,388
이제 우리는이 이상한 패턴을 가지고 있습니다.

683
00:41:48,388 --> 00:41:50,548
이러한 서로 다른 할당 작업을 수행하기를 원했지만,

684
00:41:50,548 --> 00:41:52,492
우리는이 재미있는 tf.group 것을 사용해야 만합니다.

685
00:41:52,492 --> 00:41:56,248
고통스러워서 고맙게도 TensorFlow는

686
00:41:56,248 --> 00:41:58,058
너는 그 일종의 편리한 조작을

687
00:41:58,058 --> 00:42:00,004
당신을위한 종류의 물건.

688
00:42:00,004 --> 00:42:01,706
그리고 이것을 옵티 마이저라고합니다.

689
00:42:01,706 --> 00:42:06,047
그래서 우리는 tf.train.GradientDescentOptimizer를
사용하고 있습니다.

690
00:42:06,047 --> 00:42:08,458
우리는 우리가 사용하고자하는 학습 속도를 말하고 있습니다.

691
00:42:08,458 --> 00:42:10,964
그리고 거기에 RMSprop이 있다고 상상해보십시오.

692
00:42:10,964 --> 00:42:12,784
여기에는 모든 종류의 다양한 최적화 알고리즘이 있습니다.

693
00:42:12,784 --> 00:42:16,284
그리고 이제 우리는 optimizer.minimize of loss라고 부릅니다.

694
00:42:17,311 --> 00:42:19,067
그리고 지금 이것은 꽤 마술적이고,

695
00:42:19,067 --> 00:42:21,204
이것은 꽤 불가사의 한 일입니다.

696
00:42:21,204 --> 00:42:24,527
이제이 호출은 이러한 변수가

697
00:42:24,527 --> 00:42:28,106
w1과 w2는 기본적으로 학습 가능으로 표시되어 있습니다.

698
00:42:28,106 --> 00:42:30,586
그래서 내부적으로,이 최적화 도구 안에서.

699
00:42:30,586 --> 00:42:33,104
그것은 그래프에 노드를 추가하고 있습니다.

700
00:42:33,104 --> 00:42:35,184
존중과 함께 손실의 기울기를 계산할 것이다.

701
00:42:35,184 --> 00:42:38,159
을 w1 및 w2로 변경 한 다음
해당 업데이트를 수행 중입니다.

702
00:42:38,159 --> 00:42:40,287
그룹 작업을하고 있습니다.

703
00:42:40,287 --> 00:42:42,219
당신을 위해 그리고 그것은 할당을하고 있습니다.

704
00:42:42,219 --> 00:42:44,206
그것은 내부에 많은 마법 물건을 만드는 것과 같습니다.

705
00:42:44,206 --> 00:42:46,506
하지만 그때 당신에게이 마법적인 업데이트 값을주게됩니다.

706
00:42:46,506 --> 00:42:49,542
실제로 사용하고있는 코드를 파헤 치면

707
00:42:49,542 --> 00:42:52,344
tf.group 그래서 내부적으로 매우 유사하게 보입니다.

708
00:42:52,344 --> 00:42:53,518
우리는 전에 보았습니다.

709
00:42:53,518 --> 00:42:55,946
이제 루프 내부에서 그래프를 실행할 때

710
00:42:55,946 --> 00:42:58,607
우리는 손실을 계산하기 위해 동일한 패턴을 사용합니다.

711
00:42:58,607 --> 00:43:00,004
업데이트합니다.

712
00:43:00,004 --> 00:43:03,444
그래프를 업데이트 할 때마다 업데이트를 계산할 때마다

713
00:43:03,444 --> 00:43:07,450
그러면 실제로 그래프가 업데이트됩니다.

714
00:43:07,450 --> 00:43:08,593
문제?

715
00:43:08,593 --> 00:43:10,959
[학생의 말은 마이크가 없어서 가려졌습니다.]

716
00:43:10,959 --> 00:43:14,249
네, 그래서 tf.GlobalVariablesInitializer는
무엇입니까?

717
00:43:14,249 --> 00:43:18,076
그래서 그것은 w1과 w2를 초기화하기 때문입니다.

718
00:43:18,076 --> 00:43:20,502
그래프 안에있는 변수.

719
00:43:20,502 --> 00:43:22,823
그래서 우리는 이것을 보았을 때, 우리가 창조 할 때

720
00:43:22,823 --> 00:43:25,244
tf.variable 우리는이
tf.randomnormal 있습니다.

721
00:43:25,244 --> 00:43:28,366
이것은 초기화이므로

722
00:43:28,366 --> 00:43:30,771
tf.GlobalVariablesInitializer가

723
00:43:30,771 --> 00:43:34,946
실제로 실행하고 구체적인 값을
생성하는 tf.randomnormal

724
00:43:34,946 --> 00:43:37,733
해당 변수를 초기화합니다.

725
00:43:37,733 --> 00:43:40,794
[학생의 말은 마이크가 없어서 가려졌습니다.]

726
00:43:40,794 --> 00:43:42,271
미안, 질문이 뭐야?

727
00:43:42,271 --> 00:43:45,233
[학생의 말은 마이크가 없어서 가려졌습니다.]

728
00:43:45,233 --> 00:43:47,935
따라서 자리 표시자를 먹일 것임을 알고 있습니다.

729
00:43:47,935 --> 00:43:49,978
그래프 밖에서 변수는

730
00:43:49,978 --> 00:43:51,385
그래프 내부에 살고있다.

731
00:43:51,385 --> 00:43:53,770
그래서 나는 그것이 어떻게 결정되는지에
대한 모든 세부 사항을 알지 못한다.

732
00:43:53,770 --> 00:43:56,371
정확히 그 전화와 함께 달릴 것을 결정합니다.

733
00:43:56,371 --> 00:43:57,680
나는 당신이 코드를 파헤쳐 그림을
그릴 필요가 있다고 생각한다.

734
00:43:57,680 --> 00:44:00,384
그, 어쩌면 어딘가에 문서화되어 있습니다.

735
00:44:00,384 --> 00:44:01,941
그래서 지금 우리는 이것을 얻었습니다.

736
00:44:01,941 --> 00:44:04,656
다시 우리는이 훈련의 전체 예제를 얻었습니다.

737
00:44:04,656 --> 00:44:06,130
네트워크를 TensorFlow에 추가하면

738
00:44:06,130 --> 00:44:09,328
종소리와 휘파람을 조금 더 편리하게 만들 수 있습니다.

739
00:44:09,328 --> 00:44:11,893
이전 예제에서 우리는 여기에서도 할 수 있습니다.

740
00:44:11,893 --> 00:44:14,027
우리는 우리 자신의 손실을 명시 적으로 계산했습니다.

741
00:44:14,027 --> 00:44:16,954
텐서 작동, TensorFlow 당신은
항상 그렇게 할 수 있습니다,

742
00:44:16,954 --> 00:44:19,148
기본 텐서 연산을 사용하여 계산할 수 있습니다.

743
00:44:19,148 --> 00:44:20,739
네가 원하는 무엇이든.

744
00:44:20,739 --> 00:44:22,730
그러나 TensorFlow는 또한 편의를 제공합니다.

745
00:44:22,730 --> 00:44:25,901
이러한 일반적인 신경 회로망을 계산하는 함수

746
00:44:25,901 --> 00:44:26,734
너를 위해서.

747
00:44:26,734 --> 00:44:30,040
그래서이 경우 우리는 tf.losses.mean_squared_error를
사용할 수 있습니다.

748
00:44:30,040 --> 00:44:32,531
우리에게는 L2 손실 만 있기 때문에
우리는 가지고 있지 않습니다.

749
00:44:32,531 --> 00:44:36,273
기본 텐서 연산의 측면에서 스스로를 계산할 수 있습니다.

750
00:44:36,273 --> 00:44:39,194
여기 또 다른 종류의 이상한 점은 그것이 일종의

751
00:44:39,194 --> 00:44:42,606
우리가 입력 내용을 명시 적으로 정의해야한다는

752
00:44:42,606 --> 00:44:44,729
우리의 가중치를 정의한 다음 함께 연결합니다.

753
00:44:44,729 --> 00:44:46,667
행렬 곱셈을 사용하는 순방향 패스에서.

754
00:44:46,667 --> 00:44:49,958
그리고이 예제에서는 실제로 바이어스를 넣지 않았습니다.

755
00:44:49,958 --> 00:44:52,820
그것이 여분의 종류 일 것이기 때문에 층에서,

756
00:44:52,820 --> 00:44:54,291
그러면 편향을 초기화해야합니다.

757
00:44:54,291 --> 00:44:56,330
우리는 그들을 올바른 모양으로 만들어야합니다.

758
00:44:56,330 --> 00:44:58,494
출력에 대해 편견을 방송해야합니다.

759
00:44:58,494 --> 00:45:00,568
행렬의 곱셈과 당신은 그것을 볼 수 있습니다

760
00:45:00,568 --> 00:45:01,966
좀 많은 코드가 될 것입니다.

761
00:45:01,966 --> 00:45:03,664
그것은 성가신 글씨 일 것입니다.

762
00:45:03,664 --> 00:45:05,231
일단 컨버전스가 좋아지면

763
00:45:05,231 --> 00:45:07,626
배치 정규화 및 다른 유형의 레이어

764
00:45:07,626 --> 00:45:09,653
작업의 기본 방식 의이 종류,

765
00:45:09,653 --> 00:45:12,511
이러한 변수들을 갖는 것, 이들 입출력들

766
00:45:12,511 --> 00:45:14,626
그들을 모두 기본으로 결합

767
00:45:14,626 --> 00:45:17,403
전산 그래프 작업은 약간 일 수 있습니다.

768
00:45:17,403 --> 00:45:19,749
다루기 힘들고 정말로 성가시다.

769
00:45:19,749 --> 00:45:21,274
가중치를 오른쪽으로 초기화해야합니다.

770
00:45:21,274 --> 00:45:22,954
모양과 모든 종류의 물건.

771
00:45:22,954 --> 00:45:25,353
그래서 결과적으로 더 높은 수준의 종류가 있습니다.

772
00:45:25,353 --> 00:45:27,535
TensorFlow를 둘러싼 라이브러리

773
00:45:27,535 --> 00:45:30,615
이러한 세부 정보 중 일부를 처리 할 수 있습니다.

774
00:45:30,615 --> 00:45:33,190
그래서 TensorFlow와 함께 제공되는 한 가지 예는,

775
00:45:33,190 --> 00:45:35,965
이 안에 tf 층이 있니?

776
00:45:35,965 --> 00:45:38,554
이제이 코드 예제에서 우리 코드가

777
00:45:38,554 --> 00:45:41,455
명시 적으로 X 및 Y 만 선언하고 있습니다.

778
00:45:41,455 --> 00:45:44,060
데이터 및 레이블의 자리 표시 자입니다.

779
00:45:44,060 --> 00:45:48,474
그리고 이제 우리는 H = tf. layers.dense라고 말하면서,

780
00:45:48,474 --> 00:45:53,036
우리는 입력 X를주고 그것을 단위 = H라고 말합니다.

781
00:45:53,036 --> 00:45:55,171
이것은 다시 마술적인 일종의 선이다.

782
00:45:55,171 --> 00:45:57,782
이 줄 안쪽에 일종의 설정이기 때문에

783
00:45:57,782 --> 00:46:02,048
w1과 b1, 바이어스, 그것들에
대한 변수를 설정하고 있습니다.

784
00:46:02,048 --> 00:46:05,222
그래프 안에 일종의 올바른 모양이 있습니다.

785
00:46:05,222 --> 00:46:07,411
그러나 우리에게서 조금 숨겨졌다.

786
00:46:07,411 --> 00:46:10,012
그리고이 xavier 이니셜 라이저
객체를 사용하고 있습니다.

787
00:46:10,012 --> 00:46:12,931
그것들을위한 초기화 전략을 세우는 것.

788
00:46:12,931 --> 00:46:14,730
그래서 우리가 명시 적으로 그것을하기 전에

789
00:46:14,730 --> 00:46:17,200
tf.randomnormal 사업으로,

790
00:46:17,200 --> 00:46:19,335
하지만 지금은 여기에 그 세부 사항
중 일부를 처리하는 종류입니다

791
00:46:19,335 --> 00:46:22,266
우리를 위해 그리고 그것은 단지 H를 내뱉고 있습니다.

792
00:46:22,266 --> 00:46:23,989
그것은 다시 우리가 본 것과 같은 종류의 H입니다.

793
00:46:23,989 --> 00:46:26,265
이전 계층에서는 일부 작업 만 수행하고 있습니다.

794
00:46:26,265 --> 00:46:27,515
우리를위한 세부 사항.

795
00:46:28,487 --> 00:46:30,354
그리고 여기에서 볼 수 있습니다.

796
00:46:30,354 --> 00:46:33,857
activation = tf.nn.relu
그래서 활성화를 수행하고 있습니다.

797
00:46:33,857 --> 00:46:36,910
우리를 위해이 층 안의 relu 활성화 기능.

798
00:46:36,910 --> 00:46:39,541
그래서 많은 건축물들을 돌보고 있습니다.

799
00:46:39,541 --> 00:46:41,370
우리를위한 세부 사항.

800
00:46:41,370 --> 00:46:42,784
문제?

801
00:46:42,784 --> 00:46:46,446
[학생의 말은 마이크가 없어서 가려졌습니다.]

802
00:46:46,446 --> 00:46:49,032
질문은 xavier initializer 기본값입니다

803
00:46:49,032 --> 00:46:51,168
특정 배포판에?

804
00:46:51,168 --> 00:46:53,887
나는 그것이 어떤 디폴트를 가지고 있다고
확신한다, 나는 그것이 무엇인지 모른다.

805
00:46:53,887 --> 00:46:55,850
나는 당신이 문서를 봐야 할 것 같아.

806
00:46:55,850 --> 00:46:58,010
그러나 그것은 합리적인 전략 인 것 같습니다.

807
00:46:58,010 --> 00:46:59,625
그리고 실제로이 코드를 실행하면,

808
00:46:59,625 --> 00:47:01,303
이전보다 훨씬 빠르게 수렴

809
00:47:01,303 --> 00:47:04,111
초기화가 더 좋기 때문입니다.

810
00:47:04,111 --> 00:47:06,047
그리고 우리는 두 번의 호출을
사용하고 있음을 알 수 있습니다.

811
00:47:06,047 --> 00:47:08,037
tf.layers와 이것으로 우리 모델을 만들 수 있습니다.

812
00:47:08,037 --> 00:47:10,465
이러한 모든 명시적인 부기 세부 사항을 수행하지 않고

813
00:47:10,465 --> 00:47:11,911
나 자신.

814
00:47:11,911 --> 00:47:14,273
그래서 이것은 아마도 조금 더 편리 할 것입니다.

815
00:47:14,273 --> 00:47:18,682
하지만 tf.contrib.layer는
실제로 유일한 게임이 아닙니다.

816
00:47:18,682 --> 00:47:21,260
다른 상위 라이브러리들도 많이 있습니다.

817
00:47:21,260 --> 00:47:23,349
사람들이 TensorFlow 위에 구축합니다.

818
00:47:23,349 --> 00:47:26,841
그리고 이것은 기본적인 발기 부전 불일치로 인한 것입니다

819
00:47:26,841 --> 00:47:30,315
계산 그래프는 상대적으로 낮은 레벨의 것인데,

820
00:47:30,315 --> 00:47:32,356
그러나 우리가 신경 네트워크로 작업 할 때

821
00:47:32,356 --> 00:47:34,309
우리는 레이어와 웨이트의 개념을 가지고 있습니다.

822
00:47:34,309 --> 00:47:36,426
몇몇 층에는 그들과 관련된 가중치가 있고,

823
00:47:36,426 --> 00:47:38,949
우리는 일반적으로 약간 높은 수준으로 생각합니다.

824
00:47:38,949 --> 00:47:41,866
이 원시 계산 그래프보다 추상화의.

825
00:47:41,866 --> 00:47:44,899
이것이 바로이 다양한 패키지들이 시도하는 것입니다.

826
00:47:44,899 --> 00:47:46,563
당신을 돕고 당신이이 상위 계층에서 일하게하십시오

827
00:47:46,563 --> 00:47:48,503
추상화의.

828
00:47:48,503 --> 00:47:50,781
그래서 당신이 가질 수있는 아주 인기있는 또 다른 패키지

829
00:47:50,781 --> 00:47:52,460
전에 본 Keras입니다.

830
00:47:52,460 --> 00:47:56,275
Keras는 매우 아름답고 멋진 API로,

831
00:47:56,275 --> 00:47:59,051
TensorFlow 및 이러한 종류의 건물을 처리합니다.

832
00:47:59,051 --> 00:48:02,806
백 엔드에서 당신을위한 전산 그래프.

833
00:48:02,806 --> 00:48:05,029
그런데 Keras는 Theano를 백엔드로 지원합니다.

834
00:48:05,029 --> 00:48:07,704
그래서 그것은 또한 친절합니다.

835
00:48:07,704 --> 00:48:09,693
이 예제에서 우리는 모델을 만들
수 있음을 알 수 있습니다.

836
00:48:09,693 --> 00:48:10,958
일련의 레이어로

837
00:48:10,958 --> 00:48:12,738
우리는 최적화 객체를 구현합니다.

838
00:48:12,738 --> 00:48:15,589
우리는 model.compile을
호출하고 이것은 많은 마법을 만듭니다.

839
00:48:15,589 --> 00:48:17,910
백 엔드에서 그래프를 작성하십시오.

840
00:48:17,910 --> 00:48:20,759
그리고 이제 model.fit을 호출 할 수 있습니다.

841
00:48:20,759 --> 00:48:22,797
우리를위한 마술 훈련 절차.

842
00:48:22,797 --> 00:48:24,914
그래서 나는 이것이 어떻게 작동하는지
모든 세부 사항을 알지 못한다.

843
00:48:24,914 --> 00:48:26,211
하지만 Keras가 매우 유명하다는 것을 알고 있습니다.

844
00:48:26,211 --> 00:48:27,606
그래서 당신이 이야기하고 있다면
그것을 사용하는 것이 좋습니다.

845
00:48:27,606 --> 00:48:28,523
TensorFlow.

846
00:48:29,797 --> 00:48:31,270
문제?

847
00:48:31,270 --> 00:48:35,437
[학생의 말은 마이크가 없어서 가려졌습니다.]

848
00:48:41,717 --> 00:48:43,899
네, 그렇다면 질문은 명백한 이유가없는 것과 같습니다.

849
00:48:43,899 --> 00:48:45,525
CPU, GPU가 여기에 있습니다.

850
00:48:45,525 --> 00:48:48,409
그래서 나는 코드를 깨끗하게 유지하기 위해 그것을 버렸다.

851
00:48:48,409 --> 00:48:50,042
그러나 처음 예제에서 보았습니다.

852
00:48:50,042 --> 00:48:52,147
이 모든 것들을 뒤집어 쓰는 것은 꽤 쉬웠습니다.

853
00:48:52,147 --> 00:48:54,607
CPU와 GPU 사이에 몇 가지 글로벌 플래그가있었습니다.

854
00:48:54,607 --> 00:48:56,369
또는 다른 데이터 유형

855
00:48:56,369 --> 00:48:59,449
또는 성명서가있는 것도 있고 일반적으로 비교적 간단합니다

856
00:48:59,449 --> 00:49:01,635
각각의 경우에 한 줄 씩 바꿀 수 있습니다.

857
00:49:01,635 --> 00:49:03,349
하지만 정확히 그 라인이 어떻게 생겼는지

858
00:49:03,349 --> 00:49:06,149
상황에 따라 약간 다릅니다.

859
00:49:06,149 --> 00:49:09,456
실제로이 전체 집합과 같습니다.

860
00:49:09,456 --> 00:49:12,527
보다 높은 수준의 TensorFlow
랩퍼를 볼 수 있습니다.

861
00:49:12,527 --> 00:49:14,186
거기에 야생에.

862
00:49:14,186 --> 00:49:17,109
그리고 구글 내의 사람들조차도

863
00:49:17,109 --> 00:49:21,276
어떤 것이 올바른지에 대해 정말로 동의 할 수 없습니다.

864
00:49:22,230 --> 00:49:24,690
그래서 Keras와 TFLearn은 타사 라이브러리입니다.

865
00:49:24,690 --> 00:49:26,829
다른 사람들이 인터넷에있는 곳입니다.

866
00:49:26,829 --> 00:49:29,768
하지만이 세 가지가 있습니다.

867
00:49:29,768 --> 00:49:32,563
tf.layers, TF-Slim 및 tf.contrib.learn

868
00:49:32,563 --> 00:49:35,355
모두 TensorFlow와 함께 배송됩니다.

869
00:49:35,355 --> 00:49:37,890
이것의 약간 다른 버전을하고있다.

870
00:49:37,890 --> 00:49:39,727
높은 수준의 래퍼 일.

871
00:49:39,727 --> 00:49:41,765
Google의 다른 프레임 워크도 있습니다.

872
00:49:41,765 --> 00:49:44,109
Pretty Tensor라는 TensorFlow와
함께 배송되지는 않습니다.

873
00:49:44,109 --> 00:49:46,291
그것은 같은 종류의 일을합니다.

874
00:49:46,291 --> 00:49:48,599
그리고 이들 중 어느 것도 DeepMind에게는
충분하지 않았을 것입니다.

875
00:49:48,599 --> 00:49:50,269
그들이 몇 주 전에 앞섰기 때문에

876
00:49:50,269 --> 00:49:52,488
그들 자신의 높은 수준을 썼다.

877
00:49:52,488 --> 00:49:54,530
Sonnet이라는 TensorFlow 래퍼.

878
00:49:54,530 --> 00:49:57,570
네가 혼란 스러울 정도라면 나는 너를 괴롭히지 않을거야.

879
00:49:57,570 --> 00:49:59,506
이 모든 것들로.

880
00:49:59,506 --> 00:50:00,715
다양한 선택이 있습니다.

881
00:50:00,715 --> 00:50:03,113
항상 서로 잘 어울리는 것은 아닙니다.

882
00:50:03,113 --> 00:50:07,423
하지만 옵션이 많아서 좋습니다.

883
00:50:07,423 --> 00:50:09,123
TensorFlow에는 사전 모델이 있습니다.

884
00:50:09,123 --> 00:50:11,112
TF-Slim과 Keras에 몇 가지 예가 있습니다.

885
00:50:11,112 --> 00:50:14,072
재교육 된 모델이 매우 중요하다는 것을 기억하라.

886
00:50:14,072 --> 00:50:15,874
당신이 자신의 것을 훈련 할 때.

887
00:50:15,874 --> 00:50:17,931
Tensorboard에 대한 아이디어도 있습니다.

888
00:50:17,931 --> 00:50:19,634
너를로드 할 수있는 곳,

889
00:50:19,634 --> 00:50:21,072
나는 세부 사항에 들어가고 싶지 않다,

890
00:50:21,072 --> 00:50:22,834
그러나 Tensorboard를 사용하면
계기의 종류를 추가 할 수 있습니다.

891
00:50:22,834 --> 00:50:24,733
귀하의 코드에 다음 손실과 물건을 줄거리

892
00:50:24,733 --> 00:50:27,747
당신이 훈련 과정을 통과 할 때.

893
00:50:27,747 --> 00:50:29,535
TensorFlow는 분산 형으로도 실행 해 보겠습니다.

894
00:50:29,535 --> 00:50:31,372
계산 그래프를 나눌 수있는 곳

895
00:50:31,372 --> 00:50:32,760
다른 컴퓨터에서 실행됩니다.

896
00:50:32,760 --> 00:50:35,222
그건 아주 멋지지만 아마 아무도 아닌 것 같아.

897
00:50:35,222 --> 00:50:37,613
Google 외부에서 실제로 큰 성공을 거두고 있습니다.

898
00:50:37,613 --> 00:50:40,733
요즘에는 배포 된 자료를 실행하고 싶다면

899
00:50:40,733 --> 00:50:44,193
아마 TensorFlow가 그 도시의 주요 게임입니다.

900
00:50:44,193 --> 00:50:46,834
부수적 인 부분은 TensorFlow의 많은 디자인

901
00:50:46,834 --> 00:50:49,992
이 초기 프레임 워크에서 영감을받은 것입니다.

902
00:50:49,992 --> 00:50:51,533
몬트리올에서 Theano라고 불렀습니다.

903
00:50:51,533 --> 00:50:54,008
나는 세부 사항을 여기에서 지나고 싶지 않다,

904
00:50:54,008 --> 00:50:55,933
당신이이 슬라이드를 직접 살펴 본다면,

905
00:50:55,933 --> 00:50:58,127
Theano 코드가 끝나는 것을 볼 수 있습니다.

906
00:50:58,127 --> 00:50:59,979
TensorFlow와 매우 유사합니다.

907
00:50:59,979 --> 00:51:01,420
변수를 정의 할 때,

908
00:51:01,420 --> 00:51:03,512
우리는 몇 가지 진행 경로를 수행,
우리는 몇 가지 그라디언트를 계산,

909
00:51:03,512 --> 00:51:05,978
우리는 함수를 컴파일 한 다음 함수를 실행합니다.

910
00:51:05,978 --> 00:51:08,034
반복해서 네트워크를 훈련 시키십시오.

911
00:51:08,034 --> 00:51:10,290
TensorFlow와 비슷합니다.

912
00:51:10,290 --> 00:51:13,010
그래서 우리는 아직 많이해야 할 일이 있습니다.

913
00:51:13,010 --> 00:51:14,462
그래서 저는 PyTorch로 이동할 것입니다.

914
00:51:14,462 --> 00:51:16,671
마지막에 질문을 할 수도 있습니다.

915
00:51:16,671 --> 00:51:20,770
따라서 페이스 북의 PyTorch는

916
00:51:20,770 --> 00:51:22,868
TensorFlow는 3 개의 명시 적

917
00:51:22,868 --> 00:51:26,397
PyTorch 내부의 다른 추상화 계층.

918
00:51:26,397 --> 00:51:29,415
그래서 PyTorch는이 텐서 객체를 가지고 있습니다.

919
00:51:29,415 --> 00:51:30,619
너 프디 배열.

920
00:51:30,619 --> 00:51:33,603
그것은 단지 명령형 배열 일 뿐이며 아무것도 모릅니다.

921
00:51:33,603 --> 00:51:36,770
심층적 인 학습은 있지만 GPU로 실행할 수 있습니다.

922
00:51:36,770 --> 00:51:38,676
우리는이 변수 객체를 가지고 있습니다.

923
00:51:38,676 --> 00:51:42,274
계산 그래프를 구성하는 전산 그래프,

924
00:51:42,274 --> 00:51:44,093
그래디언트를 계산할 수 있습니다.

925
00:51:44,093 --> 00:51:46,312
그리고 우리는 신경망 인 모듈 객체를 가지고 있습니다.

926
00:51:46,312 --> 00:51:48,573
이 모듈들을 함께 구성 할 수있는 레이어

927
00:51:48,573 --> 00:51:50,766
큰 네트워크를 구축 할 수 있습니다.

928
00:51:50,766 --> 00:51:52,973
그래서 여러분이 거친 균등 물에 대해 생각하고 싶다면

929
00:51:52,973 --> 00:51:55,971
PyTorch와 TensorFlow 사이에

930
00:51:55,971 --> 00:51:58,759
PyTorch 텐서가 같은 역할을 수행함

931
00:51:58,759 --> 00:52:01,457
TensorFlow의 Numpy 배열로.

932
00:52:01,457 --> 00:52:04,602
PyTorch 변수는 TensorFlow 텐서와 유사합니다

933
00:52:04,602 --> 00:52:07,549
변수 또는 자리 표시 자, 모든 종류의 노드

934
00:52:07,549 --> 00:52:08,803
전산 그래프에서.

935
00:52:08,803 --> 00:52:11,970
그리고 이제 PyTorch 모듈은 동급입니다.

936
00:52:11,970 --> 00:52:16,288
tf.slim 또는 tf.layers에서
이러한 상위 수준의 작업

937
00:52:16,288 --> 00:52:18,448
또는 소네트 또는 다른 상위 레벨 프레임 워크.

938
00:52:18,448 --> 00:52:21,102
PyTorch에 대해 알아야 할 사항이 하나 있습니다.

939
00:52:21,102 --> 00:52:24,072
이 높은 수준의 추상화와 함께 제공되기 때문입니다.

940
00:52:24,072 --> 00:52:26,696
정말 멋진 고차원 추상화 같아.

941
00:52:26,696 --> 00:52:28,947
자체적으로 모듈이라고 불리는데, 선택의 여지가 적습니다.

942
00:52:28,947 --> 00:52:29,780
뒤얽힌.

943
00:52:29,780 --> 00:52:32,534
nnmodules을 고수하면 잘 갈 수 있습니다.

944
00:52:32,534 --> 00:52:35,809
어느 상위 레벨 래퍼에 대해 걱정할 필요가 없습니다.

945
00:52:35,809 --> 00:52:36,642
사용.

946
00:52:37,777 --> 00:52:41,944
그래서 내가 말했듯이, PyTorch의
텐서는 마치 Numpy 배열과 같습니다.

947
00:52:43,660 --> 00:52:46,181
여기 오른쪽에서 우리는 전체 2
계층 네트워크를 수행했습니다.

948
00:52:46,181 --> 00:52:47,787
전적으로 PyTorch 텐서를 사용합니다.

949
00:52:47,787 --> 00:52:50,279
한 가지주의 할 점은 우리가 여기
Numpy를 수입하지 않는다는 것입니다.

950
00:52:50,279 --> 00:52:51,379
더 이상.

951
00:52:51,379 --> 00:52:53,910
우리는 PyTorch 텐서를 사용하여
이러한 모든 작업을 수행하고 있습니다.

952
00:52:53,910 --> 00:52:58,624
그리고이 코드는 2 층 네트 코드와 똑같습니다.

953
00:52:58,624 --> 00:53:01,245
너 프룻이 첫 숙제를 할 때 썼다.

954
00:53:01,245 --> 00:53:05,774
임의의 데이터를 설정 했으므로 몇 가지 연산을 사용합니다.

955
00:53:05,774 --> 00:53:07,127
전달 경로를 계산합니다.

956
00:53:07,127 --> 00:53:09,332
그리고 나서 우리는 백 워드 패스를
명시 적으로보고 있습니다.

957
00:53:09,332 --> 00:53:10,165
나 자신.

958
00:53:10,165 --> 00:53:12,794
네트워크를 통해 일종의 백 호핑 (backhopping)

959
00:53:12,794 --> 00:53:15,980
당신이 숙제 중 하나에서했던 것처럼, 작업을 통해.

960
00:53:15,980 --> 00:53:18,241
이제 우리는 가중치의 수동 업데이트를 수행하고 있습니다.

961
00:53:18,241 --> 00:53:22,672
학습 속도를 사용하고 계산 된 그라디언트를 사용합니다.

962
00:53:22,672 --> 00:53:24,681
그러나 PyTorch 텐서

963
00:53:24,681 --> 00:53:27,785
Numpy 배열은 GPU에서 실행된다는 것입니다.

964
00:53:27,785 --> 00:53:30,651
이 코드를 실행하려면

965
00:53:30,651 --> 00:53:33,034
GPU는 다른 데이터 유형을 사용합니다.

966
00:53:33,034 --> 00:53:35,092
torch.FloatTensor를 사용하는 대신,

967
00:53:35,092 --> 00:53:39,259
당신은 토치를합니다. 쿠다. 플로트
텐서, 모든 텐서 스를 던져라.

968
00:53:40,152 --> 00:53:42,174
이 새로운 데이터 유형에 모든 것이 마술처럼 작동합니다.

969
00:53:42,174 --> 00:53:43,709
GPU에서.

970
00:53:43,709 --> 00:53:47,637
PyTorch tensors는 Numpy와
GPU를 더한 것으로 생각해야합니다.

971
00:53:47,637 --> 00:53:49,401
그게 정확히 무엇인지, 어떤 것도 구체적이지 않습니다.

972
00:53:49,401 --> 00:53:50,818
깊은 학습.

973
00:53:52,638 --> 00:53:55,278
따라서 PyTorch에서 다음 추상화 계층은 변수입니다.

974
00:53:55,278 --> 00:53:58,440
그래서 우리가 일단 텐서 (tensors)에서
변수 (variables)로 이동하면

975
00:53:58,440 --> 00:54:00,702
이제 우리는 전산 그래프를 구축하고있다.

976
00:54:00,702 --> 00:54:02,194
우리는 자동으로 그라디언트를 취할 수 있습니다.

977
00:54:02,194 --> 00:54:03,460
그리고 그 모든게 좋아.

978
00:54:03,460 --> 00:54:07,627
그래서 여기에서 X가 변수이면 x.data는 텐서입니다.

979
00:54:08,890 --> 00:54:12,164
x.grad는 그라디언트를 포함하는 또 다른 변수입니다.

980
00:54:12,164 --> 00:54:14,007
그 텐서에 관한 손실의

981
00:54:14,007 --> 00:54:15,913
따라서 x.grad.data는 실제 텐서입니다.

982
00:54:15,913 --> 00:54:17,246
그 그라디언트.

983
00:54:18,972 --> 00:54:22,387
그리고 PyTorch 텐서와 변수는
똑같은 API를 가지고 있습니다.

984
00:54:22,387 --> 00:54:25,606
그래서 PyTorch 텐서에서 작업 한 코드는

985
00:54:25,606 --> 00:54:28,457
그 (것)들에게 대신에 가변을
만들고 동일한 부호를 달리십시오,

986
00:54:28,457 --> 00:54:30,292
지금은 계산 그래프를 작성하는 것을 제외하고는

987
00:54:30,292 --> 00:54:34,459
단지 이러한 명령 적 작업을 수행하는 것이 아닙니다.

988
00:54:35,943 --> 00:54:38,553
여기서 우리는이 변수들을 만들 때

989
00:54:38,553 --> 00:54:41,234
변수 생성자에 대한 각 호출은 PyTorch를 래핑합니다.

990
00:54:41,234 --> 00:54:43,652
다음에 텐서를 붙일 지 어떨지를 나타내는 플래그도 붙인다

991
00:54:43,652 --> 00:54:47,461
이 변수에 대한 그라디언트를 계산하려고합니다.

992
00:54:47,461 --> 00:54:49,412
그리고 앞으로 패스에서 그것은 정확하게 보입니다.

993
00:54:49,412 --> 00:54:52,012
그것은 이전에 텐서가있는 경우의 변수에서 수행되었습니다.

994
00:54:52,012 --> 00:54:54,073
그들은 같은 API를 가지고 있기 때문에.

995
00:54:54,073 --> 00:54:55,667
이제 우리는 예측을 계산하고 있습니다.

996
00:54:55,667 --> 00:54:58,431
우리는이 명령의 종류로 우리의 손실을 계산하고있다.

997
00:54:58,431 --> 00:54:59,683
종류의 길.

998
00:54:59,683 --> 00:55:03,492
그리고 나서 우리는 손실이라고 부릅니다.
뒤로 그리고 지금이 모든 그라데이션들

999
00:55:03,492 --> 00:55:05,251
우리를 위해 나와라.

1000
00:55:05,251 --> 00:55:06,790
그런 다음 그라데이션 업데이트 단계를 만들 수 있습니다.

1001
00:55:06,790 --> 00:55:09,214
현재 존재하는 그라데이션을 사용하는 가중치

1002
00:55:09,214 --> 00:55:11,528
w1.grad.data에서

1003
00:55:11,528 --> 00:55:16,227
그래서 이것은 Numpy 사건과 아주 유사하게 보입니다.

1004
00:55:16,227 --> 00:55:18,137
모든 그라디언트가 무료로 제공된다는 점을 제외하고는

1005
00:55:18,137 --> 00:55:20,596
그것이 주목할만한 점은

1006
00:55:20,596 --> 00:55:23,353
PyTorch와 TensorFlow는
TensorFlow의 경우입니다.

1007
00:55:23,353 --> 00:55:25,289
우리는이 명백한 그래프를 만들고있었습니다.

1008
00:55:25,289 --> 00:55:27,132
그런 다음 그래프를 여러 번 실행합니다.

1009
00:55:27,132 --> 00:55:30,308
여기 PyTorch에서는 새로운
그래프를 작성하고 있습니다.

1010
00:55:30,308 --> 00:55:32,152
우리가 앞으로 나아갈 때마다.

1011
00:55:32,152 --> 00:55:33,887
그리고 이것은 코드를 좀 더 깔끔하게 보입니다.

1012
00:55:33,887 --> 00:55:35,284
그리고 다른 의미도 있습니다.

1013
00:55:35,284 --> 00:55:37,058
조금 들어가.

1014
00:55:37,058 --> 00:55:40,630
그래서 PyTorch에서 여러분은 새로운
autograd 함수를 정의 할 수 있습니다.

1015
00:55:40,630 --> 00:55:42,933
텐서 (tensors)의 관점에서 앞뒤를 정의함으로써.

1016
00:55:42,933 --> 00:55:45,954
이것은 결국 모듈 레이어처럼 보이게됩니다.

1017
00:55:45,954 --> 00:55:48,303
숙제 2를 위해 작성한 코드.

1018
00:55:48,303 --> 00:55:50,404
다음을 사용하여 앞뒤로 구현할 수있는 위치

1019
00:55:50,404 --> 00:55:52,676
텐서 (tensor) 연산을 수행 한 다음,

1020
00:55:52,676 --> 00:55:54,433
전산 그래프.

1021
00:55:54,433 --> 00:55:56,297
그래서 우리는 우리 자신의 relu를 정의하고 있습니다.

1022
00:55:56,297 --> 00:56:00,654
그리고 나서 우리는 실제로 들어가서 우리
자신의 relu를 사용할 수 있습니다.

1023
00:56:00,654 --> 00:56:02,754
연산을 수행하고 이제 계산 그래프에 집어 넣습니다.

1024
00:56:02,754 --> 00:56:05,214
이런 식으로 우리 자신의 작업을 정의하십시오.

1025
00:56:05,214 --> 00:56:06,857
하지만 대부분의 시간은 아마 필요 없을 것입니다.

1026
00:56:06,857 --> 00:56:09,097
자신 만의 autograd 작업을 정의 할 수 있습니다.

1027
00:56:09,097 --> 00:56:10,814
대부분의 경우 필요한 작업은

1028
00:56:10,814 --> 00:56:14,246
대부분 이미 구현되어 있습니다.

1029
00:56:14,246 --> 00:56:16,293
그래서 TensorFlow에서 우리는 보았습니다,

1030
00:56:16,293 --> 00:56:19,054
우리가 Keras 또는 TF와
같은 것으로 이동할 수 있다면.

1031
00:56:19,054 --> 00:56:21,253
이것은 우리에게 더 높은 수준의 API를 제공합니다.

1032
00:56:21,253 --> 00:56:23,349
이 원시 계산 그래프보다.

1033
00:56:23,349 --> 00:56:25,433
PyTorch에서 이에 상응하는 것은 nn 패키지입니다.

1034
00:56:25,433 --> 00:56:29,448
작업을위한 이러한 높은 수준의 래퍼를 제공하는 곳

1035
00:56:29,448 --> 00:56:30,948
이걸로.

1036
00:56:31,882 --> 00:56:33,454
그러나 TensorFlow와는
달리 그 중 하나만 있습니다.

1037
00:56:33,454 --> 00:56:35,837
그리고 그것은 꽤 잘 작동합니다,
그렇다면 그것을 사용하십시오.

1038
00:56:35,837 --> 00:56:37,772
PyTorch를 사용합니다.

1039
00:56:37,772 --> 00:56:39,374
그래서 여기, 이것은 마치 Keras처럼
보이는 것을 끝내고 있습니다.

1040
00:56:39,374 --> 00:56:42,196
우리는 모델을 일련의 레이어 시퀀스로 정의합니다.

1041
00:56:42,196 --> 00:56:44,436
우리의 직선 및 relu 작업.

1042
00:56:44,436 --> 00:56:47,574
그리고 우리는 nn 패키지에 정의
된 손실 함수를 사용합니다.

1043
00:56:47,574 --> 00:56:49,816
즉 평균 제곱 오류 손실입니다.

1044
00:56:49,816 --> 00:56:51,593
이제 루프 반복마다

1045
00:56:51,593 --> 00:56:53,716
우리는 모델을 통해 데이터를 전달하여

1046
00:56:53,716 --> 00:56:55,214
우리의 예측.

1047
00:56:55,214 --> 00:56:57,511
손실 함수를 통해 예측을 진행할 수 있습니다.

1048
00:56:57,511 --> 00:56:59,054
우리의 규모 또는 손실을 얻기 위해,

1049
00:56:59,054 --> 00:57:01,177
우리는 손실을 전화 할 수 있습니다. 뒤로,
우리의 모든 그라디언트를 얻으십시오.

1050
00:57:01,177 --> 00:57:04,021
자유로운을 위해 그리고 모형의 매개 변수에 반복하십시오

1051
00:57:04,021 --> 00:57:06,143
업데이트하기 위해 명시적인 그래디언트
강하 단계를 수행하십시오.

1052
00:57:06,143 --> 00:57:07,273
모델.

1053
00:57:07,273 --> 00:57:09,054
그리고 다시 우리는 이것이 우리가 일종의

1054
00:57:09,054 --> 00:57:12,749
우리가 전방 통과를 할 때마다 새로운 전산 그래프.

1055
00:57:12,749 --> 00:57:14,714
그리고 우리가 TensorFlow에서 본 것처럼,

1056
00:57:14,714 --> 00:57:17,017
PyTorch는 이러한 최적화 작업을 제공합니다.

1057
00:57:17,017 --> 00:57:19,655
그 종류의 추상적인 업데이트 논리

1058
00:57:19,655 --> 00:57:21,758
Adam과 같은 고급 업데이트 규칙 구현

1059
00:57:21,758 --> 00:57:23,000
그리고 이것 저것.

1060
00:57:23,000 --> 00:57:25,038
여기서 우리는 최적화 객체를 생성하고 있습니다.

1061
00:57:25,038 --> 00:57:27,034
우리가 그것을 통해 최적화하기를 원한다는 것을

1062
00:57:27,034 --> 00:57:28,771
모델의 매개 변수.

1063
00:57:28,771 --> 00:57:31,115
하이퍼 매개 변수 아래에서 약간의 학습 속도를 제공합니다.

1064
00:57:31,115 --> 00:57:33,438
그리고 지금 우리가 그라디언트를 계산 한 후에

1065
00:57:33,438 --> 00:57:35,356
optimizer.step을 호출하면 업데이트됩니다.

1066
00:57:35,356 --> 00:57:39,810
우리를위한 모델의 모든 매개 변수는 바로 여기에 있습니다.

1067
00:57:39,810 --> 00:57:41,951
PyTorch에서 또 다른 공통점이 있습니다.

1068
00:57:41,951 --> 00:57:44,714
많은 것은 자신의 nn 모듈을 정의합니다.

1069
00:57:44,714 --> 00:57:47,268
따라서 일반적으로 자신의 수업을 작성할 것입니다.

1070
00:57:47,268 --> 00:57:49,961
전체 모델을 단일 모델로 정의합니다.

1071
00:57:49,961 --> 00:57:51,801
새로운 nn 모듈 클래스.

1072
00:57:51,801 --> 00:57:54,979
그리고 모듈은 단지 신경 네트워크 계층의 일종입니다.

1073
00:57:54,979 --> 00:57:57,678
다른 모듈을 포함 할 수있는

1074
00:57:57,678 --> 00:58:01,043
또는 훈련 가능한 무게 또는 다른 다른 종류의 상태.

1075
00:58:01,043 --> 00:58:04,142
그래서이 경우 우리는 2 층 네트
예제를 다시 할 수 있습니다.

1076
00:58:04,142 --> 00:58:07,051
우리 자신의 nn 모듈 클래스를 정의함으로써.

1077
00:58:07,051 --> 00:58:09,925
이제 클래스의 초기화 프로그램에 있습니다.

1078
00:58:09,925 --> 00:58:11,672
이 linear1과 linear2를 할당합니다.

1079
00:58:11,672 --> 00:58:13,853
우리는이 새로운 모듈 객체를 구성하고 있습니다.

1080
00:58:13,853 --> 00:58:17,257
그들을 우리 자신의 수업 시간 내에 보관하십시오.

1081
00:58:17,257 --> 00:58:20,335
그리고 앞으로 패스에서 우리는 우리 자신의

1082
00:58:20,335 --> 00:58:22,832
내부 모듈 및 임의의 자동 기록 작업

1083
00:58:22,832 --> 00:58:26,466
변수를 사용하여 네트워크의 출력을 계산합니다.

1084
00:58:26,466 --> 00:58:29,782
그래서 여기에 우리는이 앞쪽에있는 방법 안에서,

1085
00:58:29,782 --> 00:58:31,594
입력은 변수의 역할을하며,

1086
00:58:31,594 --> 00:58:34,213
변수를 우리 자신에게 넘깁니다. 선형 1

1087
00:58:34,213 --> 00:58:35,817
첫 번째 레이어.

1088
00:58:35,817 --> 00:58:38,129
우리는 autograd op clamp를
사용하여 relu를 완료합니다.

1089
00:58:38,129 --> 00:58:40,233
우리는 그 출력을 두 번째 선형으로 전달합니다.

1090
00:58:40,233 --> 00:58:42,233
그리고 나서 그것은 우리에게 결과를줍니다.

1091
00:58:42,233 --> 00:58:44,732
이제이 코드를 훈련시키는 나머지 부분은

1092
00:58:44,732 --> 00:58:46,633
거의 같은 것처럼 보입니다.

1093
00:58:46,633 --> 00:58:48,455
최적화 도구를 만들고 루프를 반복하는 곳

1094
00:58:48,455 --> 00:58:50,916
모델에 대한 반복 피드 데이터에서,

1095
00:58:50,916 --> 00:58:52,777
손실로 그라디언트를 계산합니다.

1096
00:58:52,777 --> 00:58:54,676
호출 최적화 기. 단계.

1097
00:58:54,676 --> 00:58:57,924
그래서 이것은 상대적으로 특징적입니다.

1098
00:58:57,924 --> 00:59:00,233
많은 PyTorch 유형에서 볼 수있는 것 중

1099
00:59:00,233 --> 00:59:01,817
교육 시나리오.

1100
00:59:01,817 --> 00:59:02,964
자신 만의 클래스를 정의 할 때,

1101
00:59:02,964 --> 00:59:04,932
다른 모듈을 포함하는 자신 만의 모델 정의하기

1102
00:59:04,932 --> 00:59:07,103
그리고 그 외의 것들은 명시적인 훈련을받습니다.

1103
00:59:07,103 --> 00:59:11,166
이것을 실행하고 업데이트하는 루프.

1104
00:59:11,166 --> 00:59:13,353
당신이 가지고있는 삶의 질 좋은 종류의 한 종류

1105
00:59:13,353 --> 00:59:16,057
PyTorch에서 dataloader입니다.

1106
00:59:16,057 --> 00:59:18,873
따라서 데이터 로더는 건물 미니
바를 처리 할 수 있습니다.

1107
00:59:18,873 --> 00:59:21,273
우리가 말한 멀티 스레딩 중 일부를 처리 할 수 있습니다.

1108
00:59:21,273 --> 00:59:23,876
당신이 실제로 여러 스레드를 사용할 수있는 곳

1109
00:59:23,876 --> 00:59:25,934
당신을 위해 많은 배치를 만드는 백그라운드에서

1110
00:59:25,934 --> 00:59:27,273
디스크에서 스트리밍.

1111
00:59:27,273 --> 00:59:30,777
그래서 여기서 dataloader는 데이터 셋을 랩핑하고

1112
00:59:30,777 --> 00:59:33,221
당신을위한 이러한 추상화 중 일부.

1113
00:59:33,221 --> 00:59:35,562
실제로 자신의 데이터를 실행할 때,

1114
00:59:35,562 --> 00:59:38,138
일반적으로 자체 데이터 집합 클래스를 작성합니다.

1115
00:59:38,138 --> 00:59:40,208
특정 유형의 데이터를 읽는 방법을 알고 있습니다.

1116
00:59:40,208 --> 00:59:42,251
원하는 모든 소스를 끄고 랩핑하십시오.

1117
00:59:42,251 --> 00:59:44,458
데이터 로더와 기차.

1118
00:59:44,458 --> 00:59:47,631
그래서, 여기서 우리는 이제 우리가
반복하는 것을 볼 수 있습니다.

1119
00:59:47,631 --> 00:59:50,444
dataloader 객체와 모든 반복마다

1120
00:59:50,444 --> 00:59:52,233
이것은 데이터의 minibatches을 낳고있다.

1121
00:59:52,233 --> 00:59:55,527
그리고 내부적으로 데이터 셔플을 처리하고 있습니다.

1122
00:59:55,527 --> 00:59:57,576
및 다중 스레드 데이터 로딩과 이러한 모든 종류의 것들

1123
00:59:57,576 --> 00:59:58,409
너를 위해서.

1124
00:59:58,409 --> 01:00:00,654
따라서 이것은 완전히 PyTorch 예제입니다.

1125
01:00:00,654 --> 01:00:02,494
PyTorch 교육 코드가 많이 보입니다.

1126
01:00:02,494 --> 01:00:04,161
이 같은.

1127
01:00:05,583 --> 01:00:07,587
PyTorch는 사전 훈련 된 모델을 제공합니다.

1128
01:00:07,587 --> 01:00:09,469
그리고 이것은 아마 가장 매끄러운
pretrained 모델입니다

1129
01:00:09,469 --> 01:00:11,521
내가 본 경험.

1130
01:00:11,521 --> 01:00:14,268
torchvision.models.alexnet
pretained = true라고 말하면됩니다.

1131
01:00:14,268 --> 01:00:16,951
그 배경에서 내려갈거야, pretrained 다운로드

1132
01:00:16,951 --> 01:00:18,759
당신이 이미 그것을 가지고 있지 않다면 당신을위한 무게,

1133
01:00:18,759 --> 01:00:21,052
그리고 바로 거기에 있습니다, 당신은 잘 가게됩니다.

1134
01:00:21,052 --> 01:00:24,242
따라서 사용하기가 쉽습니다.

1135
01:00:24,242 --> 01:00:27,094
PyTorch는 또한 Visdom이라는 패키지도 있습니다.

1136
01:00:27,094 --> 01:00:30,253
이 손실 통계 중 일부를 시각화 할 수 있습니다.

1137
01:00:30,253 --> 01:00:33,600
Tensorboard와 다소 비슷합니다.

1138
01:00:33,600 --> 01:00:35,168
그래서 친절 하네, 나는 실제로 얻지 못했다.

1139
01:00:35,168 --> 01:00:36,934
내 자신과 놀 수있는 기회. 그래서
나는 정말로 할 수 없다.

1140
01:00:36,934 --> 01:00:38,569
그것이 얼마나 유용한 지 이야기하십시오.

1141
01:00:38,569 --> 01:00:40,927
하지만 Tensorboard의 주요 차이점 중 하나는

1142
01:00:40,927 --> 01:00:43,769
Visdom은 실제로 Tensorboard를
사용하면 시각화 할 수 있습니다.

1143
01:00:43,769 --> 01:00:45,907
전산 그래프의 구조.

1144
01:00:45,907 --> 01:00:47,984
정말 멋진 디버깅 전략입니다.

1145
01:00:47,984 --> 01:00:50,989
그리고 Visdom은 아직 그
기능을 가지고 있지 않습니다.

1146
01:00:50,989 --> 01:00:53,011
하지만 난 정말 이걸 사용하지 않았어.
그래서 나는 정말로 할 수 없어.

1147
01:00:53,011 --> 01:00:54,761
그 유틸리티에 말하십시오.

1148
01:00:56,350 --> 01:00:58,627
제쳐두고, PyTorch는 일종의 진화입니다.

1149
01:00:58,627 --> 01:01:01,747
이전 프레임 워크의 최신 업데이트 버전

1150
01:01:01,747 --> 01:01:04,086
내가 마지막으로 많이 일했던 Torch

1151
01:01:04,086 --> 01:01:05,491
2 년.

1152
01:01:05,491 --> 01:01:07,577
그리고 저는 여기에 세부 사항을
설명하기를 원하지 않습니다.

1153
01:01:07,577 --> 01:01:10,569
하지만 PyTorch는 많은면에서 훨씬 뛰어납니다.

1154
01:01:10,569 --> 01:01:13,280
늙은 루아 토치보다,하지만 그들은
실제로 많은 것을 공유합니다.

1155
01:01:13,280 --> 01:01:15,657
같은 백엔드의 C 코드

1156
01:01:15,657 --> 01:01:18,100
및 tensors 및 이것 저것에 GPU 작업.

1157
01:01:18,100 --> 01:01:19,554
따라서이 Torch 예제를 살펴보면,

1158
01:01:19,554 --> 01:01:21,906
일부는 PyTorch와 비슷한 종류의 것으로 보입니다.

1159
01:01:21,906 --> 01:01:23,369
일부는 약간 다릅니다.

1160
01:01:23,369 --> 01:01:25,957
어쩌면 당신은 오프라인에서 단계를 밟을 수 있습니다.

1161
01:01:25,957 --> 01:01:28,361
그러나 사이에 높은 수준의 차이의 종류

1162
01:01:28,361 --> 01:01:31,229
Torch와 PyTorch는 Torch가
실제로 Lua에 있음을 나타냅니다.

1163
01:01:31,229 --> 01:01:33,011
이 다른 것들과 달리 파이썬이 아닙니다.

1164
01:01:33,011 --> 01:01:37,748
루아를 배우는 것은 어떤 사람들에게는 약간의 휴식입니다.

1165
01:01:37,748 --> 01:01:40,009
토치에는 자동 굴림 기능이 없습니다.

1166
01:01:40,009 --> 01:01:41,710
성화는 또한 더 오래 되었기 때문에 더 안정적입니다.

1167
01:01:41,710 --> 01:01:43,491
버그에 덜 민감하고, 예제 코드가 더 많습니다.

1168
01:01:43,491 --> 01:01:44,324
토치 용.

1169
01:01:45,230 --> 01:01:47,214
그들은 거의 같은 속도이며, 그것은
정말로 걱정거리가 아닙니다.

1170
01:01:47,214 --> 01:01:49,827
하지만 PyTorch에서는 Python을 사용합니다.

1171
01:01:49,827 --> 01:01:52,270
당신은 훨씬 더 간단하게 해주는
autograd를 가지고 있습니다.

1172
01:01:52,270 --> 01:01:54,531
복잡한 모델을 작성하는 것.

1173
01:01:54,531 --> 01:01:56,422
루아 토치에서 당신은 많은 자신의 글을 쓰게됩니다.

1174
01:01:56,422 --> 01:01:59,670
때로는 소품 코드가 돌아 다니기 때문에 조금 성가시다.

1175
01:01:59,670 --> 01:02:01,650
그러나 PyTorch는 최신
버전이며 기존 코드가 적습니다.

1176
01:02:01,650 --> 01:02:03,689
여전히 변경 될 수 있습니다.

1177
01:02:03,689 --> 01:02:06,051
그래서 좀 더 모험이 될 것입니다.

1178
01:02:06,051 --> 01:02:08,145
그러나 적어도 나를 위해, 나는 조금은 좋아한다.

1179
01:02:08,145 --> 01:02:10,162
나는 내 자신에 대한 많은 이유를 실제로 보지 못한다.

1180
01:02:10,162 --> 01:02:13,228
이 시점에서 더 이상 TorTch를
PyTorch 이상 사용하십시오.

1181
01:02:13,228 --> 01:02:15,848
그래서 저는 PyTorch를 독점적으로 사용하고 있습니다.

1182
01:02:15,848 --> 01:02:17,765
요즘 내 모든 작업.

1183
01:02:18,606 --> 01:02:20,557
우리는이 아이디어에 대해 조금 이야기했습니다.

1184
01:02:20,557 --> 01:02:22,531
정적 그래프와 동적 그래프 비교

1185
01:02:22,531 --> 01:02:24,350
그리고 이것은 주요 특징 중 하나입니다.

1186
01:02:24,350 --> 01:02:26,291
PyTorch와 TensorFlow 사이.

1187
01:02:26,291 --> 01:02:29,416
우리는 Tensor에서 보았습니다.이 두 단계가 있습니다.

1188
01:02:29,416 --> 01:02:31,667
첫 번째로

1189
01:02:31,667 --> 01:02:34,371
전산 그래프, 그럼 당신은 전산 그래프를 실행합니다.

1190
01:02:34,371 --> 01:02:37,246
반복해서 여러 번 반복하여

1191
01:02:37,246 --> 01:02:38,145
그래프.

1192
01:02:38,145 --> 01:02:40,209
정적 계산 그래프라고합니다.

1193
01:02:40,209 --> 01:02:42,403
그들 중 하나만.

1194
01:02:42,403 --> 01:02:44,940
그리고 우리는 PyTorch가 우리가 실제로
어디에 있는지 전혀 다른 것을 보았습니다.

1195
01:02:44,940 --> 01:02:46,829
이 새로운 전산 그래프를 구축하고,

1196
01:02:46,829 --> 01:02:48,771
모든 앞으로 패스 에서이 새로운 신선한.

1197
01:02:48,771 --> 01:02:52,259
이를 동적 계산 그래프라고합니다.

1198
01:02:52,259 --> 01:02:54,075
피드 종류의 종류와 함께 단순한 종류의 경우

1199
01:02:54,075 --> 01:02:57,053
신경 네트워크, 그것은 실제로 큰 차이를 만들지 않습니다,

1200
01:02:57,053 --> 01:02:58,467
코드는 비슷하게 끝납니다.

1201
01:02:58,467 --> 01:03:00,225
비슷하게 작동합니다.

1202
01:03:00,225 --> 01:03:02,079
그러나 나는 몇 가지 의미에 대해 조금 이야기하고 싶다.

1203
01:03:02,079 --> 01:03:04,227
정적 대 동적.

1204
01:03:04,227 --> 01:03:07,102
그리고이 둘의 절충안은 무엇입니까?

1205
01:03:07,102 --> 01:03:08,947
정적 그래프를 사용한 멋진 아이디어

1206
01:03:08,947 --> 01:03:11,331
우리가 일종의 건물이기 때문에

1207
01:03:11,331 --> 01:03:15,286
전산 그래프를 한 번 사용한 다음 여러 번 재사용하면,

1208
01:03:15,286 --> 01:03:17,355
프레임 워크에 들어갈 기회가있을 수 있습니다.

1209
01:03:17,355 --> 01:03:19,571
그 그래프에 대한 최적화를 수행합니다.

1210
01:03:19,571 --> 01:03:22,821
그리고 몇 가지 작업을 통합하고, 일부 작업을 재정렬하고,

1211
01:03:22,821 --> 01:03:24,520
가장 효율적인 작동 방법 알아보기

1212
01:03:24,520 --> 01:03:26,809
그 그래프는 정말 효율적 일 수 있습니다.

1213
01:03:26,809 --> 01:03:28,725
그리고 우리는 그 그래프를 재사용 할 것이기 때문에

1214
01:03:28,725 --> 01:03:31,790
여러 번, 아마 그 최적화 과정

1215
01:03:31,790 --> 01:03:33,039
앞면이 비싸다.

1216
01:03:33,039 --> 01:03:34,947
그러나 우리는 그 비용을 스피드 업으로 상각 할 수있다.

1217
01:03:34,947 --> 01:03:37,230
우리가 여러 번 그래프를 실행할 때 우리가 얻은 것.

1218
01:03:37,230 --> 01:03:40,162
그래서 구체적인 예로서,

1219
01:03:40,162 --> 01:03:41,814
어쩌면 회선이있는 그래프를 작성하면

1220
01:03:41,814 --> 01:03:44,085
그리고 후퇴 작전 종류의 하나씩,

1221
01:03:44,085 --> 01:03:48,250
멋진 그래프 최적화 도구가

1222
01:03:48,250 --> 01:03:51,865
실제로 사용자 정의 코드를 내보내는 것처럼
들어가서 실제로 출력 할 수 있습니다.

1223
01:03:51,865 --> 01:03:54,530
운영을 융합시킨, 그 회선을 융합 한

1224
01:03:54,530 --> 01:03:56,371
그리고 relu 그래서 지금
그것은 같은 것을 계산하고있다.

1225
01:03:56,371 --> 01:04:00,525
당신이 쓴 코드처럼, 지금은 될 수 있을지도 몰라.

1226
01:04:00,525 --> 01:04:03,445
보다 효율적으로 실행됩니다.

1227
01:04:03,445 --> 01:04:07,909
그래서 나는 실제로 국가가 정확히
무엇인지 확신하지 못한다.

1228
01:04:07,909 --> 01:04:10,419
TensorFlow 그래프 최적화 중 지금은,

1229
01:04:10,419 --> 01:04:14,469
그러나 적어도 원칙적으로, 이것은 하나의 장소입니다.

1230
01:04:14,469 --> 01:04:17,747
정적 그래프 정말, 당신은 잠재력을 가질 수

1231
01:04:17,747 --> 01:04:20,131
정적 그래프에서이 최적화를 수행

1232
01:04:20,131 --> 01:04:24,298
어쩌면 동적 그래프에서는 다루기가 쉽지 않을 것입니다.

1233
01:04:25,504 --> 01:04:26,937
정적 대 동적에 대한 또 다른 종류의 미묘한 점

1234
01:04:26,937 --> 01:04:28,931
이 직렬화에 대한 아이디어입니다.

1235
01:04:28,931 --> 01:04:32,347
따라서 정적 그래프를 사용하면

1236
01:04:32,347 --> 01:04:34,026
그래프를 작성하는이 코드

1237
01:04:34,026 --> 01:04:35,641
그래프를 만든 후에는

1238
01:04:35,641 --> 01:04:37,667
당신은이 데이터 구조를 나타내는 메모리에

1239
01:04:37,667 --> 01:04:39,571
네트워크의 전체 구조.

1240
01:04:39,571 --> 01:04:41,226
이제 그 데이터 구조를 사용할 수 있습니다.

1241
01:04:41,226 --> 01:04:42,428
디스크에 직렬화하면됩니다.

1242
01:04:42,428 --> 01:04:44,528
이제는 네트워크의 전체 구조를 갖게되었습니다.

1243
01:04:44,528 --> 01:04:45,996
일부 파일에 저장되었습니다.

1244
01:04:45,996 --> 01:04:48,711
그리고 나서 나중에 그걸 로딩 할 수 있습니다.

1245
01:04:48,711 --> 01:04:51,627
그런 다음 액세스하지 않고 계산 그래프를 실행하십시오.

1246
01:04:51,627 --> 01:04:53,630
원래 코드를 작성한 것입니다.

1247
01:04:53,630 --> 01:04:55,450
그래서 이것은 배치 시나리오에서 좋을 것입니다.

1248
01:04:55,450 --> 01:04:57,606
너는 너를 훈련시키고 싶을지도
모른다라고 생각할지도 모른다.

1249
01:04:57,606 --> 01:05:00,424
네트워크가 파이썬에서 작동하기 쉽기 때문에,

1250
01:05:00,424 --> 01:05:01,788
하지만 그 네트워크를 직렬화 한 후에

1251
01:05:01,788 --> 01:05:04,170
이제 C ++로 배포 할 수 있습니다.

1252
01:05:04,170 --> 01:05:06,409
원본을 사용할 필요가없는 환경

1253
01:05:06,409 --> 01:05:07,759
그래프를 작성한 코드.

1254
01:05:07,759 --> 01:05:10,909
그래서 이것은 정적 그래프의 좋은 장점입니다.

1255
01:05:10,909 --> 01:05:12,510
동적 그래프에서는 인터리빙하기 때문에

1256
01:05:12,510 --> 01:05:15,793
이러한 그래프 작성 및 그래프 실행 프로세스,

1257
01:05:15,793 --> 01:05:17,822
항상 원본 코드가 필요합니다.

1258
01:05:17,822 --> 01:05:22,012
당신이 미래에 그 모델을 재사용하고 싶다면.

1259
01:05:22,012 --> 01:05:24,390
반면에 동적 그래프의 몇 가지 장점

1260
01:05:24,390 --> 01:05:26,921
그게 일종의 만드는거야, 그냥 코드를 만든다.

1261
01:05:26,921 --> 01:05:29,163
많은 경우에 훨씬 깨끗하고 훨씬 쉽습니다.

1262
01:05:29,163 --> 01:05:31,264
예를 들어, 우리가

1263
01:05:31,264 --> 01:05:34,501
조건부 연산 값에 따라

1264
01:05:34,501 --> 01:05:37,541
어떤 변수 Z의 경우, 우리는 다른 연산을하고 싶다.

1265
01:05:37,541 --> 01:05:38,624
Y를 계산한다.

1266
01:05:39,723 --> 01:05:42,123
Z가 양수이면 하나의 가중치 행렬을 사용하고,

1267
01:05:42,123 --> 01:05:45,070
Z가 음수이면 다른 가중치 행렬을 사용하려고합니다.

1268
01:05:45,070 --> 01:05:47,981
그리고 우리는이 두 가지 대안
사이에서 전환하기를 원합니다.

1269
01:05:47,981 --> 01:05:50,720
우리가 동적 그래프를 사용하기 때문에 PyTorch에서,

1270
01:05:50,720 --> 01:05:52,011
그것은 매우 간단합니다.

1271
01:05:52,011 --> 01:05:54,101
귀하의 코드 종류가 예상했던 것과 똑같습니다.

1272
01:05:54,101 --> 01:05:56,400
너가 니피에서 뭘 할거야.

1273
01:05:56,400 --> 01:05:58,877
일반 파이썬 제어 흐름을 사용할 수 있습니다.

1274
01:05:58,877 --> 01:06:00,795
이 일을 처리합니다.

1275
01:06:00,795 --> 01:06:03,264
그리고 우리는 매번 그래프를 작성하기 때문에,

1276
01:06:03,264 --> 01:06:05,563
이 작업을 수행 할 때마다

1277
01:06:05,563 --> 01:06:08,021
어쩌면 다른 그래프를 만들지.

1278
01:06:08,021 --> 01:06:10,864
각 순회 패스에서,하지만 우리가하는 모든 그래프에 대해

1279
01:06:10,864 --> 01:06:13,104
우리가 그것을 전파 할 수있게된다.

1280
01:06:13,104 --> 01:06:14,337
잘 됐네.

1281
01:06:14,337 --> 01:06:15,941
코드는 매우 깨끗하고 작업하기 쉽습니다.

1282
01:06:15,941 --> 01:06:18,843
이제는 TensorFlow에서
상황이 조금 더 나아졌습니다.

1283
01:06:18,843 --> 01:06:23,201
우리는 그래프를 한 번 빌드하기 때문에 복잡합니다.

1284
01:06:23,201 --> 01:06:25,219
이 제어 흐름 연산자 종류가 필요합니다.

1285
01:06:25,219 --> 01:06:28,400
TensorFlow 그래프의 명시 적 연산자입니다.

1286
01:06:28,400 --> 01:06:31,301
그리고 이제는 우리가 이것을 볼 수 있습니다.

1287
01:06:31,301 --> 01:06:34,319
TensorFlow 버전과 같은 종류의 tf.cond 호출

1288
01:06:34,319 --> 01:06:36,818
if 문을 사용하지만 이제는 구운 것입니다.

1289
01:06:36,818 --> 01:06:38,838
일종의 계산 그래프를 사용하는 것보다

1290
01:06:38,838 --> 01:06:40,741
파이썬 제어 흐름.

1291
01:06:40,741 --> 01:06:43,473
그리고 문제는 그래프 만 작성하기 때문입니다.

1292
01:06:43,473 --> 01:06:46,123
일단, 제어 흐름의 모든 잠재적 인 경로는

1293
01:06:46,123 --> 01:06:48,729
우리의 프로그램은 구워야 할 필요가있을 수 있습니다.

1294
01:06:48,729 --> 01:06:51,200
우리가 전에 그래프를 만들 때

1295
01:06:51,200 --> 01:06:52,523
그것을 실행하십시오.

1296
01:06:52,523 --> 01:06:54,353
그래서 모든 종류의 제어 흐름 연산자

1297
01:06:54,353 --> 01:06:58,394
파이썬 제어 흐름이 아니길 바랄뿐입니다.

1298
01:06:58,394 --> 01:07:00,409
통신 수, 당신은 어떤 종류의 마술을 사용할 필요가 있고,

1299
01:07:00,409 --> 01:07:03,360
제어 흐름을 수행하는 특수한 텐서 흐름 연산.

1300
01:07:03,360 --> 01:07:05,527
이 경우이 tf.cond.

1301
01:07:06,713 --> 01:07:09,400
비슷한 상황이 발생하면

1302
01:07:09,400 --> 01:07:10,763
루프가있다.

1303
01:07:10,763 --> 01:07:12,684
그래서 우리가 어떤 종류의 재발을
계산하기를 원한다고 가정 해보자.

1304
01:07:12,684 --> 01:07:16,607
관계 여기서 Y T는 Y T가 1을 뺀 것과 같습니다.

1305
01:07:16,607 --> 01:07:19,839
더하기 X T는 가중치 행렬 W를 곱해서

1306
01:07:19,839 --> 01:07:23,077
우리가 이것을 할 때마다, 우리가 이것을 계산할 때마다,

1307
01:07:23,077 --> 01:07:26,436
우리는 다른 크기의 데이터 시퀀스를 가질 수 있습니다.

1308
01:07:26,436 --> 01:07:28,265
그리고 우리의 데이터 순서의 길이에 관계없이,

1309
01:07:28,265 --> 01:07:30,217
이 반복 관계를 계산하고 싶습니다.

1310
01:07:30,217 --> 01:07:33,371
입력 시퀀스의 크기에 관계없이

1311
01:07:33,371 --> 01:07:35,796
그래서 PyTorch에서 이것은 매우 쉽습니다.

1312
01:07:35,796 --> 01:07:39,489
우리는 Python에서 일반 for
루프를 사용할 수 있습니다.

1313
01:07:39,489 --> 01:07:41,534
우리가 원하는 횟수만큼 반복하면됩니다.

1314
01:07:41,534 --> 01:07:44,436
unroll 그리고 지금은 입력 데이터의 크기에 따라,

1315
01:07:44,436 --> 01:07:47,095
우리의 계산 그래프는 다른 크기로 끝날 것입니다.

1316
01:07:47,095 --> 01:07:49,737
하지만 괜찮아요, 우리는 다시 전파 할 수 있습니다.

1317
01:07:49,737 --> 01:07:51,694
각 하나, 한 번에 하나씩.

1318
01:07:51,694 --> 01:07:55,782
이제는 TensorFlow에서 조금 더 못 생겼습니다.

1319
01:07:55,782 --> 01:07:58,494
그리고 다시, 우리는 그래프를 구성해야하기 때문에

1320
01:07:58,494 --> 01:08:02,398
모두 한 번에 앞에,이 제어 흐름 루핑 구조

1321
01:08:02,398 --> 01:08:06,364
다시 TensorFlow 그래프의
명시 적 노드 여야합니다.

1322
01:08:06,364 --> 01:08:08,084
그래서 당신의 함수 프로그래밍을 기억하기 바란다.

1323
01:08:08,084 --> 01:08:10,432
이러한 종류의 연산자를 사용해야하기 때문에

1324
01:08:10,432 --> 01:08:13,517
TensorFlow에서 루핑 구문을 구현합니다.

1325
01:08:13,517 --> 01:08:16,292
이 경우,이 특정한 반복 관계

1326
01:08:16,292 --> 01:08:18,857
foldl 조작을 사용하여 패스 할 수 있습니다.

1327
01:08:18,857 --> 01:08:23,024
foldl의 관점에서이 특정 루프를 구현합니다.

1328
01:08:24,100 --> 01:08:26,200
그러나 이것이 기본적으로 의미하는 것은
당신이이 감각을 가지고 있다는 것입니다.

1329
01:08:26,201 --> 01:08:28,734
TensorFlow는 거의 자체 전체를 구축하고 있습니다.

1330
01:08:28,734 --> 01:08:31,214
프로그래밍 언어,의 언어 사용

1331
01:08:31,214 --> 01:08:33,212
전산 그래프.

1332
01:08:33,212 --> 01:08:34,821
그리고 모든 종류의 제어 흐름 연산자,

1333
01:08:34,821 --> 01:08:37,215
또는 모든 종류의 데이터 구조를 롤백해야합니다.

1334
01:08:37,215 --> 01:08:40,014
계산 그래프에 넣으세요.

1335
01:08:40,014 --> 01:08:42,595
모든 필수적인 패러다임을 명령 적으로 사용하라.

1336
01:08:42,595 --> 01:08:44,216
파이썬.

1337
01:08:44,216 --> 01:08:46,195
전체 집합으로 다시 학습해야 할 필요가 있습니다.

1338
01:08:46,196 --> 01:08:47,956
제어 흐름 연산자.

1339
01:08:47,956 --> 01:08:49,991
그리고 어떤 종류의 제어 흐름을 원한다면

1340
01:08:49,991 --> 01:08:52,804
TensorFlow를 사용하여 계산 그래프 내부에서

1341
01:08:52,804 --> 01:08:56,252
그래서 적어도 저에게는 혼란 스럽습니다.

1342
01:08:56,252 --> 01:08:58,238
내 머리를 때로는 감싸기가 힘듭니다.

1343
01:08:58,238 --> 01:09:01,259
필자는 PyTorch 동적 그래프를
사용하는 것과 비슷합니다.

1344
01:09:01,259 --> 01:09:03,555
당신은 당신이 좋아하는 명령형
프로그래밍을 사용할 수 있습니다.

1345
01:09:03,555 --> 01:09:06,722
모든 구문이 잘 작동합니다.

1346
01:09:07,737 --> 01:09:12,051
그런데 실제로는 아주 새로운 라이브러리가 있습니다.

1347
01:09:12,051 --> 01:09:15,732
이 중 하나 인 TensorFlow Fold라고 불리는

1348
01:09:15,732 --> 01:09:17,662
TensorFlow의 맨 위에있는 레이어를 사용하면

1349
01:09:17,662 --> 01:09:21,579
동적 그래프, 자신 만의 코드 작성 가능

1350
01:09:22,416 --> 01:09:24,986
TensorFlow Fold를
사용하여 동적 인 것처럼 보입니다.

1351
01:09:24,986 --> 01:09:27,977
그래프 조작 후 TensorFlow
Fold가 마법을 수행합니다.

1352
01:09:27,977 --> 01:09:30,617
당신을 위해 그리고 어떻게 든 그것을

1353
01:09:30,617 --> 01:09:32,277
정적 TensorFlow 그래프.

1354
01:09:32,277 --> 01:09:35,225
이것은 제시되고있는 초 신종 종이입니다

1355
01:09:35,225 --> 01:09:37,357
이번 주에는 프랑스에서 ICLR에서 열렸습니다.

1356
01:09:37,358 --> 01:09:39,737
그래서 나는 다이빙을하고 놀 기회를 얻지 못했습니다.

1357
01:09:39,737 --> 01:09:41,694
아직 이걸로.

1358
01:09:41,694 --> 01:09:44,252
그러나 나의 처음 인상은 그것이
약간을 더한다라는 것이었다.

1359
01:09:44,252 --> 01:09:46,455
TensorFlow에 대한 동적 그래프의 양

1360
01:09:46,455 --> 01:09:48,798
일종의 원주민보다 일하는 것이 조금 더 어색하다.

1361
01:09:48,798 --> 01:09:51,952
PyTorch에있는 동적 그래프.

1362
01:09:51,952 --> 01:09:54,527
그럼, 동기 부여하는 것이 좋을 거라 생각 했어.

1363
01:09:54,527 --> 01:09:57,257
왜 우리는 일반적으로 동적 그래프에 관심이 있습니까?

1364
01:09:57,257 --> 01:10:00,257
따라서 하나의 옵션은 반복적 인 네트워크입니다.

1365
01:10:01,177 --> 01:10:03,256
그래서 당신은 이미지 캡션과 같은 것을 볼 수 있습니다.

1366
01:10:03,256 --> 01:10:05,715
우리는 재발하는 네트워크를 사용합니다.

1367
01:10:05,715 --> 01:10:07,612
길이가 다른 시퀀스.

1368
01:10:07,612 --> 01:10:10,798
이 경우 생성하려는 문장

1369
01:10:10,798 --> 01:10:13,337
캡션은 시퀀스이고 그 시퀀스는 다를 수 있으므로

1370
01:10:13,337 --> 01:10:15,636
입력 데이터에 따라 다릅니다.

1371
01:10:15,636 --> 01:10:18,414
이제 우리는이 역 동성을 사물에서 볼 수 있습니다.

1372
01:10:18,414 --> 01:10:21,694
문장의 크기에 따라,

1373
01:10:21,694 --> 01:10:24,136
우리의 전산 그래프는 더 많은 것을 가질 필요가 있습니다.

1374
01:10:24,136 --> 01:10:25,716
또는 더 적은 수의 요소.

1375
01:10:25,716 --> 01:10:29,920
그래서 이것은 동적 그래프의 공통된 적용의 한 종류입니다.

1376
01:10:29,920 --> 01:10:34,115
지난 분기에 CS224N을 가져간 분들의 경우,

1377
01:10:34,115 --> 01:10:36,377
재귀 네트워크에 대한이 아이디어를 보았습니다.

1378
01:10:36,377 --> 01:10:38,674
때때로 자연 언어 처리에서

1379
01:10:38,674 --> 01:10:41,316
예를 들어 파싱 된 트리를 계산할 수 있습니다.

1380
01:10:41,316 --> 01:10:43,934
문장을 읽은 다음 신경을 갖기를 원한다.

1381
01:10:43,934 --> 01:10:47,337
네트워크 종류의이 파스 트리를 재귀 적으로 작동합니다.

1382
01:10:47,337 --> 01:10:49,291
그래서 신경망을 가지고 그런 종류의 작품,

1383
01:10:49,291 --> 01:10:51,817
계층의 순차적 인 순서가 아니라,

1384
01:10:51,817 --> 01:10:54,516
그러나 대신 그것은 어떤 그래프를 통해 일종의 일을한다.

1385
01:10:54,516 --> 01:10:56,856
또는 트리 구조 대신 이제 각 데이터 요소

1386
01:10:56,856 --> 01:10:58,732
다른 그래프 또는 트리 구조를 가질 수 있습니다.

1387
01:10:58,732 --> 01:11:00,756
그래서 계산 그래프의 구조

1388
01:11:00,756 --> 01:11:03,194
종류는 입력 데이터의 구조를 반영합니다.

1389
01:11:03,194 --> 01:11:05,714
데이터 포인트마다 다를 수 있습니다.

1390
01:11:05,714 --> 01:11:07,934
그래서 이런 종류의 일은 복잡해 보이고

1391
01:11:07,934 --> 01:11:10,316
TensorFlow를 사용하여 구현하기에 털이 많습니다.

1392
01:11:10,316 --> 01:11:12,478
그러나 PyTorch에서 당신은 단지 종류의 사용을 할 수 있습니다.

1393
01:11:12,478 --> 01:11:14,054
정상적인 파이썬 제어 흐름과 같이 작동합니다.

1394
01:11:14,054 --> 01:11:14,887
잘 됐네.

1395
01:11:16,574 --> 01:11:19,198
더 많은 연구 응용 프로그램의 또
다른 비트가 정말이 있습니다

1396
01:11:19,198 --> 01:11:21,614
신경 이완 네트워크라고하는 멋진 아이디어.

1397
01:11:21,614 --> 01:11:23,678
시각적 인 질문 응답.

1398
01:11:23,678 --> 01:11:26,718
여기서 우리는 몇 가지 질문을하고 싶습니다.

1399
01:11:26,718 --> 01:11:29,278
이 이미지를 입력 할 수있는 이미지에 대해

1400
01:11:29,278 --> 01:11:31,737
고양이와 개들에 대해서는 몇 가지 질문이 있습니다.

1401
01:11:31,737 --> 01:11:34,756
어떤 색깔이 고양이이고, 그 다음에는 내부적으로

1402
01:11:34,756 --> 01:11:37,614
그 질문을 읽을 수 있고, 그것들은이
다른 것들을 가지고 있습니다.

1403
01:11:37,614 --> 01:11:39,758
수행을위한 특수화 된 신경망 모듈

1404
01:11:39,758 --> 01:11:43,594
색을 묻고 고양이를 찾는 것과 같은 작업.

1405
01:11:43,594 --> 01:11:45,915
그리고 질문의 텍스트에 따라,

1406
01:11:45,915 --> 01:11:48,193
응답을 위해이 사용자 지정 아키텍처를
컴파일 할 수 있습니다.

1407
01:11:48,193 --> 01:11:49,838
질문.

1408
01:11:49,838 --> 01:11:52,294
그리고 지금 우리가 다른 질문을한다면,

1409
01:11:52,294 --> 01:11:55,094
개보다 고양이가 더 많습니까?

1410
01:11:55,094 --> 01:11:58,241
이제 우리는 아마 같은 기본 모듈
세트를 가지고있을 것입니다.

1411
01:11:58,241 --> 01:12:00,534
고양이와 개를 발견하고 세는 것,

1412
01:12:00,534 --> 01:12:03,076
그러나 그들은 다른 순서로 정렬됩니다.

1413
01:12:03,076 --> 01:12:05,177
그래서 우리는이 역 동성을 다른 데이터 포인트

1414
01:12:05,177 --> 01:12:07,716
다른 계산 그래프가 생길 수 있습니다.

1415
01:12:07,716 --> 01:12:09,635
하지만 이것은 좀 더 연구적인 것입니다.

1416
01:12:09,635 --> 01:12:12,574
어쩌면 지금은 그렇게 주요 스트림이 아닐 수도 있습니다.

1417
01:12:12,574 --> 01:12:15,037
그러나 좀 더 큰 요점으로, 나는 생각한다.

1418
01:12:15,037 --> 01:12:17,198
사람들이 좋아하는 멋지고 창의적인 응용 프로그램

1419
01:12:17,198 --> 01:12:19,214
동적 계산 그래프로 할 수있다.

1420
01:12:19,214 --> 01:12:21,577
어쩌면 지금은 그렇게 많지 않을 수도 있습니다.

1421
01:12:21,577 --> 01:12:23,471
단지 그들과 함께 일하는 것은
너무 고통 스럽기 때문입니다.

1422
01:12:23,471 --> 01:12:25,577
그래서 저는 많은 기회가 있다고 생각합니다.

1423
01:12:25,577 --> 01:12:27,396
시원하고 창조적 인 일을하기 때문에

1424
01:12:27,396 --> 01:12:30,596
동적 계산 그래프.

1425
01:12:30,596 --> 01:12:32,297
그리고 멋진 아이디어가 떠오르면,

1426
01:12:32,297 --> 01:12:34,078
우리는 내년에 강의에서 그 특징을 다룰 것입니다.

1427
01:12:34,078 --> 01:12:37,612
그래서 저는 Caffe에 대해 아주
간단히 이야기하고 싶었습니다.

1428
01:12:37,612 --> 01:12:39,854
버클리에서이 프레임 워크입니다.

1429
01:12:39,854 --> 01:12:43,795
어느 Caffe가 다른 Caffe와 다소 다른가요?

1430
01:12:43,795 --> 01:12:45,774
깊은 학습 프레임 워크는 많은 경우에

1431
01:12:45,774 --> 01:12:47,454
당신은 실제로 글쓰기없이 네트워크를 훈련시킬 수 있습니다.

1432
01:12:47,454 --> 01:12:48,815
모든 코드를 직접 작성하십시오.

1433
01:12:48,815 --> 01:12:50,798
이런 기존의 바이너리를 호출하는 것만으로도 충분합니다.

1434
01:12:50,798 --> 01:12:53,214
구성 파일을 설정하고 많은 경우에

1435
01:12:53,214 --> 01:12:56,697
자신의 코드를 작성하지 않고도
데이터를 학습 할 수 있습니다.

1436
01:12:56,697 --> 01:13:00,078
먼저 데이터를 변환하고

1437
01:13:00,078 --> 01:13:03,054
HDF5 또는 LMDB와 같은 형식으로

1438
01:13:03,054 --> 01:13:06,014
Caffe 내부의 일부 스크립트는
다음과 같이 변환 할 수 있습니다.

1439
01:13:06,014 --> 01:13:08,638
이미지 및 텍스트 파일을이 형식으로 변환 할 수 있습니다.

1440
01:13:08,638 --> 01:13:12,537
코드를 작성하는 대신 정의해야합니다.

1441
01:13:12,537 --> 01:13:14,478
계산 그래프의 구조를 정의하려면,

1442
01:13:14,478 --> 01:13:17,414
대신 prototxt라는 텍스트 파일을 편집합니다.

1443
01:13:17,414 --> 01:13:19,934
이는 계산 그래프의 구조를 설정합니다.

1444
01:13:19,934 --> 01:13:22,997
여기 구조는 우리가 어떤 입력으로부터 읽는 것입니다.

1445
01:13:22,997 --> 01:13:26,537
HDF5 파일, 우리는 일부 내부 제품을 수행,

1446
01:13:26,537 --> 01:13:28,974
우리는 약간의 손실과 전체 구조를 계산한다.

1447
01:13:28,974 --> 01:13:30,875
이 텍스트 파일에 그래프 그래프가 설정됩니다.

1448
01:13:30,875 --> 01:13:33,653
한 가지 종류의 단점은 이러한 파일

1449
01:13:33,653 --> 01:13:35,956
매우 큰 네트워크의 경우 정말 추악해질 수 있습니다.

1450
01:13:35,956 --> 01:13:38,455
그래서 152 레이어 ResNet 모델과 같은 것을 위해,

1451
01:13:38,455 --> 01:13:41,535
그건 원래 Caffe에서 훈련 받았어.

1452
01:13:41,535 --> 01:13:44,253
이 prototxt 파일은 거의 7000 줄에 이릅니다.

1453
01:13:44,253 --> 01:13:47,278
그래서 사람들은 이것을 손으로 쓰지 않습니다.

1454
01:13:47,278 --> 01:13:49,887
사람들은 때로는 파이썬 스크립트 작성을 원할 것입니다.

1455
01:13:49,887 --> 01:13:51,817
prototxt 파일을 생성합니다.

1456
01:13:51,817 --> 01:13:53,275
[웃음]

1457
01:13:53,275 --> 01:13:55,154
그렇다면 당신은 자신의 것을 굴리는 영역에 친절합니다.

1458
01:13:55,154 --> 01:13:56,318
전산 그래프 추상화.

1459
01:13:56,318 --> 01:13:58,974
그것은 아마 좋은 생각이 아니지만
이전에 그것을 보았습니다.

1460
01:13:58,974 --> 01:14:02,238
그런 다음, 옵티 마이저 오브젝트를 갖는 것보다는,

1461
01:14:02,238 --> 01:14:05,316
대신에 어떤 솔버가 있습니다. 솔버를 정의합니다.

1462
01:14:05,316 --> 01:14:07,497
다른 prototxt 안에.

1463
01:14:07,497 --> 01:14:09,118
이것은 학습 속도를 정의하며,

1464
01:14:09,118 --> 01:14:11,036
귀하의 최적화 알고리즘 및 이것 저것.

1465
01:14:11,036 --> 01:14:12,334
그리고 나서이 모든 일을하면

1466
01:14:12,334 --> 01:14:14,174
train 명령으로 Caffe
바이너리를 실행할 수 있습니다.

1467
01:14:14,174 --> 01:14:17,278
모든 것이 마술처럼 일어납니다.

1468
01:14:17,278 --> 01:14:19,577
카피 (Cafee)에는 일련의 사전
모델링 된 모델 동물원이 있으며,

1469
01:14:19,577 --> 01:14:21,294
그건 꽤 유용합니다.

1470
01:14:21,294 --> 01:14:23,454
Caffe는 파이썬 인터페이스를
가지고 있지만 슈퍼가 아닙니다.

1471
01:14:23,454 --> 01:14:25,438
잘 기록 된.

1472
01:14:25,438 --> 01:14:27,358
파이썬의 소스 코드를 읽을 필요가 있습니다.

1473
01:14:27,358 --> 01:14:29,017
인터페이스를 사용하여 수행 할 수있는 작업을 확인하고,

1474
01:14:29,017 --> 01:14:30,116
그래서 그것은 짜증나게합니다.

1475
01:14:30,116 --> 01:14:31,455
그러나 그것은 효과가있다.

1476
01:14:31,455 --> 01:14:35,622
Caffe에 관한 나의 일반적인 점은,

1477
01:14:36,596 --> 01:14:38,334
어쩌면 피드 포워드 모델에 좋을 것입니다.

1478
01:14:38,334 --> 01:14:40,174
프로덕션 시나리오에 유용 할 수 있습니다.

1479
01:14:40,174 --> 01:14:42,796
파이썬에 의존하지 않기 때문입니다.

1480
01:14:42,796 --> 01:14:45,075
하지만 요즘 연구를 위해 아마도
Caffe를 보았을 것입니다.

1481
01:14:45,075 --> 01:14:47,358
어쩌면 조금 덜 사용되었을 수도 있습니다.

1482
01:14:47,358 --> 01:14:49,597
비록 그것이 아직도 꽤 일반적으로 사용된다고 생각하지만

1483
01:14:49,597 --> 01:14:51,417
생산을 위해 업계에서 다시.

1484
01:14:51,417 --> 01:14:54,410
나는 Caffe 2에서 하나의 슬라이드,
하나 또는 두 개의 슬라이드를 약속합니다.

1485
01:14:54,410 --> 01:14:58,596
그래서 Caffe 2는 Facebook의
Caffe의 후계자입니다.

1486
01:14:58,596 --> 01:15:02,432
그것은 슈퍼 새로운, 그것은 일주일 전에 릴리스되었습니다.

1487
01:15:02,432 --> 01:15:04,436
[웃음]

1488
01:15:04,436 --> 01:15:06,617
그래서 저는 수퍼를 형성 할 시간이 없었습니다.

1489
01:15:06,617 --> 01:15:09,314
Caffe 2에 대한 교육받은 의견은 아직,

1490
01:15:09,314 --> 01:15:12,318
TensorFlow와 비슷한 정적 그래프를 사용합니다.

1491
01:15:12,318 --> 01:15:15,198
Caffe와 같은 종류의 핵심은 C ++로 작성되었습니다.

1492
01:15:15,198 --> 01:15:17,817
그들은 파이썬 인터페이스를 가지고 있습니다.

1493
01:15:17,817 --> 01:15:19,694
차이점은 이제 더 이상 필요가 없다는 것입니다.

1494
01:15:19,694 --> 01:15:21,518
자신의 파이썬 스크립트를 작성하여
prototxt 파일을 생성하십시오.

1495
01:15:21,518 --> 01:15:25,312
계산 그래프 구조를 정의 할 수 있습니다.

1496
01:15:25,312 --> 01:15:28,170
파이썬에서는 모두 보이는 API를 사용하여 찾고 있습니다.

1497
01:15:28,170 --> 01:15:29,657
TensorFlow와 같은 종류입니다.

1498
01:15:29,657 --> 01:15:31,854
하지만 그 다음에 침을 뱉을 수 있습니다.
이것을 직렬화 할 수 있습니다.

1499
01:15:31,854 --> 01:15:34,596
전산 그래프 구조를 prototxt 파일로 변환합니다.

1500
01:15:34,596 --> 01:15:36,777
그리고 일단 당신의 모델이 훈련되면 이것 저것,

1501
01:15:36,777 --> 01:15:38,676
우리는 정적에 대해 이야기 한이 유익을 얻습니다.

1502
01:15:38,676 --> 01:15:41,257
그래프는 할 수 있습니다. 원본은 필요 없습니다.

1503
01:15:41,257 --> 01:15:43,534
훈련 된 모델을 배치하기위한 교육 코드를 제공합니다.

1504
01:15:43,534 --> 01:15:46,958
그래서 흥미로운 점은 Google을 보았다는 것입니다.

1505
01:15:46,958 --> 01:15:49,417
어쩌면 하나의 중요한 깊은 실행
프레임 워크를 가질 수 있습니다.

1506
01:15:49,417 --> 01:15:52,094
TensorFlow는 페이스 북이이
두 가지를 가지고 있으며,

1507
01:15:52,094 --> 01:15:53,761
PyTorch와 Caffe 2.

1508
01:15:54,596 --> 01:15:57,252
그래서 이들은 서로 다른 철학입니다.

1509
01:15:57,252 --> 01:15:59,751
Google이 규칙으로 하나의 프레임
워크를 구축하려고 시도하고 있습니다.

1510
01:15:59,751 --> 01:16:01,569
가능한 모든 시나리오에서 작동하는 모든 것

1511
01:16:01,569 --> 01:16:02,847
깊은 학습을 위해.

1512
01:16:02,847 --> 01:16:04,609
이것은 모든 노력을 통합하기 때문에 친절합니다.

1513
01:16:04,609 --> 01:16:06,209
하나의 프레임 워크에.

1514
01:16:06,209 --> 01:16:07,852
그것은 당신이 오직 한 가지를 배울 필요가 있음을 의미합니다.

1515
01:16:07,852 --> 01:16:09,464
다양한 시나리오에서 작동합니다.

1516
01:16:09,464 --> 01:16:11,708
분산 시스템, 생산,

1517
01:16:11,708 --> 01:16:13,772
배치, 모바일, 연구, 모든 것.

1518
01:16:13,772 --> 01:16:15,706
이러한 모든 일을 수행하는 하나의
프레임 워크를 배울 필요가 있습니다.

1519
01:16:15,706 --> 01:16:18,151
반면 페이스 북은 조금 다른 접근 방식을 취하고있다.

1520
01:16:18,151 --> 01:16:20,849
PyTorch가 정말로 더 전문화 된 곳에서,

1521
01:16:20,849 --> 01:16:23,591
글쓰기의 관점에서 연구에 더욱 초점을 맞 춥니 다.

1522
01:16:23,591 --> 01:16:26,071
연구 코드를 작성하고 아이디어를
신속하게 반복 할 수 있습니다.

1523
01:16:26,071 --> 01:16:27,948
그것은 PyTorch에서 매우 쉽지만,

1524
01:16:27,948 --> 01:16:30,869
프로덕션에서 실행, 모바일 장치에서 실행,

1525
01:16:30,869 --> 01:16:32,951
PyTorch는 많은 도움을받지 못합니다.

1526
01:16:32,951 --> 01:16:35,210
대신, Caffe 2는 더 많은
것을 목표로 개발되었습니다.

1527
01:16:35,210 --> 01:16:37,710
생산 지향 유스 케이스.

1528
01:16:39,567 --> 01:16:42,929
그래서 내 일반적인 연구, 제 장군, 전반적인 조언

1529
01:16:42,929 --> 01:16:45,409
어떤 프레임 워크가 어떤 문제에 사용되는지

1530
01:16:45,409 --> 01:16:47,350
그 두 종류 다,

1531
01:16:47,350 --> 01:16:50,172
나는 TensorFlow가 단지
대략 안전한 내기라고 생각합니다.

1532
01:16:50,172 --> 01:16:53,510
새로운 프로젝트를 시작하고 싶은 프로젝트는 무엇입니까?

1533
01:16:53,510 --> 01:16:56,168
그것들을 모두 다스리는 하나의
프레임 워크의 일종이기 때문에,

1534
01:16:56,168 --> 01:16:58,849
그것은 거의 모든 상황을 위해 사용될 수 있습니다.

1535
01:16:58,849 --> 01:17:01,166
그러나, 당신은 아마 그것과 함께 쌍을해야합니다

1536
01:17:01,166 --> 01:17:03,510
높은 수준의 래퍼 및 동적 그래프가 필요한 경우,

1537
01:17:03,510 --> 01:17:05,207
어쩌면 운이 좋을 수도 있습니다.

1538
01:17:05,207 --> 01:17:07,152
코드 중 일부가 조금 더 못 생겼습니다.

1539
01:17:07,152 --> 01:17:10,226
내 의견으로는,하지만 어쩌면 그것은
일종의 코스메틱 디테일입니다.

1540
01:17:10,226 --> 01:17:13,190
그다지 중요하지 않습니다.

1541
01:17:13,190 --> 01:17:15,809
필자는 개인적으로 PyTorch가
연구에 정말 유용하다고 생각합니다.

1542
01:17:15,809 --> 01:17:18,675
연구 코드 작성에만 집중한다면,

1543
01:17:18,675 --> 01:17:21,233
PyTorch는 훌륭한 선택이라고 생각합니다.

1544
01:17:21,233 --> 01:17:23,830
그러나 조금 더 새롭고 지역 사회 지원이 적으며,

1545
01:17:23,830 --> 01:17:25,649
코드가 적어서 약간의 모험이 될 수 있습니다.

1546
01:17:25,649 --> 01:17:28,412
잘 밟은 길을 더 원한다면, TensorFlow

1547
01:17:28,412 --> 01:17:29,969
더 나은 선택 일 수 있습니다.

1548
01:17:29,969 --> 01:17:32,365
프로덕션 배포에 관심이있는 경우,

1549
01:17:32,365 --> 01:17:34,710
Caffe, Caffe 2 또는
TensorFlow를보아야합니다.

1550
01:17:34,710 --> 01:17:37,017
모바일 배포에 중점을 두는 경우,

1551
01:17:37,017 --> 01:17:39,312
나는 TensorFlow와 Caffe 2 둘
다 일부가 내장되어 있다고 생각합니다.

1552
01:17:39,312 --> 01:17:41,270
그 지원.

1553
01:17:41,270 --> 01:17:43,325
불행히도, 그런 것은 아닙니다.

1554
01:17:43,325 --> 01:17:45,009
하나의 글로벌 베스트 프레임 워크, 일종의 의존

1555
01:17:45,009 --> 01:17:47,393
네가 실제로하려는 일에

1556
01:17:47,393 --> 01:17:49,212
예상하는 응용 프로그램은 무엇입니까?

1557
01:17:49,212 --> 01:17:52,045
그 것들에 대한 나의 일반적인 충고.

1558
01:17:53,169 --> 01:17:55,691
다음 번에는 몇 가지 사례 연구에 대해 이야기하겠습니다.

1559
01:17:55,691 --> 00:00:00,000
다양한 CNN 아키텍처에 대해

