1
00:00:06,971 --> 00:00:10,106
CS231n 두번째 시간에 오신걸 환영합니다.

2
00:00:10,106 --> 00:00:12,646
화요일에 했던 것을 되짚어보자면

3
00:00:12,646 --> 00:00:15,098
컴퓨터비전이 무엇인지,

4
00:00:15,098 --> 00:00:16,187
역사는 어떠했는지,

5
00:00:16,187 --> 00:00:18,101
그리고 이 수업에 대해 다뤘었습니다.

6
00:00:18,101 --> 00:00:21,451
그리고 오늘은 본격적으로 수업을
진행하는 첫 날입니다.

7
00:00:21,451 --> 00:00:23,339
그리고 실생활에서

8
00:00:23,339 --> 00:00:27,864
우리가 배울 알고리즘이 정확히 어떻게
적용되는지를 아주 깊게 살펴 볼 것입니다.

9
00:00:27,864 --> 00:00:31,927
강의의 처음에는 큰 흐름을 다룰 것이고,

10
00:00:31,927 --> 00:00:35,574
그리고 대부분의 수업에서는 좀 더 깊은 
내용을 다룰 것이며

11
00:00:35,574 --> 00:00:39,617
다양한 알고리즘들의 
세부적인 매커니즘에 초점을 맞출 것입니다.

12
00:00:39,617 --> 00:00:43,419
오늘은 이제 첫 학습 알고리즘을 살펴볼 것인데요,
아마 재밌을 것입니다, 제 생각에는요.

13
00:00:43,419 --> 00:00:47,535
일단 진행에 앞서 몇가지 공지사항을 알려드리겠습니다.

14
00:00:47,535 --> 00:00:48,785
첫 번째는 Piazza에 관한 것인데요

15
00:00:49,645 --> 00:00:51,580
어제 확인했을때

16
00:00:51,580 --> 00:00:55,357
Piazza를 가입한 인원이 500명쯤
되었던 것으로 기억합니다.

17
00:00:55,357 --> 00:00:58,493
그리고 그것은 100여명 가량이
아직 가입을 하지 않았다는 것이지요

18
00:00:58,493 --> 00:01:01,457
저는 Piazza가 학생과 조교들의

19
00:01:01,457 --> 00:01:04,221
주요 의사소통 수단이 되었으면 좋겠습니다.

20
00:01:04,221 --> 00:01:07,401
프로젝트 아이디어나, 중간고사, 컨퍼런스와 관련된 질문이

21
00:01:07,401 --> 00:01:12,841
조교들의 메일로 너무 많이 오고 있습니다.

22
00:01:12,841 --> 00:01:16,138
그리고 이런 종류의 질문은 Piazza에서
이루어져야만 합니다.

23
00:01:16,138 --> 00:01:21,568
모든 조교들이 Piazza를  수시로 확인하고 있기 때문에, 
Piazza로 질문을 하는것이 훨씬 더 빠를 것입니다.

24
00:01:21,568 --> 00:01:26,234
이메일로 보내면 여러 메일 리스트랑 섞여서
답장을 못 할 수도 있습니다.

25
00:01:26,234 --> 00:01:28,983
또한 SCPD 학생들 중 일부는

26
00:01:28,983 --> 00:01:33,629
Piazza 가입에 어려움을 겪고 있습니다.

27
00:01:33,629 --> 00:01:37,791
SCPD 학생들은 @ stanford.edu 이메일을
부여 받아야 합니다.

28
00:01:38,804 --> 00:01:40,733
이메일 주소를 부여받으면

29
00:01:40,733 --> 00:01:44,276
그때 Stanford 이메일로 Piazaa에
가입할 수 있습니다.

30
00:01:44,276 --> 00:01:46,908
아마 이 문제는 지금 여기 앉아있는 분들은
상관 없겠지만

31
00:01:46,908 --> 00:01:50,408
SCPD로 듣고있는 학생들에게 말씀드리는 것입니다.

32
00:01:52,191 --> 00:01:55,643
다음 사항은 바로 과제에 관한 것입니다.

33
00:01:55,643 --> 00:01:58,178
과제 1이 오늘 늦게 업로드 될 것입니다.

34
00:01:58,178 --> 00:01:59,517
아마 오후가 될 것 같습니다만

35
00:01:59,517 --> 00:02:03,043
오늘 자기전에는 꼭 과제가 올라갈 것임을
약속드립니다.

36
00:02:03,043 --> 00:02:04,589
하지만 여러분이 조금 불안하고

37
00:02:04,589 --> 00:02:07,304
지금 당장 과제를 하고싶다면

38
00:02:07,304 --> 00:02:10,531
과제 1의 작년버전을 찾을 수 있을 것입니다.

39
00:02:10,531 --> 00:02:12,684
작년 버전도 거의 똑같습니다.

40
00:02:12,684 --> 00:02:15,081
단지 아주 조금만 손 보고 있습니다.

41
00:02:15,081 --> 00:02:19,082
예를 들어 Python 2.7버전에서 
Python3으로 업그레이드하는것 등이죠

42
00:02:19,082 --> 00:02:20,943
그리고 아주 작은 외관의 변화가 있고

43
00:02:20,943 --> 00:02:24,742
내용만 보면 작년과 같습니다.

44
00:02:24,742 --> 00:02:28,665
이번 과제에서 여러분은 여러분만의  k-NN을 
구현하게 될 것인데요

45
00:02:28,665 --> 00:02:30,686
우리가 이번 강의에서 다룰 것입니다.

46
00:02:30,686 --> 00:02:33,514
여러분은 또한 몇가지 선형 분류기를 구현할 것인데

47
00:02:33,514 --> 00:02:35,694
SVM과 Softmas 뿐만 아니라

48
00:02:35,694 --> 00:02:37,927
2-layer 신경망도 구현할 것입니다.

49
00:02:37,927 --> 00:02:42,160
그리고 앞으로 몇 강의동안
그 내용을 모두 다룰 것입니다.

50
00:02:43,112 --> 00:02:46,168
그리고 모든 과제는 Python과 
Numpy를 사용하고 있습니다.

51
00:02:46,168 --> 00:02:49,229
Python이나 Numpy에 익숙하지 않다면

52
00:02:49,229 --> 00:02:51,608
관련한 tutorial이 있으며, 여러분은

53
00:02:51,608 --> 00:02:54,312
우리 강의 사이트에서 찾아 볼 수 있습니다.

54
00:02:54,312 --> 00:02:56,856
그리고 이것은 사실 엄청나게 중요합니다.

55
00:02:56,856 --> 00:02:59,211
NumPy는 여러분들이 vectorized 연산을 아주
효율적으로 할 수 있게 해줄 것이며

56
00:02:59,211 --> 00:03:03,856
방대한 양의 계산은 단지 코드 몇 줄로
가능하도록 해 줄 것입니다.

57
00:03:03,856 --> 00:03:06,081
이건 완전 중요한데, 왜냐하면

58
00:03:06,081 --> 00:03:10,288
대부분의 수치계산, 기계학습과 같은 것들이

59
00:03:10,288 --> 00:03:13,424
vectorized 연산을 수행하기 때문입니다.

60
00:03:13,424 --> 00:03:16,964
그리고 여러분들은 첫 과제를 하면서
많은 연습을 할 수 있을 것입니다.

61
00:03:16,964 --> 00:03:23,449
그러니 Matlab, Numpy와 같은 vectorized 
tensor 계산이 익숙하지 않은 분들은

62
00:03:23,449 --> 00:03:27,606
일찌감치 화제를 시작하기를 추천드립니다.

63
00:03:27,606 --> 00:03:32,175
그리고 tutorial도 아주 주의깊에 읽어보시기 바랍니다.

64
00:03:32,175 --> 00:03:40,198
또 한가지 알려드릴 것은 이 수업에서 쓸
Google Cloud가 공직적으로 지원 된다는 것입니다.

65
00:03:40,198 --> 00:03:43,630
Google Cloud는 Amazon AWS와 유사합니다

66
00:03:43,630 --> 00:03:46,694
여러분은 cloud에서 가상 머신을
사용할 수 있습니다.

67
00:03:46,694 --> 00:03:50,432
이런 가상 머신은 GPU들을 지닐 수 있습니다.

68
00:03:50,432 --> 00:03:55,386
현재 Googld Cloud 사용법과 이것으로 과제를 하는
방법에 관한 Tutorial을 작성중입니다.

69
00:03:55,386 --> 00:04:04,723
여러분들이 이미지만 다운받으면 Google Cloud를 통해
과제를 차질없이 진행할 수 있도록 하기 위함입니다.

70
00:04:04,723 --> 00:04:08,437
그리고 google에서 우리 수업을
아주 잘 지원하고 있기 때문에

71
00:04:08,437 --> 00:04:15,582
Google Cloud Credits을 무료로 쓸 수 있는 쿠폰을
여러분들에게 나누어 줄 수 있을 것입니다.

72
00:04:15,582 --> 00:04:24,015
여러분들은 자유롭게 과제나 프로젝트를 수행할때 필요한
GPU나 고사양 컴퓨터와 같은 자원들을 사용할 수 있습니다.

73
00:04:24,015 --> 00:04:28,023
아마도 오늘  Pizzza에 자세한 사항을
포스팅 하도록 하겠습니다.

74
00:04:28,023 --> 00:04:29,300
여러분들에게 말씀드리고 싶었던 것은 ,

75
00:04:29,300 --> 00:04:35,566
본인의 Laptop을 사용해도 되는지와 같은 여러 질문을 받았습니다만

76
00:04:35,566 --> 00:04:41,774
제 답변은, 여러분은 Google Cloud를 사용할 수 있다는 것이고
이를 위해 쿠폰을 제공받을 것입니다.

77
00:04:43,923 --> 00:04:44,756
자 그럼

78
00:04:45,959 --> 00:04:49,716
지금까지 몇가지 공지사항을
전달해 드렸고,

79
00:04:49,716 --> 00:04:53,681
그런 다음 내용을 자세히 살펴 봅니다.

80
00:04:53,681 --> 00:04:58,125
저번 강의에서 Image Classification에 대해
조금 말씀드렸습니다

81
00:04:58,125 --> 00:05:00,450
이것은 컴퓨터비전의 핵심 과제입니다.

82
00:05:00,450 --> 00:05:04,048
그리고 우리가 가장 가장 초점을 맞출 것입니다.

83
00:05:04,048 --> 00:05:04,964
정확히,

84
00:05:04,964 --> 00:05:07,832
Image Classification을 어떻게 할 수 있을까요?

85
00:05:07,832 --> 00:05:09,750
그래서 좀더 구체적으로,

86
00:05:09,750 --> 00:05:12,142
여러분이 Image Classification을 할때

87
00:05:12,142 --> 00:05:14,259
여러분은 입력 이미지를 받습니다.

88
00:05:14,259 --> 00:05:16,457
여기 예제에 귀여운 고양이가 있습니다.

89
00:05:16,457 --> 00:05:22,455
그리고 시스템은 미리 정해놓은 
카테고리를 알고 있습니다.

90
00:05:22,455 --> 00:05:29,134
그런 카테고리는 개나 고양이 트럭, 비행기 같을 것일 수 있으며

91
00:05:29,134 --> 00:05:34,831
컴퓨터가 할 일은 사진을 보고 
그중 하나를 고르는 것입니다.

92
00:05:34,831 --> 00:05:36,444
이건 엄청 쉬워보입니다.

93
00:05:36,444 --> 00:05:44,346
왜냐면 여러분의 뇌에 있는 시각 체계는 이런 류의 
시간 인식 작업에 고도화 되어 있기 때문입니다.

94
00:05:44,346 --> 00:05:48,232
하지만 기계한테는 정말 어려운 일입니다.

95
00:05:48,232 --> 00:05:53,236
컴퓨터가 이 이미지를 볼때 무엇을 보고 있는지에 대해서
좀만 더 깊게 생각해 본다면

96
00:05:53,236 --> 00:05:57,428
컴퓨터는 우리와는 다르게 고양이 라는 생각을
하지 못 할 것입니다.

97
00:05:57,428 --> 00:06:01,755
컴퓨터는 이미지를 아주 큰 격자 모양의 숫자집합으로
표현하고 있습니다.

98
00:06:01,755 --> 00:06:05,922
800x600 이미지 같이 말입니다.

99
00:06:07,371 --> 00:06:13,176
그리고 각 픽셀은 숫자 세개로 표현되는데 
red, green, blue 값을 의미합니다.

100
00:06:13,176 --> 00:06:15,769
다시 말하자면, 컴퓨터한테 이미지는
단지 거대한 숫자 집합에 불과합니다.

101
00:06:15,769 --> 00:06:24,841
이 숫자 거대한  집합에서 고양이 라는 개념을 끄집어 
내는 것은 정말로 힘듭니다.

102
00:06:26,956 --> 00:06:30,186
그래서 우리는 이걸 의미적론적인 차이라고 부릅니다.

103
00:06:30,186 --> 00:06:32,735
이게 고양이다 라는 생각 혹은 
고양이 라는 레이블은

104
00:06:32,735 --> 00:06:35,462
우리가 이 이미지에 붙힌 의미상의 레이블입니다.

105
00:06:35,462 --> 00:06:42,897
그리고 이 이미지가 "고양이다" 라는 것과 실제 컴퓨터가
보는 픽셀 값과는 아무 큰 차이가 있습니다.

106
00:06:42,897 --> 00:06:45,503
이건 정말 어려운 문제인데 왜냐하면

107
00:06:45,503 --> 00:06:48,901
여러분이 이 사진에 아주 미묘한 변화만 주더라도

108
00:06:48,901 --> 00:06:51,916
저 픽셀 값들은 모조리 다 바뀌게 될 것입니다.

109
00:06:51,916 --> 00:06:54,038
예를들어 저 고양이와 똑같은 고양이를 데리고,

110
00:06:54,038 --> 00:06:55,622
그리고 고양이가 계속 앉아있고,

111
00:06:55,622 --> 00:06:57,352
심지어 조금의 미동도 하지 않는다면

112
00:06:57,352 --> 00:06:58,782
아무 일도 일어나진 않을 것입니다.

113
00:06:58,782 --> 00:07:06,921
하지만 우리가 카메라를 조금만 옆으로 옮긴다면 모든 픽셀
하나 하나가 전부 달라질 것입니다.

114
00:07:06,921 --> 00:07:09,431
하지만 여전히 달라진 픽셀 값도
고양이를 표현하고 있는 것이죠

115
00:07:09,431 --> 00:07:12,754
우리의 알고리즘은 이런 것들에
강인해야 합니다.

116
00:07:12,754 --> 00:07:16,260
바라보는 방향 뿐만 아니라
또다는 문제는 바로 조명입니다.

117
00:07:16,260 --> 00:07:19,298
장면에 따라 서로 다른 조명 조건이 있을 수 있습니다.

118
00:07:19,298 --> 00:07:22,500
고양이가 엄청 어두운 곳에 있던

119
00:07:22,500 --> 00:07:25,520
밝은 곳에 있던, 고양이는 여전히 고양이입니다

120
00:07:25,520 --> 00:07:28,488
우리 알고리즘은 이것에 강인해야 합니다.

121
00:07:28,488 --> 00:07:30,145
객체는 또한 변형될 수 있습니다.

122
00:07:30,145 --> 00:07:34,549
제 생각엔 저기 보이는 것 처럼 
고양이가 제일 많이 변형이 가능한 것 같습니다.

123
00:07:34,549 --> 00:07:38,940
고양이들은 다양한 자세를 취할 수 있습니다.

124
00:07:38,940 --> 00:07:42,441
그리고 우리의 알고리즘은 이런 다양한 종류의
변형에도 강인해야 합니다.

125
00:07:43,442 --> 00:07:45,823
가려짐(occlusion)에서의 문제도 있습니다.

126
00:07:45,823 --> 00:07:49,762
여러분은 고양이의 일부만 볼 수 있을수도 있습니다.
예컨데

127
00:07:49,762 --> 00:07:53,686
고양이의 얼굴 만이라던가, 극단적으로는 소파 쿠션에 숨은
고양이의 꼬리만 볼 수도 있습니다.

128
00:07:53,686 --> 00:07:58,897
사람은 이게 고양이다 라는 것을
아주 쉽게 알 수 있습니다

129
00:07:58,897 --> 00:08:01,532
여전히 이것이 고양이 이미지라는 것을 인식하고 있습니다.

130
00:08:01,532 --> 00:08:08,460
우리의 알고리즘도 이것에 강인해 져야 합니다.
제생각엔 아무 어려운 문제일 것 같긴 하지만요

131
00:08:08,460 --> 00:08:10,792
 background clutter라는 문제도 있습니다.
(배경과 비슷한 경우)

132
00:08:10,792 --> 00:08:16,677
전경 객체인 고양이가 배경과 완전히
비슷하게 생겼을 수도 있습니다.

133
00:08:16,677 --> 00:08:20,389
이것 또한 우리가 처리해야 할 또 다른
과제 중 하나입니다.

134
00:08:20,389 --> 00:08:23,637
또한 클래스 내부에서의 다양성과
관련된 문제도 있습니다.

135
00:08:23,637 --> 00:08:28,455
고양이라는 한 개념이 고양이의 다양한 모습들을 전부 
소화해 내야 합니다.

136
00:08:28,455 --> 00:08:32,158
고양이들은 다양한 외형과 크기, 색, 나이를 지니고 있습니다.

137
00:08:32,158 --> 00:08:36,345
우리의 알고리즘은 그러한 다양한 변화도 
다룰 수 있어야 합니다.

138
00:08:36,345 --> 00:08:40,033
이것 사실 엄청나게 어려운 문제입니다.

139
00:08:40,033 --> 00:08:47,931
우리 뇌는 이런 것들을 아주 잘 해내기 때문에, 컴퓨터에게는 
이 작업이 얼마나 어려울지를 까먹을 수도 있습니다.

140
00:08:47,931 --> 00:08:54,288
그러나 만약 우리가 위에서 언급한 모든 문제를 다룰 수
있는 프로그램을 원한다면, 그리고 단연 고양이 뿐만 아니라

141
00:08:54,288 --> 00:08:59,052
우리가 상상할 수 있는 어떤 객체에 대해서도 다룰 수 있는
프로그램이여야 한다면, 이것은 판타스틱하게 어려운 문제입니다.

142
00:08:59,052 --> 00:09:03,078
저 일은 다 할 수 있다면 아마 기적적일 것입니다. 
제 의견으로는요.

143
00:09:03,078 --> 00:09:04,703
그러나 실제로, 그것은 작동 할뿐만 아니라,

144
00:09:04,703 --> 00:09:09,278
그러나 그런 것들은 약간의 제한된 상황에서는 
인간의 정확도와 유사하게 동작할 수도 있습니다.

145
00:09:09,278 --> 00:09:12,379
수행하는데 수백 ms 밖에 걸리지 않습니다.

146
00:09:12,379 --> 00:09:14,921
이것은 상당히 놀랍고 대단한 기술입니다.

147
00:09:14,921 --> 00:09:22,241
수업의 나머지는 어떤 종류의 진보가 이를 가능하게 
했는지를 살펴 볼 것입니다.

148
00:09:23,492 --> 00:09:27,565
자 그러면, 만약 여러분이 Image 분류기를 위한
API에 대해 고민하고 있다고 해 보자면,

149
00:09:27,565 --> 00:09:31,131
여러분은 아마 이런식의 Python 메서드를
작성 해 보려 할 지 모르겠습니다.

150
00:09:31,131 --> 00:09:34,401
이미지를 입력받고 
어떤 놀라운 마법이 일어난 뒤

151
00:09:34,401 --> 00:09:38,180
그러고 나서는 이 "이미지는 고양이"다, "개다" 라고 말해주는 
클래스 레이블이 튀어 나오는 것입니다.

152
00:09:38,180 --> 00:09:41,695
그리고 이걸 할 수 있는 확실한 묘안이 떠오르진 
않을 것입니다. 그렇죠?

153
00:09:41,695 --> 00:09:43,476
여러분이 알고리즘 수업을 듣고 있고,

154
00:09:43,476 --> 00:09:46,837
여러분이 할 일이 "숫자를 정렬"하거나 
"convex hull을 계산"하거나

155
00:09:46,837 --> 00:09:49,535
또는 "RSA 암호화" 같은 것이었다면

156
00:09:49,535 --> 00:09:55,773
여러분은 알고리즘을 써 내려가면서, 알고리즘이
동작하려면 필요한 모든 과정들을 나열할 것입니다.

157
00:09:55,773 --> 00:10:00,390
그러나 우리가 사물을 인식하려고 할 때,
고양이 또는 이미지를 인식하거나,

158
00:10:00,390 --> 00:10:07,909
객체를 인식하는 데 있어서는 그런 직관적이고 명시적인
알고리즘이 존재하지 않습니다.

159
00:10:07,909 --> 00:10:09,874
그러니 이것은 완전히 어려운 일입니다.

160
00:10:09,874 --> 00:10:13,447
만약 여러분이, 이제 막 첫 과제를 하려고 하고 있고

161
00:10:13,447 --> 00:10:15,464
이 함수를 작성하려고 하고 있을 때

162
00:10:15,464 --> 00:10:18,869
제 생각에는 대부분이 문제에 봉착 할 것입니다.

163
00:10:18,869 --> 00:10:19,740
즉,

164
00:10:19,740 --> 00:10:27,004
확실히, 지금껏 사람들은 서로 다른 동물들을 인식해 내기 위해서 
고오급 coded rules을 만들어내려는 시도를 해 왔습니다.

165
00:10:27,004 --> 00:10:29,193
우리는 마지막 강의에서 이를 조금 다룰것이지만

166
00:10:29,193 --> 00:10:32,095
고양이를 인식하는것에 대한 한 아이디어를 보자면

167
00:10:32,095 --> 00:10:35,596
우리는 고양이가 두 귀가 있고, 입이 있고 코가 있다는 것을
알고 있습니다.

168
00:10:35,596 --> 00:10:37,945
또 우리는 Hubel과 Wiesel의 연구로부터

169
00:10:37,945 --> 00:10:41,641
시각 인식을 할때 edges가 아주 중요하다는 것도 
알 고 있습니다.

170
00:10:41,641 --> 00:10:43,820
그러니 우리가 시도해볼 수 있는 한가지가 있다면

171
00:10:43,820 --> 00:10:45,425
이미지의 edges를 계산하는 것입니다.

172
00:10:45,425 --> 00:10:50,983
그리고는 여러 코너와 경계선의 카테고리를 분류합니다.

173
00:10:50,983 --> 00:10:53,146
만약 세개의 선이 이런 식으로 만나면 이건 코너고,

174
00:10:53,146 --> 00:10:55,186
귀는 "여기에 코너 하나" "저기에 코너 하나"

175
00:10:55,186 --> 00:10:56,483
또 "저기에도 코나 하나" 가 있고

176
00:10:56,483 --> 00:11:01,608
이런 식으로, 고양이를 인식하기 위해서 
이런 식으로 "명시적인 규칙 집합" 을 써 내려 가는 것입니다.

177
00:11:01,608 --> 00:11:04,391
아지만 이런 방식은 잘 동작하지 않는 다는 것이 밝혀졌습니다.

178
00:11:04,391 --> 00:11:06,127
한가지 이유는, 이런 알고리즘은
강인하지 못하다는 것이고

179
00:11:06,127 --> 00:11:15,005
두번째는, 우리가 또 다른 객체를 가지고 한다 치면, 고양이는 신경쓰지
않은채로 트럭에 대해서, 개에 대해서 만들게 될 것입니다.

180
00:11:15,005 --> 00:11:17,081
그러면 이 모든걸 다시 시작해야 합니다.

181
00:11:17,081 --> 00:11:19,853
즉, 이런 방법은 전혀 확장성있는 방법이 아닙니다.

182
00:11:19,853 --> 00:11:25,181
우리는 이 세상에 존재하는 다양한 객체들에게 유연하게
적용될 수 있는

183
00:11:25,181 --> 00:11:29,360
확장성 있는 알고리즘이나 방법을 만들고 싶었습니다.

184
00:11:31,311 --> 00:11:34,766
이런 일을 가능할 수 있게 만든 한 통찰은 바로

185
00:11:34,766 --> 00:11:38,092
데이터 중심 접근 방식에 대한 아이디어입니다.

186
00:11:38,092 --> 00:11:45,766
고양이는 무엇이다, 물고기는 무엇이다 하면서 손으로 
직접 어떤 규칙을 써내려 하는 것 대신에

187
00:11:45,766 --> 00:11:47,845
우리는 인터넷으로 가서

188
00:11:47,845 --> 00:11:55,402
엄청 많은 고양이 데이터, 엄청 많은 비행기 데이터, 엄청 많은
사슴 데이터를 포함한 방대한 데이터 셋을 수집하였습니다.

189
00:11:55,402 --> 00:11:59,138
그리고 우리는 다양한 카테고리들의 방대한 데이터들을 모으기 위해

190
00:11:59,138 --> 00:12:03,866
Google Image Search와 같은 도구들을 이용할 수 있었습니다.

191
00:12:03,866 --> 00:12:08,338
사실 그렇게 방대한 량의 데이터를 모으는 것은 엄청나게
많은 노력이 필요합니다.

192
00:12:08,338 --> 00:12:14,105
다행이도 우리가 사용할 수 있는 고퀄의 데이터셋들이 존재합니다.

193
00:12:14,105 --> 00:12:16,073
우리가 데이터셋을 얻고 나면

194
00:12:16,073 --> 00:12:20,869
우리는 그 모든 데이터를 집어 삼킬 기계학습 분류기를
학습시킵니다.

195
00:12:20,869 --> 00:12:28,446
그럼 알고리즘은 어떤 식으로든 데이터를 요약해서는 서로 다른 객체들을
인식할 수 있는 어떤 모델을 뱉어냅니다.

196
00:12:28,446 --> 00:12:31,756
그리고 최종적으로 이 학습 모델을 새로운 이미지에 
적용시키게 되면

197
00:12:31,756 --> 00:12:35,930
이 모델은 고양이나 개를 인식해 낼 수 있을 것입니다.

198
00:12:35,930 --> 00:12:38,428
이 부분에서 API 가 조금 변경되었습니다.

199
00:12:38,428 --> 00:12:42,099
이미지를 입력하면 고양이를 인식하는 단일 함수가 있는 것 대신에

200
00:12:42,099 --> 00:12:43,621
우리는 이제 함수가 두 개 있는 것입니다.

201
00:12:43,621 --> 00:12:49,115
하나는 학습을 위한 것이고, 이미지와
레이블을 입력으로 주면 모델을 출력합니다.

202
00:12:49,115 --> 00:12:52,030
그리고 별도적으로 또 하나의 함수는
예측 함수인데

203
00:12:52,030 --> 00:12:55,276
모델을 입력하면 이미지를 예측해 주는 것입니다.

204
00:12:55,276 --> 00:12:56,780
이것은 key insight입니다.

205
00:12:56,780 --> 00:13:01,928
이 key insight은 지난 10-20년간  
모든 것이 실제로 잘 동작하게 해 줬습니다.

206
00:13:05,784 --> 00:13:11,111
이 수업은 주로 신경망, CNN, 딥러닝 과 같은 것들을
다룹니다.

207
00:13:11,111 --> 00:13:15,819
하지만 데이터 중심이라는 아이디어는 단지 딥러닝에서
뿐만 아니라 좀 더 일반적으로 퍼져있던 개념입니다.

208
00:13:15,819 --> 00:13:19,145
제생각에는 좀 더 복잡하고 어려운 것을 하기 전에

209
00:13:19,145 --> 00:13:23,058
엄청 간단한 분류기를 한번 다뤄보고 가는것이
좋을 것 같습니다.

210
00:13:23,058 --> 00:13:28,907
아마도 여러분이 상상할 수 있는 가장 단순한 분류기가
있는데 우리는 이것을 nearest neighbor라고 부릅니다.

211
00:13:28,907 --> 00:13:31,243
알고리즘은 솔직히 너무 단순하긴 합니다.

212
00:13:31,243 --> 00:13:34,315
그래서 학습 스텝에서 우리는
아무 일도 하지 않을 것입니다.

213
00:13:34,315 --> 00:13:39,108
단지 모든 학습 데이터를 기억할 뿐입니다.
엄청 쉽습니다

214
00:13:39,108 --> 00:13:43,191
예측 스텝에서는
새로운 이미지가 주어지면

215
00:13:43,191 --> 00:13:47,964
새로운 이미지와 학습 데이터의 유사성을 비교해서

216
00:13:47,964 --> 00:13:51,453
가장 유사한 이미지로 레이블링을 예측하는 것입니다.

217
00:13:51,453 --> 00:13:53,169
아주 간단한 알고리즘 입니다.

218
00:13:53,169 --> 00:13:58,771
하지만 데이터 중심 방법론이라는 관점에서는
좋은 점들이 많다고 볼 수 있습니다.

219
00:13:59,977 --> 00:14:01,680
좀 더 구체적으로 들어가 보자면

220
00:14:01,680 --> 00:14:04,951
여러분들은 CIFAR-10 데이터셋을 다루게 될 것입니다.

221
00:14:04,951 --> 00:14:09,259
이 데이터셋은 기계학습 분야에서 아주 널리 쓰이며
일종의 작은 테스트용 데이터셋입니다.

222
00:14:09,259 --> 00:14:11,579
그리고 여러분들은 과제에서 이 데이터셋을 
다루게 될 것입니다.

223
00:14:11,579 --> 00:14:15,159
CIFAR-10 데이터 셋은 
10 가지 클래스를 제공합니다.

224
00:14:15,159 --> 00:14:19,920
비행기와 자동차, 새와 고양이 등이 있습니다.

225
00:14:19,920 --> 00:14:24,990
10개의 각 카테고리가 있고
50,000여개의 학습 이미지를 제공합니다.

226
00:14:27,173 --> 00:14:30,250
50,000개의 데이터가 각 카테고리에 
균등하게 분포하고 있습니다.

227
00:14:30,250 --> 00:14:37,565
그리고 여러분들이 알고리즘을 테스트하는데 사용하게될
10,000여개의 테스트 이미지가 있습니다.

228
00:14:38,707 --> 00:14:45,741
그러면 이 CIFAR-10 테스트 이미지에 
간단한 NN 예제를 적용 해 보겠습니다.

229
00:14:45,741 --> 00:14:53,693
오른쪽 칸의 제일 왼쪽 줄은
CIFAR-10의 테스트 이미지입니다.

230
00:14:53,693 --> 00:14:58,375
그리고 그 오른쪽 이미지들은 
학습 이미지를 분류하고

231
00:14:58,375 --> 00:15:03,571
각각의 테스트 예제와 가장 유사한 학습 이미지를
보여줍니다.

232
00:15:03,571 --> 00:15:07,938
테스트 이미지들이 학습 이미지와 비교했을때
눈으로 보기에는 꽤 비슷하게 생겼습니다.

233
00:15:07,938 --> 00:15:10,974
뭐 항상 맞는것은 아니지만 말입니다. 그렇죠?

234
00:15:10,974 --> 00:15:14,687
두번째 행을 보면, 사실 이게 좀 보기 힘듭니다

235
00:15:14,687 --> 00:15:21,224
이미지들이 32 x 32 픽셀로 되어 있기 때문에 
좀 더 유심히 보고 짐작해 보시기 바랍니다.

236
00:15:21,224 --> 00:15:23,850
어찌됐든, 두번째 행의 이미지는 개 입니다.
바로 옆에 있는 NN도 개 입니다.

237
00:15:23,850 --> 00:15:30,006
하지만 그 다음에 있는건 "사슴"이나 "말"같이 보이는군요

238
00:15:30,006 --> 00:15:33,237
하지만 여러분들은 얘네가 눈으로 보기에는
아주 유사해 보인다는 것을 알 수 있습니다.

239
00:15:33,237 --> 00:15:36,370
중간에 흰색 뭉텅이가 있거나 하는 식으로 보면 말이죠

240
00:15:36,370 --> 00:15:39,651
그래서 우리가 이 이미지에 NN 알고리즘을 적용한다면,

241
00:15:39,651 --> 00:15:42,707
우리는 트레이닝 셋에서 "가장 가까운 샘플"을 찾게 될 것입니다.

242
00:15:42,707 --> 00:15:47,135
우리는 그렇게 찾은 "가장 가까운 샘플"의 레이블을 알 수 
있습니다. 이 샘플들은 학습 데이터에서 나온 것이기 때문입니다

243
00:15:47,135 --> 00:15:50,875
그래서 우리는 이 테스트 이미지 또한 "개"이다 라는 것을
쉽게 알 수 있을 것입니다.

244
00:15:50,875 --> 00:15:55,851
여러분들이 이 예제가 엄청 잘 동작하지 않을것 
같다고 생각하실 수도 있지만

245
00:15:55,851 --> 00:16:00,018
그럼에도 이 예제는 해볼만한 아주 좋은 예제입니다.

246
00:16:00,939 --> 00:16:03,724
하지만 한가지 알아야 할 것은

247
00:16:03,724 --> 00:16:06,908
이미지 쌍이 주어졌을때
어떻게 그것들을 비교할지에 관한 것입니다.

248
00:16:06,908 --> 00:16:10,573
왜냐면 우리가 테스트 이미지를 하나 고르고 
이것을 모든 학습 이미지들과 비교한다 했을때

249
00:16:10,573 --> 00:16:12,165
사실 여러가지 방법이 있을 수 있습니다.

250
00:16:12,165 --> 00:16:15,640
정확히는, 비교 함수가 어떻게 생겼는지에 달렸습니다.

251
00:16:15,640 --> 00:16:20,008
이전 슬라이드의 예제에서는
L1 Distance라는 것을 사용했습니다.

252
00:16:20,008 --> 00:16:22,547
Manhattan distance 라고도 하지요

253
00:16:22,547 --> 00:16:27,448
이미지들을 비교할 때 이 방법은 아주 쉽고
단준한 방법입니다.

254
00:16:27,448 --> 00:16:32,969
이미지간의 각각의 개별적인 픽셀 하나 하나를 
비교하는 것입니다.

255
00:16:32,969 --> 00:16:39,346
테스트 이미지가 4x4 이미지라고 생각해보면

256
00:16:39,346 --> 00:16:43,172
그러고 테스트 이미지의 픽셀 하나를

257
00:16:43,172 --> 00:16:46,242
같은 자리의 트레이닝 이미지의 픽셀로 뺴줍니다. 
그리고 그 값의 절댓값을 취합니다

258
00:16:46,242 --> 00:16:49,068
그러면 두 이미지간의 픽셀의 차이 값을 
계산한 것입니다.

259
00:16:49,068 --> 00:16:51,950
그리고 이미지 내 모든 픽셀의 수행 결과를 모두 더합니다.

260
00:16:51,950 --> 00:16:54,213
이런 방식으로 이미지를 분류하는 것은
어리석은 일이지만

261
00:16:54,213 --> 00:16:57,963
하지만 여전히 생각해볼 만한 것들도 있습니다.

262
00:16:57,963 --> 00:17:01,991
이 방법은 두 이미지의 차이를 측정할 수 있는
아주 구체적인 방법을 제시합니다.

263
00:17:01,991 --> 00:17:07,147
이 예제의 경우에는 두 이미지간에
456이라는 차이가 있습니다.

264
00:17:08,446 --> 00:17:13,233
NN 분류기를 구현한 전체 Python 코드가 있습니다.

265
00:17:13,233 --> 00:17:16,582
상당히 짧고 간결하다는 것을 알 수 있습니다.

266
00:17:16,583 --> 00:17:21,348
NumPy에서 제공하는 Vectorizaed 연산을
이용했기 떄문입니다.

267
00:17:21,348 --> 00:17:26,247
여기에서는 우리가 얼마전에 얘기했던 
학습과 관련된 함수를 볼 수 있는데

268
00:17:26,247 --> 00:17:28,946
NN의 경우 아주 단순하죠

269
00:17:28,946 --> 00:17:33,427
단지 학습 데이터를 기억하는 것입니다.
크게 할 일이 딱히 없습니다

270
00:17:33,427 --> 00:17:39,126
이제 테스트 함수에서는 이미지를 입력으로 받고
이를 L1 Distance를 이용해서 비교합니다

271
00:17:39,126 --> 00:17:45,395
학습 데이터와 테스트 이미지를 가지고 비교를 해서 
트레이닝 셋에서 가장 유사한 것들을 찾아냅니다.

272
00:17:45,395 --> 00:17:50,113
그리고 그 일은 Python code 한 두 줄이면 충분합니다.

273
00:17:50,113 --> 00:17:53,882
Numpy의 vectorized 연산을 이용해서 말이죠

274
00:17:53,882 --> 00:17:57,779
여러분들은 첫 과제에서 이 것들을
연습하게 될 것입니다.

275
00:17:58,628 --> 00:18:02,179
그러면, 이 간단한 분류기에 대해서 
몇가지 물음이 있을 수 있겠습니다.

276
00:18:02,179 --> 00:18:04,910
첫째, 우리의 트레이닝 셋에 N개의 이미지가 들어있다면

277
00:18:04,910 --> 00:18:09,077
트레이닝, 테스트 속도가 얼마나 될까 하는 것입니다

278
00:18:12,233 --> 00:18:17,878
딱히 일이 없는 트레이닝쪽은 속도는 상수시간 일 것입니다.
단지 데이터를 기억하고 있는 것이죠. O(1)

279
00:18:17,878 --> 00:18:22,703
pointer를 사용해서 복사한다면, 데이터가 얼마나 크던
상수시간으로 끝날 수 있을 것입니다.

280
00:18:22,703 --> 00:18:31,099
하지만 테스트를 할때는 N개의 모든 학습용 데이터를 
테스트 이미지와 비교해야만 합니다.

281
00:18:31,099 --> 00:18:33,766
그리고 이 일은 실제로 너무 느립니다.

282
00:18:34,991 --> 00:18:38,641
하지만 생각해보면 이건 완전 "거꾸로" 된 것입니다.
(Train TIme < Test  TIme)

283
00:18:38,641 --> 00:18:45,326
실용적인 측명으로 보면 우리는 "학습 시간이 느리고"
"테스트 시간은 빠르길" 원합니다

284
00:18:45,326 --> 00:18:49,882
왜냐하면, 여러분이 데이터 센터같은 곳에서 어떤 분류기를
학습시키고 있다고 생각해보면

285
00:18:49,882 --> 00:18:54,640
좋은 성능의 분류기를 만들기 위해서 "학습"하는데
많은 시간을 들일 수 있을 것입니다.

286
00:18:54,640 --> 00:18:57,566
하지만 여러분이 이 분류기를 
"테스트"하려 한다고 생각해보면

287
00:18:57,566 --> 00:19:02,248
여러분이 그것을 핸드폰이나, 브라우저와 같은
low power device에서 돌려보고 싶을 수 있고

288
00:19:02,248 --> 00:19:07,075
여러분은 여러분의 분류기가 어느정도 빠른 성능을
보이길 원할 것입니다.

289
00:19:07,075 --> 00:19:11,826
그런 관점에서 보면 NN 알고리즘은 사실
거꾸로 된 경우인 것입니다.

290
00:19:11,826 --> 00:19:16,420
그리고 CNN이나 그런 부류의 parametic 모델을
생각해보면

291
00:19:16,420 --> 00:19:18,286
NN과 정 반대하는 것을 알 수 있습니다.

292
00:19:18,286 --> 00:19:24,936
학습 시간은 오래 걸리겠지만 
테스트 시간을 완전 빠를 것입니다.

293
00:19:24,936 --> 00:19:30,816
 NN알고리즘을 우리가 실제로 적용했을 때 정확히
어떤식으로 보일지에 대한 물음이 남아있습니다.

294
00:19:30,816 --> 00:19:36,130
여기에 우리가 NN의 decision regions 이라는 것을
그려 보았습니다.

295
00:19:36,130 --> 00:19:42,021
여기 2차원 평면에 이 점들은
학습 데이터들 입니다.

296
00:19:42,021 --> 00:19:47,547
그리고 점의 색은 그 점의 카테고리, 즉 클래스 레이블을 나타냅니다.

297
00:19:47,547 --> 00:19:49,221
여기에는 5개의 클래스가 있다는 것을 알수 있습니다.

298
00:19:49,221 --> 00:19:51,543
저 위 구석에는 파란색이 있고

299
00:19:51,543 --> 00:19:53,921
오른쪽 위에는 보라색이 있고

300
00:19:53,921 --> 00:19:56,491
그리고 이제 전체 평면의 모든 픽셀에 대해서

301
00:19:56,491 --> 00:20:02,560
각 픽셀이 어떤 학습 데이터와 가장 가까운지를 계산했습니다.

302
00:20:02,560 --> 00:20:06,954
그런 다음 해당 클래스 레이블에 해당하는 색으로 칠했습니다.

303
00:20:06,954 --> 00:20:11,032
이는 NN 분류기를 일종의 공간을 분할하는 작업을 수행하고

304
00:20:11,032 --> 00:20:14,979
해당하는 색을 칙하는 것으로 볼 수 있습니다.

305
00:20:14,979 --> 00:20:18,320
하지만 이 분류기는 엄청 좋다고는 할 수 없습니다.

306
00:20:18,320 --> 00:20:24,676
이 그림을 보면서 우리는 이 NN 분류기에서 발생할지
모르는 문제들에 대해서 살펴볼 수 있습니다.

307
00:20:24,676 --> 00:20:31,591
하나 들자면, 가운데에는 대부분이 초록점인데
중간에 노란 점 하나가 있습니다.

308
00:20:31,591 --> 00:20:38,552
하지만 우리는 단지 "가장 가까운 이웃" 만을 보기 때문에 이는 
녹색의 무리 한개운데 "노란색 섬" 을 나타나게 합니다.

309
00:20:38,552 --> 00:20:40,087
아마 이것은 좋은 일은 아닐 것입니다.

310
00:20:40,087 --> 00:20:44,081
아마 그런 점들도 초록색이어야 할 것입니다.

311
00:20:44,081 --> 00:20:50,225
그리고 비슷하게 파란색 지역을 침범하고 있는
"손가락 모양의 초록색 지역"또한 볼 수 있는데

312
00:20:50,225 --> 00:20:55,180
이것은 한 점이 그곳에 있어서 그런데,
아마 이 점은 잡음(noise)이거나 가짜(spurious)일 것입니다.

313
00:20:55,180 --> 00:21:01,606
이런 것들은, NN의 조금 더 일반화된 버전인
k-NN 알고리즘이 만들어진 동기가 되었습니다.

314
00:21:01,606 --> 00:21:05,070
단순하기 하나의 가장 가까운 이웃을 탐색하기 보다는

315
00:21:05,070 --> 00:21:08,021
대신에 우리는 좀더 멋있는 것을 할 것입니다.

316
00:21:08,021 --> 00:21:10,627
일종의 distance metric을 이용해서 
K개의 가까운 이웃을 찾는 것입니다.

317
00:21:10,627 --> 00:21:15,057
그리고 여러 이웃들끼리 투표를 하는 것입니다.

318
00:21:15,057 --> 00:21:18,733
그리고 여러 이웃들 중 가장 많은 득표수를 가진 것으로
예측하는 것입니다.

319
00:21:18,733 --> 00:21:21,032
아마 여러분은 이를 위해서는 약간은 더 복잡한 방법이 
필요할 것이라고 생각해 볼 수 있을 것입니다.

320
00:21:21,032 --> 00:21:24,148
거리별로 가중치를 가지고 투표를 한다던가 
하는 것들이 있겠지요

321
00:21:24,148 --> 00:21:27,912
하지만 가장 잘 동작하는 제일 쉬운 방법은

322
00:21:27,912 --> 00:21:29,810
그저 가장 많은 득표수를 취하는 것입니다.

323
00:21:29,810 --> 00:21:35,899
정확히 같은 데이터를 사용한 k-nn 분류기들이 있습니다.

324
00:21:35,899 --> 00:21:39,928
K=1 일때가 있고, 중앙과 오른쪽에는  K=3일떄
K = 5 일때가 있습니다.

325
00:21:39,928 --> 00:21:43,792
K=3 일때는 살보자면

326
00:21:43,792 --> 00:21:50,966
이전에 초록색 무리 중안에 자리잡고있었던 노란색 점이 더 이상 노란 지역을
만들어내지 않는다는 것을 알 수 있습니다.

327
00:21:50,966 --> 00:21:55,852
이제는 중앙의 초록색 부분은 완전하게
초록색으로 분류되고 있습니다.

328
00:21:55,852 --> 00:21:59,272
그리고 빨간색과 파란색 영역 사이의 손가락들이

329
00:21:59,272 --> 00:22:00,823
점점 부드러워지기 시작했습니다.

330
00:22:00,823 --> 00:22:02,454
다수결로 인한 것이죠.

331
00:22:02,454 --> 00:22:05,381
K=5의 경우를 보자면

332
00:22:05,381 --> 00:22:08,437
파란색과 빨간색 영역 사이의 결정경계가

333
00:22:08,437 --> 00:22:12,064
아주 부드럽고 훌륭하게 만들어 졌습니다.

334
00:22:12,064 --> 00:22:14,727
대게 여러분이 NN분류기를 사용하게 된다면

335
00:22:14,727 --> 00:22:20,771
여러분들은 대게 1보다는 큰 값의 K를 사용하고 싶을 것입니다.

336
00:22:20,771 --> 00:22:26,064
왜냐하면 K가 1보다 큰 것이 결정 경계를 더 부드럽게 하고 
더 좋은 결과를 이끌어 주기 때문입니다.

337
00:22:29,252 --> 00:22:30,159
질문있나요?

338
00:22:30,159 --> 00:22:34,279
[질문하는 학생]

339
00:22:34,279 --> 00:22:35,208
알겠습니다. 질문은

340
00:22:35,208 --> 00:22:38,133
"흰색 지역은 어떻게 처리하나요?" 인데요

341
00:22:38,133 --> 00:22:43,247
흰색 영역은 k-nn에서 "대다수"를 결정할 수 없는 지역입니다.

342
00:22:43,247 --> 00:22:45,521
여러분들은 조금 더 좋은 방법을 생각할 수 있을 것입니다.

343
00:22:45,521 --> 00:22:50,472
어떤 식으로든 추론을 해보거나 임의로 정할 수도 있겠지요

344
00:22:50,472 --> 00:22:55,668
하지만 여기 단순한 예제에서는 이 곳에서는 가장 가까운 이웃이 
존재하지 않음을 나타내기 위해서 흰색으로 칠했습니다.

345
00:23:00,005 --> 00:23:02,439
우리가 컴퓨터비전에 대해 생각하는데 있어서

346
00:23:02,439 --> 00:23:06,616
다양한 관점을 유연하게 다루는 능력이 매우 유용하다고 생각합니다.

347
00:23:06,616 --> 00:23:09,480
하나는 고차원 공간에 존재하는 점들 이라는 측면이고

348
00:23:09,480 --> 00:23:13,049
다른 하나는 그저 실제 이미지로 보는 것입니다.

349
00:23:13,049 --> 00:23:19,048
왜냐하면 이미지의 픽셀들은 이미지를 고차원 벡터로 
여길 수 있게 해주기 때문입니다.

350
00:23:19,048 --> 00:23:23,395
이런 두가지 측면을 왔다 갔다 할 수 있는 것은 매우 유용합니다.

351
00:23:23,395 --> 00:23:27,876
다시 k-nn을 가지고 이미지로 돌아가 보자면

352
00:23:27,876 --> 00:23:29,443
여러분들은 k-nn이 별로 안좋다는 것을 알 수 있습니다.

353
00:23:29,443 --> 00:23:35,385
제가 여기에 잘 분류되었는지 
아닌지를 초록색과 빨간색으로 표시해 놨습니다.

354
00:23:35,385 --> 00:23:38,288
그리고 정말 안좋아 보입니다.

355
00:23:38,288 --> 00:23:41,459
하지만 더 큰 K값을 사용한다면

356
00:23:41,459 --> 00:23:47,264
투표를 하는데 제일 유사한 하나만 쓰는것이 아니라
상위 세개, 혹은 다섯개, 심지어는 행 전체를 이용할 수도 있을 것입니다.

357
00:23:47,264 --> 00:23:55,325
더 많은 이웃을 고려하면서 이웃들을 검색하게 되면 
여러 잡음에 더 강인해 질 것임을 상상해 볼 수 있습니다.

358
00:23:57,070 --> 00:24:01,666
우리가 k-nn 알고리즘에서 결정해야할 다른 한가지가 있습니다.

359
00:24:01,666 --> 00:24:05,727
그것은 바로 서로 다른 점들을 어떤 방식으로 비교해야 하는지 입니다.

360
00:24:05,727 --> 00:24:11,666
지금까지는 L1 Distance만을 예제로 봐 왔습니다.

361
00:24:11,666 --> 00:24:15,126
이는 픽셀간의 차이의 절대값의 합을 이용하는 것입니다.

362
00:24:15,126 --> 00:24:18,299
하지만 또다른 선택지중 하나는 바로 
L2, 즉 Euclidean distance인데

363
00:24:18,299 --> 00:24:24,491
제곱들의 합의 제곱근을 거리로 이용하는 것입니다.

364
00:24:24,491 --> 00:24:28,727
다른 거리 측정 기준을 고르는 것은 아주 흥미로운 주제입니다.

365
00:24:28,727 --> 00:24:30,072
왜냐하면 서로 다른 거리측정 기준은

366
00:24:30,072 --> 00:24:35,386
그 공간에서 기대할 수 있는 근본적인 기하학적 구조에
대한 새로운 가정을 세우기 떄문입니다.

367
00:24:35,386 --> 00:24:41,688
왼쪽에 보이는 L1 Distance는 사실 L1 Distance의
입장에서는 원입니다. (모양은 사각형)

368
00:24:41,688 --> 00:24:45,020
그리고 원점을 기준으로 하는 사각형의 모양입니다.

369
00:24:45,020 --> 00:24:47,637
각 점들은 이 사각형 위에 존재하며

370
00:24:47,637 --> 00:24:51,017
L1에 따르면 이 점들은 모두 같은 거리에 존재하는 것입니다. 
(L1의 입장에서는 모두 같은 거리에 있는 점들의 집합이니 원임)

371
00:24:51,017 --> 00:24:53,116
반면 L2, Euclidean distance의 경우에는

372
00:24:53,116 --> 00:24:55,290
이 원이 바로 우리에게 익숙한 원이죠

373
00:24:55,290 --> 00:24:57,241
L2의 모양은 우리가 예상한 그대로 입니다.

374
00:24:57,241 --> 00:25:00,798
이 두 metrics간에 아주 흥미로운 차이가 있는데

375
00:25:00,798 --> 00:25:05,135
L1 Distance는 여러분의 좌표 시스템의 선택에 
영향을 받는다는 것입니다.

376
00:25:05,135 --> 00:25:07,306
그러니까, 여러분이 좌표계를 회전시키거나 하면

377
00:25:07,306 --> 00:25:10,201
점들 간의 L1 distance가 바뀌게 되는 것입니다.

378
00:25:10,201 --> 00:25:18,281
반면 L2 Distance의 경우에는 어떤 좌표계와 아무 상관이 없습니다.

379
00:25:18,281 --> 00:25:24,791
만약 입력된 특징벡터 안의 각각 요소들이 어떤 중요한
의미를 가지고 있다면

380
00:25:24,791 --> 00:25:27,935
L1 Distance가 더 잘 맞을 수도 있을지 모릅니다.

381
00:25:27,935 --> 00:25:30,533
그러나 그것이 어떤 공간에서의 일반적인 벡터 일 경우에는

382
00:25:30,533 --> 00:25:34,121
그리고 우리가 그 벡터 요소들간에 어떤 실질적인 의미가
있는지를 잘 알지 못한다면

383
00:25:34,121 --> 00:25:37,531
아마도 L2 Distance가 조금은 더 잘 맞을 수 있습니다.

384
00:25:37,531 --> 00:25:41,839
서로 다른 거리 척도를 사용하는 것에 대해서
또다른 주목할 점은 바로

385
00:25:41,839 --> 00:25:46,343
우리는 다양한 많은 타입의 데이터를 K-NN 분류기를 통해
일반화 시킬 수 있습니다.

386
00:25:46,343 --> 00:25:48,280
단지 벡터나 이미지에서 뿐만이 아닌 것입니다.

387
00:25:48,280 --> 00:25:51,410
에를 들어 여러분이 어떤 문장을 분류하는 일이 있다고 생각해 보면

388
00:25:51,410 --> 00:25:55,366
우리가 k-nn 분류기를 이 문제에 사용하기 위해 해야할 일은 그저

389
00:25:55,366 --> 00:25:57,716
어떤 거리 척도를 사용할지를 정하는 것입니다.

390
00:25:57,716 --> 00:26:03,831
그 척도가 될 수 있는것은 두 문장간의 거리를 측정할 수 
있는 어떤것이든 가능합니다.

391
00:26:03,831 --> 00:26:12,701
어떤 거리 척도를 사용할지를 정하는 것 만으로 우리는 
이 알고리즘은 어떤 종류의 데이터에도 일반적으로 적용할 수 있습니다.

392
00:26:12,701 --> 00:26:20,283
비록 이 알고리즘이 단순하긴 하지만 어떤 새로운 문제를
접하고 있을때 시도해볼만한 좋은 수단이 될 수 있는 것입니다.

393
00:26:21,805 --> 00:26:28,441
자 그러면 다양한 거리 측정 기준을 선택함으로써 
실제 기하학적으로 어떤 변화가 일어나는지 생각해 봅시다.

394
00:26:28,441 --> 00:26:33,577
여기 같은 데이터를 가지고 비교한 두개의 모델이 있는데

395
00:26:33,577 --> 00:26:38,087
왼쪽에는 L1 Distance를 사용했고, 오른쪽은 L2 Distance를
사용한 것입니다.

396
00:26:38,087 --> 00:26:44,093
그리고 두 개의 척도 사이에 실제 결정 경계의 모양이
서로 다름을 알 수 있습니다.

397
00:26:44,093 --> 00:26:49,337
왼쪽의 L1 Distance를 보자면 이는 결정 경계가 
좌표 축의 영향을 받는 경향이 있음을 알 수 있습니다.

398
00:26:49,337 --> 00:26:53,451
다시 말하지만, L1 Distance는 우리가 좌표 시스템을
어떻게 선택하는지에 영향을 받기 때문입니다.

399
00:26:53,451 --> 00:27:00,294
반명 L2 Distance는 촤표축을 신경쓰지 않고 결정 경계를 
만들기 때문에 좀 더 자연스러워 보입니다.

400
00:27:04,161 --> 00:27:08,669
사실 지금까지 보여드린 모든 예제는

401
00:27:08,669 --> 00:27:14,618
제가 만든 웹 데모사이트에서 가져온 것입니다. 여기에서는
여러분만의 k-nn 분류기를 만들어 볼 수 있습니다.

402
00:27:14,618 --> 00:27:17,938
이 프로텍터 스크린으로 보여드리기가 참 어렵군요

403
00:27:17,938 --> 00:27:21,271
집에가서 한번 해보시기 바랍니다.

404
00:27:26,820 --> 00:27:29,403
다시 돌아가 봅시다

405
00:27:32,951 --> 00:27:35,784


406
00:28:07,103 --> 00:28:09,679
좋습니다 말썽이 좀 있었습니다

407
00:28:09,679 --> 00:28:13,496
여기에서는 웹 데모를 건너 뛸 것이지만 
여러분의 컴퓨터로 한번 꼭 해보시기 바랍니다.

408
00:28:13,496 --> 00:28:26,029
실제로는 상당히 재밌고 K 값이나 거리 척도가 바뀜에 따라서 어떻게 
결정 경계가 만들어지는지에 대한 직관을 얻을 수 있습니다.

409
00:28:30,641 --> 00:28:34,072
좋습니다. 그러면 여기에서의 질문은 바로 
우리가 실제로 알고리즘을 사용하려고 할때

410
00:28:34,072 --> 00:28:37,178
우리가 선택해야만 하는 몇가지 항목이 있다는 것입니다.

411
00:28:37,178 --> 00:28:39,426
이전에 다양한 K값의 대해서 이야기했습니다.

412
00:28:39,426 --> 00:28:41,520
L1/L2와 같은 거리의 척도에 관해서도 이야기 했습니다.

413
00:28:41,520 --> 00:28:47,286
그리고 그 질문은 "어떻게 우리의 문제나 데이터에 맞는 
그런 결정을 할 수 있는지" 가 됩니다.

414
00:28:47,286 --> 00:28:53,937
K나 거리척도와 긑은 것들을 우리는 
"하이퍼 파라미터" 라고 부릅니다.

415
00:28:53,937 --> 00:28:57,289
하이퍼 파라미터는 트레이닝 데이터로부터 학슴할 수 있는
것이 아니기 떄문에

416
00:28:57,289 --> 00:29:01,313
대신 여러분이 학습하기 전에 사전에 선택해야만 합니다.

417
00:29:01,313 --> 00:29:05,623
데이터에서 직접적으로 배울 방법은 없습니다.

418
00:29:05,623 --> 00:29:10,260
그럼 다음 질문은 "실제로는 하이퍼 파라미터는 어떻게 정하는지" 
입니다.

419
00:29:10,260 --> 00:29:12,277
그리고 그건 매우 문제의존적(problem-dependent)입니다.

420
00:29:12,277 --> 00:29:17,957
대부분의 사람들이 하는 가장 단순한 방법은 바로  데이터에 맞게
다양한 하이퍼파라미터 값을 시도해 보는 것입니다.

421
00:29:17,957 --> 00:29:20,950
그리고 어떤것이 가장 좋은지를 알아내는 것입니다.

422
00:29:20,950 --> 00:29:22,404
질문 있습니까?

423
00:29:22,404 --> 00:29:26,071
[질문하는 학생]

424
00:29:29,589 --> 00:29:34,447
질문은 "어디에서 L1 Distance가 L2 Distance보다 
더 좋은지" 입니다.

425
00:29:34,447 --> 00:29:36,800
제 생각에 그것은 정말 문제 의존적이고,

426
00:29:36,800 --> 00:29:41,204
이 경우에는 L1을 쓰고 이 경우에는 L2를 써라 라고
말하기가 참 어렵습니다만

427
00:29:41,204 --> 00:29:50,185
L1은 좌표계에 의존적이기 때문에 
여러분의 데이터가 좌표계에 의존적인지가 관건이라 생각합니다.

428
00:29:50,185 --> 00:29:55,513
만약 어떤 특징 벡터가 있고, 이 벡터의 각 요소가 어떤 의미를
지니고 있다면

429
00:29:55,513 --> 00:29:58,583
예를들어 여러분이 어떤 이유가 되었든
직원들을 분류하고자 한다면

430
00:29:58,583 --> 00:30:03,976
그 벡터의 다양한 요소 각각이 직원들의 서로다른 특징들에 
영향을 미치게 됩니다.

431
00:30:03,976 --> 00:30:08,778
그 요소는 봉급이라던지 회사 근속년수와 
같은 것들이 될 수 있겠지요

432
00:30:08,778 --> 00:30:11,860
각각의 요소가 어떤 의미를 가지고 있다면,

433
00:30:11,860 --> 00:30:15,850
L1 을 사용하는것이 좀 더 괜찮을 지도 모릅니다.

434
00:30:15,850 --> 00:30:17,623
그러나 일반적으로, 이것은 다시 하이퍼 파라미터입니다

435
00:30:17,623 --> 00:30:19,989
실제로 귀하의 문제와 귀하의 데이터에 달려 있습니다.

436
00:30:19,989 --> 00:30:22,214
가장 좋은 대답은 둘 다 시도하는 것입니다.

437
00:30:22,214 --> 00:30:24,381
더 잘 작동하는지 확인하십시오.

438
00:30:28,381 --> 00:30:30,092
다른 가치를 시험해 보려는이 아이디어조차도

439
00:30:30,092 --> 00:30:32,413
하이퍼 패러미터를 확인하고 가장
잘 작동하는 것이 무엇인지,

440
00:30:32,413 --> 00:30:34,238
여기에는 다양한 선택 사항이 있습니다.

441
00:30:34,238 --> 00:30:36,287
하이퍼 파라미터를 시도하는 것이 정확히 무엇을 의미합니까?

442
00:30:36,287 --> 00:30:38,268
무엇이 가장 잘 작동하는지 확인하십시오.

443
00:30:38,268 --> 00:30:40,391
글쎄, 당신이 생각할지도 모르는 첫 번째 생각

444
00:30:40,391 --> 00:30:42,911
단순히 당신에게주는 하이퍼 파라미터를 선택하는 것입니다.

445
00:30:42,911 --> 00:30:45,032
최상의 정확도 또는 최상의 성능

446
00:30:45,032 --> 00:30:47,691
귀하의 교육 데이터에.

447
00:30:47,691 --> 00:30:49,961
이것은 실제로 실제로 끔찍한 생각입니다.

448
00:30:49,961 --> 00:30:52,137
당신은 절대로 이것을해서는 안됩니다.

449
00:30:52,137 --> 00:30:54,081
가장 가까운 이웃의 구체적인 경우

450
00:30:54,081 --> 00:30:55,717
분류 자, 예를 들어,

451
00:30:55,717 --> 00:30:59,324
K = 1로 설정하면 항상 훈련 데이터를 분류합니다.

452
00:30:59,324 --> 00:31:00,157
아주.

453
00:31:01,124 --> 00:31:04,420
따라서이 전략을 사용하면 항상 K = 1을 선택합니다.

454
00:31:04,420 --> 00:31:06,390
그러나 앞에서 본 예에서 보았 듯이,

455
00:31:06,390 --> 00:31:10,446
실제로는 K를 더 큰 값으로 설정하는 것 같습니다.

456
00:31:10,446 --> 00:31:13,184
우리가 훈련 데이터의 일부를 잘못
분류하게 만들 수도 있습니다.

457
00:31:13,184 --> 00:31:14,713
그러나 실제로 더 나은 성능으로 이어집니다

458
00:31:14,713 --> 00:31:17,915
교육 데이터에없는 포인트

459
00:31:17,915 --> 00:31:19,208
궁극적으로 기계 학습

460
00:31:19,208 --> 00:31:21,434
우리는 훈련 데이터를 맞추는 것에 신경 쓰지 않고,

461
00:31:21,434 --> 00:31:23,317
우리는 우리의 분류 자,

462
00:31:23,317 --> 00:31:24,463
또는 우리의 방법,

463
00:31:24,463 --> 00:31:27,051
교육 후에 보이지 않는 데이터에 대해 수행합니다.

464
00:31:27,051 --> 00:31:30,495
그래서, 이것은 끔찍한 생각입니다. 그렇게하지 마십시오.

465
00:31:30,495 --> 00:31:32,687
그래서, 당신이 생각할지도 모르는 또 다른 생각은,

466
00:31:32,687 --> 00:31:34,856
어쩌면 전체 데이터 세트를 가져올 것입니다.

467
00:31:34,856 --> 00:31:36,532
그것을 몇 가지 교육 데이터로 나눕니다.

468
00:31:36,532 --> 00:31:38,390
및 일부 테스트 데이터.

469
00:31:38,390 --> 00:31:42,294
이제 다른 알고리즘으로 알고리즘을 연습 해 보겠습니다.

470
00:31:42,294 --> 00:31:44,409
교육 자료의 하이퍼 매개 변수 선택

471
00:31:44,409 --> 00:31:47,263
그리고 나는 그 훈련 된 분류자를 적용 할 것이다.

472
00:31:47,263 --> 00:31:49,584
테스트 데이터에 대해 지금 선택하겠습니다.

473
00:31:49,584 --> 00:31:52,299
최고의 성능을 발휘하는 하이퍼 패러미터 세트

474
00:31:52,299 --> 00:31:53,716
테스트 데이터에.

475
00:31:54,582 --> 00:31:57,414
이것은 아마도 더 합리적인 전략 인 것처럼 보입니다.

476
00:31:57,414 --> 00:31:59,546
그러나 사실, 이것은 또한 끔찍한 생각입니다.

477
00:31:59,546 --> 00:32:01,087
당신은 이것을 절대해서는 안됩니다.

478
00:32:01,087 --> 00:32:03,723
다시, 기계 학습 시스템의 요점

479
00:32:03,723 --> 00:32:06,515
알고리즘이 어떻게 수행되는지 알고 싶다는 것입니다.

480
00:32:06,515 --> 00:32:08,017
그래서, 테스트 세트의 포인트

481
00:32:08,017 --> 00:32:10,760
우리의 방법이 어떻게 할 것인가에
대한 견적을 우리에게 줄 것입니다.

482
00:32:10,760 --> 00:32:14,523
야생에서 나오는 보이지 않는 데이터에 대해

483
00:32:14,523 --> 00:32:17,710
그리고 우리가이 여러 가지 훈련 전략을 사용한다면

484
00:32:17,710 --> 00:32:19,749
다른 하이퍼 파라미터를 갖는 알고리즘,

485
00:32:19,749 --> 00:32:22,133
그리고 나서, 최선을 다하는 것을 선택하십시오.

486
00:32:22,133 --> 00:32:23,363
테스트 데이터에서,

487
00:32:23,363 --> 00:32:26,404
그렇다면 우리가 방금 뽑은 것일 수도 있습니다.

488
00:32:26,404 --> 00:32:28,045
하이퍼 파라미터의 올바른 세트

489
00:32:28,045 --> 00:32:30,319
우리 알고리즘이 아주 잘 작동하게 만들었습니다.

490
00:32:30,319 --> 00:32:31,663
이 테스트 세트에서

491
00:32:31,663 --> 00:32:33,776
이제이 테스트 세트에 대한 우리의 성과

492
00:32:33,776 --> 00:32:35,488
더 이상 대표가되지 않을 것이다.

493
00:32:35,488 --> 00:32:38,280
새로운 보이지 않는 데이터에 대한 우리의 성과.

494
00:32:38,280 --> 00:32:41,491
그래서, 다시, 당신은 이것을하지
말아야합니다, 이것은 나쁜 생각입니다,

495
00:32:41,491 --> 00:32:44,672
당신이 그렇게하면 곤경에 빠지게 될 것입니다.

496
00:32:44,672 --> 00:32:47,025
훨씬 더 일반적인 것은 실제로 데이터를 분리하는 것입니다.

497
00:32:47,025 --> 00:32:49,192
3 개의 다른 세트로.

498
00:32:50,185 --> 00:32:54,165
대부분의 데이터를 교육 세트로 분할합니다.

499
00:32:54,165 --> 00:32:56,159
그런 다음 유효성 검사 집합을 만듭니다

500
00:32:56,159 --> 00:32:57,305
및 테스트 세트.

501
00:32:57,305 --> 00:33:01,465
그리고 이제 우리가 일반적으로하는 일은 우리의
알고리즘을 훈련시키고 훈련시키는 것입니다.

502
00:33:01,465 --> 00:33:03,500
다양한 하이퍼 파라미터 선택 가능

503
00:33:03,500 --> 00:33:05,172
훈련 세트에,

504
00:33:05,172 --> 00:33:07,227
검증 세트에 대해 평가하고,

505
00:33:07,227 --> 00:33:08,802
이제 하이퍼 매개 변수 세트를 선택하십시오.

506
00:33:08,802 --> 00:33:11,621
이는 유효성 검증 세트에서 가장 잘 수행됩니다.

507
00:33:11,621 --> 00:33:13,719
그리고 지금, 당신이 모든 개발을 완료 한 후에,

508
00:33:13,719 --> 00:33:14,899
당신은 모든 디버깅을 완료했습니다.

509
00:33:14,899 --> 00:33:16,627
당신이 모든 것을 돔으로 만든 후에,

510
00:33:16,627 --> 00:33:20,000
그런 다음 가장 실적이 우수한 분류 기준을 사용합니다.

511
00:33:20,000 --> 00:33:21,614
검증 세트에

512
00:33:21,614 --> 00:33:23,401
테스트 세트에서 한 번 실행하십시오.

513
00:33:23,401 --> 00:33:25,005
이제는 종이에 들어가는 숫자입니다.

514
00:33:25,005 --> 00:33:27,139
그것은 귀하의 보고서에 들어가는 번호입니다.

515
00:33:27,139 --> 00:33:29,542
그 숫자가 실제로 당신에게 어떻게
말하고 있는지 보여줍니다.

516
00:33:29,542 --> 00:33:32,393
귀하의 알고리즘은 보이지 않는 데이터에 대해 수행하고 있습니다.

517
00:33:32,393 --> 00:33:34,335
그리고 이것은 실제로 정말로 정말로 중요합니다.

518
00:33:34,335 --> 00:33:36,516
너는 아주 엄격한 분리를 유지한다.

519
00:33:36,516 --> 00:33:38,847
유효성 확인 데이터 및 테스트 데이터

520
00:33:38,847 --> 00:33:41,437
예를 들어 연구 논문을 연구 할 때,

521
00:33:41,437 --> 00:33:43,303
우리는 일반적으로 테스트 세트 만 터치합니다.

522
00:33:43,303 --> 00:33:45,653
아주 마지막 순간에.

523
00:33:45,653 --> 00:33:47,029
제가 논문을 쓸 때,

524
00:33:47,029 --> 00:33:49,472
내 문제에 대해서만 테스트 세트를 만지는 경향이있다.

525
00:33:49,472 --> 00:33:51,809
마감일 전까지 주일에

526
00:33:51,809 --> 00:33:54,159
정말로 우리가

527
00:33:54,159 --> 00:33:56,665
여기 부정직하고 숫자를보고하지 않습니다.

528
00:33:56,665 --> 00:33:57,961
불공평하다.

529
00:33:57,961 --> 00:34:00,102
그래서 이것은 실제로 매우 중요합니다.

530
00:34:00,102 --> 00:34:02,216
테스트 데이터를 확실히 유지하려고합니다.

531
00:34:02,216 --> 00:34:03,883
상당히 통제하에있다.

532
00:34:06,468 --> 00:34:08,721
하이퍼 파라미터를 설정하는 또 다른 전략

533
00:34:08,721 --> 00:34:10,840
교차 검증이라고합니다.

534
00:34:10,840 --> 00:34:13,868
그리고 이것은 조금 더 보편적으로 사용됩니다.

535
00:34:13,868 --> 00:34:17,317
작은 데이터 세트의 경우 심층적 인
학습에는 그다지 많이 사용되지 않습니다.

536
00:34:17,317 --> 00:34:20,201
그래서 여기서 아이디어는 우리가 테스트
데이터를 취할 것이라는 것입니다,

537
00:34:20,201 --> 00:34:21,674
또는 우리는 우리의 데이터 세트를 취할 것입니다.

538
00:34:21,674 --> 00:34:25,534
늘 그렇듯이, 맨 마지막에 사용할
몇 가지 테스트 세트를 들고,

539
00:34:25,534 --> 00:34:27,023
이제 나머지 데이터는

540
00:34:27,023 --> 00:34:29,188
단일 교육으로 분할하는 것보다

541
00:34:29,188 --> 00:34:31,265
유효성 검사 파티션,

542
00:34:31,266 --> 00:34:33,851
대신, 우리는 우리의 훈련 데이터를 분리 할 수 있습니다.

543
00:34:33,851 --> 00:34:35,515
많은 다른 주름으로.

544
00:34:35,516 --> 00:34:38,879
그리고 이제,이 방법으로, 우리는

545
00:34:38,879 --> 00:34:41,415
fold가 유효성 검사 집합이 될 것입니다.

546
00:34:41,415 --> 00:34:43,286
이제이 예에서,

547
00:34:43,286 --> 00:34:45,498
우리는 5 배 교차 검증을 사용하고 있습니다.

548
00:34:45,498 --> 00:34:47,594
그래서 당신은 한 세트의 알고리즘으로
당신의 알고리즘을 훈련시킬 것입니다.

549
00:34:47,594 --> 00:34:49,984
처음 4 개의 주름에 하이퍼 매개 변수,

550
00:34:49,985 --> 00:34:51,928
4 배의 성능 평가,

551
00:34:51,928 --> 00:34:54,177
이제 폴드에 알고리즘을 재교육하십시오.

552
00:34:54,177 --> 00:34:55,712
하나, 둘, 셋, 다섯,

553
00:34:55,712 --> 00:34:57,769
4 번 폴드에 대해 평가하고,

554
00:34:57,769 --> 00:35:00,293
모든 다른 주름을 순환하십시오.

555
00:35:00,293 --> 00:35:01,765
그리고, 당신이 그것을 이렇게하면,

556
00:35:01,765 --> 00:35:04,098
너는 훨씬 더 높은 자신감을 얻는다.

557
00:35:04,098 --> 00:35:06,215
어느 하이퍼 패러미터가 수행 할 것인가?

558
00:35:06,215 --> 00:35:07,511
더 튼튼하게.

559
00:35:07,511 --> 00:35:09,714
그래서 이것은 사용하는 금본위 제의 종류입니다.

560
00:35:09,714 --> 00:35:11,749
그러나 실제로는 깊은 학습에서

561
00:35:11,749 --> 00:35:13,285
우리가 큰 모델을 훈련 할 때

562
00:35:13,285 --> 00:35:15,972
훈련은 매우 계산적으로 비쌉니다.

563
00:35:15,972 --> 00:35:18,652
이들은 실제로 너무 많이 사용되지 않습니다.

564
00:35:18,652 --> 00:35:19,519
문제?

565
00:35:19,519 --> 00:35:23,186
[질문하는 학생]

566
00:35:29,515 --> 00:35:31,371
그래, 문제는,

567
00:35:31,371 --> 00:35:32,634
조금 더 구체적으로,

568
00:35:32,634 --> 00:35:34,320
교육과 교육의 차이점은 무엇입니까?

569
00:35:34,320 --> 00:35:35,728
유효성 검사가 설정 되었습니까?

570
00:35:35,728 --> 00:35:39,895
그래서, 당신이 k- 최근 이웃 분류 자에 대해 생각한다면

571
00:35:41,060 --> 00:35:45,300
트레이닝 세트는 레이블이있는이 이미지 세트입니다.

572
00:35:45,300 --> 00:35:46,974
우리가 레이블을 암기하는 곳.

573
00:35:46,974 --> 00:35:48,607
그리고 지금, 이미지를 분류하기 위해,

574
00:35:48,607 --> 00:35:51,405
우리는 이미지를 가져 와서 각 요소와 비교할 것입니다.

575
00:35:51,405 --> 00:35:52,451
훈련 데이터에서,

576
00:35:52,451 --> 00:35:56,618
그런 다음 가장 가까운 교육
포인트에서 레이블을 전송하십시오.

577
00:36:00,052 --> 00:36:02,102
이제 알고리즘이 모든 것을 암기합니다.

578
00:36:02,102 --> 00:36:03,184
훈련 세트에서,

579
00:36:03,184 --> 00:36:05,934
유효성 검사 집합의 각 요소를 가져옵니다.

580
00:36:05,934 --> 00:36:08,062
학습 데이터의 각 요소와 비교

581
00:36:08,062 --> 00:36:12,081
이것을 사용하여 정확도를 결정하십시오

582
00:36:12,081 --> 00:36:16,815
유효성 검사 집합에 적용 할 때 분류 자의

583
00:36:16,815 --> 00:36:18,746
이것이 교육의 차이점입니다.

584
00:36:18,746 --> 00:36:19,919
검증.

585
00:36:19,919 --> 00:36:22,364
알고리즘에서 라벨을 볼 수있는 위치

586
00:36:22,364 --> 00:36:24,207
훈련 세트의

587
00:36:24,207 --> 00:36:25,528
유효성 검사 세트의 경우,

588
00:36:25,528 --> 00:36:28,473
귀하의 알고리즘은 레이블에 직접 액세스 할 수 없습니다.

589
00:36:28,473 --> 00:36:30,818
유효성 검사 집합의 레이블 만 사용합니다.

590
00:36:30,818 --> 00:36:34,043
우리의 알고리즘이 얼마나 잘 작동하고 있는지 확인합니다.

591
00:36:34,043 --> 00:36:34,907
질문 하나?

592
00:36:34,907 --> 00:36:38,574
[질문하는 학생]

593
00:36:44,373 --> 00:36:47,736
문제는 테스트 세트,

594
00:36:47,736 --> 00:36:49,442
테스트 세트가 없을 수도 있습니다.

595
00:36:49,442 --> 00:36:52,941
야생에있는 데이터의 대표?

596
00:36:52,941 --> 00:36:55,955
이것은 실제로는 실제로 문제가 될 수 있지만,

597
00:36:55,955 --> 00:36:58,252
근본적인 통계적 가정은

598
00:36:58,252 --> 00:37:01,863
귀하의 데이터는 모두 독립적으로 동일하게 배포되며,

599
00:37:01,863 --> 00:37:05,280
모든 데이터 요소가

600
00:37:06,631 --> 00:37:10,125
동일한 기본 확률 분포에서 끌어 낸다.

601
00:37:10,125 --> 00:37:12,948
물론 실제로는 항상 그런 것은 아니지만,

602
00:37:12,948 --> 00:37:14,768
그리고 당신은 분명히 사건에 참여할 수 있습니다.

603
00:37:14,768 --> 00:37:18,798
테스트 세트는 최고 대표가 아닐 수도있다.

604
00:37:18,798 --> 00:37:20,951
당신이 야생에서 보는 것의

605
00:37:20,951 --> 00:37:23,826
따라서 이것은 데이터 집합 생성자 및

606
00:37:23,826 --> 00:37:25,473
데이터 세트 큐레이터는 생각할 필요가 있습니다.

607
00:37:25,473 --> 00:37:27,712
그러나 데이터 세트를 만들 때, 예를 들어,

608
00:37:27,712 --> 00:37:28,626
내가하는 한 가지는,

609
00:37:28,626 --> 00:37:31,454
내가 가서 한꺼번에 모든 데이터를 수집 할 것인가,

610
00:37:31,454 --> 00:37:34,252
데이터를 수집하는 것과 똑같은 방법론을 사용하여,

611
00:37:34,252 --> 00:37:36,656
그 다음에는 무작위로 파티션을 나눕니다.

612
00:37:36,656 --> 00:37:38,690
기차와 시험 사이.

613
00:37:38,690 --> 00:37:40,962
너를 망칠 수있는 한가지는

614
00:37:40,962 --> 00:37:43,024
시간이 지남에 따라 데이터를 수집하는 경우

615
00:37:43,024 --> 00:37:45,489
그리고 당신은 먼저 수집 한 데이터를 만들고,

616
00:37:45,489 --> 00:37:46,343
훈련 데이터,

617
00:37:46,343 --> 00:37:48,601
나중에 수집하는 데이터가 테스트 데이터가됩니다.

618
00:37:48,601 --> 00:37:50,300
그러면이 변화에 실제로 빠질 수 있습니다.

619
00:37:50,300 --> 00:37:51,613
문제를 일으킬 수 있습니다.

620
00:37:51,613 --> 00:37:53,971
하지만이 파티션이 무작위이면

621
00:37:53,971 --> 00:37:55,831
전체 데이터 요소 중에서

622
00:37:55,831 --> 00:37:58,762
그러면 우리가이 문제를 완화하려고하는 것입니다.

623
00:37:58,762 --> 00:37:59,762
실제로.

624
00:38:04,297 --> 00:38:06,619
그럼 일단 당신이이 일을 겪었 으면

625
00:38:06,619 --> 00:38:08,351
교차 검증 절차,

626
00:38:08,351 --> 00:38:11,493
다음과 같이 보이는 그래프로 끝납니다.

627
00:38:11,493 --> 00:38:14,856
여기 X 축에서 우리는 K의 값을 보여줍니다.

628
00:38:14,856 --> 00:38:17,354
어떤 문제에 대한 k- 최근 접 이웃 분류 자에 대해,

629
00:38:17,354 --> 00:38:21,565
이제 Y 축에서 정확도가 무엇인지 보여줍니다.

630
00:38:21,565 --> 00:38:25,102
일부 데이터 세트의 분류 기준

631
00:38:25,102 --> 00:38:26,961
K의 다른 값에 대해서.

632
00:38:26,961 --> 00:38:28,332
그리고 당신은이 경우에,

633
00:38:28,332 --> 00:38:31,705
우리는 데이터에 대해 5 배 교차 검증을 수행했습니다.

634
00:38:31,705 --> 00:38:34,639
그래서 K의 각 값에 대해 5 가지 다른 예가 있습니다.

635
00:38:34,639 --> 00:38:38,346
이 알고리즘이 얼마나 잘하고 있는지

636
00:38:38,346 --> 00:38:41,050
그리고 실제로, 질문에 대한

637
00:38:41,050 --> 00:38:43,165
더 좋거나 더 나쁜 몇 가지 테스트 세트를 가지고있다.

638
00:38:43,165 --> 00:38:44,748
귀하의 알고리즘,

639
00:38:46,239 --> 00:38:47,683
K 배 교차 유효성 검사 사용

640
00:38:47,683 --> 00:38:50,276
아마도 그 양을 정량화하는 데 도움이되는
한 가지 방법 일 것입니다.

641
00:38:50,276 --> 00:38:54,935
그리고, 우리는이 알고리즘이 어떻게
변했는지 볼 수 있습니다.

642
00:38:54,935 --> 00:38:57,606
다른 유효성 검사 폴드에서 수행됩니다.

643
00:38:57,606 --> 00:38:58,981
그리고 그것은 당신에게 어떤 감각을줍니다.

644
00:38:58,981 --> 00:39:00,359
최선의 것이 아니라,

645
00:39:00,359 --> 00:39:04,301
그러나 또한, 그 성과의 분포는 무엇입니까.

646
00:39:04,301 --> 00:39:06,442
그래서, 당신이 기계 학습 모델을 훈련 할 때마다

647
00:39:06,442 --> 00:39:08,151
이런 식으로 음모를 꾸미면,

648
00:39:08,151 --> 00:39:09,744
그들은 당신에게 당신의 정확성,

649
00:39:09,744 --> 00:39:12,380
또는 귀하의 성능을 귀하의 하이퍼 파라미터의 함수로,

650
00:39:12,380 --> 00:39:14,857
그리고 당신은 가고 모델을 고르고 싶습니다.

651
00:39:14,857 --> 00:39:16,151
또는 하이퍼 파라미터들의 세트,

652
00:39:16,151 --> 00:39:17,198
하루의 끝에서,

653
00:39:17,198 --> 00:39:19,995
이는 유효성 검증 세트에서 최상을 수행합니다.

654
00:39:19,995 --> 00:39:23,195
그래서, 여기에서 우리는 K = 7 일
가능성이 있음을 알 수 있습니다.

655
00:39:23,195 --> 00:39:25,528
이 문제에 대한 최선의 방법.

656
00:39:29,713 --> 00:39:32,007
따라서, 이미지상의 k- 최근 인접 분류기

657
00:39:32,007 --> 00:39:34,458
실제로 실제로는 거의 사용되지 않습니다.

658
00:39:34,458 --> 00:39:38,233
왜냐하면, 우리가 이야기 한 모든 문제들 때문입니다.

659
00:39:38,233 --> 00:39:41,393
그래서, 한 가지 문제는 테스트
시간에 매우 느린다는 것입니다.

660
00:39:41,393 --> 00:39:43,115
그것은 우리가 원하는 것과는 반대로,

661
00:39:43,115 --> 00:39:44,287
이전에 우리가 얘기했습니다.

662
00:39:44,287 --> 00:39:45,582
또 다른 문제는

663
00:39:45,582 --> 00:39:48,859
이러한 것들은 유클리드 거리, 또는 L1 거리,

664
00:39:48,859 --> 00:39:51,648
정말 좋은 방법은 아닙니다.

665
00:39:51,648 --> 00:39:54,495
이미지 간의 거리를 측정합니다.

666
00:39:54,495 --> 00:39:57,247
이러한 일종의 벡터 거리 함수

667
00:39:57,247 --> 00:39:59,857
지각상의 유사성에 잘 일치하지 않는다.

668
00:39:59,857 --> 00:40:01,254
이미지 간.

669
00:40:01,254 --> 00:40:04,076
이미지 간의 차이를 어떻게 감지 할 수 있습니까?

670
00:40:04,076 --> 00:40:06,910
따라서이 예에서 우리는

671
00:40:06,910 --> 00:40:08,796
여자의 왼쪽에이 이미지가 있어요.

672
00:40:08,796 --> 00:40:11,234
오른쪽에있는 3 개의 다른 왜곡 된 이미지

673
00:40:11,234 --> 00:40:12,930
우리가 입을 막은 곳,

674
00:40:12,930 --> 00:40:16,029
우리는 실제로 몇 픽셀 아래로 이동했습니다.

675
00:40:16,029 --> 00:40:18,416
또는 전체 이미지를 파란색으로 채 웁니다.

676
00:40:18,416 --> 00:40:20,849
실제로 유클리드 거리를 계산하면

677
00:40:20,849 --> 00:40:22,785
원본과 박스 사이,

678
00:40:22,785 --> 00:40:24,145
원래 및 셔플,

679
00:40:24,145 --> 00:40:25,425
착색 된 원본,

680
00:40:25,425 --> 00:40:27,735
그들은 모두 동일한 L2 거리를가집니다.

681
00:40:27,735 --> 00:40:29,469
어느 쪽이 좋을지 모르겠군요.

682
00:40:29,469 --> 00:40:32,585
그게 당신에게 감각을주기 때문에

683
00:40:32,585 --> 00:40:34,952
L2 거리는 정말 좋은 일을하고 있지 않습니다.

684
00:40:34,952 --> 00:40:39,119
이미지 사이의 이러한 지각 거리를 포착 할 때

685
00:40:40,642 --> 00:40:42,818
k- 가장 가까운 이웃과 관련된 다른 종류의 문제

686
00:40:42,818 --> 00:40:45,338
분급은 우리가 저주라고 부르는 것과 관련이있다.

687
00:40:45,338 --> 00:40:46,819
차원의.

688
00:40:46,819 --> 00:40:49,744
그래서, 우리가 가진이 관점을 다시 생각해 보면

689
00:40:49,744 --> 00:40:51,279
k- 최근 인접 카테고리 분류기,

690
00:40:51,279 --> 00:40:53,811
그것은 각각의 훈련 주위에 떨어지는 페인트의 일종이다.

691
00:40:53,811 --> 00:40:57,186
데이터 포인트를 사용하여 파티션을
정렬하는 데 공간을 사용합니다.

692
00:40:57,186 --> 00:40:59,517
그래서 우리가 k- 가장 가까운 이웃을 기대한다면

693
00:40:59,517 --> 00:41:01,105
분류기가 잘 작동하려면,

694
00:41:01,105 --> 00:41:04,160
우리는 공간을 커버하기위한 훈련 예가 필요합니다.

695
00:41:04,160 --> 00:41:05,646
꽤 조밀 해.

696
00:41:05,646 --> 00:41:10,525
그렇지 않으면 가장 가까운 이웃들이 실제로는
아주 멀리 떨어져있을 수 있습니다.

697
00:41:10,525 --> 00:41:14,171
떨어져있을 수도 있고 실제로 우리의 테스트와
매우 유사하지 않을 수도 있습니다.

698
00:41:14,171 --> 00:41:15,004
전철기.

699
00:41:16,738 --> 00:41:17,571
그리고 문제는,

700
00:41:17,571 --> 00:41:19,197
실제로 공간을 조밀하게 덮고있는,

701
00:41:19,197 --> 00:41:21,467
우리가 여러 가지 훈련 예를 필요로한다는 것을 의미합니다.

702
00:41:21,467 --> 00:41:24,666
이것은 문제의 차원에서 기하 급수적이다.

703
00:41:24,666 --> 00:41:29,217
따라서 이것은 매우 나쁘다. 기하
급수적 인 성장은 언제나 나쁘다.

704
00:41:29,217 --> 00:41:31,310
기본적으로 충분한 이미지를 얻지 못할 것입니다.

705
00:41:31,310 --> 00:41:33,422
픽셀의이 공간을 조밀하게 덮으려고

706
00:41:33,422 --> 00:41:35,587
이 고차원 공간에서.

707
00:41:35,587 --> 00:41:37,487
그래서 이것은 아마도 염두에 두어야 할 또 다른 것입니다.

708
00:41:37,487 --> 00:41:41,300
k- 가장 가까운 이웃을 사용할 때 생각할 때.

709
00:41:41,300 --> 00:41:42,749
그래서 요약의 종류는 우리가

710
00:41:42,749 --> 00:41:44,460
이 아이디어를 소개하는 k- 가장 가까운 이웃

711
00:41:44,460 --> 00:41:45,701
이미지 분류.

712
00:41:45,701 --> 00:41:47,634
우리는 이미지와 라벨의 훈련 세트를 가지고 있습니다.

713
00:41:47,634 --> 00:41:48,926
우리는 그것을 사용합니다.

714
00:41:48,926 --> 00:41:51,445
테스트 세트에서 이러한 라벨을 예측합니다.

715
00:41:51,445 --> 00:41:52,278
문제?

716
00:41:52,278 --> 00:41:54,519
[질문하는 학생]

717
00:41:54,519 --> 00:41:55,352
오, 미안하지만 문제는,

718
00:41:55,352 --> 00:41:57,271
이 사진으로 무엇이 진행되고 있었습니까?

719
00:41:57,271 --> 00:41:59,168
녹색 점과 파란색 점은 무엇입니까?

720
00:41:59,168 --> 00:42:02,335
여기에 우리는 몇 가지 훈련 견본을 가지고 있습니다.

721
00:42:03,463 --> 00:42:05,596
포인트로 표현되는

722
00:42:05,596 --> 00:42:08,128
도트의 색이 카테고리를 나타낼 수도있다

723
00:42:08,128 --> 00:42:11,004
이 훈련 견본의 요점.

724
00:42:11,004 --> 00:42:12,629
그래서 우리가 한 차원에 있다면,

725
00:42:12,629 --> 00:42:15,161
그러면 네 가지 훈련 샘플 만 필요할 것입니다.

726
00:42:15,161 --> 00:42:17,525
공간을 조밀하게 덮으려면,

727
00:42:17,525 --> 00:42:19,478
그러나 우리가 2 차원으로 이동한다면,

728
00:42:19,478 --> 00:42:22,701
다음, 우리는 지금, 네 번 네
가지가 필요 16 훈련 예입니다

729
00:42:22,701 --> 00:42:25,529
이 공간을 조밀하게 덮으십시오.

730
00:42:25,529 --> 00:42:28,767
그리고 우리가 3, 4, 5, 더 많은 차원으로 이동하면,

731
00:42:28,767 --> 00:42:30,431
우리가 필요로하는 훈련 사례의 수

732
00:42:30,431 --> 00:42:32,344
공간을 조밀하게 덮으려면,

733
00:42:32,344 --> 00:42:35,200
차원과 함께 기하 급수적으로 커집니다.

734
00:42:35,200 --> 00:42:36,536
그래서, 이것은 당신에게 감각을주는 종류입니다,

735
00:42:36,536 --> 00:42:37,601
어쩌면 2 차원으로

736
00:42:37,601 --> 00:42:40,501
우리는 이런 종류의 재미있는
곡선 모양을 가질 수 있습니다.

737
00:42:40,501 --> 00:42:44,848
또는 당신은 라벨의 임의적 인 다양성을 가질 수 있습니다.

738
00:42:44,848 --> 00:42:47,641
다른 차원 공간에서.

739
00:42:47,641 --> 00:42:49,403
k- 최근 접 알고리즘

740
00:42:49,403 --> 00:42:51,704
실제로 이것들에 대한 어떤 가정도하지 않는다.

741
00:42:51,704 --> 00:42:53,029
하부 매니 폴드,

742
00:42:53,029 --> 00:42:54,565
제대로 수행 할 수있는 유일한 방법

743
00:42:54,565 --> 00:42:57,713
훈련 점이 꽤 빽빽한 샘플인가?

744
00:42:57,713 --> 00:42:58,796
함께 일해.

745
00:43:01,741 --> 00:43:04,915
그래서 이것은 k- 가장 가까운 이웃들의 개요입니다

746
00:43:04,915 --> 00:43:07,015
실제로 이것을 구현할 기회를 얻게 될 것입니다.

747
00:43:07,015 --> 00:43:11,214
첫 번째 과제에서 이미지에 시도해보십시오.

748
00:43:11,214 --> 00:43:12,953
K와 N에 관한 마지막 질문이 있다면,

749
00:43:12,953 --> 00:43:16,059
나는 다음 주제로 나아갈 것이다.

750
00:43:16,059 --> 00:43:16,892
문제?

751
00:43:16,892 --> 00:43:21,014
[학생이 질문하고있다]

752
00:43:21,014 --> 00:43:22,034
미안, 다시 말해봐.

753
00:43:22,034 --> 00:43:25,951
[학생이 질문하고있다]

754
00:43:28,437 --> 00:43:29,270
그래, 문제는,

755
00:43:29,270 --> 00:43:32,033
왜이 이미지들은 동일한 L2 거리를 가지고 있습니까?

756
00:43:32,033 --> 00:43:34,036
그리고 대답은, 나는 조심스럽게 그들을 지었다.

757
00:43:34,036 --> 00:43:35,915
동일한 L2 거리를 유지해야합니다.

758
00:43:35,915 --> 00:43:38,716
[웃음]

759
00:43:38,716 --> 00:43:41,812
하지만 그것은 단지 L2 거리

760
00:43:41,812 --> 00:43:45,096
이미지들 사이의 유사성을 측정하는 좋은 방법은 아닙니다.

761
00:43:45,096 --> 00:43:47,306
그리고이 이미지들은 실제로

762
00:43:47,306 --> 00:43:50,223
서로 아주 다른 방식으로

763
00:43:52,470 --> 00:43:54,498
K와 N을 사용하는 경우,

764
00:43:54,498 --> 00:43:56,654
거리를 측정해야하는 유일한 방법입니다.

765
00:43:56,654 --> 00:43:57,649
이미지 간,

766
00:43:57,649 --> 00:44:00,236
이 단일 거리 측정 기준입니다.

767
00:44:00,236 --> 00:44:03,200
그리고 이런 종류의 예는

768
00:44:03,200 --> 00:44:05,229
해당 거리 측정 항목은 실제로 캡처하지 않습니다.

769
00:44:05,229 --> 00:44:07,331
거리 또는 차이에 대한 자세한 설명

770
00:44:07,331 --> 00:44:08,949
이미지 간.

771
00:44:08,949 --> 00:44:12,042
그래서,이 경우에, 나는 단지
이것들을 조심스럽게 만들었습니다.

772
00:44:12,042 --> 00:44:15,384
번역 및 이러한 오프셋을 정확히 일치시킵니다.

773
00:44:15,384 --> 00:44:16,217
문제?

774
00:44:16,217 --> 00:44:19,884
[질문하는 학생]

775
00:44:28,672 --> 00:44:29,925
그래서 문제는,

776
00:44:29,925 --> 00:44:31,249
아마 이것은 실제로 좋다.

777
00:44:31,249 --> 00:44:33,228
이 모든 것들 때문에

778
00:44:33,228 --> 00:44:36,615
실제로 이미지까지의 거리가 같습니다.

779
00:44:36,615 --> 00:44:38,024
이것은 아마도이 예에 해당 할 것입니다.

780
00:44:38,024 --> 00:44:40,390
하지만 저는 여러분이 예제를 만들 수 있다고 생각합니다.

781
00:44:40,390 --> 00:44:41,810
어쩌면 두 개의 원본 이미지가있을 수 있습니다.

782
00:44:41,810 --> 00:44:43,352
상자를 올바른 위치에 놓음으로써

783
00:44:43,352 --> 00:44:44,270
또는 그들을 착색,

784
00:44:44,270 --> 00:44:46,652
우리는 그것이 훨씬 더 가까워 질 수 있습니다.

785
00:44:46,652 --> 00:44:48,316
니가 원하는 무엇이라도, 그렇지?

786
00:44:48,316 --> 00:44:50,850
이 예에서 우리는 일종의 임의적 인 일을 할 수 있습니다.

787
00:44:50,850 --> 00:44:52,007
이동 및 착색

788
00:44:52,007 --> 00:44:54,612
이 거리를 거의 임의로 변화시키는 것

789
00:44:54,612 --> 00:44:57,469
이러한 이미지의 인식 적 특성을 변경하지 않고

790
00:44:57,469 --> 00:44:59,256
그래서,이게 실제로 너를 망칠 수 있다고 생각해.

791
00:44:59,256 --> 00:45:03,172
원본 이미지가 여러 개인 경우

792
00:45:03,172 --> 00:45:04,039
문제?

793
00:45:04,039 --> 00:45:07,956
[학생이 질문하고있다]

794
00:45:15,207 --> 00:45:16,040
질문은 ~이야,

795
00:45:16,040 --> 00:45:17,339
실제 사례에서 공통적인지 여부

796
00:45:17,339 --> 00:45:20,664
돌아가서 전체 데이터 세트를 재교육

797
00:45:20,664 --> 00:45:24,098
그 최고의 하이퍼 파라미터를 찾았습니까?

798
00:45:24,098 --> 00:45:27,765
그래서 사람들은 때로는 이것을 실제로 수행합니다.

799
00:45:28,787 --> 00:45:30,982
그러나 그것은 다소 맛이 있습니다.

800
00:45:30,982 --> 00:45:32,568
그 마감 기한을 정말로 서두르고 있다면

801
00:45:32,568 --> 00:45:34,430
그리고 당신은 정말로이 모델을 문 밖으로 나오게해야합니다.

802
00:45:34,430 --> 00:45:36,781
그런 다음 모델을 재 훈련하는 데 오랜 시간이 걸리면

803
00:45:36,781 --> 00:45:38,176
전체 데이터 세트에서,

804
00:45:38,176 --> 00:45:39,875
어쩌면 당신은 그것을하지 않을 것입니다.

805
00:45:39,875 --> 00:45:41,776
하지만 시간이 조금 있으면

806
00:45:41,776 --> 00:45:43,432
그리고 조금 더 여분의 계산,

807
00:45:43,432 --> 00:45:45,929
그리고 당신은 그 여분의 1 %

808
00:45:45,929 --> 00:45:50,012
성능면, 당신이 사용할 수있는 속임수입니다.

809
00:45:53,288 --> 00:45:54,858
그래서 우리는 k- 가장 가까운 이웃이

810
00:45:54,858 --> 00:45:56,758
좋은 속성이 많이 있습니다.

811
00:45:56,758 --> 00:45:58,409
기계 학습 알고리즘의

812
00:45:58,409 --> 00:46:00,490
그러나 실제로는 그렇게 크지는 않습니다.

813
00:46:00,490 --> 00:46:03,823
실제로 이미지에서 많이 사용되지 않습니다.

814
00:46:05,258 --> 00:46:07,134
그래서 내가 이야기하고 싶은 것은 다음과 같습니다.

815
00:46:07,134 --> 00:46:08,895
선형 분류.

816
00:46:08,895 --> 00:46:11,753
그리고 선형 분류는 다시 한번 아주 간단한 학습입니다

817
00:46:11,753 --> 00:46:14,981
알고리즘을 사용하지만 이것은 매우 중요해질 것입니다

818
00:46:14,981 --> 00:46:17,257
우리가 전체 신경 네트워크를 구축하도록 도와주세요.

819
00:46:17,257 --> 00:46:19,845
전체 컨벌루션 네트워크.

820
00:46:19,845 --> 00:46:21,736
그래서 한 비유 사람들이 자주 이야기합니다.

821
00:46:21,736 --> 00:46:23,470
신경망으로 작업 할 때

822
00:46:23,470 --> 00:46:26,242
우리는 그것들을 레고 블록과
같은 것으로 생각하고 있습니다.

823
00:46:26,242 --> 00:46:28,154
다른 종류의 구성 요소를 가질 수 있습니다.

824
00:46:28,154 --> 00:46:30,345
신경 네트워크 및 이러한 구성 요소 스틱 수 있습니다.

825
00:46:30,345 --> 00:46:33,814
이 큰 타워들을 함께 지어서

826
00:46:33,814 --> 00:46:36,378
컨벌루션 네트워크.

827
00:46:36,378 --> 00:46:38,404
우리가 볼 수있는 가장 기본적인 빌딩 블록 중 하나

828
00:46:38,404 --> 00:46:41,513
다양한 유형의 심층 학습 애플리케이션

829
00:46:41,513 --> 00:46:43,215
이 선형 분류 자입니다.

830
00:46:43,215 --> 00:46:44,996
그래서, 나는 그것이 실제로 정말로 중요하다고 생각합니다.

831
00:46:44,996 --> 00:46:46,905
무슨 일이 일어나고 있는지 잘 이해해야한다.

832
00:46:46,905 --> 00:46:48,270
선형 분류.

833
00:46:48,270 --> 00:46:50,236
이것들은 아주 잘 일반화 될 것이기 때문에

834
00:46:50,236 --> 00:46:52,712
전체 신경 네트워크에.

835
00:46:52,712 --> 00:46:55,243
이 모듈 적 성격의 또 다른 예가 있습니다.

836
00:46:55,243 --> 00:46:56,139
신경 네트워크

837
00:46:56,139 --> 00:46:58,728
이미지 캡션에 관한 우리 자신의
연구실에서의 일부 연구에서 나온 것이지만,

838
00:46:58,728 --> 00:47:00,281
약간의 미리보기처럼.

839
00:47:00,281 --> 00:47:02,847
여기서는 이미지를 입력하고 싶습니다.

840
00:47:02,847 --> 00:47:04,789
서술문을 출력한다.

841
00:47:04,789 --> 00:47:06,226
이미지를 설명합니다.

842
00:47:06,226 --> 00:47:08,162
그리고 이런 종류의 작품은

843
00:47:08,162 --> 00:47:10,710
우리는 하나의 길쌈 신경 네트워크를 찾고 있습니다.

844
00:47:10,710 --> 00:47:11,548
이미지에서,

845
00:47:11,548 --> 00:47:13,444
및 알고있는 반복적 인 신경망

846
00:47:13,444 --> 00:47:14,496
언어에 관해서.

847
00:47:14,496 --> 00:47:16,664
우리는이 두 조각을 함께 붙일 수 있습니다.

848
00:47:16,664 --> 00:47:18,776
레고 블록처럼 모든 것을 함께 훈련 시켜라.

849
00:47:18,776 --> 00:47:20,919
끝내주는 멋진 시스템

850
00:47:20,919 --> 00:47:22,767
그것은 사소한 일을 할 수 있습니다.

851
00:47:22,767 --> 00:47:25,077
그리고 우리는이 모델의 세부
사항을 통해 작업 할 것입니다.

852
00:47:25,077 --> 00:47:26,388
클래스에서 앞으로,

853
00:47:26,388 --> 00:47:27,711
그러나 이것은 당신에게 감각을줍니다.

854
00:47:27,711 --> 00:47:30,209
이러한 심층 신경 네트워크는 레고와 비슷합니다.

855
00:47:30,209 --> 00:47:31,999
및이 선형 분류기

856
00:47:31,999 --> 00:47:34,082
가장 기본적인 빌딩 블록과 비슷합니다.

857
00:47:34,082 --> 00:47:36,082
이 거대한 네트워크 중.

858
00:47:37,096 --> 00:47:39,189
그러나 그것은 강의 2에 대해 너무 흥미 롭습니다.

859
00:47:39,189 --> 00:47:41,257
그래서 우리는 잠시 CIFAR-10으로 돌아 가야합니다.

860
00:47:41,257 --> 00:47:42,375
[웃음]

861
00:47:42,375 --> 00:47:45,641
CIFAR-10에는 5 만 건의 교육 사례가 있으며,

862
00:47:45,641 --> 00:47:49,808
각 이미지는 32 x 32 픽셀과
3 개의 색상 채널입니다.

863
00:47:52,068 --> 00:47:53,963
선형 분류에서, 우리는 약간의

864
00:47:53,963 --> 00:47:56,696
k- 가장 가까운 이웃과는 다른 접근법을 사용한다.

865
00:47:56,696 --> 00:48:01,646
따라서 선형 분류기는 가장 간단한 예제 중 하나입니다.

866
00:48:01,646 --> 00:48:04,734
우리가 파라 메트릭 모델이라고 부르는 것의

867
00:48:04,734 --> 00:48:07,548
이제 우리의 파라 메트릭 모델은 실제로 두 개의 다른

868
00:48:07,548 --> 00:48:08,685
구성 요소.

869
00:48:08,685 --> 00:48:12,201
이 이미지는 아마도 왼쪽의 고양이를 잡을 것입니다.

870
00:48:12,201 --> 00:48:13,539
이,

871
00:48:13,539 --> 00:48:17,384
우리가 보통 입력 데이터를 위해 X로 쓰는 것,

872
00:48:17,384 --> 00:48:20,220
또한 일련의 매개 변수 또는 가중치,

873
00:48:20,220 --> 00:48:23,475
보통 W라고도하며 때로는 세타라고도하며,

874
00:48:23,475 --> 00:48:24,767
문헌에 따라.

875
00:48:24,767 --> 00:48:26,974
이제 우리는 몇 가지 기능을 적을 것입니다.

876
00:48:26,974 --> 00:48:30,780
이것은 데이터 X와 매개 변수 W를 모두 취하는데,

877
00:48:30,780 --> 00:48:34,180
이제 10 개의 숫자가 나옵니다.

878
00:48:34,180 --> 00:48:37,950
각 10 점에 해당하는 점수는 무엇입니까?

879
00:48:37,950 --> 00:48:39,991
CIFAR-10의 카테고리

880
00:48:39,991 --> 00:48:44,232
고양이의 더 큰 점수와 마찬가지로,

881
00:48:44,232 --> 00:48:48,583
그 입력 X가 고양이 일 확률이 더 높음을 나타냅니다.

882
00:48:48,583 --> 00:48:49,827
그리고 지금 질문 하나?

883
00:48:49,827 --> 00:48:53,494
[질문하는 학생]

884
00:48:55,380 --> 00:48:56,717
미안, 그걸 반복 할 수 있니?

885
00:48:56,717 --> 00:48:58,863
[질문하는 학생]

886
00:48:58,863 --> 00:49:01,524
오, 그래서 문제는 세 가지가 무엇입니까?

887
00:49:01,524 --> 00:49:04,986
이 예에서 3 개는 3 색에 해당합니다.

888
00:49:04,986 --> 00:49:06,943
채널, 빨간색, 녹색 및 파란색.

889
00:49:06,943 --> 00:49:08,469
우리는 일반적으로 컬러 이미지 작업을하기 때문에,

890
00:49:08,469 --> 00:49:12,636
당신이 버리고 싶지 않은 멋진 정보입니다.

891
00:49:15,323 --> 00:49:17,243
따라서, k- 최근 접 설정

892
00:49:17,243 --> 00:49:18,999
대신 매개 변수가 없으며,

893
00:49:18,999 --> 00:49:21,686
우리는 단지 전체 훈련 데이터를 계속 유지하고 있습니다.

894
00:49:21,686 --> 00:49:22,657
전체 훈련 세트,

895
00:49:22,657 --> 00:49:24,092
시험 시간에 사용하십시오.

896
00:49:24,092 --> 00:49:25,825
그러나 이제는 파라 메트릭 접근법에서,

897
00:49:25,825 --> 00:49:28,196
우리는 훈련 자료에 대한 우리의 지식을 요약하려고합니다

898
00:49:28,196 --> 00:49:31,105
그 모든 지식을이 매개 변수들에 집어 넣어 라.

899
00:49:31,105 --> 00:49:33,607
이제 테스트 시간에 더 이상 실제
테스트가 필요하지 않습니다.

900
00:49:33,607 --> 00:49:35,371
훈련 데이터, 우리는 그것을 버릴 수 있습니다.

901
00:49:35,371 --> 00:49:37,938
테스트 시간에 이러한 매개 변수 W 만 있으면됩니다.

902
00:49:37,938 --> 00:49:40,561
따라서 우리 모델의 효율성을 높일 수 있습니다.

903
00:49:40,561 --> 00:49:44,684
실제로 휴대폰과 같은 소형 장치에서 실행됩니다.

904
00:49:44,684 --> 00:49:47,277
그래서 일종의, 깊은 학습에 관한 전체 이야기

905
00:49:47,277 --> 00:49:49,404
이것에 맞는 구조가 생겨났다.

906
00:49:49,404 --> 00:49:50,906
기능, F.

907
00:49:50,906 --> 00:49:53,451
다른 기능적 양식을 적어 놓을 수 있습니다.

908
00:49:53,451 --> 00:49:55,406
서로 다른 가중치와 데이터를 결합하는 방법

909
00:49:55,406 --> 00:49:58,608
복잡한 방법, 그리고 이들은 서로 다른

910
00:49:58,608 --> 00:50:01,169
네트워크 아키텍처.

911
00:50:01,169 --> 00:50:02,489
그러나 가장 간단한 예제

912
00:50:02,489 --> 00:50:04,132
이 두 가지를 결합하는 것

913
00:50:04,132 --> 00:50:05,729
그냥, 아마, 그들을 번식하는 것입니다.

914
00:50:05,729 --> 00:50:08,833
그리고 이것은 선형 분류 자입니다.

915
00:50:08,833 --> 00:50:13,181
그래서 우리의 F of X, W는
W times X와 같습니다.

916
00:50:13,181 --> 00:50:15,770
아마 당신이 상상할 수있는 가장
단순한 방정식 일 것입니다.

917
00:50:15,770 --> 00:50:16,603
그래서 여기,

918
00:50:16,603 --> 00:50:18,921
만약 당신이 이런 것들의 치수를 풀 수 있다면,

919
00:50:18,921 --> 00:50:23,088
우리는 우리의 이미지가 32 x 32의
값을 가졌을 것이라고 생각합니다.

920
00:50:24,871 --> 00:50:28,207
그러면 우리는 그 가치들을 취하고 그
다음에 스트레칭을 할 것입니다.

921
00:50:28,207 --> 00:50:31,424
그들을 긴 열 벡터로 밖으로 내 보낸다.

922
00:50:31,424 --> 00:50:34,715
그 중 한 항목은 3,072입니다.

923
00:50:34,715 --> 00:50:38,632
이제 우리는 10 개의 학점으로 끝내기를 원합니다.

924
00:50:39,746 --> 00:50:41,742
이 이미지에 대해 10 개의 숫자로 끝내기를 원합니다.

925
00:50:41,742 --> 00:50:44,236
10 가지 범주 각각에 대해 점수를줍니다.

926
00:50:44,236 --> 00:50:46,279
이것은 우리 행렬 W가,

927
00:50:46,279 --> 00:50:49,032
3072 년까지 10 년이어야합니다.

928
00:50:49,032 --> 00:50:51,299
그래서이 두 가지를 곱하면

929
00:50:51,299 --> 00:50:53,243
우리는 하나의 열 벡터로 끝날 것입니다.

930
00:50:53,243 --> 00:50:57,086
10 점 만점에 10 점을주었습니다.

931
00:50:57,086 --> 00:51:00,317
또한 때로는 일반적으로 이것을 볼 수 있습니다.

932
00:51:00,317 --> 00:51:01,910
우리는 종종 편견 용어를 추가 할 것입니다.

933
00:51:01,910 --> 00:51:04,724
10 요소의 상수 벡터가 될 것입니다

934
00:51:04,724 --> 00:51:06,669
교육 자료와 상호 작용하지 않는

935
00:51:06,669 --> 00:51:09,656
대신에 우리에게 일종의 독립적 인 데이터를 제공합니다.

936
00:51:09,656 --> 00:51:12,235
다른 클래스에 대한 일부 클래스의 환경 설정.

937
00:51:12,235 --> 00:51:13,878
그래서 당신이 상상할 수도 있습니다
당신이 있다면 데이터 세트

938
00:51:13,878 --> 00:51:16,300
균형이 맞지 않고 개보다 고양이가 더 많았습니다.

939
00:51:16,300 --> 00:51:19,259
예를 들어, 대응하는 바이어스 요소

940
00:51:19,259 --> 00:51:23,553
고양이는 다른 고양이보다 높을 것입니다.

941
00:51:23,553 --> 00:51:25,445
그래서 당신이 그림에 대해 생각한다면

942
00:51:25,445 --> 00:51:27,778
이 기능이하는 일,

943
00:51:28,930 --> 00:51:31,483
이 그림에서는 왼쪽에 예제가 있습니다.

944
00:51:31,483 --> 00:51:35,729
2 x 2 이미지 만있는 단순 이미지의

945
00:51:35,729 --> 00:51:37,099
그래서 그것은 총 4 개의 픽셀을 가지고 있습니다.

946
00:51:37,099 --> 00:51:39,832
선형 분류기가 작동하는 방식

947
00:51:39,832 --> 00:51:42,273
이 두 이미지를 두 이미지로 찍는 것입니다.

948
00:51:42,273 --> 00:51:45,202
우리는 그것을 열 벡터로 뻗는다.

949
00:51:45,202 --> 00:51:46,577
네 가지 요소,

950
00:51:46,577 --> 00:51:49,301
이제이 예에서 우리는

951
00:51:49,301 --> 00:51:51,765
3 개의 종류, 고양이, 개 및 배,

952
00:51:51,765 --> 00:51:54,030
슬라이드에 10을 넣을 수 없기 때문에,

953
00:51:54,030 --> 00:51:58,197
이제 우리의 체중 행렬은 4에 3이 될 것입니다.

954
00:51:59,890 --> 00:52:02,236
그래서 우리는 4 개의 픽셀과 3 개의 클래스를가집니다.

955
00:52:02,236 --> 00:52:05,567
그리고 다시, 우리는 세 요소
바이어스 벡터를 가지고 있습니다.

956
00:52:05,567 --> 00:52:08,858
그것은 우리에게 데이터 독립 바이어스 조건을 제공합니다.

957
00:52:08,858 --> 00:52:11,125
각 카테고리에 대해.

958
00:52:11,125 --> 00:52:13,467
이제 우리는 고양이 점수가 들어가는 것을 보았습니다.

959
00:52:13,467 --> 00:52:16,717
이미지의 픽셀 사이의 제품

960
00:52:18,436 --> 00:52:20,650
이 행은 가중치 행렬에 있습니다.

961
00:52:20,650 --> 00:52:22,814
이 편견 용어와 함께 추가되었습니다.

962
00:52:22,814 --> 00:52:25,134
그래서, 당신이 그것을이 방법으로 볼 때

963
00:52:25,134 --> 00:52:28,146
선형 분류를 이해할 수 있습니다.

964
00:52:28,146 --> 00:52:30,157
거의 템플리트 매칭 방식입니다.

965
00:52:30,157 --> 00:52:32,444
이 행렬의 각 행

966
00:52:32,444 --> 00:52:35,652
이미지의 일부 템플리트에 해당합니다.

967
00:52:35,652 --> 00:52:38,113
이제 제품 또는 내 제품 입력

968
00:52:38,113 --> 00:52:41,099
행렬의 행과 열 사이

969
00:52:41,099 --> 00:52:43,183
이미지의 픽셀을 제공하고,

970
00:52:43,183 --> 00:52:45,165
이 점 제품 종류를 계산하면 우리에게 준다.

971
00:52:45,165 --> 00:52:47,874
클래스에 대한이 템플릿 간의 유사성

972
00:52:47,874 --> 00:52:50,458
및 우리의 심상의 화소.

973
00:52:50,458 --> 00:52:52,906
그리고 나서 바이어스가이 데이터를 제공합니다.

974
00:52:52,906 --> 00:52:57,073
각 클래스에 대한 독립 스케일링 오프셋.

975
00:53:00,837 --> 00:53:02,401
선형 분류를 생각하면

976
00:53:02,401 --> 00:53:04,705
이 템플릿 매칭의 관점에서

977
00:53:04,705 --> 00:53:07,532
우리는 실제로 그 무게 매트릭스의 행을 취할 수 있습니다.

978
00:53:07,532 --> 00:53:09,965
그들을 다시 이미지로 풀다.

979
00:53:09,965 --> 00:53:12,799
실제로 이러한 템플릿을 이미지로 시각화합니다.

980
00:53:12,799 --> 00:53:14,734
그리고 이것은 우리에게 어떤 선형의 어떤 감각을줍니다.

981
00:53:14,734 --> 00:53:16,769
분류 작업이 실제로 수행 중일 수 있습니다.

982
00:53:16,769 --> 00:53:18,908
우리의 데이터를 이해하려고 노력합니다.

983
00:53:18,908 --> 00:53:21,057
따라서이 예에서 우리는 앞서 나가고 훈련을 받았습니다.

984
00:53:21,057 --> 00:53:23,208
우리의 이미지에 선형 분류기.

985
00:53:23,208 --> 00:53:25,355
그리고 이제 바닥에 우리는 시각화하고 있습니다.

986
00:53:25,355 --> 00:53:28,974
학습 된 가중치 행렬에있는 행은 무엇입니까?

987
00:53:28,974 --> 00:53:30,892
10 개 범주 각각에 해당

988
00:53:30,892 --> 00:53:32,274
CIFAR-10에서.

989
00:53:32,274 --> 00:53:34,117
그리고 이런 방식으로 우리는

990
00:53:34,117 --> 00:53:35,686
이 심상에서 계속.

991
00:53:35,686 --> 00:53:38,841
예를 들어, 왼쪽, 왼쪽 하단,

992
00:53:38,841 --> 00:53:40,978
우리는 평면 클래스의 템플릿을 봅니다.

993
00:53:40,978 --> 00:53:42,941
이런 종류의 푸른 얼룩,

994
00:53:42,941 --> 00:53:44,856
중간에 이런 종류의 blobby

995
00:53:44,856 --> 00:53:46,739
그리고 아마 백그라운드에서 파란색,

996
00:53:46,739 --> 00:53:49,212
이 선형 분류기가

997
00:53:49,212 --> 00:53:51,574
비행기가 파란 물건을 찾고있는 것 같아.

998
00:53:51,574 --> 00:53:54,585
및 blobby 물건, 그리고 그 기능을 일으킬거야

999
00:53:54,585 --> 00:53:57,606
비행기를 더 좋아하는 분류 자.

1000
00:53:57,606 --> 00:53:59,444
또는 우리가이 차보기를 보면,

1001
00:53:59,444 --> 00:54:02,441
우리는 붉은 색 얼룩 무늬가있는 것을 보았습니다.

1002
00:54:02,441 --> 00:54:04,801
상단에서 중간과 파란색 blobby 물건을 통해

1003
00:54:04,801 --> 00:54:08,721
어쩌면 흐린 바람막이 일 수 있습니다.

1004
00:54:08,721 --> 00:54:09,654
그러나 이것은 조금 이상합니다.

1005
00:54:09,654 --> 00:54:11,271
이것은 실제로 차처럼 보이지 않습니다.

1006
00:54:11,271 --> 00:54:13,716
어떤 개인의 차도 실제로 이처럼 보이지 않는다.

1007
00:54:13,716 --> 00:54:15,628
그래서 문제는 선형 분류기

1008
00:54:15,628 --> 00:54:18,317
각 클래스에 대해 하나의 템플릿 만 학습하고 있습니다.

1009
00:54:18,317 --> 00:54:20,823
그래서 그 수업의 종류에 변화가 있다면

1010
00:54:20,823 --> 00:54:21,748
나타날 수 있습니다.

1011
00:54:21,748 --> 00:54:24,340
그 모든 다른 유사 콘텐츠를 평균화하려하고 있습니다.

1012
00:54:24,340 --> 00:54:25,658
모든 다른 모습,

1013
00:54:25,658 --> 00:54:27,461
하나의 단일 템플릿 만 사용하십시오.

1014
00:54:27,461 --> 00:54:29,675
각 범주를 인식합니다.

1015
00:54:29,675 --> 00:54:31,961
우리는 말에서 이것을 꽤 명백하게 볼 수 있습니다.

1016
00:54:31,961 --> 00:54:33,139
분류 자.

1017
00:54:33,139 --> 00:54:35,776
그래서 말 분류 자에서 우리는
바닥에 녹색 물건을 보았습니다.

1018
00:54:35,776 --> 00:54:37,340
말은 보통 풀밭에 있기 때문입니다.

1019
00:54:37,340 --> 00:54:39,289
그리고주의 깊게 보면 말은 실제로

1020
00:54:39,289 --> 00:54:43,125
어쩌면 두 개의 머리, 각면에
머리가 하나있는 것 같습니다.

1021
00:54:43,125 --> 00:54:45,855
그리고 저는 두 마리의 머리를
가진 말을 본 적이 없습니다.

1022
00:54:45,855 --> 00:54:48,063
하지만 선형 분류기는 최선을 다하고 있습니다.

1023
00:54:48,063 --> 00:54:50,513
그것은 오직 배울 수 있기 때문에 할 수 있습니다.

1024
00:54:50,513 --> 00:54:52,788
카테고리 당 하나의 템플릿.

1025
00:54:52,788 --> 00:54:55,085
그리고 우리가 신경 네트워크로 나아갈 때

1026
00:54:55,085 --> 00:54:56,367
더 복잡한 모델,

1027
00:54:56,367 --> 00:54:59,460
우리는 더 나은 정확도를 달성 할 수있을 것이다.

1028
00:54:59,460 --> 00:55:01,230
그들은 더 이상이 제한이 없기 때문에

1029
00:55:01,230 --> 00:55:05,230
카테고리 당 하나의 템플릿 만 학습하면됩니다.

1030
00:55:09,030 --> 00:55:11,223
선형 분류기의 또 다른 관점

1031
00:55:11,223 --> 00:55:13,523
이 이미지에 대한 생각으로 돌아가는 것입니다.

1032
00:55:13,523 --> 00:55:15,649
포인트와 높은 차원 공간으로.

1033
00:55:15,649 --> 00:55:19,232
그리고 각각의 이미지가

1034
00:55:20,191 --> 00:55:23,328
이 고차원 공간의 한 점과 같습니다.

1035
00:55:23,328 --> 00:55:26,341
이제 선형 분류기는

1036
00:55:26,341 --> 00:55:29,305
선형 결정 경계선을 선형으로 그리려 함

1037
00:55:29,305 --> 00:55:31,402
한 카테고리 간의 분리

1038
00:55:31,402 --> 00:55:33,125
나머지 범주는

1039
00:55:33,125 --> 00:55:35,838
어쩌면 왼쪽 상단에 올라있을 수도 있습니다.

1040
00:55:35,838 --> 00:55:38,897
우리는 비행기의 훈련 예를 본다.

1041
00:55:38,897 --> 00:55:41,446
훈련 과정 전반에 걸쳐

1042
00:55:41,446 --> 00:55:43,845
선형 분류자가 가서 이것을 그리려고 할 것입니다.

1043
00:55:43,845 --> 00:55:46,692
한 줄로 분리하는 파란색 선

1044
00:55:46,692 --> 00:55:49,493
나머지 모든 클래스의 비행기 클래스.

1045
00:55:49,493 --> 00:55:51,492
그리고 실제로는 재미있는 일입니다.

1046
00:55:51,492 --> 00:55:53,902
이 과정은 무작위로 시작됩니다.

1047
00:55:53,902 --> 00:55:55,818
가서 분리하여 분리하려고 시도하십시오.

1048
00:55:55,818 --> 00:55:57,318
데이터를 올바르게.

1049
00:55:58,709 --> 00:56:00,921
그러나 선형 분류에 대해 생각할 때

1050
00:56:00,921 --> 00:56:04,000
이러한 방식으로,이 고차원 적 관점에서,

1051
00:56:04,000 --> 00:56:06,768
당신은 몇 가지 문제가 무엇인지 다시 볼 수 있습니다.

1052
00:56:06,768 --> 00:56:09,758
선형 분류가 생길 수도 있습니다.

1053
00:56:09,758 --> 00:56:11,672
그리고 예제를 구성하는 것이 어렵지 않습니다.

1054
00:56:11,672 --> 00:56:15,232
선형 분류기가 완전히 실패하는 데이터 세트를

1055
00:56:15,232 --> 00:56:16,998
한 가지 예를 들면, 여기 왼쪽에,

1056
00:56:16,998 --> 00:56:20,324
우리가 두 가지 범주의 데이터 집합을
가지고 있다고 가정 해 보겠습니다.

1057
00:56:20,324 --> 00:56:22,067
그리고 이것들은 모두 다소 인공적 일 수 있습니다.

1058
00:56:22,067 --> 00:56:24,990
하지만 아마도 우리의 데이터 세트에는 두 가지 범주가 있습니다.

1059
00:56:24,990 --> 00:56:26,095
파란색과 빨간색.

1060
00:56:26,095 --> 00:56:30,414
파란색 범주는 픽셀 수입니다.

1061
00:56:30,414 --> 00:56:33,122
0보다 큰 이미지에서 홀수입니다.

1062
00:56:33,122 --> 00:56:34,944
그리고 픽셀 수가 더 많은 곳에서는

1063
00:56:34,944 --> 00:56:38,714
0보다 큽니다. 빨간색 카테고리로 분류하고 싶습니다.

1064
00:56:38,714 --> 00:56:42,881
그래서 당신이 실제로 가서이 다른 것을 그린다면

1065
00:56:43,991 --> 00:56:46,259
결정 영역은 비행기에서와 같이 보이지만,

1066
00:56:46,259 --> 00:56:49,907
당신은 홀수 개의 픽셀을 가진 우리의
푸른 클래스를 볼 수 있습니다.

1067
00:56:49,907 --> 00:56:53,426
비행기에서이 두 사분면이 될 것입니다.

1068
00:56:53,426 --> 00:56:56,063
심지어 반대편 두 사분면이 될 것입니다.

1069
00:56:56,063 --> 00:56:59,609
이제는 하나의 선형 선을 그릴 수있는 방법이 없습니다.

1070
00:56:59,609 --> 00:57:01,364
파란색에서 빨간색을 분리합니다.

1071
00:57:01,364 --> 00:57:03,412
그래서 이것은 선형 분류 자

1072
00:57:03,412 --> 00:57:05,273
정말 어려울 것입니다.

1073
00:57:05,273 --> 00:57:09,535
그리고 이것은 어쩌면 그런 인공적인
것이 아닐 수도 있습니다.

1074
00:57:09,535 --> 00:57:10,912
화소를 계수하는 대신에,

1075
00:57:10,912 --> 00:57:13,042
어쩌면 우리는 실제로 그 숫자가

1076
00:57:13,042 --> 00:57:16,424
동물이나 사람의 이미지가 이상하거나 균등합니다.

1077
00:57:16,424 --> 00:57:19,004
그래서 이런 종류의 패리티 문제

1078
00:57:19,004 --> 00:57:21,145
확율로부터 확률을 분리하는 방법

1079
00:57:21,145 --> 00:57:22,725
그 선형 분류

1080
00:57:22,725 --> 00:57:25,725
전통적으로 정말로 투쟁합니다.

1081
00:57:28,376 --> 00:57:31,438
선형 분류기가 실제로 어려움을 겪는 다른 상황

1082
00:57:31,438 --> 00:57:33,974
multimodal 상황입니다.

1083
00:57:33,974 --> 00:57:35,146
그래서 여기 오른쪽에,

1084
00:57:35,146 --> 00:57:39,541
아마 우리의 파란 종류에는이 3 개의 다른 섬이 있습니다

1085
00:57:39,541 --> 00:57:41,915
파란 종류가 사는 곳에,

1086
00:57:41,915 --> 00:57:44,791
다른 모든 것은 다른 카테고리입니다.

1087
00:57:44,791 --> 00:57:46,529
그래서, 말과 같은 것을 위해,

1088
00:57:46,529 --> 00:57:47,961
우리는 앞의 예를 보았습니다.

1089
00:57:47,961 --> 00:57:50,106
이것이 실제로 일어날 수있는 무언가입니다.

1090
00:57:50,106 --> 00:57:50,959
실제로.

1091
00:57:50,959 --> 00:57:53,560
픽셀 공간에 하나의 섬이있는 곳

1092
00:57:53,560 --> 00:57:55,159
왼쪽을 보면서 말,

1093
00:57:55,159 --> 00:57:57,268
그리고 오른쪽으로 보이는 또 다른 말 섬.

1094
00:57:57,268 --> 00:58:00,360
이제는 단일 선형을 그리는 좋은 방법이 없습니다.

1095
00:58:00,360 --> 00:58:03,757
이 두 분리 된 섬 사이의 경계.

1096
00:58:03,757 --> 00:58:07,257
멀티 모달 데이터가있는 곳이라면 언제든지

1097
00:58:08,098 --> 00:58:08,931
한 반처럼

1098
00:58:08,931 --> 00:58:10,854
우주의 다른 지역에서 나타날 수있는

1099
00:58:10,854 --> 00:58:13,963
선형 분류가 어려울 수도있는 또 다른 장소입니다.

1100
00:58:13,963 --> 00:58:15,932
그래서 여러 가지 문제가 있습니다.

1101
00:58:15,932 --> 00:58:18,575
선형 분류 자 (linear classifier)이지만,
그것은 매우 간단한 알고리즘이며,

1102
00:58:18,575 --> 00:58:22,259
슈퍼 멋지고 해석하기 쉽고 이해하기 쉽습니다.

1103
00:58:22,259 --> 00:58:24,412
그래서 당신은 실제로 이런 것들을 구현할 것입니다.

1104
00:58:24,412 --> 00:58:27,245
너의 첫번째 숙제에.

1105
00:58:28,852 --> 00:58:29,971
이 지점에서,

1106
00:58:29,971 --> 00:58:30,980
우리는 얼마간 얘기했다.

1107
00:58:30,980 --> 00:58:33,313
기능적 형태는 무엇입니까?

1108
00:58:33,313 --> 00:58:34,402
선형 분류 자.

1109
00:58:34,402 --> 00:58:36,711
그리고 우리는이 기능적 형태가

1110
00:58:36,711 --> 00:58:39,185
행렬 벡터 곱셈의

1111
00:58:39,185 --> 00:58:41,541
이 템플릿 매칭 개념에 해당함

1112
00:58:41,541 --> 00:58:43,364
각 카테고리에 대한 단일 템플릿 학습

1113
00:58:43,364 --> 00:58:44,922
귀하의 데이터에.

1114
00:58:44,922 --> 00:58:48,339
그리고 일단 우리가이 훈련 된 행렬을 가지면

1115
00:58:49,485 --> 00:58:52,499
당신은 실제로 가서 점수를 얻는데 사용할 수 있습니다.

1116
00:58:52,499 --> 00:58:55,738
어떤 새로운 훈련 예도.

1117
00:58:55,738 --> 00:58:58,104
하지만 우리가 당신에게 말하지 않은 것은

1118
00:58:58,104 --> 00:59:00,597
당신은 실제로 올바른 W를 선택하는
방법에 대해 실제로 어떻게 생각합니까?

1119
00:59:00,597 --> 00:59:01,951
귀하의 데이터 세트.

1120
00:59:01,951 --> 00:59:03,908
우리는 방금 기능적인 형태가
무엇인지에 대해 이야기했습니다.

1121
00:59:03,908 --> 00:59:06,907
그리고이 일에 무슨 일이 일어나고 있는지.

1122
00:59:06,907 --> 00:59:11,086
그래서 우리는 다음 번에 정말로 집중할 것입니다.

1123
00:59:11,086 --> 00:59:12,933
그리고 우리가 이야기 할 다음 강의

1124
00:59:12,933 --> 00:59:14,668
전략과 알고리즘은 무엇인가?

1125
00:59:14,668 --> 00:59:16,581
오른쪽 W를 선택하십시오.

1126
00:59:16,581 --> 00:59:17,708
그리고 이것은 우리를 질문으로 이끌 것입니다.

1127
00:59:17,708 --> 00:59:19,544
손실 함수 및 최적화

1128
00:59:19,544 --> 00:59:21,322
결국 ConvNets.

1129
00:59:21,322 --> 00:59:25,044
그래서, 다음주에 미리보기가 약간 있습니다.

1130
00:59:25,044 --> 00:00:00,000
그게 오늘 우리가 가진 전부입니다.

