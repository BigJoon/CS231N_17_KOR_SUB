1
00:00:15,562 --> 00:00:22,506
9강입니다. 오늘은 CNN 아키텍쳐들을
한번 알아보겠습니다.

2
00:00:22,506 --> 00:00:28,516
수업에 앞서 몇 가지 공지사항을 전달해 드리겠습니다.
우선 과제 2는 목요일까지 입니다.

3
00:00:28,516 --> 00:00:37,665
중간고사는 다음 주인 5월 9일 목요일 수업시간이 진행하도록 하겠습니다.
그 전까지 중간고사 범위 진도를 모두 나갈 예정입니다.

4
00:00:37,665 --> 00:00:42,160
Recurrent neural network 까지 차질없이
진행할 수 있도록 하겠습니다.

5
00:00:42,160 --> 00:00:49,931
그리고 우리 수업의 포스터세션이 6월 6일 12시부터 3시까지
진행될 예정입니다. 우리 수업의 마지막 주가 되겠군요

6
00:00:49,931 --> 00:00:54,638
이번에는 포스터세션이 조금 일찍 열립니다.

7
00:00:54,638 --> 00:01:00,942
포스터 세션까지 아직 시간이 남아있으니 
기말 레포트에 최선을 다해주시기 바랍니다.

8
00:01:04,135 --> 00:01:06,622
지난 시간에 배운 내용을 복습해 보겠습니다.

9
00:01:06,622 --> 00:01:10,134
지난 시간에 다양한 딥러닝 프레임워크들을 배웠습니다.

10
00:01:10,134 --> 00:01:13,500
도PyTorch, TensorFlow, Caffe2 등이 있었죠

11
00:01:15,324 --> 00:01:19,572
이런 프레임워크를 이용하게 되면 NN, CNN같은 규모가 큰 
computraional graphs를 아주 쉽게 구성할 수 있었습니다.

12
00:01:19,572 --> 00:01:26,594
또한 그래프에서 gradients을 계산하기에도 
아주 수월했습니다.

13
00:01:26,594 --> 00:01:33,225
네트워크 중간의 가중치와 입력변수들의 그레디언트를
알아서 계산해 주기 때문에 Train에 사용만 하면 됩니다.

14
00:01:33,225 --> 00:01:36,475
그리고 이 모든 것을 GPU를 통해 아주 효율적으로
동작시킬 수 있습니다.

15
00:01:38,468 --> 00:01:45,788
그리고 프레임워크들은 대게 이런 식으로 
모듈화된 레이어를 통해 동작합니다.

16
00:01:45,788 --> 00:01:50,738
여러분이 과제로 작성했던 backward/forward pass와
아주 유사한 모습입니다.

17
00:01:50,738 --> 00:01:59,214
모델 아키텍쳐를 구성하기 위해서는 단지 그 레이어들을
하나의 시퀀스로 정의하고 묶어주기만 하면 됩니다.

18
00:01:59,214 --> 00:02:05,747
이를 통해 아주 복잡한 아키텍쳐라고 해도 
손쉽게 구성할 수 있습니다.

19
00:02:07,436 --> 00:02:15,330
오늘은 최신 CNN 아키텍쳐들에 대해서 배워보겠습니다.

20
00:02:15,330 --> 00:02:20,441
사람들이 가장 많이 사용하는 아키텍쳐들을 
아주 심도깊게 살펴볼 것입니다.

21
00:02:20,441 --> 00:02:22,935
 이들은 모두 ImageNet 첼린지에서 우승한 모델들이죠

22
00:02:22,935 --> 00:02:28,895
여기에는 제가 연대순으로 정렬했습니다. AlexNet, VGGNet,
GoogLeNet 그리고 ResNet이 있습니다.

23
00:02:28,895 --> 00:02:44,581
그리고 또한 엄청 잘 사용하지는 않지만 역사적인 관점에서
아주 흥미로운 모델들, 그리고 아주 최신의 모델들도 다룰 것입니다.

24
00:02:47,632 --> 00:02:51,649
아주 오래 전 강의에서 LeNet을 다룬 적이 있었습니다.

25
00:02:51,649 --> 00:02:56,413
LeNet은 산업에 아주 성공적으로 적용된 최초의 ConvNet입니다.

26
00:02:56,413 --> 00:03:06,588
LeNet은 이미지를 입력으로 받아서 stride = 1 인 5 x 5 필터를
거치고 몇 개의 Conv Layer와 pooling layer를 거칩니다.

27
00:03:06,588 --> 00:03:10,145
그리고 끝 단에 FC Layer가 붙습니다.

28
00:03:10,145 --> 00:03:15,130
엄청 간단한 모델이지만 숫자 인식에서
엄청난 성공을 거두었습니다.

29
00:03:17,840 --> 00:03:23,685
2012년에 AlexNet이 나왔습니다. 이 모델도 이전 강의에서
다들 본 적 있으실 것ㅇ비니다.

30
00:03:23,685 --> 00:03:31,989
AlexNet은 최초의 Large scale CNN 입니다. ImageNet
Classification Task을 아주 잘 수행했습니다.

31
00:03:31,989 --> 00:03:41,421
AlexNet은 2012년에 등장해서는 기존의 non-딥러닝 모델들을 
능가하는 놀라운 성능을 보여줬습니다.

32
00:03:41,421 --> 00:03:48,822
AlexNet은 ConvNet 연구의 부흥을 일으킨 장본입니다.

33
00:03:48,822 --> 00:03:57,237
AlexNet은 기본적으로 conv - pool - normalization
구조가 두 번 반복됩니다.

34
00:03:59,231 --> 00:04:01,816
그리고 conv layer가 조금 더 붙고(CONV 3,4,5) 
그 뒤에 pooling layer가 있습니다. (Max POOL3)

35
00:04:01,816 --> 00:04:04,232
그리고 마지막에는 FC-layer가 몇 개 붙습니다.
(FC6, FC7, FC8)

36
00:04:04,232 --> 00:04:10,576
생긴 것만 봐서는 기존의 LeNet과 상당히 유사합니다.
레이어만 더 많아졌습니다.

37
00:04:10,576 --> 00:04:19,197
AlexNet는 5개의 Conv Layer와 
2개의 FC-Layer로 구성됩니다.

38
00:04:22,699 --> 00:04:26,740
그렇다면 이제 AlexNet 의 모델 크기를 한번 살펴봅시다.

39
00:04:26,740 --> 00:04:33,938
AlexNet의 ImageNet으로 학습시키는 경우 
입력의 크기가 227 x 227 x 3 입니다.

40
00:04:33,938 --> 00:04:44,003
AlexNet의 첫 레이어를 살펴보면 11 x 11 필터가
stride = 4 로 96개가 존재합니다.

41
00:04:44,003 --> 00:04:50,133
그렇다면 잠시 멈춰서 생각해봅시다. 
첫 레이어의 출력사이즈는 어떻게 될까요?

42
00:04:52,598 --> 00:04:54,181
힌트도 있습니다.

43
00:04:58,579 --> 00:05:12,251
입력과 Conv 필터의 사이즈를 알고 있습니다.
그리고 출력값의 차원도 여기 힌트로 있습니다.

44
00:05:12,251 --> 00:05:18,442
이 공식은 (전체 이미지 크기 - 필터 크기) / Stride + 1 이죠

45
00:05:18,442 --> 00:05:27,729
결국 출력차원은 55입니다.
답이 뭘까요?

46
00:05:27,729 --> 00:05:30,633
[학생이 대답]

47
00:05:30,633 --> 00:05:33,776
55 x 55 x 96 이라고 답했습니다. 맞습니다.

48
00:05:33,776 --> 00:05:38,923
출력값의 width, height는 각각 55 입니다.

49
00:05:38,923 --> 00:05:46,201
그리고 필터가 총 96개 이므로 depth가 96이 됩니다.

50
00:05:46,201 --> 00:05:50,296
자 그러면 이 레이어의 전체 파라미터 갯수는 몇 개일까요?

51
00:05:50,296 --> 00:05:53,629
명심해야 할 점은 
11 x 11 필터가 총 96개 있다는 것입니다.

52
00:05:55,661 --> 00:05:58,563
[학생이 대답]

53
00:05:58,563 --> 00:06:01,563
96 x 11 x11 이라고 답했습니다. 
거의 맞췄습니다.

54
00:06:02,755 --> 00:06:06,107
네 맞습니다. 3을 더 곱해줘야죠

55
00:06:06,107 --> 00:06:14,442
필터 하나가 11 x 11 x 3 을 통과합니다.
입력의 Depth가 3이죠

56
00:06:14,442 --> 00:06:19,793
답은 전체 필터의 크기 X 96 이 됩니다.

57
00:06:19,793 --> 00:06:23,960
첫 레이어에 35K의 파라미터가 있는 것입니다.

58
00:06:26,828 --> 00:06:31,043
자 그러면 두 번째 레이어를 한번 살펴봅시다. 
두 번째 레이어는 Pooling Layer 입니다.

59
00:06:31,043 --> 00:06:34,814
여기에는 stride = 2 인 3 x 3 필터가 있습니다.

60
00:06:34,814 --> 00:06:38,981
이 레이어의 출력값의 크기는 어떻게 될까요?

61
00:06:41,511 --> 00:06:45,678
힌트도 드렸습니다. 지난 문제와 아주 유사합니다.

62
00:06:52,061 --> 00:06:57,077
27 x 27 x 96 이라고 답했습니다.
네 맞습니다.

63
00:06:58,526 --> 00:07:02,338
Pooling Layer에서는 힌트에 나와있는 공식이
적용될 것입니다.

64
00:07:02,338 --> 00:07:17,465
이 공식을 이용해서 width, height를 구할 수 있습니다.
다만 depth는 입력과 변하지 않습니다.

65
00:07:17,465 --> 00:07:22,337
입력의 Depth가 96 이었으니 출력의 Depth도 96 입니다.

66
00:07:23,635 --> 00:07:28,937
그렇다면 이 레이어의 파라미터는 몇 개일까요?

67
00:07:32,256 --> 00:07:35,164
[학생이 대답]

68
00:07:35,164 --> 00:07:37,715
없다고 대답했습니다. 네 맞습니다.

69
00:07:37,715 --> 00:07:41,611
Pooling layer에는 파라미터가 없죠. 
훼이크였습니다.

70
00:07:43,549 --> 00:07:46,082
네 질문있나요?

71
00:07:46,082 --> 00:07:48,002
[학생이 질문]

72
00:07:48,002 --> 00:07:52,990
질문은 바로 왜 pooling layer에는 파라미터가
없는지 입니다.

73
00:07:52,990 --> 00:07:55,361
파라미터는 우리가 학습시키는 가중치입니다.

74
00:07:55,361 --> 00:07:57,321
Conv Layer에는 학습할 수 있는 가중치가 있습니다.

75
00:07:57,321 --> 00:08:03,046
반면 pooling의 경우에는 가중치가 없고 그저 
특정 지역에서 큰 값을 뽑아내는 역할만 합니다.

76
00:08:03,046 --> 00:08:06,520
따라서 학습시킬 파라미터가 없는 것이죠

77
00:08:06,520 --> 00:08:15,060
다들 집에가서 모든 레이어의 파라미터 사이즈를 계산해
보면 아주 큰 도움이 될 것입니다.

78
00:08:17,283 --> 00:08:23,498
이는 AlexNet의 전체 구조입니다.

79
00:08:23,498 --> 00:08:32,730
Conv layer들의 파라미터 크기는  앞서 계산한 값과 유사할 것입니다.

80
00:08:32,730 --> 00:08:39,932
그리고 끝에 몇 개의 FC-Layer가 있었습니다.
4096개의 노드를 가진 레이어입니다.

81
00:08:39,933 --> 00:08:42,350
그리고 FC8 는 Softmax를 통과합니다.

82
00:08:43,499 --> 00:08:47,166
1000 ImageNet 클래스로 이동합니다.

83
00:08:48,849 --> 00:08:57,162
AlexNet을 조금 더 자세히 살펴보자면 우선 ReLU 를 사용했습니다.
ReLU는 딥러닝 모델에서 아주 보편화된 방법입니다.

84
00:08:57,162 --> 00:09:08,201
local response normalization layer는 채널간의 
 normalization을 위한 것인데 요즘은 잘 사용하지 않습니다.

85
00:09:08,201 --> 00:09:12,747
큰 효과가 없는 것으로 알려졌기 때문입니다.

86
00:09:12,747 --> 00:09:22,579
data augumentation을 엄청 했습니다. 논문이 더 자세하지만 
flipping, jittering, color norm 등을 적용하였습니다.

87
00:09:22,579 --> 00:09:29,537
data augumentation은 여러분이 프로젝트를
진행할 때 아주 유용한 기법입니다.

88
00:09:29,537 --> 00:09:33,229
AlexNet은 Dropout을 사용했습니다. 
학습 시 Batch size는 128 입니다.

89
00:09:33,229 --> 00:09:37,993
그리고 우리도 지난 강의에서 배웠던 
SGD momentum을 사용했습니다.

90
00:09:37,993 --> 00:09:43,105
그리고 초기 Learning rate 는 1e-2 입니다.

91
00:09:43,105 --> 00:09:50,955
그리고 val accuracy가 올라가지 않는 지점에서는 학습이 종료되는
시점까지 Learning rate를 1e-10까지 줄입니다.

92
00:09:50,955 --> 00:09:59,822
그리고 wight decay를 사용했고, 마지막에는 
모델 앙상블로 성능을 향상시켰습니다.

93
00:09:59,822 --> 00:10:03,972
모델을 여러개 학습시켜서 앙상블시켜
성능을 개선했습니다.

94
00:10:05,215 --> 00:10:07,138
그래서 내가 지적하고자하는 또 하나의

95
00:10:07,138 --> 00:10:09,591
만약이 AlexNet 다이어그램을 여기서 보면,

96
00:10:09,591 --> 00:10:13,390
그것은 정상적인 comNet 다이어그램처럼 보입니다.

97
00:10:13,390 --> 00:10:16,045
우리가 보았던 것, 단 하나의 차이점을 제외하고,

98
00:10:16,045 --> 00:10:18,580
그것은 그것이 다름 아닌 일종의 분할임을 알 수 있습니다.

99
00:10:18,580 --> 00:10:22,747
이 두 개의 서로 다른 행이나 열에서

100
00:10:23,987 --> 00:10:27,431
그리고 이것에 대한 이유는 대부분 역사적인 기록입니다.

101
00:10:27,431 --> 00:10:31,431
그래서 AlexNet은 GTX580 GPUs
구형 GPU에 대한 교육을 받았습니다.

102
00:10:32,632 --> 00:10:34,916
그것은 단지 3 기가의 공연을 가지고있었습니다.

103
00:10:34,916 --> 00:10:38,065
따라서 실제로이 전체 네트워크를
여기에 맞출 수는 없습니다.

104
00:10:38,065 --> 00:10:39,757
그래서 그들이 결국 무엇을했는지,

105
00:10:39,757 --> 00:10:42,583
그들은 두 GPU에 걸쳐 네트워크를 보급 했습니까?

106
00:10:42,583 --> 00:10:45,091
따라서 각 GPU에서 절반의 뉴런을 갖게됩니다.

107
00:10:45,091 --> 00:10:47,265
또는 기능 맵의 절반.

108
00:10:47,265 --> 00:10:50,040
예를 들어이 첫 번째 전환 레이어를 보면

109
00:10:50,040 --> 00:10:52,540
우리는 55 x 56 출력을 가지고 있습니다.

110
00:10:55,199 --> 00:10:57,568
그러나이 도표를 신중하게 보면,

111
00:10:57,568 --> 00:11:00,144
실제 종이에서 나중에 확대 할 수 있습니다.

112
00:11:00,144 --> 00:11:02,965
당신은 그것을 볼 수 있습니다, 그것은 실제로 단지 48입니다.

113
00:11:02,965 --> 00:11:04,965
각 GPU에서 심층적으로,

114
00:11:05,859 --> 00:11:07,986
그래서 그들은 그것을 확산 시켰고, 특징지도들,

115
00:11:07,986 --> 00:11:09,403
반으로 직접.

116
00:11:11,098 --> 00:11:13,269
그래서 이러한 계층의 대부분에서,

117
00:11:13,269 --> 00:11:16,580
예를 들어, 1, 2, 4 및 5,

118
00:11:16,580 --> 00:11:18,177
연결은 기능 맵과 만 연결됩니다.

119
00:11:18,177 --> 00:11:21,816
같은 GPU에서, 당신은 입력으로 받아 들일 것이고,

120
00:11:21,816 --> 00:11:26,072
동일한 GPU에 있던 기능 맵의 절반

121
00:11:26,072 --> 00:11:29,304
이전처럼 그리고 당신은 전체 96을 보지 않는다.

122
00:11:29,304 --> 00:11:30,493
예를 들어 기능지도.

123
00:11:30,493 --> 00:11:34,660
첫 번째 레이어에서 48을 입력하면됩니다.

124
00:11:35,577 --> 00:11:38,527
그리고 몇 가지 레이어가 있으므로 com 3,

125
00:11:38,527 --> 00:11:42,006
뿐만 아니라 FC 6, 7, 8,

126
00:11:42,006 --> 00:11:45,195
여기 GPU는 서로 이야기를 나눕니다.

127
00:11:45,195 --> 00:11:47,673
모든 기능 맵과의 연결이 있습니다.

128
00:11:47,673 --> 00:11:48,506
이전 계층에서

129
00:11:48,506 --> 00:11:50,125
그래서 GPU를 통한 통신이 있습니다.

130
00:11:50,125 --> 00:11:52,015
이 뉴런들 각각이 연결된다.

131
00:11:52,015 --> 00:11:55,001
이전 입력 레이어의 전체 깊이까지

132
00:11:55,001 --> 00:11:56,437
문제.

133
00:11:56,437 --> 00:11:59,141
- [학생] 그것은 단순화 된 AlexNetwork

134
00:11:59,141 --> 00:12:00,085
건축물.

135
00:12:00,085 --> 00:12:02,252
[mumbles]

136
00:12:06,393 --> 00:12:07,781
- 오케이, 질문은 왜 그런 말인가?

137
00:12:07,781 --> 00:12:10,843
전체 단순화 된 AlexNet 아키텍처는 여기에 있습니까?

138
00:12:10,843 --> 00:12:13,139
내가 모든 세부 사항을 넣지 않았기 때문에

139
00:12:13,139 --> 00:12:17,192
여기에, 예를 들어 이것은 전체 세트의 레이어입니다.

140
00:12:17,192 --> 00:12:19,846
건축에서, 그리고 걷는 것 등등.

141
00:12:19,846 --> 00:12:22,911
예를 들어 정규화 레이어에는 다른 레이어가 있습니다.

142
00:12:22,911 --> 00:12:26,078
이 세부 사항은 여기에 쓰여 있지 않습니다.

143
00:12:31,447 --> 00:12:33,244
그리고 나서 하나의 작은 메모,

144
00:12:33,244 --> 00:12:35,576
신문을보고 시도하고 쓰려면

145
00:12:35,576 --> 00:12:38,659
수학과 건축 등등.

146
00:12:39,668 --> 00:12:43,178
첫 번째 문제에 약간의 문제가 있습니다.

147
00:12:43,178 --> 00:12:45,474
당신이 그림을 보면 그들이 말하는 계층

148
00:12:45,474 --> 00:12:47,276
그들은 224에서 224,

149
00:12:47,276 --> 00:12:49,439
실제로 재미있는 패턴이 있습니다.

150
00:12:49,439 --> 00:12:51,364
계속해서 번호가 실제로 작동합니다.

151
00:12:51,364 --> 00:12:53,531
네가 227로 본다면.

152
00:12:55,792 --> 00:12:59,959
AlexNet은 ImageNet 분류의 우승자였습니다.

153
00:13:00,944 --> 00:13:03,396
2012 년 벤치 마크에서

154
00:13:03,396 --> 00:13:06,056
오류율을 상당히 크게 줄였습니다.

155
00:13:06,056 --> 00:13:10,223
최초의 CNN베이스 우승자였으며 널리 사용되었습니다.

156
00:13:11,750 --> 00:13:13,430
우리 아키텍처의 기초

157
00:13:13,430 --> 00:13:16,530
몇 년 전까지 거의 유비 쿼터스에서.

158
00:13:16,530 --> 00:13:18,790
그것은 아직도 꽤 많이 사용됩니다.

159
00:13:18,790 --> 00:13:22,066
다양한 작업을위한 전송 학습에 사용됩니다.

160
00:13:22,066 --> 00:13:24,881
그래서 기본적으로 오랜 시간 동안 사용되었습니다.

161
00:13:24,881 --> 00:13:28,487
그리고 그것은 매우 유명했습니다.

162
00:13:28,487 --> 00:13:31,677
일반적으로 가지고있는 좀 더 최근의 아키텍처

163
00:13:31,677 --> 00:13:34,012
성능이 좋았으므로 여기에 대해 이야기하겠습니다.

164
00:13:34,012 --> 00:13:36,592
다음은 더 일반적인 아키텍처가 될 것입니다.

165
00:13:36,592 --> 00:13:40,092
실제로 사용하고 싶을 것입니다.

166
00:13:41,663 --> 00:13:45,838
2013 년 처음으로 ImageNet의 도전

167
00:13:45,838 --> 00:13:48,623
ZFNet이라고 불리는 것으로 이겼습니다.

168
00:13:48,623 --> 00:13:49,528
네, 질문.

169
00:13:49,528 --> 00:13:53,539
[학생은 마이크에서 소리 쳤다]

170
00:13:53,539 --> 00:13:55,298
- 그래서 질문은 AlexNet이 왜 직관인지에 대한 것입니다.

171
00:13:55,298 --> 00:13:57,422
전에 온 사람들보다 훨씬 좋았습니다.

172
00:13:57,422 --> 00:14:01,600
DefLearning comNets [mumbles] 이것은 단지

173
00:14:01,600 --> 00:14:05,596
아키텍처에서 매우 다른 종류의 접근 방식.

174
00:14:05,596 --> 00:14:07,564
그래서 이것은 최초의 깊은 학습 기반 접근 방식이었습니다.

175
00:14:07,564 --> 00:14:09,814
첫 번째로 사용 된 comNet.

176
00:14:13,255 --> 00:14:16,126
그래서 2013 년에 도전 과제는

177
00:14:16,126 --> 00:14:19,108
ZFNet [Zeller Fergus Net]
창조자의 이름을 딴 것.

178
00:14:19,108 --> 00:14:23,537
그래서 대부분이 하이퍼 매개 변수를 개선하고있었습니다.

179
00:14:23,537 --> 00:14:24,559
AlexNet을 통해.

180
00:14:24,559 --> 00:14:26,194
그것은 같은 수의 레이어를 가지고 있었고,

181
00:14:26,194 --> 00:14:28,566
같은 일반적인 구조와 그들이 만든 몇 가지

182
00:14:28,566 --> 00:14:31,518
보폭 크기 변경,

183
00:14:31,518 --> 00:14:34,909
필터의 수와 재생 횟수

184
00:14:34,909 --> 00:14:36,545
이 하이퍼 매개 변수가 더 많으면,

185
00:14:36,545 --> 00:14:40,751
그들은 오류율을 향상시킬 수있었습니다.

186
00:14:40,751 --> 00:14:42,179
그러나 여전히 기본적으로 같은 생각입니다.

187
00:14:42,179 --> 00:14:44,720
그래서 2014 년에는 몇 가지 아키텍처가 있습니다.

188
00:14:44,720 --> 00:14:47,291
더 현저하게 다른

189
00:14:47,291 --> 00:14:50,653
공연에서 또 하나의 도약을했다.

190
00:14:50,653 --> 00:14:54,820
이러한 네트워크와의 주요 차이점

191
00:14:56,699 --> 00:14:58,988
훨씬 더 깊은 네트워크였습니다.

192
00:14:58,988 --> 00:15:01,926
그래서 2012 년에 있었던 8 층 네트워크에서

193
00:15:01,926 --> 00:15:06,093
2013 년 현재 2014 년에 두
명의 매우 근접한 수상자가있었습니다.

194
00:15:07,606 --> 00:15:10,999
그것은 약 19 층과 22 층이었습니다.

195
00:15:10,999 --> 00:15:13,131
그래서 훨씬 더 깊어졌습니다.

196
00:15:13,131 --> 00:15:17,312
그리고이 우승자는 Google의 GoogleNet입니다.

197
00:15:17,312 --> 00:15:20,986
하지만 매우 뒤에 VGGNet라는 것이있었습니다.

198
00:15:20,986 --> 00:15:24,750
옥스포드에서, 그리고 실제로 현지화 도전에

199
00:15:24,750 --> 00:15:28,231
VGG는 다른 트랙 중 일부에서 1 위를 차지했습니다.

200
00:15:28,231 --> 00:15:32,768
그래서 이것들은 매우 강력한 네트워크였습니다.

201
00:15:32,768 --> 00:15:35,473
먼저 VGG에 대해 좀 더 자세히 살펴 보겠습니다.

202
00:15:35,473 --> 00:15:39,458
그래서 VGG 네트워크는 훨씬 더
깊은 네트워크의 아이디어입니다.

203
00:15:39,458 --> 00:15:41,628
필터가 훨씬 작습니다.

204
00:15:41,628 --> 00:15:44,795
그래서 그들은 층수를 늘렸다.

205
00:15:45,632 --> 00:15:48,854
AlexNet의 8 개 레이어에서 지금까지

206
00:15:48,854 --> 00:15:52,021
VGGNet에서 16-19 층의 모델

207
00:15:53,100 --> 00:15:55,196
그리고 한 가지 중요한 사실은 그들이
아주 작게 유지되었다는 것입니다.

208
00:15:55,196 --> 00:15:58,307
필터를 세 번에 걸쳐 세 번만 클릭하면됩니다.

209
00:15:58,307 --> 00:16:01,459
기본적으로 가장 작은 COM 필터 크기입니다

210
00:16:01,459 --> 00:16:04,726
이웃 한 픽셀을 조금 보아요.

211
00:16:04,726 --> 00:16:07,100
그리고 그들은이 매우 단순한 구조를 유지했습니다.

212
00:16:07,100 --> 00:16:09,553
주기적으로 풀링하는 3 ~ 3 개의 전환

213
00:16:09,553 --> 00:16:12,295
네트워크를 통해

214
00:16:12,295 --> 00:16:15,486
그리고 그것은 매우 단순한 우아한 네트워크 아키텍처,

215
00:16:15,486 --> 00:16:18,591
7.3 %의 상위 5 개 오류를 얻을 수있었습니다.

216
00:16:18,591 --> 00:16:20,758
ImageNet 도전에.

217
00:16:23,461 --> 00:16:28,252
먼저 작은 필터를 사용해야하는
이유에 대한 첫 번째 질문입니다.

218
00:16:28,252 --> 00:16:31,931
그래서 우리가이 작은 필터들을 취할 때 우리는

219
00:16:31,931 --> 00:16:34,181
매개 변수가 적어지고 더 많은
매개 변수를 시도하고 스택합니다.

220
00:16:34,181 --> 00:16:35,659
더 큰 필터를 갖는 대신에,

221
00:16:35,659 --> 00:16:37,869
대신에 깊이가 더 작은 필터를 사용하십시오.

222
00:16:37,869 --> 00:16:40,154
대신에 이러한 필터가 더 많이 필요합니다.

223
00:16:40,154 --> 00:16:43,384
당신이 똑같은 효과를 가지게되는 것입니다.

224
00:16:43,384 --> 00:16:46,414
수용체 장은 마치 7 대 7이있는 것처럼

225
00:16:46,414 --> 00:16:48,012
길쌈 층.

226
00:16:48,012 --> 00:16:52,099
그래서 여기에 질문이 있습니다.
효과적인 수용 필드는 무엇입니까?

227
00:16:52,099 --> 00:16:55,416
이 3 개의 3 개 중 3 개의 전환 층

228
00:16:55,416 --> 00:16:56,276
큰 걸음으로?

229
00:16:56,276 --> 00:16:58,854
따라서 3 ~ 3 개의 전환 층을 쌓으려면

230
00:16:58,854 --> 00:17:01,999
효과적인 수용체 란 무엇인가?

231
00:17:01,999 --> 00:17:06,166
입력의 전체 면적, 입력의 공간 면적

232
00:17:07,232 --> 00:17:10,463
세 계층의 최상위 계층에서 실행됩니다.

233
00:17:10,463 --> 00:17:11,630
찾고있다.

234
00:17:13,123 --> 00:17:16,797
그래서 나는 15 개의 픽셀,
15 개의 픽셀을 들었습니까?

235
00:17:16,797 --> 00:17:21,419
- 알았어, 그 이유는

236
00:17:21,419 --> 00:17:22,276
그들은 겹쳐진다.

237
00:17:22,276 --> 00:17:26,662
좋아, 그 이유는 그들이 겹치기 때문 이었어.

238
00:17:26,662 --> 00:17:28,179
그래서 그것은 바른 길 위에 있습니다.

239
00:17:28,179 --> 00:17:32,350
실제로 일어나고있는 것은 비록 당신이보아야 만하지만,

240
00:17:32,350 --> 00:17:34,803
첫 번째 레이어에서 수용 필드는

241
00:17:34,803 --> 00:17:36,478
3시 3 분 오른쪽?

242
00:17:36,478 --> 00:17:38,558
그리고 두 번째 레이어에서,

243
00:17:38,558 --> 00:17:41,379
두 번째 층에있는 각각의 뉴런

244
00:17:41,379 --> 00:17:44,003
다른 세 번째 레이어에서 3을 보게 될 것입니다.

245
00:17:44,003 --> 00:17:47,278
필터,하지만이 세 모서리에 의해

246
00:17:47,278 --> 00:17:49,690
각면에 추가 픽셀이 있고,

247
00:17:49,690 --> 00:17:52,486
원래의 입력 레이어를보고 있습니다.

248
00:17:52,486 --> 00:17:55,476
그래서 두 번째 레이어는 실제로 5를 5로보고 있습니다.

249
00:17:55,476 --> 00:17:57,233
수용력 분야에 대한 이해를 돕고,

250
00:17:57,233 --> 00:18:01,119
세 번째 계층은 세 번째로보고있다.

251
00:18:01,119 --> 00:18:04,850
두 번째 레이어에서하지만 이것은 진행될 것입니다.

252
00:18:04,850 --> 00:18:06,245
네가이 피라미드를보고 있다면

253
00:18:06,245 --> 00:18:07,717
입력 레이어에서 7 x 7

254
00:18:07,717 --> 00:18:09,565
그래서 효과적인 수용 필드는 여기에 있습니다.

255
00:18:09,565 --> 00:18:12,470
7에 7이 될 것입니다.

256
00:18:12,470 --> 00:18:16,836
어느 것이 7 × 7 전환 층과 같은지.

257
00:18:16,836 --> 00:18:18,676
그래서 무슨 일이 일어나는가는
이것도 똑같은 효과가 있습니다.

258
00:18:18,676 --> 00:18:21,128
7 ~ 7 회 전환 층으로서의 수용 필드

259
00:18:21,128 --> 00:18:22,356
그러나 그것은 더 깊다.

260
00:18:22,356 --> 00:18:24,801
거기에 더 많은 비선형 성을 가질 수 있으며,

261
00:18:24,801 --> 00:18:27,011
또한 매개 변수도 적습니다.

262
00:18:27,011 --> 00:18:28,599
따라서 총 매개 변수 수를 보면,

263
00:18:28,599 --> 00:18:33,179
3 개씩이 conv 필터 각각

264
00:18:33,179 --> 00:18:37,346
각 전환에 9 개의 매개 변수를 갖게 될 것입니다.
[mumbles]

265
00:18:38,975 --> 00:18:42,338
세 번 세 번 누른 다음 입력 깊이를 곱합니다.

266
00:18:42,338 --> 00:18:45,458
3 배 3 배 C,이 총 횟수 곱하기

267
00:18:45,458 --> 00:18:48,859
출력 된 기능 맵은 다시 C

268
00:18:48,859 --> 00:18:50,491
우리는 총계를 보존 할 것인가?

269
00:18:50,491 --> 00:18:51,844
채널 수.

270
00:18:51,844 --> 00:18:54,300
그래서 당신은 3 배 3을 얻습니다.

271
00:18:54,300 --> 00:18:57,128
각 층에 대해 C 배 C,

272
00:18:57,128 --> 00:18:59,089
우리는 3 개의 레이어를 가지므로

273
00:18:59,089 --> 00:19:00,975
이 숫자의 3 배,

274
00:19:00,975 --> 00:19:04,167
당신이 단 하나의 7x7 층을 가졌다면

275
00:19:04,167 --> 00:19:06,090
똑같은 추론을하면

276
00:19:06,090 --> 00:19:08,219
7 제곱 시간 C 제곱.

277
00:19:08,219 --> 00:19:10,675
따라서 총 매개 변수의 수를 줄이려고합니다.

278
00:19:10,675 --> 00:19:11,842
멋지다.

279
00:19:16,380 --> 00:19:20,511
이제 전체 네트워크를 살펴 보겠습니다.

280
00:19:20,511 --> 00:19:22,721
여기에 숫자가 많아서 돌아갈 수 있어요.

281
00:19:22,721 --> 00:19:24,971
더 조심스럽게 보아라.하지만 우리 모두를 보면

282
00:19:24,971 --> 00:19:28,572
매개 변수의 크기와 수를 같은 방식으로

283
00:19:28,572 --> 00:19:31,526
AlexNet에 대한 예제를 계산 한 결과,

284
00:19:31,526 --> 00:19:33,327
이것은 통과하는 좋은 운동이다,

285
00:19:33,327 --> 00:19:36,563
우리는 당신이 같은 길로가는 것을 압니다.

286
00:19:36,563 --> 00:19:40,656
우리는이 conv 레이어와 pooling
레이어를 가지고 있습니다.

287
00:19:40,656 --> 00:19:43,973
몇 가지 더 많은 전환 레이어,
레이어 풀링, 몇 가지 추가

288
00:19:43,973 --> 00:19:45,120
전환 레이어 등이 있습니다.

289
00:19:45,120 --> 00:19:46,644
그래서 이것은 계속 올라갑니다.

290
00:19:46,644 --> 00:19:49,068
그리고 당신이 총 회선 수를 계산한다면

291
00:19:49,068 --> 00:19:51,680
완전히 연결된 레이어라면 16 개가 될 것입니다.

292
00:19:51,680 --> 00:19:53,241
이 경우, VGG (16)

293
00:19:53,241 --> 00:19:57,121
그리고 나서 VGG 19, 그것은 매우 비슷합니다.

294
00:19:57,121 --> 00:20:01,288
아키텍처가 있지만 거기에 몇 가지
더 많은 전환 레이어가 있습니다.

295
00:20:03,831 --> 00:20:06,415
그리고이 네트워크의 총 메모리 사용량,

296
00:20:06,415 --> 00:20:10,467
그래서 앞으로 카운트 통과를 통해 패스

297
00:20:10,467 --> 00:20:13,538
이 번호들은 모두 여기에있는 메모리 번호에 들어 있습니다.

298
00:20:13,538 --> 00:20:16,414
총 수의 관점에서 작성된,

299
00:20:16,414 --> 00:20:18,006
이전에 계산 한 것처럼,

300
00:20:18,006 --> 00:20:20,170
그리고 숫자 당 4 바이트를 보면,

301
00:20:20,170 --> 00:20:23,935
이것은 이미지 당 약 100 megs가 될 것입니다.

302
00:20:23,935 --> 00:20:27,657
그래서 이것은 메모리 사용의 규모입니다

303
00:20:27,657 --> 00:20:29,537
그 일이 일어나고 있으며 이는
앞으로의 패스 만을위한 것입니다.

304
00:20:29,537 --> 00:20:31,707
당신이 후진 통과를 할 때 당신은 저장해야 할 것입니다.

305
00:20:31,707 --> 00:20:36,280
그리고 이것은 꽤 많은 메모리를 현명하게합니다.

306
00:20:36,280 --> 00:20:39,947
이미지 당 100 메가, 5 기가에 있다면

307
00:20:41,200 --> 00:20:44,223
총 메모리의 양을 줄이면

308
00:20:44,223 --> 00:20:46,473
약 50 개를 저장합니다.

309
00:20:48,110 --> 00:20:50,766
그리고 우리가 가지고있는 매개 변수의
총 수 또한 여기에 있습니다.

310
00:20:50,766 --> 00:20:54,324
이 네트워크에서 1 억 3 천 8 백만 개의 매개 변수입니다.

311
00:20:54,324 --> 00:20:56,941
이는 AlexNet의 경우 6 천만과 비교됩니다.

312
00:20:56,941 --> 00:20:58,291
문제?

313
00:20:58,291 --> 00:21:01,708
[학생은 마이크에서 소리 쳤다]

314
00:21:07,014 --> 00:21:09,097
- 문제는 우리가 더 깊은 의미가 무엇인지,

315
00:21:09,097 --> 00:21:10,730
그것은 필터의 수, 레이어의 수입니까?

316
00:21:10,730 --> 00:21:14,897
이 경우에는 깊이가 항상 레이어를 참조합니다.

317
00:21:16,415 --> 00:21:19,082
따라서 단어 깊이의 두 가지 용도가 있습니다.

318
00:21:19,082 --> 00:21:23,249
혼란스러운 것은 채널 당 깊이 비율이며,

319
00:21:24,975 --> 00:21:26,892
너비를 깊이별로 높이면 사용할 수 있습니다.

320
00:21:26,892 --> 00:21:27,752
여기서 단어 깊이,

321
00:21:27,752 --> 00:21:30,078
그러나 일반적으로 우리는 네트워크의
깊이에 대해 이야기합니다.

322
00:21:30,078 --> 00:21:33,065
이것은 총 레이어 수입니다.

323
00:21:33,065 --> 00:21:35,108
네트워크 및 일반적으로 특히

324
00:21:35,108 --> 00:21:38,781
우리는 무게 층의 총 수를 세고있다.

325
00:21:38,781 --> 00:21:41,195
따라서 훈련 가능한 무게를 가진 레이어의 총 수,

326
00:21:41,195 --> 00:21:44,178
그래서 컨벌루션 레이어와 완전히 연결된 레이어.

327
00:21:44,178 --> 00:21:47,678
[학생이 마이크에서 엉망으로]

328
00:22:01,620 --> 00:22:04,118
- 그래, 문제는 각 레이어 내에서

329
00:22:04,118 --> 00:22:06,984
다른 필터에는 어떤 것이 필요합니까?

330
00:22:06,984 --> 00:22:11,238
그래서 우리는 comNet
강연에서이 이야기를 나누었습니다.

331
00:22:11,238 --> 00:22:13,853
그래서 당신은 또한 돌아가서 그것을 참조 할 수 있습니다.

332
00:22:13,853 --> 00:22:18,356
그러나 각 필터는 3 ~ 3 개의 전환 (convs)

333
00:22:18,356 --> 00:22:20,900
그래서 각 필터는

334
00:22:20,900 --> 00:22:24,253
세 개의 값으로 된 입력을보고있는 체중 집합입니다.

335
00:22:24,253 --> 00:22:28,426
입력 깊이, 그리고 이것은 하나의 피쳐 맵을 생성하며,

336
00:22:28,426 --> 00:22:30,759
그 (것)들의 모든 응답의 1 개의 활성화지도.

337
00:22:30,759 --> 00:22:32,764
다른 공간 위치.

338
00:22:32,764 --> 00:22:35,994
그리고 우리는 우리가 원하는만큼의
필터를 가질 수 있습니다.

339
00:22:35,994 --> 00:22:38,328
그렇습니다. 예를 들어 96과

340
00:22:38,328 --> 00:22:40,456
는 기능 맵을 생성 할 것입니다.

341
00:22:40,456 --> 00:22:42,748
그리고 각 필터가 해당하는 것과 같습니다.

342
00:22:42,748 --> 00:22:45,249
우리가 찾고있는 다른 패턴으로

343
00:22:45,249 --> 00:22:47,620
입력에서 우리가 convolve하고 우리가 볼

344
00:22:47,620 --> 00:22:49,178
입력에있는 모든 곳의 응답,

345
00:22:49,178 --> 00:22:52,824
우리는 이들의지도를 만들고 다른 필터를 만듭니다.

346
00:22:52,824 --> 00:22:56,991
우리는 이미지를 뒤집어서 다른지도를 만들 것입니다.

347
00:22:59,571 --> 00:23:01,036
문제.

348
00:23:01,036 --> 00:23:04,453
[학생은 마이크에서 소리 쳤다]

349
00:23:08,275 --> 00:23:10,808
- 그래서 질문은, 직감이 뒤에 있는지,

350
00:23:10,808 --> 00:23:13,376
네트워크에 깊이 들어가면 더 많은 채널 깊이가 있습니다.

351
00:23:13,376 --> 00:23:17,543
필터 수가 너무 많아서 가질 수 있습니다.

352
00:23:18,486 --> 00:23:22,576
당신이 원하는 어떤 디자인이라도 그렇게하지 않아도됩니다.

353
00:23:22,576 --> 00:23:25,151
실제로 이것은 많은 시간이 걸리는 것을 볼 수 있습니다.

354
00:23:25,151 --> 00:23:28,547
이유 중 하나는 사람들이 시도하고 유지하는 것입니다.

355
00:23:28,547 --> 00:23:31,408
비교적 일정한 수준의 계산,

356
00:23:31,408 --> 00:23:34,718
그래서 당신이 당신의 네트워크로 더 깊이 들어갈수록,

357
00:23:34,718 --> 00:23:38,801
당신은 또한 일반적으로 기본적으로
다운 샘플링을 사용하고 있습니다.

358
00:23:40,416 --> 00:23:43,869
전체 공간 면적이 더 작 으면

359
00:23:43,869 --> 00:23:46,569
그들은 또한 당신이 조금씩 깊어 져서 증가합니다.

360
00:23:46,569 --> 00:23:48,525
깊이에 의해 증가하는 것은 지금 비싸지 않다.

361
00:23:48,525 --> 00:23:51,321
그것은 공간적으로 더 작기 때문에,

362
00:23:51,321 --> 00:23:54,177
그래, 그게 이유 일 뿐이야.

363
00:23:54,177 --> 00:23:55,526
문제.

364
00:23:55,526 --> 00:23:58,943
[학생은 마이크에서 소리 쳤다]

365
00:24:00,682 --> 00:24:02,478
- 성능면에서는 현명한 사용법이 필요합니다.

366
00:24:02,478 --> 00:24:05,463
SouthMax 대신에 SBN
[mumbles], [mumbles]

367
00:24:05,463 --> 00:24:07,671
그래서 아니, 분류자를 위해 당신은
어느 쪽이든을 사용할 수 있습니다,

368
00:24:07,671 --> 00:24:10,571
그리고 당신도 수업 초기에 그렇게 했어.

369
00:24:10,571 --> 00:24:13,191
그러나 일반적으로 SouthMax 손실,

370
00:24:13,191 --> 00:24:16,052
일반적으로 잘 작동하고 표준 사용되었습니다.

371
00:24:16,052 --> 00:24:18,052
분류는 여기에.

372
00:24:19,319 --> 00:24:20,833
좋아, 한 번 더 질문.

373
00:24:20,833 --> 00:24:24,333
[학생이 마이크에서 엉망으로]

374
00:24:38,712 --> 00:24:41,091
- 예, 질문은 저장하지 않아도됩니다.

375
00:24:41,091 --> 00:24:43,714
우리가 부품을 버릴 수있는 것처럼 모든 기억

376
00:24:43,714 --> 00:24:46,208
우리는 필요 없어요?

377
00:24:46,208 --> 00:24:48,230
그리고 네, 이것이 사실입니다.

378
00:24:48,230 --> 00:24:50,031
이 중 일부는 보관할 필요가 없습니다.

379
00:24:50,031 --> 00:24:53,095
그러나 당신은 또한 거꾸로 통과 할 것입니다.

380
00:24:53,095 --> 00:24:55,229
대부분은 도자기를 통해,

381
00:24:55,229 --> 00:24:56,586
네가 체인을 할 때

382
00:24:56,586 --> 00:24:58,141
규칙 등등 이러한 많은 활성화가 필요했습니다.

383
00:24:58,141 --> 00:25:01,631
그것의 부분으로 그리고 이렇게 많은 부분에서

384
00:25:01,631 --> 00:25:03,381
지켜야 할 필요가있다.

385
00:25:04,816 --> 00:25:08,990
따라서 메모리가 사용되는 곳의 분포를 보면

386
00:25:08,990 --> 00:25:11,612
매개 변수가 어디에 있는지 알 수 있습니다.

387
00:25:11,612 --> 00:25:15,250
이 초기 층의 추억은 아직도 당신이 가지고있는 바로

388
00:25:15,250 --> 00:25:19,656
더 많은 메모리 사용량을 갖게 될 공간 차원

389
00:25:19,656 --> 00:25:21,994
그리고 나서 많은 매개 변수들이 실제로

390
00:25:21,994 --> 00:25:24,864
마지막 층, 완전히 연결된 층

391
00:25:24,864 --> 00:25:26,821
엄청난 수의 매개 변수를 가지고 있습니다.

392
00:25:26,821 --> 00:25:29,647
왜냐하면 우리 모두가 밀집 해 있기 때문입니다.

393
00:25:29,647 --> 00:25:33,995
그래서 그것은 단지 알고있는 것이고 그 다음에는

394
00:25:33,995 --> 00:25:37,809
나중에 우리는 실제로 일부 네트워크를 볼 것입니다.

395
00:25:37,809 --> 00:25:39,771
이 완전히 연결된 레이어를 제거하고

396
00:25:39,771 --> 00:25:43,155
매개 변수의 수를 많이 절약 할 수 있습니다.

397
00:25:43,155 --> 00:25:45,029
그리고 마지막으로 지적해야 할 것이 하나 있습니다.

398
00:25:45,029 --> 00:25:47,483
당신은 또한 다양한 전화 방법을 보게 될 것입니다.

399
00:25:47,483 --> 00:25:48,869
이 모든 층들이 옳다.

400
00:25:48,869 --> 00:25:51,831
그래서 여기서 나는 그 층이 무엇인지 정확하게 기술했다.

401
00:25:51,831 --> 00:25:55,151
conv3-64는 세 x 세 개의 conv를 의미합니다.

402
00:25:55,151 --> 00:25:57,000
총 64 개의 필터가 있습니다.

403
00:25:57,000 --> 00:26:01,499
그러나이 다이어그램의 VGGNet은
여기 오른쪽에 있습니다.

404
00:26:01,499 --> 00:26:03,868
사람들이 볼 일반적인 방법도 있습니다.

405
00:26:03,868 --> 00:26:06,000
필터의 각 그룹에서,

406
00:26:06,000 --> 00:26:08,992
각 주황색 블록은 conv1과 마찬가지로 여기에 있습니다.

407
00:26:08,992 --> 00:26:11,734
1 부, 그래서 conv1-1, conv1-2,

408
00:26:11,734 --> 00:26:12,632
등등.

409
00:26:12,632 --> 00:26:15,465
그래서 염두에 두어야 할 것이 있습니다.

410
00:26:17,404 --> 00:26:20,515
그래서 VGGNet은

411
00:26:20,515 --> 00:26:22,930
ImageNet 2014 분류 과제,

412
00:26:22,930 --> 00:26:25,593
처음으로 현지화되었습니다.

413
00:26:25,593 --> 00:26:27,106
그들은 매우 유사한 훈련 절차를 따랐다.

414
00:26:27,106 --> 00:26:29,847
AlexNet을위한 Alex Krizhevsky로서.

415
00:26:29,847 --> 00:26:33,895
그들은 지역적 대응 표준화를 사용하지 않았고,

416
00:26:33,895 --> 00:26:35,333
그래서 앞서 언급했듯이,

417
00:26:35,333 --> 00:26:37,367
그들은 이것이 정말로 도움이되지 않는다는 것을 알았습니다.

418
00:26:37,367 --> 00:26:39,574
그래서 그들은 그것을 꺼냈다.

419
00:26:39,574 --> 00:26:44,281
VGG 16과 VGG 19는 일반적인 변형입니다.

420
00:26:44,281 --> 00:26:46,657
여기 사이클의, 그리고 이것은 단지

421
00:26:46,657 --> 00:26:50,425
층 수, 19는 16보다 약간 깊습니다.

422
00:26:50,425 --> 00:26:54,348
실제로 VGG 19는 조금 더 잘 작동하지만,

423
00:26:54,348 --> 00:26:57,009
그리고 약간의 메모리 사용량이 있습니다.

424
00:26:57,009 --> 00:27:01,176
그래서 둘 중 하나를 사용할 수 있지만
16은 매우 일반적으로 사용됩니다.

425
00:27:02,280 --> 00:27:06,247
AlexNet과 같은 최상의 결과를 얻기
위해 그들은 앙상블 작업을 수행했습니다.

426
00:27:06,247 --> 00:27:09,198
여러 모델을 평균하기 위해서,

427
00:27:09,198 --> 00:27:10,920
당신은 더 나은 결과를 얻습니다.

428
00:27:10,920 --> 00:27:14,432
그리고 그들은 또한 그들의 연구에서

429
00:27:14,432 --> 00:27:18,112
마지막으로 완전히 연결된 레이어의 FC7 기능

430
00:27:18,112 --> 00:27:20,968
1000 ImageNet 클래스로갑니다.

431
00:27:20,968 --> 00:27:24,771
그 직전의 4096 사이즈 레이어는,

432
00:27:24,771 --> 00:27:27,273
좋은 특징 표현,

433
00:27:27,273 --> 00:27:29,648
그것은 단지 그대로 사용될 수 있습니다.

434
00:27:29,648 --> 00:27:32,714
다른 데이터에서 이러한 기능을 추출하려면,

435
00:27:32,714 --> 00:27:35,865
이 다른 작업들을 일반화했다.

436
00:27:35,865 --> 00:27:38,602
그래서 FC7은 훌륭한 기능을 나타냅니다.

437
00:27:38,602 --> 00:27:39,952
그래.

438
00:27:39,952 --> 00:27:42,742
[학생은 마이크에서 소리 쳤다]

439
00:27:42,742 --> 00:27:45,242
- 뭐가 유감 이었어?

440
00:27:46,749 --> 00:27:50,846
좋아, 그럼 여기서 현지화 란 무엇입니까?

441
00:27:50,846 --> 00:27:53,921
그래서 이것은 과제이며 우리는
그것에 대해 이야기 할 것입니다.

442
00:27:53,921 --> 00:27:55,684
추후 강연에서 조금 더

443
00:27:55,684 --> 00:27:57,973
탐지 및 로컬라이제이션에서

444
00:27:57,973 --> 00:27:59,568
여기에 자세히 설명되어 있지만 기본적으로 이미지입니다.

445
00:27:59,568 --> 00:28:04,015
단순히 분류하는 것이 아니라 이미지의 클래스가 무엇인지,

446
00:28:04,015 --> 00:28:08,518
또한 주변에 테두리 상자를 그립니다.

447
00:28:08,518 --> 00:28:10,243
개체가 이미지에 있습니다.

448
00:28:10,243 --> 00:28:11,677
그리고 탐지의 차이,

449
00:28:11,677 --> 00:28:13,641
이것은 매우 관련있는 작업입니다.

450
00:28:13,641 --> 00:28:16,963
이미지에이 객체의 인스턴스가 여러 개있을 수 있습니다.

451
00:28:16,963 --> 00:28:18,938
현지화 우리는 단지 하나만 있다고 가정하고 있습니다.

452
00:28:18,938 --> 00:28:21,481
이 분류 그러나 우리는 어떻게이

453
00:28:21,481 --> 00:28:23,481
추가 경계 상자.

454
00:28:26,153 --> 00:28:29,390
그래서 우리는 깊은 네트워크 중
하나였던 VGG를 보았습니다.

455
00:28:29,390 --> 00:28:33,192
2014 년부터 GoogleNet에 대해 이야기하겠습니다.

456
00:28:33,192 --> 00:28:34,996
다른 하나는 이겼다.

457
00:28:34,996 --> 00:28:37,413
분류 도전.

458
00:28:38,422 --> 00:28:41,288
그래서 GoogleNet은 훨씬 더 깊은 네트워크였습니다.

459
00:28:41,288 --> 00:28:44,871
22 개의 레이어가 있지만 주요 통찰력 중 하나

460
00:28:45,853 --> 00:28:48,586
GoogleNet에 대한 특별한 점은

461
00:28:48,586 --> 00:28:51,653
이 계산상의 문제를 보았다.

462
00:28:51,653 --> 00:28:54,959
네트워크 아키텍처를 설계하려고 시도했다.

463
00:28:54,959 --> 00:28:58,676
계산량이 매우 효율적입니다.

464
00:28:58,676 --> 00:29:01,869
그래서 그들은이 시작 모듈을 사용하여 이것을했습니다.

465
00:29:01,869 --> 00:29:05,833
더 자세히 설명하고 기본적으로 스태킹 할 것입니다.

466
00:29:05,833 --> 00:29:09,146
이 위에 많은 시작 모듈들이 서로의 위에 있습니다.

467
00:29:09,146 --> 00:29:12,292
이 네트워크에는 완전히 연결된 레이어가 없습니다.

468
00:29:12,292 --> 00:29:14,515
그래서 그들은 많은 것을 구할 수 있었다.

469
00:29:14,515 --> 00:29:17,133
매개 변수의 합계가 5 백만에 불과합니다.

470
00:29:17,133 --> 00:29:20,651
AlexNet보다 12 배 적은 매개 변수,

471
00:29:20,651 --> 00:29:25,118
비록 지금은 훨씬 더 깊다.

472
00:29:25,118 --> 00:29:27,785
6.7 %의 상위 5 개 오류가 발생했습니다.

473
00:29:32,202 --> 00:29:34,414
그렇다면 시작 모듈은 무엇입니까?

474
00:29:34,414 --> 00:29:36,173
그래서 시작 모듈 뒤에있는 아이디어

475
00:29:36,173 --> 00:29:40,833
좋은 로컬 네트워크 유형을 디자인하고 싶었습니다.

476
00:29:40,833 --> 00:29:43,203
이 로컬 토폴로지에 대한 아이디어가 있습니다.

477
00:29:43,203 --> 00:29:47,177
그게 네가 네트워크라고 생각할
수 있다는 걸 너는 알고있다.

478
00:29:47,177 --> 00:29:48,857
네트워크 내에서 이들을 많이 스택

479
00:29:48,857 --> 00:29:53,151
지역 유형학은 서로의 위에 하나씩있다.

480
00:29:53,151 --> 00:29:55,890
그래서 그들이 말하는 지역 네트워크에서

481
00:29:55,890 --> 00:29:59,197
시작 모듈은 그들이하는 일은 기본적으로

482
00:29:59,197 --> 00:30:02,673
여러 종류의 필터 조작 적용

483
00:30:02,673 --> 00:30:05,823
들어오는 동일한 입력 위에 병렬로

484
00:30:05,823 --> 00:30:07,948
이 같은 계층.

485
00:30:07,948 --> 00:30:09,715
따라서 우리는 이전 레이어에서 들어오는 입력을받습니다.

486
00:30:09,715 --> 00:30:12,706
그리고 나서 우리는 다른 종류의 회선을 할 것입니다.

487
00:30:12,706 --> 00:30:16,838
그래서 하나 하나의 conv, 오른쪽 3 x 3 conv,

488
00:30:16,838 --> 00:30:18,684
5 ~ 5 대가 나왔고 그들도

489
00:30:18,684 --> 00:30:22,208
이 경우 풀링 작업이 3 ~ 3 개 있습니다.

490
00:30:22,208 --> 00:30:24,290
풀링, 그래서 당신은이 모든 다른 것들을 얻습니다.

491
00:30:24,290 --> 00:30:26,457
이들 상이한 층들로부터의 출력들,

492
00:30:26,457 --> 00:30:29,565
그런 다음 그들이하는 일은이 모든 것을 연결하는 것입니다.

493
00:30:29,565 --> 00:30:32,309
깊이와 함께 필터 출력

494
00:30:32,309 --> 00:30:36,036
그 다음에는 끝 부분에 하나의 텐서 출력이 생성됩니다

495
00:30:36,036 --> 00:30:39,703
그것은 톰이 다음 레이어로 넘어가는 것입니다.

496
00:30:41,830 --> 00:30:44,368
그래서 우리가이 일을하는 순진한 방법을 본다면

497
00:30:44,368 --> 00:30:46,616
우리는 단지 우리가이 모든 다른 것들을
가지고 있음을 정확하게 나타냅니다.

498
00:30:46,616 --> 00:30:50,825
우리가 출력을 합친 결과를 얻을 수 있습니다.

499
00:30:50,825 --> 00:30:53,196
그래서이 문제는 무엇입니까?

500
00:30:53,196 --> 00:30:56,027
그리고 계산상의 복잡성

501
00:30:56,027 --> 00:30:58,527
여기서 문제가 될 것입니다.

502
00:30:59,792 --> 00:31:01,514
따라서 예를 더 자세히 살펴보면,

503
00:31:01,514 --> 00:31:05,473
여기 예를 들어 하나 하나의 conv를 넣었습니다.

504
00:31:05,473 --> 00:31:09,476
128 필터, 3 필터, 3 필터, 192 필터,

505
00:31:09,476 --> 00:31:11,966
5 x 5 convs 및 96 개의 필터가 있습니다.

506
00:31:11,966 --> 00:31:14,582
모든 것이 기본적으로 보폭이라고 가정합니다.

507
00:31:14,582 --> 00:31:17,041
그것은 공간적 차원을 유지하려고합니다.

508
00:31:17,041 --> 00:31:20,208
그리고 우리는이 입력이 들어오는 것을 우리는 가지고 있습니다.

509
00:31:22,151 --> 00:31:24,858
그래서 하나씩 필터의 출력 크기는 얼마입니까?

510
00:31:24,858 --> 00:31:28,708
128 개, 하나씩 128 개 필터 포함

511
00:31:28,708 --> 00:31:30,041
누가 추측 할 수 있니?

512
00:31:36,720 --> 00:31:40,720
좋아, 나는 28 세에 28 세가 맞았다.

513
00:31:41,798 --> 00:31:44,043
그래서 하나씩 차례대로 우리는 유지하려고합니다.

514
00:31:44,043 --> 00:31:46,821
공간 차원 그리고 그 위에,

515
00:31:46,821 --> 00:31:50,374
각 전환 필터는

516
00:31:50,374 --> 00:31:53,969
입력의 전체 256 깊이,

517
00:31:53,969 --> 00:31:55,723
그러나 그 결과가 될 것입니다.

518
00:31:55,723 --> 00:31:57,684
28x28 크기의지도가 있습니다.

519
00:31:57,684 --> 00:32:00,145
우리가 가지고있는 128 개의 필터 각각에 대해

520
00:32:00,145 --> 00:32:01,004
이 전환 층에

521
00:32:01,004 --> 00:32:03,171
그래서 우리는 28만큼 28만큼 128을 얻습니다.

522
00:32:06,279 --> 00:32:08,368
이제 우리가 똑같은 일을한다면

523
00:32:08,368 --> 00:32:11,771
우리는 출력 크기의 필터 크기를 보았습니다. 미안 해요.

524
00:32:11,771 --> 00:32:15,749
여기에있는 모든 다른 필터 중

525
00:32:15,749 --> 00:32:17,803
3 대 3의 전환율로이 볼륨을 확보하게 될 것입니다.

526
00:32:17,803 --> 00:32:21,189
28 개, 28 개, 192, 192, 5, 5, 전환

527
00:32:21,189 --> 00:32:23,323
여기에는 96 개의 필터가 있습니다.

528
00:32:23,323 --> 00:32:25,369
그래서 28 by 96,

529
00:32:25,369 --> 00:32:28,851
그런 다음 풀링 레이어를 사용하면됩니다.

530
00:32:28,851 --> 00:32:32,821
동일한 공간 차원을 유지하려면 레이어를 풀

531
00:32:32,821 --> 00:32:35,522
그것을 깊이에서 보존 할 것이고,

532
00:32:35,522 --> 00:32:36,835
그리고 여기에 우리의 보폭 때문에,

533
00:32:36,835 --> 00:32:41,002
우리는 또한 공간 차원을 보존하려고합니다.

534
00:32:42,035 --> 00:32:44,288
이제 필터 후 출력 크기를 보면

535
00:32:44,288 --> 00:32:48,589
우리가 얻을 연결은 28 by 28이고,

536
00:32:48,589 --> 00:32:52,308
이것들은 모두 28x28 크기이며,
우리는 깊이있는 연결을합니다.

537
00:32:52,308 --> 00:32:56,681
그래서 우리는 28 배나 28 배가됩니다.

538
00:32:56,681 --> 00:32:58,807
총 출력 크기는

539
00:32:58,807 --> 00:33:00,140
28 by 672.

540
00:33:01,923 --> 00:33:05,798
그래서 우리의 시작 모듈에 대한
입력은 28x28이었습니다.

541
00:33:05,798 --> 00:33:10,185
256이면이 모듈의 출력은 28 by 28입니다.

542
00:33:10,185 --> 00:33:11,018
672.

543
00:33:12,276 --> 00:33:15,148
그래서 우리는 동일한 공간적 차원을 유지했습니다.

544
00:33:15,148 --> 00:33:18,064
우리는 깊이를 날려 버렸습니다.

545
00:33:18,064 --> 00:33:18,998
문제.

546
00:33:18,998 --> 00:33:22,715
[학생은 마이크에서 소리 쳤다]

547
00:33:22,715 --> 00:33:24,393
좋아, 그럼이 경우 네, 질문은,

548
00:33:24,393 --> 00:33:26,356
우리는 어떻게해서 모든 것이 28 개가 될까요?

549
00:33:26,356 --> 00:33:28,436
여기서 우리는 모든 제로 패딩을하고 있습니다.

550
00:33:28,436 --> 00:33:30,117
공간적 차원을 유지하기 위해,

551
00:33:30,117 --> 00:33:32,130
그러면 우리는이 필터를 할 수 있습니다.

552
00:33:32,130 --> 00:33:34,213
연결 깊이.

553
00:33:35,205 --> 00:33:37,043
뒤에서 질문해라.

554
00:33:37,043 --> 00:33:40,460
[학생은 마이크에서 소리 쳤다]

555
00:33:45,634 --> 00:33:48,615
- OK 입력에 256 깊이가 무엇인지,

556
00:33:48,615 --> 00:33:51,228
따라서 이것은 네트워크에 대한 입력이 아닙니다.

557
00:33:51,228 --> 00:33:53,677
이 로컬 모듈에 대한 입력입니다.

558
00:33:53,677 --> 00:33:54,624
나는보고있다.

559
00:33:54,624 --> 00:33:58,460
따라서이 경우 256은 이전의

560
00:33:58,460 --> 00:34:01,316
이 바로 앞에 오는 시작 모듈.

561
00:34:01,316 --> 00:34:05,491
그리고 이제 우리는 28 세, 28 세, 672 세,

562
00:34:05,491 --> 00:34:07,045
그리고 그것은 다음에 입력이 될 것입니다.

563
00:34:07,045 --> 00:34:09,248
시작 모듈.

564
00:34:09,248 --> 00:34:10,725
문제.

565
00:34:10,726 --> 00:34:14,143
[학생은 마이크에서 소리 쳤다]

566
00:34:17,849 --> 00:34:21,533
- 좋아, 문제는 28x28x128

567
00:34:21,533 --> 00:34:23,991
첫 번째 전환에 대해 첫 번째 전환에 대해

568
00:34:23,991 --> 00:34:28,614
이것은 기본적으로 하나의 회선 (convolution)

569
00:34:28,614 --> 00:34:30,701
그래서 우리는 이것을 하나의 회선으로 취하려고합니다.

570
00:34:30,701 --> 00:34:34,868
28 개의 입력을 공간적으로 28 번 긋다

571
00:34:36,295 --> 00:34:38,547
그것이 각 위치에있는 곳에, 그것은 번식 할 것입니다,

572
00:34:38,547 --> 00:34:41,005
그것은 [mumbles]를 할 것입니다.

573
00:34:41,005 --> 00:34:42,766
전체 256 깊이를 통해, 그래서 우리는 이것을한다.

574
00:34:42,766 --> 00:34:44,527
하나씩 차례로 공간 위로 슬라이드

575
00:34:44,527 --> 00:34:47,793
그리고 우리는 28에 의해 28의 특징지도를 얻습니다.

576
00:34:47,793 --> 00:34:52,459
나오는 각 공간 위치에 하나의 숫자가 있습니다.

577
00:34:52,460 --> 00:34:54,955
각 필터는 28 중 하나를 28 씩 생성합니다.

578
00:34:54,955 --> 00:34:59,122
하나의지도로, 우리는 총 128
개의 필터를 가지고 있습니다.

579
00:35:01,860 --> 00:35:05,610
128에 의해 28만큼 28을 생산할 것입니다.

580
00:35:06,619 --> 00:35:08,461
운영 횟수를 보면 알 수 있습니다.

581
00:35:08,461 --> 00:35:11,213
길쌈 (convolutional) 계층에서 일어나고있는

582
00:35:11,213 --> 00:35:15,951
첫 번째 예를 하나씩 살펴 보겠습니다.

583
00:35:15,951 --> 00:35:20,696
내가하는 각 위치에서 방금 말하고 있었던 것처럼

584
00:35:20,696 --> 00:35:23,363
한 점 한점 256 점의 제품.

585
00:35:25,355 --> 00:35:29,168
256 개의 곱셈 연산이 여기에서 발생합니다.

586
00:35:29,168 --> 00:35:32,903
그런 다음 각 필터 맵에 대해 28 x 28

587
00:35:32,903 --> 00:35:36,673
공간 위치, 그래서 처음 28 시간 28

588
00:35:36,673 --> 00:35:38,675
첫 번째 두 숫자는 여기에 곱해집니다.

589
00:35:38,675 --> 00:35:41,500
이들은 각각의 필터 맵에 대한 공간 위치이며,

590
00:35:41,500 --> 00:35:45,921
그래서 우리는 25 60 곱셈을해야합니다.

591
00:35:45,921 --> 00:35:50,415
이들 각각은 128 개의 전체 필터를 가지고 있습니다.

592
00:35:50,415 --> 00:35:53,276
이 층에서, 또는 우리는 총 128

593
00:35:53,276 --> 00:35:54,669
기능지도.

594
00:35:54,669 --> 00:35:58,809
그리고이 작업의 총 수는 여기에 있습니다.

595
00:35:58,809 --> 00:36:00,364
28 번 28 번이 될거야.

596
00:36:00,364 --> 00:36:02,031
시간 128 회 256 회.

597
00:36:02,939 --> 00:36:05,267
그리고 이것은 이것이 똑같을 것입니다,

598
00:36:05,267 --> 00:36:07,805
당신은 3 대 3에 대해 이것을 생각할 수 있습니다.

599
00:36:07,805 --> 00:36:10,053
그리고 5 대 5의 conv는 정확히 동일합니다.

600
00:36:10,053 --> 00:36:11,159
원리.

601
00:36:11,159 --> 00:36:15,500
전체적으로 우리는 8 억 8400
만 건의 운영을 할 것입니다.

602
00:36:15,500 --> 00:36:17,500
여기서 일어나는 일들.

603
00:36:18,778 --> 00:36:22,001
- [Student] 그리고 128,
192 및 96은 값입니다.

604
00:36:22,001 --> 00:36:22,941
[mumbles]

605
00:36:22,941 --> 00:36:25,841
질문 : 128, 192 및
256은 내가 선택한 값입니다.

606
00:36:25,841 --> 00:36:29,854
네, 이것들은 제가 방금 제시 한 가치가 아닙니다.

607
00:36:29,854 --> 00:36:32,553
그들은 당신이 볼 수있는 것과 비슷합니다.

608
00:36:32,553 --> 00:36:36,404
처음 그물망의 특정 층처럼,

609
00:36:36,404 --> 00:36:38,943
그래서 GoogleNet에서는 기본적으로 각 모듈마다 다른

610
00:36:38,943 --> 00:36:41,163
이러한 종류의 매개 변수 세트,
그리고 하나를 골라 냈습니다.

611
00:36:41,163 --> 00:36:43,913
이는 이들 중 하나와 유사합니다.

612
00:36:45,899 --> 00:36:48,341
그래서 이것은 계산적으로 매우 비쌉니다.

613
00:36:48,341 --> 00:36:49,856
이러한 작업.

614
00:36:49,856 --> 00:36:51,321
그리고 내가 주목하고 싶은 또 다른 것

615
00:36:51,321 --> 00:36:55,786
풀링 계층도이 문제에 추가됩니다.

616
00:36:55,786 --> 00:36:57,872
전체 피처 깊이를 보존합니다.

617
00:36:57,872 --> 00:37:02,864
따라서 모든 레이어에서 전체 깊이가 커질 수 있습니다.

618
00:37:02,864 --> 00:37:04,329
너는 가득 차있는 특색 지어진 깊이를
가지고 가기 위하여려고하고있다

619
00:37:04,329 --> 00:37:07,316
풀링 레이어뿐만 아니라 모든 추가 레이어

620
00:37:07,316 --> 00:37:11,323
전환 (conv) 레이어의지도 기능을
추가하고이를 함께 추가 할 수 있습니다.

621
00:37:11,323 --> 00:37:14,515
그래서 우리의 입력은 256 깊이 였고 출력은

622
00:37:14,515 --> 00:37:18,687
672 깊이와 당신은 단지 이것을 계속 유지하려고합니다.

623
00:37:18,687 --> 00:37:19,770
당신이 올라갈 때.

624
00:37:22,730 --> 00:37:24,979
그러면 우리는 어떻게이 문제에 대처할 것인가?

625
00:37:24,979 --> 00:37:26,251
더 다루기 쉬운가?

626
00:37:26,251 --> 00:37:30,161
GoogleNet이 사용한 주요 통찰력 중 하나

627
00:37:30,161 --> 00:37:33,140
우리는이 문제를 해결하기 위해

628
00:37:33,140 --> 00:37:36,991
병목 현상을 방지하고 이러한 기능
맵을 시도하고 계획하십시오.

629
00:37:36,991 --> 00:37:41,401
우리의 길쌈 작업 전에 차원을 낮추고,

630
00:37:41,401 --> 00:37:43,984
그래서 비싼 층 앞에.

631
00:37:45,817 --> 00:37:47,452
그러면 그게 정확히 무슨 뜻입니까?

632
00:37:47,452 --> 00:37:51,035
그래서 하나 하나의 회선을 되새기는 것 같아요.

633
00:37:52,165 --> 00:37:54,529
우리는 단지 이것을 통과했지만 귀하의 의견을

634
00:37:54,529 --> 00:37:57,630
볼륨에서 각 공간에서 점 생성을 수행하고 있습니다.

635
00:37:57,630 --> 00:38:00,951
위치 및 그것이하는 일은 공간 차원을 보존합니다.

636
00:38:00,951 --> 00:38:03,970
그러나 그것은 깊이를 줄이고 그것은
투영함으로써 그것을 감소시킨다.

637
00:38:03,970 --> 00:38:06,949
입력 깊이를 더 낮은 차원으로.

638
00:38:06,949 --> 00:38:09,075
그냥 기본적으로 선형 결합과 같습니다.

639
00:38:09,075 --> 00:38:11,325
귀하의 입력 기능지도.

640
00:38:13,690 --> 00:38:15,900
그래서이 주요 아이디어는 그것이
당신을 투사한다는 것입니다.

641
00:38:15,900 --> 00:38:19,009
깊이가 낮아서 시작 모듈

642
00:38:19,009 --> 00:38:23,553
이것들을 하나씩 보내고 이것들을 무리에 더한다.

643
00:38:23,553 --> 00:38:26,535
이 모듈에있는 장소가 어디에 있을지,

644
00:38:26,535 --> 00:38:29,895
이 값 비싼 계산을 줄이기 위해

645
00:38:29,895 --> 00:38:33,208
따라서 3 ~ 5 및 5 ~ 5 개의 전환 레이어 전에,

646
00:38:33,208 --> 00:38:36,972
그것들은 하나의 회선에 의해 이들 중 하나에 놓입니다.

647
00:38:36,972 --> 00:38:40,042
그리고 풀링 레이어 후에도

648
00:38:40,042 --> 00:38:43,125
추가적인 하나씩의 회선 (convolution).

649
00:38:44,094 --> 00:38:46,919
맞아요, 이것들은 하나의 병목 현상 레이어입니다.

650
00:38:46,919 --> 00:38:48,419
에 추가됩니다.

651
00:38:49,372 --> 00:38:51,791
그리고 이것은 어떻게 수학을 바꾸는가?

652
00:38:51,791 --> 00:38:53,546
우리가 이전에보고 있었던?

653
00:38:53,546 --> 00:38:55,841
이제 기본적으로 일어나고있는 일은 우리가 여전히

654
00:38:55,841 --> 00:38:59,399
여기에 같은 입력을 28 by 28 by 256,

655
00:38:59,399 --> 00:39:03,137
그러나이 하나 하나의 convs는 깊이를 줄이려고합니다.

656
00:39:03,137 --> 00:39:07,181
차원 및 3시 3 분 전에 볼 수 있도록

657
00:39:07,181 --> 00:39:09,802
convs, 만약 내가 하나의
conv에 64 개의 필터를 넣으면,

658
00:39:09,802 --> 00:39:12,416
내 결과물이 될거야,

659
00:39:12,416 --> 00:39:13,666
28 by 28 by 64.

660
00:39:14,994 --> 00:39:18,148
그래서 이제는 3 대 3의 조합으로 들어가는 대신

661
00:39:18,148 --> 00:39:21,304
이후에 28에 의해 28 대신에 256 들어오고,

662
00:39:21,304 --> 00:39:25,964
우리는 28 블록 씩을 가지고 있습니다.

663
00:39:25,964 --> 00:39:30,255
그래서 이제는 더 작은 입력을 줄이고 있습니다.

664
00:39:30,255 --> 00:39:32,264
이러한 전환 레이어로 들어가면

665
00:39:32,264 --> 00:39:35,614
5 x 5 conv, 그리고 풀링 레이어의 경우,

666
00:39:35,614 --> 00:39:38,976
풀링이 나온 후에 우리는

667
00:39:38,976 --> 00:39:41,309
이 후에 깊이를 줄이십시오.

668
00:39:42,372 --> 00:39:44,703
그래서, 같은 방법으로 수학을하면

669
00:39:44,703 --> 00:39:47,202
여기에있는 모든 길잡이를 위해,

670
00:39:47,202 --> 00:39:49,411
이 모든 것을 하나씩 추가하는 것

671
00:39:49,411 --> 00:39:52,024
3 위는 3 위, 5 위는 5 위.

672
00:39:52,024 --> 00:39:57,021
총 운영 횟수는 3 억 8,800 만 건이며,

673
00:39:57,021 --> 00:40:01,060
그래서 그것은 우리가 가진 854 백만보다 훨씬 적습니다.

674
00:40:01,060 --> 00:40:03,309
순진한 버전에서, 당신은 어떻게 볼 수 있습니다.

675
00:40:03,309 --> 00:40:07,440
이것을 하나씩 전환 할 수 있고 필터 크기

676
00:40:07,440 --> 00:40:11,248
귀하의 계산을 제어 할 수 있습니다.

677
00:40:11,248 --> 00:40:12,928
예, 뒷문에 질문이 있습니다.

678
00:40:12,928 --> 00:40:16,345
[학생은 마이크에서 소리 쳤다]

679
00:40:24,335 --> 00:40:27,317
- 네, 질문이 있습니다.

680
00:40:27,317 --> 00:40:30,028
이 정보를 하나씩 처리하면 어떤
정보가 손실 될 수 있습니다.

681
00:40:30,028 --> 00:40:31,789
처음에는 전환 할 수 있습니다.

682
00:40:31,789 --> 00:40:35,922
그래서 정보가 손실 될 수도 있습니다.

683
00:40:35,922 --> 00:40:38,455
그러나 동시에이 계획을하고 있다면

684
00:40:38,455 --> 00:40:41,565
당신은이 입력들의 선형 결합을 취하고 있습니다.

685
00:40:41,565 --> 00:40:45,065
그들에 중복성이있는 특징지도,

686
00:40:46,675 --> 00:40:48,433
당신은 그들의 조합을 복용하고 있습니다.

687
00:40:48,433 --> 00:40:50,647
추가 비선형 성을 도입하고 있습니다.

688
00:40:50,647 --> 00:40:53,399
하나씩 전환 한 후에도 실제로

689
00:40:53,399 --> 00:40:55,565
조금 더 추가하면 이런 식으로 도움이됩니다.

690
00:40:55,565 --> 00:41:00,232
깊이가 있기 때문에 엄격한 분석이 없다고 생각합니다.

691
00:41:00,232 --> 00:41:03,954
이것의, 그러나 기본적으로
일반적으로 이것은 잘 작동합니다.

692
00:41:03,954 --> 00:41:08,124
왜 도움이되는지에 대한 이유가 있습니다.

693
00:41:08,124 --> 00:41:11,693
여기까지 OK. 기본적으로 하나씩 사용하고 있습니다.

694
00:41:11,693 --> 00:41:16,437
우리의 계산상의 복잡성을 관리하는
데 도움이되는 convs,

695
00:41:16,437 --> 00:41:18,972
GooleNet이하는 일은 다음과 같습니다.

696
00:41:18,972 --> 00:41:20,156
시작 모듈과 스택 될 것입니다.

697
00:41:20,156 --> 00:41:21,260
모두 함께.

698
00:41:21,260 --> 00:41:23,637
이것이 완전한 시작 구조입니다.

699
00:41:23,637 --> 00:41:28,222
그리고 우리가 조금 더 자세하게 본다면,

700
00:41:28,222 --> 00:41:29,651
그래서 여기 나는 그것을 뒤집었다.

701
00:41:29,651 --> 00:41:31,701
그것이 너무 커서, 적합하지 않을 것이기 때문에

702
00:41:31,701 --> 00:41:33,583
더 이상 슬라이드에서 수직으로.

703
00:41:33,583 --> 00:41:35,868
먼저 우리는이 줄기를 가지고 있습니다.

704
00:41:35,868 --> 00:41:39,717
네트워크이므로 바닐라 평원의 종류가 더 많습니다.

705
00:41:39,717 --> 00:41:42,012
우리가 이전에 본 conv net [mumbles]

706
00:41:42,012 --> 00:41:44,066
레이어의 6 시퀀스입니다.

707
00:41:44,066 --> 00:41:46,888
그래서 다른 수영장에 몇 가지 convs conv 수영장

708
00:41:46,888 --> 00:41:49,380
시작하기 만하면됩니다.

709
00:41:49,380 --> 00:41:52,321
우리는 우리의 다양한 다른 시작을 가지고 있습니다.

710
00:41:52,321 --> 00:41:55,721
모듈은 모두 서로 겹쳐 쌓여 있으며,

711
00:41:55,721 --> 00:41:59,243
위에서 우리는 분류 자 결과를 얻습니다.

712
00:41:59,243 --> 00:42:01,288
그리고 그들이 정말로 제거했음을 여기에 주목하십시오.

713
00:42:01,288 --> 00:42:03,002
고가의 완전 연결 층

714
00:42:03,002 --> 00:42:06,887
그 모델이 그들없이 훌륭하게 작동한다는 것이 밝혀졌습니다.

715
00:42:06,887 --> 00:42:09,792
심지어 당신은 많은 매개 변수를 줄입니다.

716
00:42:09,792 --> 00:42:12,875
그리고 그들이 또한 여기에있는 것은,

717
00:42:14,643 --> 00:42:17,057
당신은 여분의 줄기가 나온 것을 볼 수 있습니다.

718
00:42:17,057 --> 00:42:19,676
이들은 보조 분류 결과입니다

719
00:42:19,676 --> 00:42:24,083
그리고 이것들은 또한 당신이 알고있는
단지 작은 미니 네트워크들입니다.

720
00:42:24,083 --> 00:42:27,036
평균 풀링, 하나씩의 전환,

721
00:42:27,036 --> 00:42:30,027
여기에 완전히 연결된 레이어 몇 개

722
00:42:30,027 --> 00:42:33,360
부드러운 최대 및 또한 1000 방법 SoftMax

723
00:42:35,218 --> 00:42:36,512
ImageNet 클래스로.

724
00:42:36,512 --> 00:42:39,376
그리고 실제로 ImageNet 교육을 사용하고 있습니다.

725
00:42:39,376 --> 00:42:42,160
여기 세 곳의 분류 손실.

726
00:42:42,160 --> 00:42:46,384
네트워크의 표준 엔드뿐만 아니라

727
00:42:46,384 --> 00:42:49,699
네트워크에서 두 곳 더 일찍, 그 이유는

728
00:42:49,699 --> 00:42:52,562
그들은 이것이 깊은 네트워크라는 것입니다.

729
00:42:52,562 --> 00:42:54,891
그들은이 보조적인 보조물

730
00:42:54,891 --> 00:42:58,641
분류 출력, 당신은 더 많은 그라데이션을 얻을

731
00:43:00,861 --> 00:43:02,950
이전 계층에 주입 된 교육,

732
00:43:02,950 --> 00:43:05,771
그래서 더 많은 도움이되는 신호가 흐르고 있습니다.

733
00:43:05,771 --> 00:43:08,521
이 중간층은 또한

734
00:43:08,521 --> 00:43:09,354
도움이됩니다.

735
00:43:10,323 --> 00:43:11,708
당신은 분류를 할 수 있어야합니다.

736
00:43:11,708 --> 00:43:14,294
이들 중 일부를 기반으로합니다.

737
00:43:14,294 --> 00:43:17,885
그리고 이것이 전체 아키텍처입니다.

738
00:43:17,885 --> 00:43:21,521
가중치가있는 22 개의 레이어가 있습니다.

739
00:43:21,521 --> 00:43:25,166
이들 각각의 모듈 내에서 각각 하나씩,

740
00:43:25,166 --> 00:43:27,622
3 x 3, 5 x 5는 무게 층,

741
00:43:27,622 --> 00:43:30,284
이러한 모든 병렬 레이어를 포함하는 것만으로도,

742
00:43:30,284 --> 00:43:34,451
일반적으로 비교적 신중하게 설계된 제품입니다.

743
00:43:38,470 --> 00:43:41,908
아키텍처 및 일부는이 중 일부를 기반으로합니다.

744
00:43:41,908 --> 00:43:44,938
우리가 말하는 직관력과 그 일부

745
00:43:44,938 --> 00:43:48,387
또한 그들이 알고있는 구글 저자들

746
00:43:48,387 --> 00:43:51,046
거대한 클러스터와 교차 검증

747
00:43:51,046 --> 00:43:53,826
모든 종류의 디자인 선택과 이것이 무엇인가?

748
00:43:53,826 --> 00:43:56,321
결국 잘 작동했다.

749
00:43:56,321 --> 00:43:57,915
문제?

750
00:43:57,915 --> 00:44:01,332
[학생은 마이크에서 소리 쳤다]

751
00:44:25,252 --> 00:44:28,071
- 그래, 문제는 보조 출력

752
00:44:28,071 --> 00:44:31,224
실제로 최종 분류에 유용합니다.

753
00:44:31,224 --> 00:44:33,267
이것도 사용 하시겠습니까?

754
00:44:33,267 --> 00:44:35,026
나는 그들이 그들을 훈련 할 때 생각한다.

755
00:44:35,026 --> 00:44:38,214
그들은 나오는 모든 손실을 평균 처리합니다.

756
00:44:38,214 --> 00:44:39,974
나는 그들이 도움이된다고 생각한다.

757
00:44:39,974 --> 00:44:42,180
나는 최종 아키텍처에서,

758
00:44:42,180 --> 00:44:44,765
그들이이 모든 것들의 평균을 내건 하나를 택하든,

759
00:44:44,765 --> 00:44:47,178
그들이 모두를 사용할 가능성이 매우 높습니다.

760
00:44:47,178 --> 00:44:50,082
그러나 당신은 그것을 확인해야 할 것입니다.

761
00:44:50,082 --> 00:44:53,499
[학생은 마이크에서 소리 쳤다]

762
00:44:59,162 --> 00:45:01,411
- 그래서 문제는 병목 층에 관한 것이고,

763
00:45:01,411 --> 00:45:05,389
다른 유형의 차원을 사용할 수 있습니까?

764
00:45:05,389 --> 00:45:09,518
축소 및 예 다른 차원의 차원 사용할 수 있습니다.

765
00:45:09,518 --> 00:45:11,029
절감.

766
00:45:11,029 --> 00:45:13,893
이 하나 하나의 conv의 장점은 다음과 같습니다.

767
00:45:13,893 --> 00:45:16,232
당신은이 효과를 얻고 있습니다. 그러나 그것은 모두입니다.

768
00:45:16,232 --> 00:45:17,948
다른 레이어와 마찬가지로 전환 레이어입니다.

769
00:45:17,948 --> 00:45:19,664
너는 이것들의 영혼 네트워크를 가지고있다.

770
00:45:19,664 --> 00:45:21,350
너는이 전체 네트워크를 훈련시킨다.

771
00:45:21,350 --> 00:45:23,646
모든 것을 통해 다시 [mumbles]

772
00:45:23,646 --> 00:45:25,157
그것을 결합하는 방법을 배우고 있습니다.

773
00:45:25,157 --> 00:45:26,990
이전 기능 맵.

774
00:45:29,411 --> 00:45:31,540
그래, 뒷문에 질문 해.

775
00:45:31,540 --> 00:45:34,957
[학생은 마이크에서 소리 쳤다]

776
00:45:36,617 --> 00:45:40,284
- 예, 질문은 공유되는 모든 가중치입니다.

777
00:45:41,969 --> 00:45:43,359
또는 그들 모두는 모두 분리되어 있습니다.

778
00:45:43,359 --> 00:45:46,352
이 모든 레이어에는 별도의 가중치가 있습니다.

779
00:45:46,352 --> 00:45:47,500
문제.

780
00:45:47,500 --> 00:45:50,917
[학생은 마이크에서 소리 쳤다]

781
00:45:57,594 --> 00:45:59,273
- 그렇습니다. 그렇다면 왜 우리는

782
00:45:59,273 --> 00:46:00,953
이전 레이어에 그라디언트를 주입하려면 어떻게해야합니까?

783
00:46:00,953 --> 00:46:04,703
결국 우리의 분류 결과는 결국,

784
00:46:06,517 --> 00:46:08,896
여기에 그라디언트가있는 곳은

785
00:46:08,896 --> 00:46:10,409
체인 롤을 통해 다시 통과했다.

786
00:46:10,409 --> 00:46:12,824
그러나 문제는 당신이 매우 깊은
네트워크를 가지고있을 때입니다.

787
00:46:12,824 --> 00:46:15,282
그리고 너는이 모든 길로 돌아가고있다.

788
00:46:15,282 --> 00:46:18,916
이 기울기 신호 중 일부가 최소화 될 수 있습니다.

789
00:46:18,916 --> 00:46:21,988
처음에 더 가깝게 잃어 버렸습니다.

790
00:46:21,988 --> 00:46:25,588
이전 부분의 이러한 추가 항목

791
00:46:25,588 --> 00:46:29,187
추가 신호를 제공하는 데 도움이 될 수 있습니다.

792
00:46:29,187 --> 00:46:33,477
[학생이 마이크에서 엉망으로]

793
00:46:33,477 --> 00:46:35,486
- 그래서 질문은 당신이 항상
소품을 되 찾는가하는 것입니다.

794
00:46:35,486 --> 00:46:36,663
각 출력에 대해

795
00:46:36,663 --> 00:46:38,824
아니요. 단 한 개의 등 받침대가
끝까지 지나치지 않습니다.

796
00:46:38,824 --> 00:46:42,256
당신은이 세 가지를 생각할 수 있습니다.

797
00:46:42,256 --> 00:46:44,790
너는 그런 종류의 존재라고 생각할 수있다.

798
00:46:44,790 --> 00:46:46,754
네가 할 일이 있다면 끝까지 추가해라.

799
00:46:46,754 --> 00:46:48,885
계산 그래프를 작성하면

800
00:46:48,885 --> 00:46:52,364
최종 신호 및 이러한 모든 걸 취할 수 있습니다.

801
00:46:52,364 --> 00:46:54,814
그라디언트를 사용하여 다시 모든 것을 통과시켜줍니다.

802
00:46:54,814 --> 00:46:57,719
마치 마지막에 함께 추가 된 것 같습니다.

803
00:46:57,719 --> 00:46:59,780
전산 그래프에서.

804
00:46:59,780 --> 00:47:02,066
우리가 아직 가지고 있기 때문에
시간의 이익을 위해서 그렇습니다.

805
00:47:02,066 --> 00:47:06,233
통과하기에 많은, 오프라인에서
다른 질문을 할 수 있습니다.

806
00:47:08,163 --> 00:47:11,330
좋아, GoogleNet 기본적으로 22 레이어.

807
00:47:12,251 --> 00:47:14,627
그것은 효율적인 시작 모듈을 가지고 있으며,

808
00:47:14,627 --> 00:47:16,793
완전히 연결된 레이어가 없습니다.

809
00:47:16,793 --> 00:47:18,919
AlexNet보다 12 배 적은 매개 변수,

810
00:47:18,919 --> 00:47:22,836
ILSVRC 2014 분류 수상자입니다.

811
00:47:26,038 --> 00:47:28,775
이제 2015 년 우승자를 살펴 보겠습니다.

812
00:47:28,775 --> 00:47:31,679
ResNet 네트워크이므로 여기에서

813
00:47:31,679 --> 00:47:35,964
이 아이디어는 정말로 깊음 그물의 혁명입니다.

814
00:47:35,964 --> 00:47:39,149
2014 년에 우리는 심도를 높이기 시작했습니다.

815
00:47:39,149 --> 00:47:43,477
152 개의 레이어에서이 엄청나게 깊은 모델을 가졌습니다.

816
00:47:43,477 --> 00:47:46,426
ResNet 아키텍처였습니다.

817
00:47:46,426 --> 00:47:49,656
이제 좀 더 자세하게 살펴 보겠습니다.

818
00:47:49,656 --> 00:47:52,521
그래서 ResNet 아키텍처는 극도로 발전하고 있습니다.

819
00:47:52,521 --> 00:47:55,096
다른 네트워크보다 훨씬 더 깊은 네트워크

820
00:47:55,096 --> 00:47:58,534
전에이 아이디어를 사용하여 이것을하고있다.

821
00:47:58,534 --> 00:48:01,289
우리가 이야기 할 잔여 연결의.

822
00:48:01,289 --> 00:48:04,968
그래서 그들은 ImageNet을위한
152 레이어 모델을 가지고있었습니다.

823
00:48:04,968 --> 00:48:08,779
그들은 이것에 7 % 정상 5
과실의 3.5를 얻을 수있었습니다

824
00:48:08,779 --> 00:48:11,927
정말로 특별한 것은 그들이 휩쓸었던 것입니다.

825
00:48:11,927 --> 00:48:15,004
모든 분류 및 탐지 경연 대회는

826
00:48:15,004 --> 00:48:17,862
ImageNet 마트 벤치 마크 및이 다른 벤치 마크

827
00:48:17,862 --> 00:48:18,924
코코.

828
00:48:18,924 --> 00:48:20,189
그것은 기본적으로 모든 것을 얻었습니다.

829
00:48:20,189 --> 00:48:24,356
그래서 다른 모든 것보다 분명히 좋았습니다.

830
00:48:25,865 --> 00:48:29,258
그리고 이제는 조금의 동기 부여로 들어가 봅시다.

831
00:48:29,258 --> 00:48:31,869
ResNet과 잔여 연결 뒤에

832
00:48:31,869 --> 00:48:33,348
우리가 얘기 할게.

833
00:48:33,348 --> 00:48:36,504
그리고 그들이 대답하려고 시도하면서 시작한 질문

834
00:48:36,504 --> 00:48:39,769
우리가 더 깊고 깊게 쌓아 올릴 때 일어나는 일입니다.

835
00:48:39,769 --> 00:48:42,749
일반 convolutional 신경 네트워크에 레이어?

836
00:48:42,749 --> 00:48:44,584
그래서 우리가 VGG와 같은 것을 가져 가면

837
00:48:44,584 --> 00:48:47,937
또는 conv의 스택 인 일반 네트워크 및

838
00:48:47,937 --> 00:48:50,517
서로 겹치는 수영장 층은 우리가 계속해서

839
00:48:50,517 --> 00:48:54,684
이것들을 확장하고, 더 깊은 층을
얻고, 더 잘할 수 있을까요?

840
00:48:56,411 --> 00:48:59,231
그리고 대답은 '아니오'입니다.

841
00:48:59,231 --> 00:49:00,832
그래서 만약에 당신이 그렇게한다면 어떤 일이 일어날지를보십시오.

842
00:49:00,832 --> 00:49:03,242
당신이 더 깊어 질 때, 여기 나는 20 층

843
00:49:03,242 --> 00:49:07,409
네트워크와 56 레이어 네트워크를 지원합니다.

844
00:49:10,308 --> 00:49:12,718
네트워크의 종류 당신은 여기에
테스트 오류에서 볼 수 있습니다

845
00:49:12,718 --> 00:49:15,864
오른쪽에서 56 층 네트워크가 더 나 빠지고 있습니다.

846
00:49:15,864 --> 00:49:17,627
28 층 네트워크보다

847
00:49:17,627 --> 00:49:20,581
그래서 더 깊은 네트워크는 더 잘할 수 없었습니다.

848
00:49:20,581 --> 00:49:23,748
하지만 그때 정말 이상한 일은 지금입니다.

849
00:49:24,639 --> 00:49:27,178
당신이 훈련 오류를 오른쪽으로 보면

850
00:49:27,178 --> 00:49:28,778
우리는 여기서 다시 20 계층 네트워크

851
00:49:28,778 --> 00:49:30,490
및 56 계층 네트워크.

852
00:49:30,490 --> 00:49:33,927
56 층 네트워크, 당신이
생각하는 분명한 문제 중 하나는,

853
00:49:33,927 --> 00:49:38,094
나는 정말 깊은 네트워크를 가지고 있고,
나는 많은 매개 변수를 가지고있다.

854
00:49:39,117 --> 00:49:42,104
어쩌면 아마도 어느 시점에 맞지 않을 수 있습니다.

855
00:49:42,104 --> 00:49:44,682
그러나 실제로 일어나는 것은 당신이 피팅을 끝내면

856
00:49:44,682 --> 00:49:46,276
당신은 아주 좋은 것을 기대할 것입니다.

857
00:49:46,276 --> 00:49:49,795
매우 낮은 훈련 오류율, 그리고 단지 나쁜 시험 오류,

858
00:49:49,795 --> 00:49:51,995
그러나 여기에서 일어나는 것은 훈련 오류에서의 것입니다.

859
00:49:51,995 --> 00:49:54,571
56 층 네트워크 역시

860
00:49:54,571 --> 00:49:56,321
20 층 네트워크.

861
00:49:57,643 --> 00:49:59,438
그리고 더 깊은 모델이 더 나 빠지더라도,

862
00:49:59,438 --> 00:50:02,355
이는 과도한 피팅으로 인한 것이 아닙니다.

863
00:50:04,272 --> 00:50:07,957
그리고 ResNet 제작자의 가설

864
00:50:07,957 --> 00:50:11,063
실제로 문제는 최적화 문제입니다.

865
00:50:11,063 --> 00:50:14,171
더 깊은 모델은 최적화하기가 더 어렵습니다.

866
00:50:14,171 --> 00:50:16,421
더 얕은 네트워크보다

867
00:50:17,645 --> 00:50:19,317
그리고 그 추론은 잘,

868
00:50:19,317 --> 00:50:21,489
더 깊은 모델은 적어도 수행 할 수 있어야합니다.

869
00:50:21,489 --> 00:50:24,073
더 얕은 모델 일 수 있습니다.

870
00:50:24,073 --> 00:50:26,238
당신은 건설을 통해 실제로 해결책을 가질 수 있습니다.

871
00:50:26,238 --> 00:50:28,645
여기서 학습 한 레이어를 가져갈 수 있습니다.

872
00:50:28,645 --> 00:50:30,486
너의 얕은 모델에서

873
00:50:30,486 --> 00:50:33,140
이들을 복사 한 다음 나머지 추가로 복사하십시오.

874
00:50:33,140 --> 00:50:36,002
더 깊은 계층에서는 ID 매핑을 추가하기 만하면됩니다.

875
00:50:36,002 --> 00:50:38,902
그래서 이것은 단지 잘 작동해야합니다.

876
00:50:38,902 --> 00:50:40,343
더 얕은 층으로.

877
00:50:40,343 --> 00:50:43,087
그리고 제대로 배울 수 없었던 당신의 모델,

878
00:50:43,087 --> 00:50:47,105
최소한 이것을 배울 수 있어야합니다.

879
00:50:47,105 --> 00:50:50,688
그리고 이것에 의해 그렇게 동기 부여 된 해결책은

880
00:50:52,418 --> 00:50:56,994
어떻게하면 우리 아키텍처를 어떻게
더 쉽게 만들 수 있을까요?

881
00:50:56,994 --> 00:50:59,896
우리의 모델은 이러한 종류의 솔루션을 배우고,

882
00:50:59,896 --> 00:51:01,404
또는 적어도 이것과 같은 무엇인가?

883
00:51:01,404 --> 00:51:06,303
그래서 그들의 생각은 스태킹 대신에 잘됩니다.

884
00:51:06,303 --> 00:51:08,675
이 모든 층들은 서로 겹쳐지며

885
00:51:08,675 --> 00:51:12,604
모든 계층에서 기본 매핑을 시도하고 배우십시오.

886
00:51:12,604 --> 00:51:17,564
대신에 원하는 블록을 가질 수 있습니다.

887
00:51:17,564 --> 00:51:20,472
우리가 잔여 매핑을 시도하고 적합하게 만들 때,

888
00:51:20,472 --> 00:51:22,518
직접 매핑 대신.

889
00:51:22,518 --> 00:51:24,863
그래서이 모양이이 오른쪽에 있습니다.

890
00:51:24,863 --> 00:51:29,030
이 블록에 대한 입력은 들어오는 입력 일뿐입니다.

891
00:51:30,628 --> 00:51:34,795
그리고 여기서 우리는 우리 편을 여기에서 사용하려고합니다.

892
00:51:38,148 --> 00:51:41,051
우리는 시도하고 맞추기 위해 레이어를 사용하려고합니다.

893
00:51:41,051 --> 00:51:44,218
X의 H에 대한 우리의 욕망의 일부 잔여 물,

894
00:51:45,142 --> 00:51:49,309
X의 원하는 함수 H 대신에 X를 뺀 값.

895
00:51:50,260 --> 00:51:53,814
그리고 기본적으로이 블록의 끝에서 우리는

896
00:51:53,814 --> 00:51:56,637
이 오른쪽의 단계 연결,이 루프,

897
00:51:56,637 --> 00:51:59,787
우리가 입력을 받아들이면 그냥 통과시킵니다.

898
00:51:59,787 --> 00:52:03,346
정체성으로, 그래서 우리는 무게 레이어가 없다면

899
00:52:03,346 --> 00:52:05,102
그 사이에 정체성이 될 것입니다.

900
00:52:05,102 --> 00:52:08,051
출력과 같은 것이지만, 이제는

901
00:52:08,051 --> 00:52:10,955
우리의 추가 무게 레이어는 델타를 배우고,

902
00:52:10,955 --> 00:52:13,372
우리 X의 일부 잔여 물.

903
00:52:14,877 --> 00:52:16,604
그리고 지금은 이것의 출력이 될 것입니다.

904
00:52:16,604 --> 00:52:19,937
우리의 원래 R X와 약간의 잔여 물

905
00:52:20,895 --> 00:52:22,003
우리는 그것을 호출하려고합니다.

906
00:52:22,003 --> 00:52:25,312
기본적으로 델타이므로 아이디어는

907
00:52:25,312 --> 00:52:29,238
이제는 출력이 쉬워야합니다. 예를 들어,

908
00:52:29,238 --> 00:52:32,238
동일성이 이상적인 경우,

909
00:52:33,320 --> 00:52:37,078
X의 F의 모든 가중치를 스쿼시하기 만하면됩니다.

910
00:52:37,078 --> 00:52:40,059
우리 체중 레이어에서 모든 제로로 설정

911
00:52:40,059 --> 00:52:42,308
예를 들어, 우리는 정체성을 얻고 자합니다.

912
00:52:42,308 --> 00:52:44,319
출력으로, 우리는 뭔가를 얻을 수 있습니다,

913
00:52:44,319 --> 00:52:47,831
예를 들어,이 솔루션에 가까운 건설

914
00:52:47,831 --> 00:52:49,388
우리가 전에 가지고 있었던.

915
00:52:49,388 --> 00:52:51,314
맞아, 이것은 단지 네트워크 아키텍처 일뿐입니다.

916
00:52:51,314 --> 00:52:53,448
알았어, 이걸 시도해 보자.

917
00:52:53,448 --> 00:52:57,574
우리의 체중 레이어가 어떻게 잔류하는지
배우고, 무언가가 되십시오.

918
00:52:57,574 --> 00:53:01,772
닫기를하면 X에 가까울 가능성이 커집니다.

919
00:53:01,772 --> 00:53:04,151
그것은 정확히 X를 수정하는 것입니다.

920
00:53:04,151 --> 00:53:06,198
무엇이 있어야하는지에 대한이 전체 맵핑.

921
00:53:06,198 --> 00:53:09,059
좋아, 이것에 대해 질문이 있으십니까?

922
00:53:09,059 --> 00:53:09,999
[학생은 마이크에서 소리 쳤다]

923
00:53:09,999 --> 00:53:13,499
- 문제는 같은 차원입니까?

924
00:53:14,580 --> 00:53:18,413
그렇습니다.이 두 경로는 같은 차원입니다.

925
00:53:19,562 --> 00:53:22,383
일반적으로 그것은 동일한 차원이거나,

926
00:53:22,383 --> 00:53:24,666
또는 그들이 실제로하는 것은 그들이

927
00:53:24,666 --> 00:53:27,571
투영 및 바로 가기 및 다른 방법이 있습니다.

928
00:53:27,571 --> 00:53:31,790
같은 치수로 작동하도록 패딩 처리

929
00:53:31,790 --> 00:53:33,098
깊이 현명한.

930
00:53:33,098 --> 00:53:34,205
예

931
00:53:34,205 --> 00:53:35,763
- [학생] 잔차라는 단어를 사용하면

932
00:53:35,763 --> 00:53:39,930
너는 마이크에 대해 이야기하고 있었다.

933
00:53:46,667 --> 00:53:49,407
- 그래서 질문은 정확히 우리가 의미하는 바입니다.

934
00:53:49,407 --> 00:53:52,968
이 변환 결과의 잔차

935
00:53:52,968 --> 00:53:54,448
잔여 물입니까?

936
00:53:54,448 --> 00:53:58,419
따라서 우리는 X의 F만큼이나 우리의
결과를 여기에서 생각할 수 있습니다.

937
00:53:58,419 --> 00:54:02,709
더하기 X, 여기서 X의 F는 변환의 결과입니다.

938
00:54:02,709 --> 00:54:06,346
그리고 나서 X는 우리의 입력입니다.

939
00:54:06,346 --> 00:54:07,460
정체성에 의해.

940
00:54:07,460 --> 00:54:10,605
그래서 평범한 레이어를 사용하고 싶습니다.

941
00:54:10,605 --> 00:54:12,854
우리가하려는 것은 뭔가를 배우는 것입니다.

942
00:54:12,854 --> 00:54:16,287
X의 H와 비슷하지만 이전에
보았던 것은 어렵다는 것입니다.

943
00:54:16,287 --> 00:54:18,008
X의 H를 배우기.

944
00:54:18,008 --> 00:54:21,481
우리가 매우 깊은 네트워크를 갖게되면 X의 좋은 H입니다.

945
00:54:21,481 --> 00:54:23,771
그래서 여기 아이디어는 시도하고
그것을 무너 뜨리는 것입니다.

946
00:54:23,771 --> 00:54:27,301
X의 H가 X의 F와 같지 않고 대신에,

947
00:54:27,301 --> 00:54:30,248
그리고 X의 F를 배우려고하자.

948
00:54:30,248 --> 00:54:33,648
그래서 직접적으로이 H의 X를 배우는 대신

949
00:54:33,648 --> 00:54:36,341
우리는 단지 우리가 추가해야 할
것이 무엇인지 배우고 싶습니다.

950
00:54:36,341 --> 00:54:40,551
또는 다음 단계로 넘어갈 때 입력 값을 뺍니다.

951
00:54:40,551 --> 00:54:44,160
따라서 여러분은 이것을이 입력을
수정하는 것으로 생각할 수 있습니다.

952
00:54:44,160 --> 00:54:45,469
의미에서 그 자리에.

953
00:54:45,469 --> 00:54:46,699
우리는 -

954
00:54:46,699 --> 00:54:49,931
[학생이 중얼 거려서 마이크에 중얼 거림]

955
00:54:49,931 --> 00:54:51,654
- 문제는 우리가 잔차라는 말을 할 때입니다.

956
00:54:51,654 --> 00:54:53,248
우리는 F of X에 대해 이야기하고 있습니까?

957
00:54:53,248 --> 00:54:54,357
네.

958
00:54:54,357 --> 00:54:56,522
따라서 X의 F는 우리가 잔차라고 부르는 것입니다.

959
00:54:56,522 --> 00:54:58,939
그리고 그 의미가 있습니다.

960
00:55:02,287 --> 00:55:04,751
네, 또 다른 질문입니다.

961
00:55:04,751 --> 00:55:08,251
[학생이 마이크에서 엉망으로]

962
00:55:12,129 --> 00:55:14,334
- 문제는 실제적으로 우리가 합계하는 것입니다.

963
00:55:14,334 --> 00:55:17,480
X와 X의 F를 함께 사용하거나, 가중치를 배웁니다.

964
00:55:17,480 --> 00:55:20,955
조합을 선택하면 직접 합계를 계산할 수 있습니다.

965
00:55:20,955 --> 00:55:23,599
왜냐하면 당신이 직접 합계를 할 때,

966
00:55:23,599 --> 00:55:26,869
이것은 내가 무엇인지 알게하는 아이디어입니다.

967
00:55:26,869 --> 00:55:29,619
나는 X에 더하거나 뺄 필요가있다.

968
00:55:31,462 --> 00:55:35,273
이 모든 것이 주된 직관입니까?

969
00:55:35,273 --> 00:55:36,171
문제.

970
00:55:36,171 --> 00:55:39,588
[학생은 마이크에서 소리 쳤다]

971
00:55:41,531 --> 00:55:43,865
- 그래, 왜 그게 문제인지 분명하지 않다.

972
00:55:43,865 --> 00:55:45,823
잔류 물을 배우는 것이 더 쉬워야한다고

973
00:55:45,823 --> 00:55:47,909
직접 매핑을 배우는 것보다?

974
00:55:47,909 --> 00:55:50,609
그리고 이것은 단지 그들의 가설입니다.

975
00:55:50,609 --> 00:55:55,181
가설은 우리가 잔차를 배우면

976
00:55:55,181 --> 00:55:59,557
X에 대한 델타가 무엇인지 배워야합니다.

977
00:55:59,557 --> 00:56:02,947
우리의 가설이 일반적으로

978
00:56:02,947 --> 00:56:07,169
심지어 우리의 솔루션과 같은 건설,

979
00:56:07,169 --> 00:56:10,690
우리는 몇몇 얕은 층들을 가지고있었습니다.

980
00:56:10,690 --> 00:56:13,718
우리는이 모든 신원 매핑을 배웠습니다.

981
00:56:13,718 --> 00:56:16,911
상단에 이것이 있어야했던 해결책이었습니다.

982
00:56:16,911 --> 00:56:20,219
좋은, 그리고 그래서 그것은 아마
많은 이들 층을 암시합니다.

983
00:56:20,219 --> 00:56:23,157
실제로 정체성에 가까운 것,

984
00:56:23,157 --> 00:56:24,795
좋은 층이 될 것입니다.

985
00:56:24,795 --> 00:56:27,092
그래서 그것 때문에, 우리는 이것을 공식화합니다.

986
00:56:27,092 --> 00:56:30,168
정체성을 배울 수있는 것으로서

987
00:56:30,168 --> 00:56:31,764
약간의 델타 플러스.

988
00:56:31,764 --> 00:56:35,125
정말로 정체성이 가장 좋은 경우

989
00:56:35,125 --> 00:56:37,573
F는 X 스쿼시 변환을 0으로 만듭니다.

990
00:56:37,573 --> 00:56:39,662
상대적으로,

991
00:56:39,662 --> 00:56:41,173
배우기가 더 쉬워 보일 수도 있지만,

992
00:56:41,173 --> 00:56:43,509
또한 우리는 가까운 것을 얻을 수 있습니다.

993
00:56:43,509 --> 00:56:45,594
신원 매핑.

994
00:56:45,594 --> 00:56:47,927
그리고 다시 이것은 반드시 필연적 인 것이 아닙니다.

995
00:56:47,927 --> 00:56:51,776
입증 된 것 또는 무엇 이건
그것은 직관과 가설 일뿐입니다.

996
00:56:51,776 --> 00:56:54,848
그리고 나서 우리는 나중에 사람들이

997
00:56:54,848 --> 00:56:56,533
실제로 이것에 도전하고 오 말할 수 있습니다.

998
00:56:56,533 --> 00:56:59,518
그것은 실제로 필요한 잔차가 아니며,

999
00:56:59,518 --> 00:57:03,069
그러나 적어도 이것은이 논문의 가설이며,

1000
00:57:03,069 --> 00:57:06,105
실제로이 모델을 사용하여,

1001
00:57:06,105 --> 00:57:08,317
그것은 아주 잘 할 수있었습니다.

1002
00:57:08,317 --> 00:57:09,620
문제.

1003
00:57:09,620 --> 00:57:13,037
[학생은 마이크에서 소리 쳤다]

1004
00:57:42,623 --> 00:57:45,394
- 그렇습니다. 그래서 사람들은
다른 방법을 시도해 보았습니다.

1005
00:57:45,394 --> 00:57:49,938
이전 레이어의 입력 및 예

1006
00:57:49,938 --> 00:57:52,556
그래서 이것은 기본적으로 매우 활발한 연구 분야입니다.

1007
00:57:52,556 --> 00:57:55,018
어떻게 우리가이 모든 연결을 공식화하는지,

1008
00:57:55,018 --> 00:57:57,557
그리고이 모든 구조에서 무엇이 연결되어 있는지

1009
00:57:57,557 --> 00:58:00,174
그래서 우리는 다른 네트워크의 몇
가지 예를 보게 될 것입니다.

1010
00:58:00,174 --> 00:58:04,505
잠시 후에 아키텍처가 활성화되지만
이는 활성화 된 영역입니다.

1011
00:58:04,505 --> 00:58:05,505
연구.

1012
00:58:06,468 --> 00:58:10,245
좋습니다. 그래서 우리는 기본적으로
모든 잔차 블록을 가지고 있습니다.

1013
00:58:10,245 --> 00:58:12,903
서로 위에 쌓인

1014
00:58:12,903 --> 00:58:15,598
우리는 완전한 상주 아키텍처를 볼 수 있습니다.

1015
00:58:15,598 --> 00:58:18,750
이 나머지 블록들 각각은 두 개씩 세 개씩 있습니다.

1016
00:58:18,750 --> 00:58:23,285
레이어를이 블록의 일부로 사용하면 작업이 완료됩니다.

1017
00:58:23,285 --> 00:58:26,479
이것이 단지 좋은 구성이라고 말하는 것입니다.

1018
00:58:26,479 --> 00:58:28,109
잘 작동합니다.

1019
00:58:28,109 --> 00:58:30,638
우리는이 모든 블록들을 매우 깊이 쌓아 놓습니다.

1020
00:58:30,638 --> 00:58:34,161
이 아주 깊은 건축물과 같은 또 다른 것

1021
00:58:34,161 --> 00:58:37,911
그것은 기본적으로 최대 150
개의 레이어를 가능하게합니다.

1022
00:58:42,231 --> 00:58:45,981
이것의 깊이, 그리고 우리가하는
일은 우리가 쌓는 것입니다.

1023
00:58:47,392 --> 00:58:49,725
이 모든 것들과 주기적으로 우리는 또한 두 배

1024
00:58:49,725 --> 00:58:52,042
필터 수 및 공간적으로 샘플 다운

1025
00:58:52,042 --> 00:58:54,792
우리가 그렇게 할 때 보폭 2를 사용합니다.

1026
00:58:56,666 --> 00:58:59,319
그리고 나서 우리는이 추가적인 [벙어리]

1027
00:58:59,319 --> 00:59:02,835
우리 네트워크의 맨 처음에

1028
00:59:02,835 --> 00:59:03,668
그리고 결국 우리는 또한 듣습니다.

1029
00:59:03,668 --> 00:59:04,677
완전히 연결된 레이어가 없다.

1030
00:59:04,677 --> 00:59:06,844
전역 평균 풀링 레이어가 있습니다.

1031
00:59:06,844 --> 00:59:09,451
그것은 공간적으로 모든 것을 평균 할 것입니다.

1032
00:59:09,451 --> 00:59:13,618
마지막 1000 방법 분류에 입력하십시오.

1033
00:59:15,504 --> 00:59:17,801
이것이 전체 ResNet 아키텍처입니다.

1034
00:59:17,801 --> 00:59:20,333
그리고 그것은 매우 단순하고 우아합니다.

1035
00:59:20,333 --> 00:59:22,745
이 모든 ResNet 블록은 서로의 위에 있습니다.

1036
00:59:22,745 --> 00:59:27,032
그들은 총 깊이가 34, 50, 100,

1037
00:59:27,032 --> 00:59:30,199
ImageNet에 대해 최대 152 개를 시도했습니다.

1038
00:59:35,040 --> 00:59:39,369
좋아, 한 가지 더 알고 싶은 것은

1039
00:59:39,369 --> 00:59:41,869
매우 깊은 네트워크의 경우, 더 많은 네트워크

1040
00:59:41,869 --> 00:59:44,774
깊이가 50 이상인 경우 병목 현상 레이어도 사용합니다.

1041
00:59:44,774 --> 00:59:47,473
효율성을 높이기 위해 GoogleNet이
수행 한 것과 유사합니다.

1042
00:59:47,473 --> 00:59:51,865
이제 각 블록 내에서 당신은 가고 있습니다.

1043
00:59:51,865 --> 00:59:54,645
그들이 한 일은 하나의 전환 필터 (conv filter)

1044
00:59:54,645 --> 00:59:58,005
그것을 먼저 더 작은 깊이로 투영합니다.

1045
00:59:58,005 --> 01:00:01,774
그래서 다시 우리가보고 있다면 28
시까 지 28 살이라고합시다.

1046
01:00:01,774 --> 01:00:05,087
256 임플란트로, 우리는 이것을 하나씩 처리합니다.

1047
01:00:05,087 --> 01:00:06,926
그것은 깊이를 투사하고있다.

1048
01:00:06,926 --> 01:00:08,759
우리는 28에 의해 28만큼 64를 얻습니다.

1049
01:00:09,917 --> 01:00:12,220
이제 당신의 회선은 3 대 3의 conv,

1050
01:00:12,220 --> 01:00:15,809
여기에 그들은 오직 하나만 가지고
있으며, 이것 이상으로 작동합니다.

1051
01:00:15,809 --> 01:00:19,296
축소 된 단계로 인해 비용이 적게 듭니다.

1052
01:00:19,296 --> 01:00:21,881
그리고 나서 그들은 다른

1053
01:00:21,881 --> 01:00:24,621
깊이있는 백업 프로젝트

1054
01:00:24,621 --> 01:00:28,180
~ 256, 실제 블록입니다.

1055
01:00:28,180 --> 01:00:30,680
더 깊은 네트워크에서 보게 될 것입니다.

1056
01:00:33,831 --> 01:00:38,618
따라서 실제로 ResNet은 일괄 정규화를 사용합니다.

1057
01:00:38,618 --> 01:00:42,092
모든 전환 레이어 후에는 Xavier 초기화를 사용합니다.

1058
01:00:42,092 --> 01:00:46,259
그들이 도움을 주었던 여분의 스케일링 요소

1059
01:00:47,221 --> 01:00:51,388
SGD + 모멘텀으로 훈련 된 초기화를 향상시킵니다.

1060
01:00:52,414 --> 01:00:54,622
그들의 학습 속도는 비슷한 학습 속도를 사용합니다.

1061
01:00:54,622 --> 01:00:57,280
학습 속도를 떨어 뜨리는 일정 유형

1062
01:00:57,280 --> 01:01:00,280
귀하의 유효성 오류가 평원 때.

1063
01:01:02,561 --> 01:01:05,351
미니 배치 크기 256, 체중 감량 조금

1064
01:01:05,351 --> 01:01:06,684
아무런 탈락도 없습니다.

1065
01:01:08,455 --> 01:01:10,990
그리고 실험적으로 그들은 그들이

1066
01:01:10,990 --> 01:01:12,710
이 매우 깊은 네트워크를 훈련 할 수 있었고,

1067
01:01:12,710 --> 01:01:14,391
퇴화하지 않고.

1068
01:01:14,391 --> 01:01:17,582
그들은 기본적으로 좋은 그라디언트
흐름을 가질 수있었습니다.

1069
01:01:17,582 --> 01:01:19,870
네트워크를 통해 모든 길로 되돌아옵니다.

1070
01:01:19,870 --> 01:01:23,435
그들은 ImageNet에서 최대 152 개의 레이어를 시도했으며,

1071
01:01:23,435 --> 01:01:27,442
1200 Cifar에, 당신은 그것으로 놀았 어,

1072
01:01:27,442 --> 01:01:31,525
그러나 더 작은 데이터 세트 및
그들은 또한 지금 그것을 보았다.

1073
01:01:32,804 --> 01:01:35,556
당신은 더 깊은 네트워크가 더
낮은 훈련을 이룰 수 있습니다.

1074
01:01:35,556 --> 01:01:37,113
예상대로 오류가 발생했습니다.

1075
01:01:37,113 --> 01:01:40,228
그래서 당신은 똑같은 이상한 그림을 가지고 있지 않습니다.

1076
01:01:40,228 --> 01:01:43,841
우리가 이전에 본 행동

1077
01:01:43,841 --> 01:01:45,353
잘못된 방향이었다.

1078
01:01:45,353 --> 01:01:48,260
그래서 여기에서 그들은 1 위를 쓸어 버릴 수있었습니다.

1079
01:01:48,260 --> 01:01:50,344
모든 ILSVRC 대회에서,

1080
01:01:50,344 --> 01:01:53,570
2015 년 모든 COCO 대회

1081
01:01:53,570 --> 01:01:55,653
상당한 이윤으로

1082
01:01:56,962 --> 01:02:01,574
총 상위 5 개 오류는 분류에서 3.6 %였습니다

1083
01:02:01,574 --> 01:02:05,626
실제로 이것은 인간의 성능보다 낫습니다.

1084
01:02:05,626 --> 01:02:07,459
ImageNet 논문에서.

1085
01:02:09,712 --> 01:02:12,864
또한 인간의 측정 기준이있었습니다.

1086
01:02:12,864 --> 01:02:17,031
사실 우리 실험실 안드레 카파시가 썼다.

1087
01:02:18,023 --> 01:02:21,940
일주일 내내 스스로 훈련하고 기본적으로

1088
01:02:23,127 --> 01:02:25,540
모두 다 했어.이 일을 혼자서 했어.

1089
01:02:25,540 --> 01:02:29,341
그리고 나는 어딘가에 5-ish %를 생각했다.

1090
01:02:29,341 --> 01:02:31,584
그래서 저는 기본적으로 할 수있었습니다.

1091
01:02:31,584 --> 01:02:35,001
적어도 인간보다.

1092
01:02:36,985 --> 01:02:41,072
좋아요, 이것들은 주요 네트워크의 종류입니다.

1093
01:02:41,072 --> 01:02:42,879
최근에 사용 된

1094
01:02:42,879 --> 01:02:45,211
우리는 처음에 AlexNet을 시작했고,

1095
01:02:45,211 --> 01:02:48,814
VGG와 GoogleNet은 여전히 인기가 있지만,

1096
01:02:48,814 --> 01:02:51,967
그러나 ResNet은 가장 최근에
가장 우수한 수행 모델입니다.

1097
01:02:51,967 --> 01:02:56,100
새로운 네트워크를 훈련시키는 무언가를 찾고 있다면

1098
01:02:56,100 --> 01:02:57,695
ResNet을 사용할 수 있습니다.

1099
01:02:57,695 --> 01:02:59,028
그것으로 작업.

1100
01:03:00,964 --> 01:03:04,844
따라서이 중 일부를 더 빨리 살펴 보는 것이 더 좋습니다.

1101
01:03:04,844 --> 01:03:07,213
복잡성에 대한 감각.

1102
01:03:07,213 --> 01:03:09,098
여기에 우리는

1103
01:03:09,098 --> 01:03:12,848
성능 그래서 이것이 여기에 최고 정확도입니다.

1104
01:03:14,003 --> 01:03:16,085
그리고 더 높은 것이 좋습니다.

1105
01:03:16,085 --> 01:03:17,887
그래서 우리는 이러한 모델을 많이 보게 될 것입니다.

1106
01:03:17,887 --> 01:03:19,817
일부 다른 버전뿐만 아니라

1107
01:03:19,817 --> 01:03:22,350
그 (것)들의 이렇게,이 GoogleNet 처음 것은,

1108
01:03:22,350 --> 01:03:26,266
내 생각에 V2, V3 및 여기서는 최고라고 생각합니다.

1109
01:03:26,266 --> 01:03:29,135
V4는 실제로 ResNet 더하기 시작입니다.

1110
01:03:29,135 --> 01:03:32,199
조합, 그래서 이들은 단지 종류의

1111
01:03:32,199 --> 01:03:34,820
점진적이고 작은 변화

1112
01:03:34,820 --> 01:03:37,555
그 (것)들의 위에 건설 해, 이렇게 최상이다

1113
01:03:37,555 --> 01:03:39,969
여기서 모델을 수행합니다.

1114
01:03:39,969 --> 01:03:42,756
그리고 우리가 오른쪽을 보면, 그들의

1115
01:03:42,756 --> 01:03:46,256
여기서 계산상의 복잡성은 정렬됩니다.

1116
01:03:48,496 --> 01:03:51,609
Y 축이 가장 정확합니다.

1117
01:03:51,609 --> 01:03:53,123
그래서 더 높습니다.

1118
01:03:53,123 --> 01:03:57,223
X 축은 사용자의 작업이므로 오른쪽으로 갈수록,

1119
01:03:57,223 --> 01:03:59,749
당신이하고있는 작전이 많을수록 더 계산적으로

1120
01:03:59,749 --> 01:04:02,456
비싼 다음 큰 원,

1121
01:04:02,456 --> 01:04:03,884
귀하의 서클은 귀하의 메모리 사용량,

1122
01:04:03,884 --> 01:04:06,097
회색 원이 여기서 참조됩니다.

1123
01:04:06,097 --> 01:04:08,061
그러나 원이 클수록 메모리 사용량이 많아집니다.

1124
01:04:08,061 --> 01:04:11,824
그래서 여기서 우리는 VGG가
녹색의 것들임을 알 수 있습니다.

1125
01:04:11,824 --> 01:04:14,152
일종의 효율성이 가장 낮습니다.

1126
01:04:14,152 --> 01:04:15,909
그들은 가장 큰 기억을 가지고 있으며,

1127
01:04:15,909 --> 01:04:17,016
대부분의 작업,

1128
01:04:17,016 --> 01:04:19,433
그러나 그들은 꽤 잘합니다.

1129
01:04:20,648 --> 01:04:23,221
GoogleNet이 가장 효율적입니다.

1130
01:04:23,221 --> 01:04:25,757
그것은 작전 측면에서 길이다.

1131
01:04:25,757 --> 01:04:30,085
메모리 사용을위한 작은 작은 원이 있습니다.

1132
01:04:30,085 --> 01:04:34,059
이전 모델 인 AlexNet은 정확도가 가장 낮습니다.

1133
01:04:34,059 --> 01:04:35,985
상대적으로 계산량이 적습니다.

1134
01:04:35,985 --> 01:04:38,804
그것은 더 작은 네트워크이지만,
또한 특별히 그렇지 않습니다.

1135
01:04:38,804 --> 01:04:40,221
메모리 효율적인.

1136
01:04:42,119 --> 01:04:47,026
그리고 ResNet 여기, 우리는
적당한 효율성을 가지고 있습니다.

1137
01:04:47,026 --> 01:04:49,310
메모리의 측면에서 보면 중간에있는 것입니다.

1138
01:04:49,310 --> 01:04:53,310
작업을 수행 할 수 있으며 정확도가 가장 높습니다.

1139
01:04:56,839 --> 01:04:58,838
여기에 몇 가지 추가 플롯도 있습니다.

1140
01:04:58,838 --> 01:05:01,618
당신은 당신의 자신의 시간에 이것들을 더 볼 수 있습니다.

1141
01:05:01,618 --> 01:05:04,600
왼쪽의이 그림은 정방향 통과 시간을 보여줍니다.

1142
01:05:04,600 --> 01:05:07,825
그래서 이것은 밀리 세컨드 (milliseconds)이며
맨 위에 올 수 있습니다.

1143
01:05:07,825 --> 01:05:11,253
VGG 앞으로 약 200 밀리 초를 얻을 수 있습니다.

1144
01:05:11,253 --> 01:05:13,466
이것으로 초당 5 프레임,

1145
01:05:13,466 --> 01:05:15,678
이것은 순서대로 정렬됩니다.

1146
01:05:15,678 --> 01:05:18,459
권력을 바라보고있는이 줄거리도있다.

1147
01:05:18,459 --> 01:05:22,584
소비와 당신이 여기에서이 종이를 더 보는 경우에,

1148
01:05:22,584 --> 01:05:25,693
이러한 종류의 계산에 대한 추가 분석이 있습니다.

1149
01:05:25,693 --> 01:05:26,693
비교.

1150
01:05:31,414 --> 01:05:34,318
그래서 이것들은 여러분이해야 할 주요 아키텍처였습니다.

1151
01:05:34,318 --> 01:05:36,776
정말로 깊이 알고 있고 익숙하다.

1152
01:05:36,776 --> 01:05:39,560
적극적으로 사용하려고 생각하고 있습니다.

1153
01:05:39,560 --> 01:05:41,278
하지만 이제는 간단히 살펴 보겠습니다.

1154
01:05:41,278 --> 01:05:43,156
좋은 다른 아키텍쳐

1155
01:05:43,156 --> 01:05:46,323
역사적 영감을 아는 것

1156
01:05:47,290 --> 01:05:50,040
또는 최근 연구 분야.

1157
01:05:51,526 --> 01:05:53,569
첫 번째 네트워크 인 Network in Network,

1158
01:05:53,569 --> 01:05:57,152
이것은 2014 년 이후이며이 배경의 아이디어입니다.

1159
01:06:01,339 --> 01:06:06,334
이 바닐라 길쌈 레이어가 있다는 것입니다.

1160
01:06:06,334 --> 01:06:10,261
그러나 우리는 이것들을 가지고 있습니다. 이것은

1161
01:06:10,261 --> 01:06:13,493
그들이 마이크로 네트워크라고 부르는 MLP 전환 계층

1162
01:06:13,493 --> 01:06:15,497
또는 기본적으로 networth 내의 네트워크,

1163
01:06:15,497 --> 01:06:16,928
종이의 이름.

1164
01:06:16,928 --> 01:06:21,088
각 전환 레이어 내에서 MLP를 스택하려는 위치

1165
01:06:21,088 --> 01:06:23,962
맨 위에 두 개의 완전히 연결된 레이어가있는

1166
01:06:23,962 --> 01:06:26,293
표준 전환과 계산이 가능하다.

1167
01:06:26,293 --> 01:06:28,989
이 지역의 더 추상적 인 기능

1168
01:06:28,989 --> 01:06:29,977
패치가 맞아.

1169
01:06:29,977 --> 01:06:32,017
그래서 conv 필터를 미끄러지는 대신에,

1170
01:06:32,017 --> 01:06:36,153
그것은 약간 더 복잡한 계층 구조로 미끄러지고있다.

1171
01:06:36,153 --> 01:06:40,780
주변의 필터 세트를 사용하여

1172
01:06:40,780 --> 01:06:42,785
활성화지도.

1173
01:06:42,785 --> 01:06:46,092
그리고 이렇게, 이것은 완전히 연결된,

1174
01:06:46,092 --> 01:06:48,751
또는 기본적으로 하나의 전환 유형의 층으로 구성됩니다.

1175
01:06:48,751 --> 01:06:51,001
그것들을 모두 쌓아 올릴 것입니다.

1176
01:06:51,001 --> 01:06:53,701
우리가이 네트워크를 가지고있는 아래쪽 다이어그램

1177
01:06:53,701 --> 01:06:58,006
각 레이어에 쌓인 네트워크 내에서

1178
01:06:58,006 --> 01:07:01,475
그리고 이것을 알아야하는 주된 이유는

1179
01:07:01,475 --> 01:07:04,825
그것은 GoogleNet과 ResNet의 선구자였습니다.

1180
01:07:04,825 --> 01:07:08,584
병목 현상에 대한 아이디어가 담긴 2014 년

1181
01:07:08,584 --> 01:07:10,912
네가 거기에서 아주 많이 사용되는 것을 보았다.

1182
01:07:10,912 --> 01:07:13,828
그리고 그것은 또한 약간의 철학적 영감을주었습니다.

1183
01:07:13,828 --> 01:07:17,388
로컬 네트워크 유형학에 대한 GoogleNet의 아이디어

1184
01:07:17,388 --> 01:07:19,963
네트워크에서 사용 된 네트워크,

1185
01:07:19,963 --> 01:07:22,880
다른 종류의 구조.

1186
01:07:25,048 --> 01:07:29,422
이제 일련의 작품에 대해 이야기하겠습니다.

1187
01:07:29,422 --> 01:07:32,125
대부분 설치되어있는 ResNet
이후에 작동하거나 작동합니다.

1188
01:07:32,125 --> 01:07:34,655
resNet을 개선하기 위해

1189
01:07:34,655 --> 01:07:37,569
그 이후로 연구가 이루어졌습니다.

1190
01:07:37,569 --> 01:07:38,923
나는이 꽤 빨리 갈 것이다,

1191
01:07:38,923 --> 01:07:40,721
그래서 매우 높은 수준에 있습니다.

1192
01:07:40,721 --> 01:07:42,147
당신이 이것들에 관심이 있다면

1193
01:07:42,147 --> 01:07:45,564
논문을보고 자세한 내용을 알아보십시오.

1194
01:07:46,565 --> 01:07:50,686
그래서 ResNet의 저자는 잠시 후에

1195
01:07:50,686 --> 01:07:54,853
2016 년에는이 신문을 통해

1196
01:07:55,876 --> 01:07:57,552
ResNet 블록 설계.

1197
01:07:57,552 --> 01:08:01,327
그래서 그들은 기본적으로 레이어가 무엇인지 조정했습니다.

1198
01:08:01,327 --> 01:08:03,825
ResNet 블록 경로에 있던

1199
01:08:03,825 --> 01:08:07,299
이 새로운 구조가

1200
01:08:07,299 --> 01:08:11,024
정보 전달을위한보다 직접적인 경로

1201
01:08:11,024 --> 01:08:14,711
네트워크 전반에 걸쳐, 당신은 좋은 것을 원한다.

1202
01:08:14,711 --> 01:08:16,883
경로를 통해 모든 방법으로 정보를 전파하고,

1203
01:08:16,883 --> 01:08:19,671
그런 다음 다시 맨 아래로 백업하십시오.

1204
01:08:19,671 --> 01:08:22,244
그래서 그들은이 새로운 블록이 더
좋았다는 것을 보여주었습니다.

1205
01:08:22,245 --> 01:08:26,129
더 나은 성능을 제공 할 수있었습니다.

1206
01:08:26,129 --> 01:08:29,769
이 논문에서는 Wide
Residual 네트워크도 있습니다.

1207
01:08:29,769 --> 01:08:34,401
ResNets은 네트워크를 훨씬 더 깊게 만들었지 만

1208
01:08:34,401 --> 01:08:36,687
이러한 잔여 연결을 추가 할뿐만 아니라

1209
01:08:36,687 --> 01:08:40,206
그들의 주장은 잔차가 실제로 있다는 것입니다.

1210
01:08:40,206 --> 01:08:41,038
중요한 요인.

1211
01:08:41,038 --> 01:08:42,334
이 잔존물 구조를 가짐으로써,

1212
01:08:42,335 --> 01:08:46,100
반드시 아주 깊은 네트워크가있는 것은 아닙니다.

1213
01:08:46,100 --> 01:08:50,277
그래서 그들은 더 넓은 잔차 블록을 사용했습니다.

1214
01:08:50,277 --> 01:08:52,702
따라서 이것이 의미하는 바는 모든 필터가

1215
01:08:52,702 --> 01:08:53,604
전환 층.

1216
01:08:53,604 --> 01:08:56,806
따라서 레이어 당 F 필터를 사용하기 전에

1217
01:08:56,807 --> 01:09:00,118
그들은 K의 이러한 인자들을 사용하여 우물을 말했고,

1218
01:09:00,118 --> 01:09:03,472
모든 레이어 그것은 F times K 필터가 될 것입니다.

1219
01:09:03,473 --> 01:09:07,432
그래서이 넓은 층을 사용하여

1220
01:09:07,432 --> 01:09:09,812
그들의 50 층 너비의 ResNet이
out-perform 할 수있었습니다.

1221
01:09:09,812 --> 01:09:12,312
152 레이어 원래 ResNet,

1222
01:09:14,564 --> 01:09:17,099
그것은 또한 다음과 같은 추가적인 장점을 가지고있다.

1223
01:09:17,099 --> 01:09:21,513
동일한 금액으로도이 금액이 증가합니다.

1224
01:09:21,513 --> 01:09:23,845
매개 변수, 더 가슴의 계산 효율

1225
01:09:23,845 --> 01:09:26,587
이러한 작업을 병렬 처리 할 수 있기 때문에

1226
01:09:26,587 --> 01:09:27,732
더 쉽게.

1227
01:09:27,733 --> 01:09:31,143
더 많은 뉴런을 가진 바로 그 콘볼 루트

1228
01:09:31,143 --> 01:09:33,560
더 많은 커널에 퍼져있다.

1229
01:09:33,560 --> 01:09:36,430
보다 순차적 인 깊이와는 달리,

1230
01:09:36,430 --> 01:09:39,416
따라서 계산 효율이 향상됩니다.

1231
01:09:39,416 --> 01:09:40,356
너비.

1232
01:09:40,356 --> 01:09:42,104
그래서 여기에서이 작품이 시작되는 것을 볼 수 있습니다.

1233
01:09:42,104 --> 01:09:44,402
너비의 공헌을 이해하려고 노력하다.

1234
01:09:44,403 --> 01:09:47,550
깊이와 잔여 연결부,

1235
01:09:47,550 --> 01:09:50,627
한 쪽에서 다른 쪽에서 몇 가지 주장하기.

1236
01:09:50,627 --> 01:09:54,127
그리고이 다른 신문은 같은 시간에,

1237
01:09:55,874 --> 01:09:58,935
아마 조금 나중에, ResNeXt,

1238
01:09:58,935 --> 01:10:02,601
다시 이것은 ResNet의 제작자입니다.

1239
01:10:02,601 --> 01:10:05,193
계속해서 아키텍처를 추진하고 있습니다.

1240
01:10:05,193 --> 01:10:09,110
그리고 여기에도 그들은 괜찮은 생각이 들었습니다.

1241
01:10:10,509 --> 01:10:13,742
참으로이 폭 너비를 단지 대신에 더 많이 다루겠습니다.

1242
01:10:13,742 --> 01:10:15,829
이 잔여 블록의 너비를 늘린다.

1243
01:10:15,829 --> 01:10:19,386
더 많은 필터를 통해 그들은 구조를 가지고 있습니다.

1244
01:10:19,386 --> 01:10:23,335
그리고 각 나머지 블록 내에서, 다중 병렬

1245
01:10:23,335 --> 01:10:25,344
통로와 그들이 전화 할거야

1246
01:10:25,344 --> 01:10:27,225
이 경로의 총 수는 카디널리티입니다.

1247
01:10:27,225 --> 01:10:31,308
그래서 기본적으로 하나의 ResNet
블록을 사용하고 있습니다.

1248
01:10:33,386 --> 01:10:36,341
병목 현상이 있고 상대적으로 더 얇은 것은

1249
01:10:36,341 --> 01:10:39,205
그러나 이들 중 여러 개를 병렬로 수행해야합니다.

1250
01:10:39,205 --> 01:10:43,373
그리고 여기에도이 두 가지가 있다는 것을 알 수 있습니다.

1251
01:10:43,373 --> 01:10:45,262
광범위한 네트워크에 대한이 아이디어와의 관계,

1252
01:10:45,262 --> 01:10:50,097
또한 시작 모듈에 약간의 연결이있다.

1253
01:10:50,097 --> 01:10:52,461
우리가이 평행선을 가지고있는 곳에서도 말이죠.

1254
01:10:52,461 --> 01:10:54,833
이들 층은 병행하여 작동한다.

1255
01:10:54,833 --> 01:10:59,000
그리고 이제이 ResNeXt에는 그 취향이 있습니다.

1256
01:11:01,648 --> 01:11:05,653
ResNets을 개선하기위한 또 다른 접근법

1257
01:11:05,653 --> 01:11:09,445
확률 적 깊이라고 불리는이
아이디어가이 작업에서 나왔습니다.

1258
01:11:09,445 --> 01:11:12,557
동기 부여는 잘 보입니다.

1259
01:11:12,557 --> 01:11:14,688
이 깊이 문제에서.

1260
01:11:14,688 --> 01:11:18,855
일반적인 문제를 더 깊고 깊게 느끼면

1261
01:11:20,385 --> 01:11:22,347
당신은 사라지는 그라디언트를 가지고있을 것입니다.

1262
01:11:22,347 --> 01:11:26,874
당신은 할 수 없습니다. 당신의
그라디언트는 점점 작아 질 것입니다.

1263
01:11:26,874 --> 01:11:29,077
당신이 다시 전파하려고하는 동안 결국 사라집니다.

1264
01:11:29,077 --> 01:11:32,881
그것들은 매우 긴 레이어들 또는
많은 수의 레이어들 위에 있습니다.

1265
01:11:32,881 --> 01:11:36,003
그래서 그들의 동기가 무엇인지 잘 알고 자 노력합니다.

1266
01:11:36,003 --> 01:11:40,418
짧은 네트워크 동안 교육 및 그들은이 아이디어를 사용하여

1267
01:11:40,418 --> 01:11:43,855
트레이닝 중에 레이어의 하위 집합을 삭제합니다.

1268
01:11:43,855 --> 01:11:46,474
따라서 레이어의 하위 집합에 대해서는 그냥 사라집니다.

1269
01:11:46,474 --> 01:11:49,246
무게와 그들은 정체성 연결로 설정했습니다.

1270
01:11:49,246 --> 01:11:53,379
그리고 지금 당신이 얻는 것은이 짧은
네트워크를 가지고 있다는 것입니다.

1271
01:11:53,379 --> 01:11:55,425
교육을받는 동안

1272
01:11:55,425 --> 01:11:56,936
그라디언트가 더 좋습니다.

1273
01:11:56,936 --> 01:12:00,343
또한 조금 더 효율적입니다.

1274
01:12:00,343 --> 01:12:02,345
일종의 드롭 아웃 권리처럼.

1275
01:12:02,345 --> 01:12:04,884
그것은 전에 보았던 이런 종류의 향이 있습니다.

1276
01:12:04,884 --> 01:12:07,251
그리고 테스트 시간에 전체 깊은
네트워크를 사용하고자합니다.

1277
01:12:07,251 --> 01:12:08,918
너는 훈련을 받았다.

1278
01:12:11,256 --> 01:12:13,591
그래서 이들은 다음을 보는 작품들입니다.

1279
01:12:13,591 --> 01:12:15,640
상주 아키텍처, 다른 것을 이해하려고 시도하다.

1280
01:12:15,640 --> 01:12:19,848
ResNet 교육을 향상시키기 위해 노력하고 있습니다.

1281
01:12:19,848 --> 01:12:23,241
그래서 현재 진행중인 작품도 있습니다.

1282
01:12:23,241 --> 01:12:25,856
ResNet을 넘어서서,

1283
01:12:25,856 --> 01:12:30,116
어쩌면 더 잘 작동 할 수있는 ResNet 아키텍처,

1284
01:12:30,116 --> 01:12:33,063
또는 ResNets과 비슷하거나 더 낫습니다.

1285
01:12:33,063 --> 01:12:36,579
그리고 한 가지 아이디어는 FractalNet입니다.

1286
01:12:36,579 --> 01:12:39,282
꽤 최근에, 그리고 FractalNet의 논쟁

1287
01:12:39,282 --> 01:12:42,312
그 잔여 표현은 어쩌면

1288
01:12:42,312 --> 01:12:44,239
실제로는 필요하지 않으므로 다시 돌아갑니다.

1289
01:12:44,239 --> 01:12:46,083
우리가 이전에 말한 것에.

1290
01:12:46,083 --> 01:12:48,586
잔여 네트워크의 동기는 무엇입니까?

1291
01:12:48,586 --> 01:12:50,756
말하자면, 알다시피,

1292
01:12:50,756 --> 01:12:53,455
이것이 왜 도움이되는지에 대한
좋은 이유가 아니라이 백서에서

1293
01:12:53,455 --> 01:12:56,600
그들은 여기에 다른 아키텍처가 있다고 말하고 있습니다.

1294
01:12:56,600 --> 01:12:59,217
우리가 소개하고있는 것은 잔여 표현이 아닙니다.

1295
01:12:59,217 --> 01:13:01,884
우리는 그 열쇠가 전환에 더 많은 것이라고 생각합니다.

1296
01:13:01,884 --> 01:13:04,708
효과적으로 얕은 네트워크에서 깊은 네트워크로,

1297
01:13:04,708 --> 01:13:07,203
그래서 그들은이 프랙탈 아키텍처를 가지고 있습니다.

1298
01:13:07,203 --> 01:13:09,901
네가 여기 오른쪽을 보면,

1299
01:13:09,901 --> 01:13:14,068
그들이이 프랙탈 패션에서 그것을 구성하는 이들 층.

1300
01:13:15,579 --> 01:13:18,199
그래서 얕은 통로와 깊은 통로가 있습니다.

1301
01:13:18,199 --> 01:13:19,449
귀하의 산출물에

1302
01:13:20,855 --> 01:13:23,269
그래서 그들은 서로 다른 길이의 경로를 가지고 있습니다.

1303
01:13:23,269 --> 01:13:26,046
그들은 하위 경로를 버리고 훈련시키고,

1304
01:13:26,046 --> 01:13:30,378
그리고 다시 한번이 드롭 아웃 종류의 향이 있습니다.

1305
01:13:30,378 --> 01:13:33,151
테스트 시간에 그들은 전체 프랙탈
네트워크를 사용할 것입니다.

1306
01:13:33,151 --> 01:13:35,846
그들은 이것이 가능하다는 것을 보여줍니다.

1307
01:13:35,846 --> 01:13:38,013
아주 좋은 성능을 얻으십시오.

1308
01:13:39,857 --> 01:13:42,953
Densely Connected라고하는
또 다른 아이디어가 있습니다.

1309
01:13:42,953 --> 01:13:45,696
길쌈 네트워크, DenseNet 및이 아이디어

1310
01:13:45,696 --> 01:13:48,397
이제 우리는이 블럭들이 호출됩니다.

1311
01:13:48,397 --> 01:13:49,377
고밀도 블록.

1312
01:13:49,377 --> 01:13:51,588
그리고 각 블록 내에서 각 레이어는

1313
01:13:51,588 --> 01:13:55,238
그 후에 모든 다른 레이어에 연결되어,

1314
01:13:55,238 --> 01:13:56,750
이 피드 포워드 방식으로

1315
01:13:56,750 --> 01:13:58,312
따라서이 블록 내에서 블록에 대한 입력

1316
01:13:58,312 --> 01:14:01,172
다른 모든 전환 레이어에 대한 입력이기도합니다.

1317
01:14:01,172 --> 01:14:04,764
그리고 당신이 각각의 conv 출력을 계산할 때,

1318
01:14:04,764 --> 01:14:06,609
그 출력은 이제 모든

1319
01:14:06,609 --> 01:14:09,589
후에 레이어를 연결 한 다음 모두 연결합니다.

1320
01:14:09,589 --> 01:14:12,500
전환 레이어에 대한 입력으로

1321
01:14:12,500 --> 01:14:16,262
그들에게는 줄이기위한 다른 과정들이있다.

1322
01:14:16,262 --> 01:14:19,453
차원과 효율적인 유지.

1323
01:14:19,453 --> 01:14:23,329
그리고 이것으로부터 그들의 주요 테이크 아웃은,

1324
01:14:23,329 --> 01:14:26,843
그들은 이것이 완화되고 있다고 주장한다.

1325
01:14:26,843 --> 01:14:29,790
이 모든 것이 있기 때문에 사라지는 그라디언트 문제

1326
01:14:29,790 --> 01:14:31,673
매우 조밀 한 연결.

1327
01:14:31,673 --> 01:14:35,722
그것은 특징 전파를 강화하고 또한 격려한다.

1328
01:14:35,722 --> 01:14:38,134
미래의 사용 권리는 너무 많기 때문에

1329
01:14:38,134 --> 01:14:41,807
학습하고있는 각 기능지도를 연결합니다.

1330
01:14:41,807 --> 01:14:44,630
나중에 여러 개의 레이어에 입력되고

1331
01:14:44,630 --> 01:14:46,297
여러 번 사용되었습니다.

1332
01:14:48,716 --> 01:14:50,813
그래서 이것들은 단지 몇 가지 아이디어 일뿐입니다.

1333
01:14:50,813 --> 01:14:55,025
당신은 대안을 알고 있습니다. 그렇지
않으면 우리가 할 수있는 것이 없습니다.

1334
01:14:55,025 --> 01:14:57,836
ResNets과 아직 수행 중입니다

1335
01:14:57,836 --> 01:15:00,784
ResNets과 비슷하거나 더 낫습니다.

1336
01:15:00,784 --> 01:15:03,816
현재 연구 중 또 다른 매우 활발한 분야.

1337
01:15:03,816 --> 01:15:05,501
이 많은 것들이보고 있다는 것을 알 수 있습니다.

1338
01:15:05,501 --> 01:15:09,057
서로 다른 레이어가 서로 연결되는 방식으로

1339
01:15:09,057 --> 01:15:12,640
그리고 이러한 네트워크에서 깊이가
관리되는 방법에 대해 설명합니다.

1340
01:15:14,338 --> 01:15:15,808
그리고 내가 언급하고 싶은 마지막 한가지

1341
01:15:15,808 --> 01:15:18,801
빠르게 효율적인 네트워크 일뿐입니다.

1342
01:15:18,801 --> 01:15:21,831
효율성에 대한 아이디어와 GoogleNet을 보았습니다.

1343
01:15:21,831 --> 01:15:24,216
이 방향을 연구 한 작품이었습니다.

1344
01:15:24,216 --> 01:15:27,240
우리가 어떻게 중요한 네트워크를 효율적으로 만들 수 있는지

1345
01:15:27,240 --> 01:15:30,260
당신은 실용적인 사용법을 많이 알고 있습니다.

1346
01:15:30,260 --> 01:15:34,804
특히 배포와 마찬가지로

1347
01:15:34,804 --> 01:15:38,737
SqueezeNet이라고하는 또 다른 최근 네트워크

1348
01:15:38,737 --> 01:15:40,744
매우 효율적인 네트워크를 찾고 있습니다.

1349
01:15:40,744 --> 01:15:42,428
그들은 화재 모듈 (fire modules)

1350
01:15:42,428 --> 01:15:45,095
많은 스퀴즈 레이어로 구성되어 있습니다.

1351
01:15:45,095 --> 01:15:47,674
하나씩 필터를 적용한 다음 피드를

1352
01:15:47,674 --> 01:15:50,455
하나씩 및 3 x 3 필터를 갖는 확장 레이어,

1353
01:15:50,455 --> 01:15:53,490
그리고 그들은 이런 종류의 건축물로
그것을 보여주고 있습니다.

1354
01:15:53,490 --> 01:15:57,820
그들은 ImageNet에서 AlexNet
수준의 정확성을 얻을 수 있으며,

1355
01:15:57,820 --> 01:16:00,030
그러나 50 배 적은 매개 변수로,

1356
01:16:00,030 --> 01:16:03,140
그런 다음 네트워크 압축을 추가로 수행 할 수 있습니다.

1357
01:16:03,140 --> 01:16:06,903
AlexNet보다 500 배나 더 작아 지도록

1358
01:16:06,903 --> 01:16:10,905
전체 네트워크가 0.5 메가가되도록하십시오.

1359
01:16:10,905 --> 01:16:13,691
그래서 이것은 우리가 어떻게 할 것인가의 방향입니다.

1360
01:16:13,691 --> 01:16:15,572
효율적인 네트워크 모델 압축

1361
01:16:15,572 --> 01:16:17,955
나중에 강의에서 더 자세히 다루겠습니다.

1362
01:16:17,955 --> 01:16:20,872
그러나 당신에게 그 힌트를주는 것뿐입니다.

1363
01:16:22,666 --> 01:16:26,351
오늘 우리는 여러 종류의 이야기를했습니다.

1364
01:16:26,351 --> 01:16:27,619
CNN 아키텍처의

1365
01:16:27,619 --> 01:16:30,568
주요 아키텍처 중 네 가지를 자세히 살펴 보았습니다.

1366
01:16:30,568 --> 01:16:32,365
넓은 사용법에서 볼 수 있습니다.

1367
01:16:32,365 --> 01:16:36,363
초기 인기 네트워크 중 하나 인 AlexNet.

1368
01:16:36,363 --> 01:16:39,642
VGG 및 GoogleNet과 같이 널리 사용됩니다.

1369
01:16:39,642 --> 01:16:43,695
그러나 ResNet은 일종의 인계 역할을합니다.

1370
01:16:43,695 --> 01:16:46,716
당신이 할 수있을 때 가장 많이보고 있어야합니다.

1371
01:16:46,716 --> 01:16:47,943
우리는 또한이 다른 네트워크들을 보았습니다.

1372
01:16:47,943 --> 01:16:50,147
간단히 조금 더 깊이 들어

1373
01:16:50,147 --> 01:16:51,397
레벨 개요.

1374
01:16:52,731 --> 01:16:55,315
그리고이 테이크 아웃을 사용할 수있는 이러한 모델

1375
01:16:55,315 --> 01:16:57,974
그들은 당신이 그들을 사용할 수 있도록
많은 [mumbles]에 있습니다.

1376
01:16:57,974 --> 01:16:59,038
필요할 때.

1377
01:16:59,038 --> 01:17:01,085
극도로 깊은 네트워크 경향이 있습니다.

1378
01:17:01,085 --> 01:17:04,804
그러나 또한 중요한 연구가 주변에있다.

1379
01:17:04,804 --> 01:17:07,637
우리가 어떻게 레이어를 연결하는지에 대한 디자인,

1380
01:17:07,637 --> 01:17:10,785
연결을 건너 뛰고, 무엇이 무엇에 연결되어 있는지,

1381
01:17:10,785 --> 01:17:14,343
또한 이들을 사용하여 아키텍처를 설계하십시오.

1382
01:17:14,343 --> 01:17:16,229
그래디언트 흐름을 개선합니다.

1383
01:17:16,229 --> 01:17:19,013
최근의 추세에 대한 검토가 있습니다.

1384
01:17:19,013 --> 01:17:22,246
깊이 대 너비의 필요성은 무엇입니까?

1385
01:17:22,246 --> 01:17:23,558
잔여 연결.

1386
01:17:23,558 --> 01:17:25,358
트레이드 오프, 실제로 문제를 돕는 것,

1387
01:17:25,358 --> 01:17:28,356
최근의 많은 작품들이 있습니다.

1388
01:17:28,356 --> 01:17:30,153
이 방향으로 볼 수있다.

1389
01:17:30,153 --> 01:17:32,190
당신이 관심이 있다면 내가 지적한 것들 중 일부.

1390
01:17:32,190 --> 01:17:34,407
그리고 다음에 우리는 재발 신경
네트워크에 대해서 이야기 할 것입니다.

1391
01:17:34,407 --> 00:00:00,000
감사.

