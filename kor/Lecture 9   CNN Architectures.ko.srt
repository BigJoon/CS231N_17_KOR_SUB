1
00:00:14,752 --> 00:00:21,696
9강입니다. 오늘은 CNN 아키텍쳐들을
한번 알아보겠습니다.

2
00:00:21,696 --> 00:00:27,706
수업에 앞서 몇 가지 공지사항을 전달해 드리겠습니다.
우선 과제 2는 목요일까지 입니다.

3
00:00:27,706 --> 00:00:36,855
중간고사는 다음 주인 5월 9일 목요일 수업시간이 진행하도록 하겠습니다.
그 전까지 중간고사 범위 진도를 모두 나갈 예정입니다.

4
00:00:36,855 --> 00:00:41,350
Recurrent neural network 까지 차질없이
진행할 수 있도록 하겠습니다.

5
00:00:41,350 --> 00:00:49,121
그리고 우리 수업의 포스터세션이 6월 6일 12시부터 3시까지
진행될 예정입니다. 우리 수업의 마지막 주가 되겠군요

6
00:00:49,121 --> 00:00:53,828
이번에는 포스터세션이 조금 일찍 열립니다.

7
00:00:53,828 --> 00:01:00,132
포스터 세션까지 아직 시간이 남아있으니
기말 레포트에 최선을 다해주시기 바랍니다.

8
00:01:03,325 --> 00:01:05,812
지난 시간에 배운 내용을 복습해 보겠습니다.

9
00:01:05,812 --> 00:01:09,324
지난 시간에 다양한 딥러닝 프레임워크들을 배웠습니다.

10
00:01:09,324 --> 00:01:12,690
도PyTorch, TensorFlow, Caffe2 등이 있었죠

11
00:01:14,514 --> 00:01:18,762
이런 프레임워크를 이용하게 되면 NN, CNN같은 규모가 큰
computraional graphs를 아주 쉽게 구성할 수 있었습니다.

12
00:01:18,762 --> 00:01:25,784
또한 그래프에서 gradients을 계산하기에도
아주 수월했습니다.

13
00:01:25,784 --> 00:01:32,415
네트워크 중간의 가중치와 입력변수들의 그레디언트를
알아서 계산해 주기 때문에 Train에 사용만 하면 됩니다.

14
00:01:32,415 --> 00:01:35,665
그리고 이 모든 것을 GPU를 통해 아주 효율적으로
동작시킬 수 있습니다.

15
00:01:37,658 --> 00:01:44,978
그리고 프레임워크들은 대게 이런 식으로
모듈화된 레이어를 통해 동작합니다.

16
00:01:44,978 --> 00:01:49,928
여러분이 과제로 작성했던 backward/forward pass와
아주 유사한 모습입니다.

17
00:01:49,928 --> 00:01:58,404
모델 아키텍쳐를 구성하기 위해서는 단지 그 레이어들을
하나의 시퀀스로 정의하고 묶어주기만 하면 됩니다.

18
00:01:58,404 --> 00:02:04,937
이를 통해 아주 복잡한 아키텍쳐라고 해도
손쉽게 구성할 수 있습니다.

19
00:02:06,626 --> 00:02:14,520
오늘은 최신 CNN 아키텍쳐들에 대해서 배워보겠습니다.

20
00:02:14,520 --> 00:02:19,631
사람들이 가장 많이 사용하는 아키텍쳐들을
아주 심도깊게 살펴볼 것입니다.

21
00:02:19,631 --> 00:02:22,125
 이들은 모두 ImageNet 첼린지에서 우승한 모델들이죠

22
00:02:22,125 --> 00:02:28,085
여기에는 제가 연대순으로 정렬했습니다. AlexNet, VGGNet,
GoogLeNet 그리고 ResNet이 있습니다.

23
00:02:28,085 --> 00:02:43,771
그리고 또한 엄청 잘 사용하지는 않지만 역사적인 관점에서
아주 흥미로운 모델들, 그리고 아주 최신의 모델들도 다룰 것입니다.

24
00:02:46,822 --> 00:02:50,839
아주 오래 전 강의에서 LeNet을 다룬 적이 있었습니다.

25
00:02:50,839 --> 00:02:55,603
LeNet은 산업에 아주 성공적으로 적용된 최초의 ConvNet입니다.

26
00:02:55,603 --> 00:03:05,778
LeNet은 이미지를 입력으로 받아서 stride = 1 인 5 x 5 필터를
거치고 몇 개의 Conv Layer와 pooling layer를 거칩니다.

27
00:03:05,778 --> 00:03:09,335
그리고 끝 단에 FC Layer가 붙습니다.

28
00:03:09,335 --> 00:03:14,320
엄청 간단한 모델이지만 숫자 인식에서
엄청난 성공을 거두었습니다.

29
00:03:17,030 --> 00:03:22,875
2012년에 AlexNet이 나왔습니다. 이 모델도 이전 강의에서
다들 본 적 있으실 것ㅇ비니다.

30
00:03:22,875 --> 00:03:31,179
AlexNet은 최초의 Large scale CNN 입니다. ImageNet
Classification Task을 아주 잘 수행했습니다.

31
00:03:31,179 --> 00:03:40,611
AlexNet은 2012년에 등장해서는 기존의 non-딥러닝 모델들을
능가하는 놀라운 성능을 보여줬습니다.

32
00:03:40,611 --> 00:03:48,012
AlexNet은 ConvNet 연구의 부흥을 일으킨 장본입니다.

33
00:03:48,012 --> 00:03:56,427
AlexNet은 기본적으로 conv - pool - normalization
구조가 두 번 반복됩니다.

34
00:03:58,421 --> 00:04:01,006
그리고 conv layer가 조금 더 붙고(CONV 3,4,5)
그 뒤에 pooling layer가 있습니다. (Max POOL3)

35
00:04:01,006 --> 00:04:03,422
그리고 마지막에는 FC-layer가 몇 개 붙습니다.
(FC6, FC7, FC8)

36
00:04:03,422 --> 00:04:09,766
생긴 것만 봐서는 기존의 LeNet과 상당히 유사합니다.
레이어만 더 많아졌습니다.

37
00:04:09,766 --> 00:04:18,387
AlexNet는 5개의 Conv Layer와
2개의 FC-Layer로 구성됩니다.

38
00:04:21,889 --> 00:04:25,930
그렇다면 이제 AlexNet 의 모델 크기를 한번 살펴봅시다.

39
00:04:25,930 --> 00:04:33,128
AlexNet의 ImageNet으로 학습시키는 경우
입력의 크기가 227 x 227 x 3 입니다.

40
00:04:33,128 --> 00:04:43,193
AlexNet의 첫 레이어를 살펴보면 11 x 11 필터가
stride = 4 로 96개가 존재합니다.

41
00:04:43,193 --> 00:04:49,323
그렇다면 잠시 멈춰서 생각해봅시다.
첫 레이어의 출력사이즈는 어떻게 될까요?

42
00:04:51,788 --> 00:04:53,371
힌트도 있습니다.

43
00:04:57,769 --> 00:05:11,441
입력과 Conv 필터의 사이즈를 알고 있습니다.
그리고 출력값의 차원도 여기 힌트로 있습니다.

44
00:05:11,441 --> 00:05:17,632
이 공식은 (전체 이미지 크기 - 필터 크기) / Stride + 1 이죠

45
00:05:17,632 --> 00:05:26,919
결국 출력차원은 55입니다.
답이 뭘까요?

46
00:05:26,919 --> 00:05:29,823
[학생이 대답]

47
00:05:29,823 --> 00:05:32,966
55 x 55 x 96 이라고 답했습니다. 맞습니다.

48
00:05:32,966 --> 00:05:38,113
출력값의 width, height는 각각 55 입니다.

49
00:05:38,113 --> 00:05:45,391
그리고 필터가 총 96개 이므로 depth가 96이 됩니다.

50
00:05:45,391 --> 00:05:49,486
자 그러면 이 레이어의 전체 파라미터 갯수는 몇 개일까요?

51
00:05:49,486 --> 00:05:52,819
명심해야 할 점은
11 x 11 필터가 총 96개 있다는 것입니다.

52
00:05:54,851 --> 00:05:57,753
[학생이 대답]

53
00:05:57,753 --> 00:06:00,753
96 x 11 x11 이라고 답했습니다.
거의 맞췄습니다.

54
00:06:01,945 --> 00:06:05,297
네 맞습니다. 3을 더 곱해줘야죠

55
00:06:05,297 --> 00:06:13,632
필터 하나가 11 x 11 x 3 을 통과합니다.
입력의 Depth가 3이죠

56
00:06:13,632 --> 00:06:18,983
답은 전체 필터의 크기 X 96 이 됩니다.

57
00:06:18,983 --> 00:06:23,150
첫 레이어에 35K의 파라미터가 있는 것입니다.

58
00:06:26,018 --> 00:06:30,233
자 그러면 두 번째 레이어를 한번 살펴봅시다.
두 번째 레이어는 Pooling Layer 입니다.

59
00:06:30,233 --> 00:06:34,004
여기에는 stride = 2 인 3 x 3 필터가 있습니다.

60
00:06:34,004 --> 00:06:38,171
이 레이어의 출력값의 크기는 어떻게 될까요?

61
00:06:40,701 --> 00:06:44,868
힌트도 드렸습니다. 지난 문제와 아주 유사합니다.

62
00:06:51,251 --> 00:06:56,267
27 x 27 x 96 이라고 답했습니다.
네 맞습니다.

63
00:06:57,716 --> 00:07:01,528
Pooling Layer에서는 힌트에 나와있는 공식이
적용될 것입니다.

64
00:07:01,528 --> 00:07:16,655
이 공식을 이용해서 width, height를 구할 수 있습니다.
다만 depth는 입력과 변하지 않습니다.

65
00:07:16,655 --> 00:07:21,527
입력의 Depth가 96 이었으니 출력의 Depth도 96 입니다.

66
00:07:22,825 --> 00:07:28,127
그렇다면 이 레이어의 파라미터는 몇 개일까요?

67
00:07:31,446 --> 00:07:34,354
[학생이 대답]

68
00:07:34,354 --> 00:07:36,905
없다고 대답했습니다. 네 맞습니다.

69
00:07:36,905 --> 00:07:40,801
Pooling layer에는 파라미터가 없죠.
훼이크였습니다.

70
00:07:42,739 --> 00:07:45,272
네 질문있나요?

71
00:07:45,272 --> 00:07:47,192
[학생이 질문]

72
00:07:47,192 --> 00:07:52,180
질문은 바로 왜 pooling layer에는 파라미터가
없는지 입니다.

73
00:07:52,180 --> 00:07:54,551
파라미터는 우리가 학습시키는 가중치입니다.

74
00:07:54,551 --> 00:07:56,511
Conv Layer에는 학습할 수 있는 가중치가 있습니다.

75
00:07:56,511 --> 00:08:02,236
반면 pooling의 경우에는 가중치가 없고 그저
특정 지역에서 큰 값을 뽑아내는 역할만 합니다.

76
00:08:02,236 --> 00:08:05,710
따라서 학습시킬 파라미터가 없는 것이죠

77
00:08:05,710 --> 00:08:14,250
다들 집에가서 모든 레이어의 파라미터 사이즈를 계산해
보면 아주 큰 도움이 될 것입니다.

78
00:08:16,473 --> 00:08:22,688
이는 AlexNet의 전체 구조입니다.

79
00:08:22,688 --> 00:08:31,920
Conv layer들의 파라미터 크기는  앞서 계산한 값과 유사할 것입니다.

80
00:08:31,920 --> 00:08:39,122
그리고 끝에 몇 개의 FC-Layer가 있었습니다.
4096개의 노드를 가진 레이어입니다.

81
00:08:39,123 --> 00:08:41,540
그리고 FC8 는 Softmax를 통과합니다.

82
00:08:42,689 --> 00:08:46,356
1000 ImageNet 클래스로 이동합니다.

83
00:08:48,039 --> 00:08:56,352
AlexNet을 조금 더 자세히 살펴보자면 우선 ReLU 를 사용했습니다.
ReLU는 딥러닝 모델에서 아주 보편화된 방법입니다.

84
00:08:56,352 --> 00:09:07,391
local response normalization layer는 채널간의
 normalization을 위한 것인데 요즘은 잘 사용하지 않습니다.

85
00:09:07,391 --> 00:09:11,937
큰 효과가 없는 것으로 알려졌기 때문입니다.

86
00:09:11,937 --> 00:09:21,769
data augumentation을 엄청 했습니다. 논문이 더 자세하지만
flipping, jittering, color norm 등을 적용하였습니다.

87
00:09:21,769 --> 00:09:28,727
data augumentation은 여러분이 프로젝트를
진행할 때 아주 유용한 기법입니다.

88
00:09:28,727 --> 00:09:32,419
AlexNet은 Dropout을 사용했습니다.
학습 시 Batch size는 128 입니다.

89
00:09:32,419 --> 00:09:37,183
그리고 우리도 지난 강의에서 배웠던
SGD momentum을 사용했습니다.

90
00:09:37,183 --> 00:09:42,295
그리고 초기 Learning rate 는 1e-2 입니다.

91
00:09:42,295 --> 00:09:50,145
그리고 val accuracy가 올라가지 않는 지점에서는 학습이 종료되는
시점까지 Learning rate를 1e-10까지 줄입니다.

92
00:09:50,145 --> 00:09:59,012
그리고 wight decay를 사용했고, 마지막에는
모델 앙상블로 성능을 향상시켰습니다.

93
00:09:59,012 --> 00:10:03,162
그리고 여러개의 모델을 앙상블시켜서
성능을 개선했습니다.

94
00:10:04,405 --> 00:10:08,781
그리고 한가지 더 말씀드릴 점은 여기 AlexNet
다이어그램을 보시면

95
00:10:08,781 --> 00:10:15,235
대체로 다른 Conv Net의 다이어그램과 유사하긴 하지만
한 가지 차이점이 있습니다.

96
00:10:15,235 --> 00:10:21,937
그것은 바로 모델이 두개로 나눠져서 서로 교차하는 것입니다.

97
00:10:23,177 --> 00:10:32,905
역사적으로 들여다보면 AlexNet을 학습할 당시에 GTX850
으로 학습시켰습니다. 이 GPU는 메모리가 3GB 뿐이죠

98
00:10:34,106 --> 00:10:37,255
전체 레이어를 GPU에 다 넣을 수 없었습니다.

99
00:10:37,255 --> 00:10:41,773
그래서 네트워크를 GPU에 분산시켜서 넣었습니다.

100
00:10:41,773 --> 00:10:46,455
각 GPU가 모델의 뉴런과 Feature Map을 반반씩 나눠가집니다.

101
00:10:46,455 --> 00:10:51,730
첫 번째 레이어를 살펴보면 출력이 55 x 55 x 96입니다.

102
00:10:54,389 --> 00:11:04,155
다이어그램을 유심히 살펴보면 각 GPU에서의
Depth가 48이라는 것을 알 수 있습니다.

103
00:11:05,049 --> 00:11:08,593
Feature Map을 절반씩 가지고 있는 것이죠

104
00:11:10,288 --> 00:11:17,367
AlexNet의 Conv 1,2,4,5 에서
어떤 일이 발생하는지 살펴봅시다.

105
00:11:17,367 --> 00:11:29,683
여기에서는 같은 GPU 내에 있는 Feature Map만 사용합니다.
 Conv 1,2,4,5는 전체 96 feature map을 볼 수 없습니다.

106
00:11:29,683 --> 00:11:33,850
 Conv 1,2,4,5는 48개의 Feature Map만 사용하는 셈입니다.

107
00:11:34,767 --> 00:11:47,696
이제 conv 3와 FC 6, 7, 8 를 한번 살펴봅시다. 이 레이어들은
이전 계층의 "전체 Feature map"과 연결되어 있습니다.

108
00:11:47,696 --> 00:11:54,191
이 레이어들에서는 GPU간의 통신을 하기 때문에 이전 입력
레이어의 전체 Depth를 전부 가져올 수 있는 것입니다.

109
00:11:54,191 --> 00:11:55,627
질문있나요?

110
00:11:55,627 --> 00:12:01,442
[학생이 질문]

111
00:12:05,583 --> 00:12:10,033
질문은 왜 여기에 예시가 full "simplified"
AlexNet architecture 인지 입니다.

112
00:12:10,033 --> 00:12:19,036
Simplified 라고 하는 이유는 여기에 AlexNet의
세부적인 것들을 전부 표시하지 않았기 때문입니다.

113
00:12:19,036 --> 00:12:25,268
가령  normalization layer 같은 경우에
자세히 기입해 놓지 않았습니다.

114
00:12:30,637 --> 00:12:37,849
그리고 AlexNet 논문의 아키텍쳐와 관련한 조그만
이슈가 하나 있습니다.

115
00:12:38,858 --> 00:12:52,721
논문의 그림에서는 첫 레이어가 224 x 224 라고 되어 있습니다.
하지만 실제 입력은 227 x 227 입니다.

116
00:12:54,982 --> 00:13:04,261
AlexNet은 Image Classification Benchmark의
2012년도에 우승한 모델입니다.

117
00:13:05,246 --> 00:13:14,193
AlexNet은 최초의 CNN기반 우승 모델이고 수년 전까지 대부분의
CNN 아키텍쳐의 베이스모델로 사용되어 왔습니다.

118
00:13:15,720 --> 00:13:17,980
물론 아직까지도 꽤 사용합니다.

119
00:13:17,980 --> 00:13:24,071
AlexNet은 다양한 Task의 transfer learning에 많이
사용되었습니다. 아주 오랜 시간 많은 사람들이 사용했고 -

120
00:13:24,071 --> 00:13:33,202
정말 유명한 모델이었습니다. 지금은 더 최신의 아키텍쳐가 많습니다.
일반적으로 AlexNet보다 성능이 더 뛰어나죠.

121
00:13:33,202 --> 00:13:39,282
이제는 더 최신의 모델들에 대해 살펴보겠습니다. 여러분이
실제도 더 자주 사용하게될 모델들입니다.

122
00:13:40,853 --> 00:13:47,813
그리고 2013년의 ImageNet Challange의 승자는
ZFNet 이라는 모델입니다.

123
00:13:47,813 --> 00:13:48,718
질문있나요?

124
00:13:48,718 --> 00:13:52,729
[학생이 질문]

125
00:13:52,729 --> 00:13:56,612
질문은 AlexNet이 기존의 모델들보다 뛰어날 수 있었던
이유가 무엇인지 입니다.

126
00:13:56,612 --> 00:14:04,786
이는 딥러닝와 Conv Net 때문입니다.
이들은 기존의 방법들과 완전히 다른 접근방법입니다.

127
00:14:04,786 --> 00:14:09,004
AlexNet이 최초로 딥러닝과 Conv Net을 적용하였습니다.

128
00:14:12,445 --> 00:14:18,298
2013년에는 ZFNet이 우승트로피를 획득했습니다.
ZF는 저자들의 이름을 딴 명칭입니다.

129
00:14:18,298 --> 00:14:23,749
ZFNet은 대부분 AlexNet의 하이퍼파라미터를 개선한 모델입니다.

130
00:14:23,749 --> 00:14:35,735
AlexNet과 같은 레이어 수이고 기존적인 구조도 같습니다.
다만 stride size, 필터 수 같은 하이퍼파라미터를 조절해서-

131
00:14:35,735 --> 00:14:41,369
AlexNet의 Error rate를 좀 더 개선시켰습니다.
AlexNet의 아이디어와는 기본적으로 같습니다.

132
00:14:41,369 --> 00:14:49,843
2014년에는 많은 변화가 있었습니다. 아키텍쳐도 많이 변했고
성능을 훨씬 향상되었죠

133
00:14:49,843 --> 00:14:58,178
가장 큰 차이점이라면 우선 네트워크가 훨씬 더 깊어졌다는 것입니다.

134
00:14:58,178 --> 00:15:12,321
2012/2013년에는 8개의 레이어였습니다. 하지만 2014년에는
19레이어와 22 레이어로 늘어났습니다. 훨씬 더 깊어졌죠

135
00:15:12,321 --> 00:15:16,502
2014년도의 우승자는 Google의 GoogLenet이었습니다.

136
00:15:16,502 --> 00:15:20,176
그리고 Oxford의 VGGNet이 2등을 차지했죠

137
00:15:20,176 --> 00:15:27,421
VGGNet은 Localization Challenge 등 일부 다른
트랙에서는 1위를 차지했습니다.

138
00:15:27,421 --> 00:15:31,958
이 둘 모두 아주 강력한 네트워크입니다.

139
00:15:31,958 --> 00:15:34,663
우선 VGG을 한번 살펴봅시다.

140
00:15:34,663 --> 00:15:40,818
VGGNet의 특징은 우선 훨씬 더 깊어졌고
그리고 더 작은 필터를 사용한다는 것입니다.

141
00:15:40,818 --> 00:15:50,374
AlexNet에서는 8개의 레이어였지만 VGGNet은
16에서 19개의 레이어를 가집니다.

142
00:15:52,290 --> 00:16:03,916
그리고 VGGNet은 아주 작은 필터만 사용합니다. 항상 3 x 3 필터만
사용합니다. 이웃픽셀을 포함할 수 있는 가장 작은 필터입니다.

143
00:16:03,916 --> 00:16:11,485
이렇게 작은 필터를 유지해 주고 주기적으로 Pooling을
수행하면서 전체 네트워크를 구성하게 됩니다.

144
00:16:11,485 --> 00:16:19,948
VGGNet은 아주 심플하면서도 고급진 아키텍쳐이고
ImageNet에서 7.3%의 Top 5 Error를 기록했습니다.

145
00:16:22,651 --> 00:16:27,442
그렇다면 VGGNet은 왜 더 작은 필터를 사용했을까요?

146
00:16:27,442 --> 00:16:33,371
우선 필터의 크기가 작으면 파라미터의 수가 더 적습니다.
따라서 큰 필터에 비해 레이어를 조금 더 많이 쌓을 수 있겠죠

147
00:16:33,371 --> 00:16:39,344
다시 말해 작은 필터를 사용하면 Depth를 더 키울 수 있는 것입니다.

148
00:16:39,344 --> 00:16:47,202
3 x 3 필터를 여러 개 쌓은 것은 결국 7 x 7 필터를 사용하는 것과
실질적으로 동일한 Receptive Filter를 가지는 것입니다.

149
00:16:47,202 --> 00:16:55,466
그렇다면 질문입니다. stride = 1 인 3 x 3필터를 세 개의
Receptive Filed는 어떻게 될까요?

150
00:16:55,466 --> 00:17:01,189
Stride가 1인 필터 세 개를 쌓게 되면
실질적인 Receptive Field가 어떻게 될까요?

151
00:17:01,189 --> 00:17:09,754
Receptive Field은 filter가 한번에 볼 수 있는 입력의
"Sparical area" 입니다.

152
00:17:12,313 --> 00:17:15,987
누군가가 15 픽셀이라고 했습니다.
왜 15 필셀인가요?

153
00:17:15,987 --> 00:17:20,609
[학생이 대답]

154
00:17:20,609 --> 00:17:27,369
네 맞습니다 필터들이 서로 겹치는 것입니다.

155
00:17:27,369 --> 00:17:35,668
실제로 어떤 일이 발생하는지 한번 살펴봅시다. 우선 첫 번째
레이어의 Receptive Field는 3 x 3 입니다.

156
00:17:35,668 --> 00:17:43,193
두 번째 레이어의 경우는 각 뉴런이 첫 번째 레이어 출력의
3 x 3 만큼을 보게 될 것입니다.

157
00:17:43,193 --> 00:17:51,676
그리고 3 x 3 중에 각 사이드는 한 픽셀씩 더 볼 수 있게 됩니다.

158
00:17:51,676 --> 00:17:56,423
따라서 두번째 레이어의 경우는 실제로
5 x 5의 receptive filed를 가지게 되는 것입니다.

159
00:17:56,423 --> 00:18:04,040
세 번째 레이어의 경우 두 번째 레이어의 3 x 3 을 보게됩니다.

160
00:18:04,040 --> 00:18:06,907
그리고 이 과정을 피라미드처럼 그려보면
결국 입력 레이어의 7 x 7을 보게되는 것이죠

161
00:18:06,907 --> 00:18:16,026
따라서 실질적인 Receptive Field는 여기에서 7 x 7 이 됩니다.
하나의 7 x 7 필터를 사용하는 것과 동일합니다.

162
00:18:16,026 --> 00:18:21,546
따라서 이는 7 x 7 필터와 실직적으로 동일한 receptive
filed를 가지면서도 더 깊은 레이어를 쌓을 수 있게 됩니다.

163
00:18:21,546 --> 00:18:26,201
더 깊게 쌓으므로써 Non-Linearity를 더 추가할 수 있고
파라미터 수도 더 적어지게 됩니다.

164
00:18:26,201 --> 00:18:36,536
전체 파라미터의 갯수를 살펴보면 3 x 3 필터에는
9개의 파라미터가 있습니다.

165
00:18:38,165 --> 00:18:44,648
3 x 3  가 되죠 그리고 Depth인 C가 있으니
3 x 3 x C 가 됩니다.

166
00:18:44,648 --> 00:18:51,034
그리고 출력 Feature Map의 갯수를 곱해줘야 하는데
이 경우에는 입력 Depth와 같습니다. C이죠

167
00:18:51,034 --> 00:19:00,165
따라서 각 레이터 당 3 x 3 x C x C 가 됩니다. 그리고 레이어가
세개 이므로 3을 더 곱해줍니다.

168
00:19:00,165 --> 00:19:07,409
 7 x 7 필터인 경우에는 7 x 7 x C x C 입니다.

169
00:19:07,409 --> 00:19:11,032
따라서 더 적은 파라미터를 가지게 됩니다.

170
00:19:15,570 --> 00:19:24,161
이제 전체 네트워크를 한번 살펴보도록 하겠습니다. 여기 숫자가
너무 많죠 다들 집으로 돌아가서 유심히 살펴보시기 바랍니다.

171
00:19:24,161 --> 00:19:30,716
AlexNet에서 우리가 계산해 봤던 것 처럼 파라미터의
크기와 수를 계산해 보실 수 있을 것입니다.

172
00:19:30,716 --> 00:19:32,517
연습으로 해보시면 아주 좋습니다.

173
00:19:32,517 --> 00:19:45,834
비슷한 패턴이 반복됩니다. Conv Layer와 Pooling Layer가
반복적으로 진행되는 것이죠

174
00:19:45,834 --> 00:19:52,431
VGG16에서 모든 Layer의 수를 세어보면 16개 입니다.

175
00:19:52,431 --> 00:20:00,478
VGG19의 경우 유사한 아키텍쳐이지만 Conv Layer가
조금 더 추가되었습니다.

176
00:20:03,021 --> 00:20:05,605
네트워크이 전체 메모리 사용량을 살펴봅시다.

177
00:20:05,605 --> 00:20:17,196
Forward pass 시 필요한 전체 메모리를 계산한 것입니다.

178
00:20:17,196 --> 00:20:23,125
그리고 각 노드가 4 bytes 의 메모리를 차지하므로
전체 약 100 MB의 메모리가 필요합니다.

179
00:20:23,125 --> 00:20:28,727
100MB가 전체 메모리 사용량이죠. 하지만 이 값은
Forward Pass 만 계산한 것입니다.

180
00:20:28,727 --> 00:20:35,470
Backward Pass를 고려한다면 더 많은 메모리가 필요할 것입니다.
VGG16은 메모리사용량이 많은 편입니다.

181
00:20:35,470 --> 00:20:44,410
전체 메모리가 5GB라면 이미지 하나당 100MB이므로
50장 밖에 처리할 수 없습니다.

182
00:20:47,300 --> 00:20:56,131
그리고 전체 파라미터의 갯수는 138M 개 입니다.
AlexNet의 경우에는 60M 개었죠

183
00:20:56,131 --> 00:20:57,481
질문있나요?

184
00:20:57,481 --> 00:21:00,898
[학생이 질문]

185
00:21:06,204 --> 00:21:09,920
질문은 "네트워크가 더 깊다"는게  "필터의 갯수가 더 많은 것"을
의미하는 것인지 "레이어의 갯수가 더 많은 것"을 의미하는 것인지 입니다.

186
00:21:09,920 --> 00:21:14,087
이 경우에는 레이어의 갯수를 의미합니다.

187
00:21:15,605 --> 00:21:25,216
Depth 라는 용어는 두 가지도 사용할 수 있습니다. 첫 째로는 채널의
Depth입니다. Width, Height, Depth 할때 Depth이죠

188
00:21:26,942 --> 00:21:34,298
반면 일반적으로 "네트워크의 깊이(Depth)"라고 할 때는
네트워크의 전체 레이어의 갯수 를 의미합니다.

189
00:21:34,298 --> 00:21:43,368
"학습 가능한 가중치를 가진 레이어의 갯수" 를 의미합니다.
가령 Conv Layer와 FC Layer 등 말입니다.

190
00:21:43,368 --> 00:21:46,868
[학생이 질문]

191
00:22:00,810 --> 00:22:06,174
질문은 하나의 Conv Layer 내에 여러개의 필터가 존재하는
이유가 무엇인지 입니다.

192
00:22:06,174 --> 00:22:13,043
지난 Conv Net 강의에서 다룬 적이 있습니다. 집에 돌아가서
한번 참고해 보시는 것이 좋을 것 같습니다. 어쩄든 -

193
00:22:13,043 --> 00:22:27,616
3 x 3 Conv 필터가 있다고 해봅시다. 필터는 한번에
3 x 3 x Depth를 보고 하나의 Feature Map을 만들어냅니다.

194
00:22:27,616 --> 00:22:31,954
그리고 입력 전체를 돌면서 하나의 Feature Map을
완성시킵니다.

195
00:22:31,954 --> 00:22:39,646
우리는 여러개의 필터를 사용할 수 있습니다. 가령 96개를 사용할 수
있겠죠. 그리고 각 필터는 하나의 Feature Map을 만듭니다.

196
00:22:39,646 --> 00:22:48,368
그리고 각 필터가 존재하는 이유는 서로 다른 패턴을
인식하기 위해서 라고 할 수 있습니다.

197
00:22:48,368 --> 00:22:56,181
각 필터는 각각의 Feature Map을 만들게 되는 것입니다.

198
00:22:58,761 --> 00:23:00,226
질문있나요?

199
00:23:00,226 --> 00:23:03,643
[학생이 질문]

200
00:23:07,465 --> 00:23:16,733
질문은 "네트워크가 깊어질수록 레이어의 필터 갯수를 늘려야
하는지" 입니다.(Channel Depth를 늘려야 하는지)

201
00:23:17,676 --> 00:23:21,766
여러분이 디자인하기 나름이고 반드시 그럴 필요는 없습니다.

202
00:23:21,766 --> 00:23:24,341
하지만 실제로 사람들이 Depth를 많이 늘리는 경우가 많습니다.

203
00:23:24,341 --> 00:23:30,598
Depth를 늘리는 이유 중 하나는 계산량을 일정하게 유지시키기
위해서 입니다(constant level of compute).

204
00:23:30,598 --> 00:23:37,991
보통 네트워크가 깊어질수록 각 레이어의 입력을
Down sampling하게 됩니다.

205
00:23:39,606 --> 00:23:45,759
Spatial area가 작아질수록 필터의 depth를
조금씩 늘려주게 됩니다.

206
00:23:45,759 --> 00:23:53,367
Width Height가 작아지기 때문에 Depth를 늘려도
부담이 없습니다.

207
00:23:53,367 --> 00:23:54,716
질문있나요?

208
00:23:54,716 --> 00:23:58,133
[학생이 질문]

209
00:23:59,872 --> 00:24:04,653
질문은 네트워크에 SoftMax Loss 대신
SVM Loss를 사용해도 되는지 입니다.

210
00:24:04,653 --> 00:24:09,761
지난 강의에서 다룬 내용입니다만
둘 다 사용할 수 있습니다.

211
00:24:09,761 --> 00:24:17,242
하지만 보통 SoftMax Loss를 일반적으로 사용하는 편입니다.

212
00:24:18,509 --> 00:24:20,023
네 질문있나요?

213
00:24:20,023 --> 00:24:23,523
[학생이 질문]

214
00:24:37,902 --> 00:24:45,398
질문은 "앞서 계산한 메모리 중에 굳이 가지고 있지 않고
버려도 되는 부분이 있는지" 입니다.

215
00:24:45,398 --> 00:24:49,221
예 맞습니다.
일부는 굳이 가지고있지 않아도 됩니다.

216
00:24:49,221 --> 00:25:02,571
하지만 Backword pass시 chain rule을 계산할 때
대부분은 이용됩니다. 따라서 대부분은 반드시 가지고있어야겠죠

217
00:25:04,006 --> 00:25:14,440
파라미터가 존재하는 곳들의 메모리 사용분포를 살펴보면
초기 레이어에서 많은 메모리를 사용하는 것을 알 수 있습니다.

218
00:25:14,440 --> 00:25:24,054
Sparial dimention이 큰 곳들이 메모리를 더 많이 사용합니다.
그리고 마지막 레이어는 많은 파라미터를 사용합니다.

219
00:25:24,054 --> 00:25:28,837
FC-Layer이엄청난 양의 파라미터를 사용합니다.
dense connection이기 때문이죠

220
00:25:28,837 --> 00:25:36,999
그리고 나중에 더 알아보겠지만 최근에 일부 네트워크들은

221
00:25:36,999 --> 00:25:42,345
아얘 FC Layer를 없애버리기도 합니다.
너무 많은 파라미터를 줄이기 위해서죠

222
00:25:42,345 --> 00:25:48,059
그리도 또 한가지 말씀드릴 점은 여기에 각 레이어들을
부르는 명칭이 있을 수 있습니다.

223
00:25:48,059 --> 00:25:56,190
제가 여기에 써 놓은것 처럼 conv3-64는
64개의 필터를 가진 3 x 3 conv 필터입니다.

224
00:25:56,190 --> 00:26:05,190
그리고 다이어그램의 오른쪽을 보시면 각 필터를 묶어놓았습니다.
사람들이 많이 쓰는 방법입니다.

225
00:26:05,190 --> 00:26:11,822
여기 오렌지색 블락을 보시면 첫번째 그룹(part 1)의
conv들은 conv1-1, conv1-2 이렇게 표현합니다.

226
00:26:11,822 --> 00:26:14,655
알고계시면 좋습니다.

227
00:26:16,594 --> 00:26:22,120
VGGNet은 ImageNet 2014 Classification
Challenge에서 2등을 했습니다.

228
00:26:22,120 --> 00:26:24,783
Localization에서는 우승을했죠

229
00:26:24,783 --> 00:26:29,037
학습 과정은 AlexNet과 유사합니다.

230
00:26:29,037 --> 00:26:38,764
다만 Local response normalization은 사용하지 않습니다.
앞서 언급했듯 도움이 크게 안되기 떄문이었죠

231
00:26:38,764 --> 00:26:49,615
VGG16과 VGG19은 아주 유사합니다. 다만 VGG19가
조금 더 깊을 뿐이죠

232
00:26:49,615 --> 00:27:00,366
VGG19가 아주 조금 더 좋습니다. 메모리도 조금 더 쓰죠.  여러분은
두 모델 모두 사용할 수 있겠지만 보통 16을 더 많이 사용합니다.

233
00:27:01,470 --> 00:27:10,110
그리고 AlexNet에서 처럼 모델 성능을 위해서 앙상블 기법을 사용했습니다.

234
00:27:10,110 --> 00:27:20,158
그리고 VGG의 마지막 FC-Layer인 FC7은 이미지넷 1000 class
의 바로 직전에 위치한 레이어입니다.

235
00:27:20,158 --> 00:27:26,463
이  FC7은 4096 사이즈의 레이어인데 아주 좋은
feature represetation을 가지고 있는 것으로 알려져있습니다.

236
00:27:26,463 --> 00:27:35,055
다른 데이터에서도 특징(feature) 추출이 잘되며
다른 Task에서도 일반화 능력이 뛰어난 것으로 알려져있습니다.

237
00:27:35,055 --> 00:27:37,792
FC7은 아주 좋은 feature representation입니다.

238
00:27:37,792 --> 00:27:39,142
예 질문있나요?

239
00:27:39,142 --> 00:27:44,432
[학생이 질문]

240
00:27:45,939 --> 00:27:50,036
질문은 "localization이 무엇인지" 입니다.
(VGG가 localization task에서 우승)

241
00:27:50,036 --> 00:27:57,163
localization은 task입니다. 나중 강의에서 Detection과
Localization을 다룰 예정입니다.

242
00:27:57,163 --> 00:28:03,205
여기에서 더 자세히 말씀드리지는 않겠지만 기본적으로
"이미지에 고양이가 있는지?" 를 분류하는 것 뿐만 아니라

243
00:28:03,205 --> 00:28:09,433
정확히 고양이가 어디에 있는지 네모박스를
그리는 것입니다.

244
00:28:09,433 --> 00:28:16,153
Detection과는 조금 다릅니다. Detection은 이미지 내에
다수의 객체가 존재할 수 있습니다.

245
00:28:16,153 --> 00:28:22,671
localization은 이미지에 객체가 하나만 있다고 가정하고
이미지를 분류하고 추가적으로 네모박스도 쳐야합니다.

246
00:28:25,343 --> 00:28:32,382
지금까지는 VGGNet에 대해서 알아보았습니다. 이제는
GoogLeNet에 대해서 알아보겠습니다.

247
00:28:32,382 --> 00:28:36,603
2014년 Classification Challenge에서 우승한 모델입니다.

248
00:28:37,612 --> 00:28:47,776
저GoogLeNet도 엄청 깊은 네트워크입니다. 22개의 레이어를 가지고있죠.
그런데 GoogLeNet에서 가장 중요한 것은

249
00:28:47,776 --> 00:28:57,866
효율적인 계산에 관한 그들의 특별한 관점이 있다는 것과 높은 계산량을
아주 효율적으로 수행하도록  네트워크를 디자인했다는 점입니다.

250
00:28:57,866 --> 00:29:05,023
GoogLeNet은 Inception module을 사용합니다. 앞으로 더
깊이 배울 내용입니다. 기본적으로 GoogLeNet은

251
00:29:05,023 --> 00:29:08,336
Inception module을 여러개 쌓아서 만듭니다.

252
00:29:08,336 --> 00:29:19,841
GoogLeNet에는 FC-Layer가 없습니다. 파라미터를 줄이기 위해서죠.
전체 파라미터 수가 5M 정도입니다. 60M인 AlexNet보다 적죠

253
00:29:19,841 --> 00:29:24,308
그럼에도 불구하고 훨씬 더 깊습니다.

254
00:29:24,308 --> 00:29:26,975
 ILVRC 14에서 6.7%의 top-5 error로 우승을 거머쥡니다.

255
00:29:31,392 --> 00:29:35,363
그렇다면 inception module이 무엇일까요?
inception module가 만들어지게된 배경을 살펴보면

256
00:29:35,363 --> 00:29:40,023
그들은 "a good local network typology"를
디자인하고 싶었습니다.

257
00:29:40,023 --> 00:29:52,341
그리고 "network within a network" 라는 개념으로
local topology를 구현했고 이를 쌓아올렸습니다.

258
00:29:52,341 --> 00:29:58,387
이 Local Network를 Inception Module이라고 합니다.

259
00:29:58,387 --> 00:30:07,138
Inception Module 내부에는 동일한 입력을 받는 서로 다른
다양한 필터들이 "병렬로" 존재합니다.

260
00:30:07,138 --> 00:30:11,896
이전 레이어의 입력을 받아서 다양한 Conv 연산을 수행하는 것이죠

261
00:30:11,896 --> 00:30:25,647
1x1 / 3x3 / 5x5 conv에 Pooling도 있습니다. 여기에서는
3x3 pooling이죠 각 레이어에서 각각의 출력 값들이 나오는데

262
00:30:25,647 --> 00:30:31,499
그 출력들을 모두 Depth 방향으로 합칩니다(concatenate).

263
00:30:31,499 --> 00:30:38,893
그렇게 합치면 하나의 tensor로 출력이 결정되고
이 하나의 출력을 다음 레이어로 전달하는 것입니다.

264
00:30:41,020 --> 00:30:50,015
지금까지는 다양한 연산을 수행하고 이를 하나로 합쳐준다는
아주 단순한 방식(naive way)을 살펴봤습니다.

265
00:30:50,015 --> 00:30:52,386
그렇다면 이 방법의 문제가 무엇일까요?

266
00:30:52,386 --> 00:30:57,717
문제는 바로 계산 비용에 있습니다.

267
00:30:58,982 --> 00:31:11,156
예제를 자세히 들여다봅시다. 우선 128개의 1x1 필터가 있습니다.
192개의 3x3 필터와 96개의 5x5 필터도 있습니다.

268
00:31:11,156 --> 00:31:19,398
그리고 stride를 조절하여 입/출력 간의
spatial dimention을 유지시켜줍니다.

269
00:31:21,341 --> 00:31:29,231
이 경우에 1 x 1 x 128 conv의 출력은 어떻게 될까요?

270
00:31:35,910 --> 00:31:39,910
네 맞습니다. 28 x 28 x 128이 되겠죠.

271
00:31:40,988 --> 00:31:53,159
1x1 conv의 경우 입력에 맞춰 depth는 256입니다.

272
00:31:53,159 --> 00:32:00,194
그리고 128개의 필터 하나 당 28 x 28 Feature map을
생성하게 될 것입니다.

273
00:32:00,194 --> 00:32:02,361
그렇다면 출력은 28 x 28 x 128 이 되겠죠

274
00:32:05,469 --> 00:32:14,939
이런 방식으로 각 필터의 출력 값을 계산해보면

275
00:32:14,939 --> 00:32:20,379
3 x 3 conv의 경우에 출력이 28 x 28 x 192이 될 것이고

276
00:32:20,379 --> 00:32:24,559
5 x 5 conv의 경우에 96개의 필터이므로
 출력이 28 x 28 x 96 이 될 것입니다.

277
00:32:24,559 --> 00:32:34,712
Pooling Layer는 input에서 depth가 변하지 않습니다.

278
00:32:34,712 --> 00:32:40,192
그리고 Stride를 잘 조절해서 Spatial dimention를
유지하면 입력과 출력의 크기는 같습니다.

279
00:32:41,225 --> 00:32:51,498
그렇다면 모든 출력 값들을 합친(concat) 사이즈를 계산해봅시다.
28 x 28 은 동일하고 depth가 점점 쌓이게 됩니다.

280
00:32:51,498 --> 00:32:59,330
28 x 28 에 모든 depth를 더하면
최종적으로 28 x 28 x 672이 됩니다.

281
00:33:01,113 --> 00:33:10,208
Inception module의 입력은 28 x 28 x 256 이었으나
출력은 28 x 28 x 672 이 된 것입니다.

282
00:33:11,466 --> 00:33:17,254
spatial dimention은 변하지 않았지만
depth가 엄청나게 불어난 것이죠

283
00:33:17,254 --> 00:33:18,188
질문있나요?

284
00:33:18,188 --> 00:33:21,905
[학생이 질문]

285
00:33:21,905 --> 00:33:25,546
질문은 어떻게 출력의 spatial dimention이
28 x 28이 될 수 있는지 입니다.

286
00:33:25,546 --> 00:33:29,307
이 경우는 spatial dimention을 유지하기 위해서
zero padding을 한 경우입니다.

287
00:33:29,307 --> 00:33:33,403
그리고 depth-wise로 합쳤습니다(concat).

288
00:33:34,395 --> 00:33:36,233
질문있나요?

289
00:33:36,233 --> 00:33:39,650
[학생이 질문]

290
00:33:44,824 --> 00:33:47,805
질문은 입력의 depth가 256인 이유가 무엇인지 입니다.

291
00:33:47,805 --> 00:33:53,814
현재 입력은 네트워크의 입력이 아닙니다. 네트워크
중간에 있는 어떤 한 레이어의 입력인 것이죠

292
00:33:53,814 --> 00:34:00,506
256이라는 값은 바로 직전에 있던 inception module의
출력 depth라고 할 수 있습니다.

293
00:34:00,506 --> 00:34:08,438
현재 레이어의 출력이 28 x 28 x 672 이었습니다.
이 값이 다음 레이어로 넘어가는 것입니다 .

294
00:34:08,438 --> 00:34:09,915
질문있나요?

295
00:34:09,916 --> 00:34:13,333
[학생이 질문]

296
00:34:17,039 --> 00:34:23,181
질문은 어떻게 1 x 1 conv의 출력이
28 x 28 x 128이 되는지 입니다.

297
00:34:23,181 --> 00:34:34,058
이 필터는 1 x 1 conv 필터입니다. 이 필터가
입력 28 x 28 x 256 돌아다니면서 conv연산을 수행하겠죠

298
00:34:35,485 --> 00:34:41,956
1 x 1 conv는 입력의 depth인 256 만 가지고
내적을 한다고 보시면 됩니다.

299
00:34:41,956 --> 00:34:46,983
그렇게 되면 필터 하나 당 28 x 28 x 1 의
feature map을 얻게 될 것입니다.

300
00:34:46,983 --> 00:34:58,311
다시 말해 입력의 각 픽셀마다 값이 하나 씩 계산됩니다.
그러면 필터당 28 x 28 x 1이 되겠죠. 그리고 필터가 128개 입니다.

301
00:35:01,050 --> 00:35:04,800
따라서 28 x 28 x 128 이 되는 것입니다.

302
00:35:05,809 --> 00:35:10,403
그리고 이 레이어들의 계산량을 한번 살펴봅시다

303
00:35:10,403 --> 00:35:22,553
첫 번째 예시로 1 x 1 conv를 살펴봅시다. 1 x 1 conv는
각 픽셀마다 1 x 1 x 256 개의 내적연산을 수행합니다.

304
00:35:24,545 --> 00:35:28,358
따라서 픽셀 당 256번의 곱셈 연산이 수행되는 것이죠
(Conv Ops:맨 뒤의 256)

305
00:35:28,358 --> 00:35:37,865
그리고 픽셀이 총 28 x 28 이므로
처음 "28  x 28" 이 여기에 해당합니다.

306
00:35:37,865 --> 00:35:53,859
그리고 이런 연산을 수행하는 필터가 총 256개 있으므로

307
00:35:53,859 --> 00:36:01,221
1 x 1 conv에서의 전체 연산량은
28 x 28 x 128 x 256 입니다.

308
00:36:02,129 --> 00:36:10,349
이런 식으로 3x3/5x5 conv 의 연산량도 계산해 볼 수 있습니다.

309
00:36:10,349 --> 00:36:16,690
따라서 하나의 Inception Module에서의
전체 연산량은 854M 가 됩니다.

310
00:36:17,968 --> 00:36:21,191
[학생이 질문]
각 필터의 depth인 128, 192, 96이 의미가 있는지

311
00:36:22,131 --> 00:36:29,044
이 값들을 제가 임의로 정한 값들입니다.

312
00:36:29,044 --> 00:36:35,594
하지만 실제 Inception Net에서도 비슷한 값이긴 합니다.

313
00:36:35,594 --> 00:36:43,103
GoogLeNet의 각 Inception Module에는 파라미터 값이
다양하기 때문에 그 중 일부를 참고한 값들입니다.

314
00:36:45,089 --> 00:36:49,046
아무튼 이는 연산량이 아주 많습니다.

315
00:36:49,046 --> 00:36:55,507
그리고 Pooling layer 또한 문제를 악화시킵니다. 왜냐하면
입력의 Depth를 그대로 유지하기 때문입니다.

316
00:36:57,062 --> 00:37:03,519
레이어를 거칠때마다 Depth가 점점 늘어만 갑니다.

317
00:37:03,519 --> 00:37:10,513
Pooling의 출력은 이미 입력의 Depth와 동일하고 여기에
다른 레이어의 출력이 계속해서 더해지게 되는 것입니다.

318
00:37:10,513 --> 00:37:18,960
이 경우에는 입력의 Depth는 256이었지만 출력은 672이 됩니다.
그리고 레이어를 거칠수록 점점 더 늘어나게 되는 것이죠

319
00:37:21,920 --> 00:37:25,441
이 문제를 어떻게 해결할 수 있을까요?

320
00:37:25,441 --> 00:37:36,181
GoogLeNet에서 사용한 key insight는 "bottleneck
layer" 를 이용하는 것입니다. Conv 연산을 수행하기에 앞서

321
00:37:36,181 --> 00:37:43,174
입력을 더 낮은 차원으로 보내는 것이죠

322
00:37:45,007 --> 00:37:46,642
낮은 차원으로 보낸다는 것이 어떤 의미일까요?

323
00:37:46,642 --> 00:37:58,080
1x1 conv를 다시 한번 살펴봅시다. 1x1 conv는 각
spatial location에서만 내적을 수행합니다.

324
00:38:00,141 --> 00:38:06,139
그러면서 depth만 줄일 수 있습니다. 입력의 depth를
더 낮은 차원으로 projection 하는 것입니다.

325
00:38:06,139 --> 00:38:10,515
Input feature map들 간의
선형결합(linear combination) 이라고 할 수 있습니다.

326
00:38:12,880 --> 00:38:18,199
주요 아이디어는 바로 입력의 depth를 줄이는 것입니다.

327
00:38:18,199 --> 00:38:29,085
각 레이어의 계산량은 1x1 conv를 통해 줄어듭니다.

328
00:38:29,085 --> 00:38:36,162
3x3/5x5 conv 이전에 1x2이 추가됩니다.

329
00:38:36,162 --> 00:38:42,315
그리고 polling layer 후에도 1x1 conv가 추가되죠

330
00:38:43,284 --> 00:38:47,609
1x1 conv가 bottleneck layers의 역할로 추가되는 것입니다.

331
00:38:48,562 --> 00:38:52,736
그렇다면 다시한번 계산량을 세어봅시다

332
00:38:52,736 --> 00:38:58,589
우선 이전과 입력은 동일합니다.
28 x 28 x 256 이죠

333
00:38:58,589 --> 00:39:12,856
1 x 1 conv가 depth 의 차원을 줄여줍니다. 3 x 3 conv
앞쪽의 1 x 1 conv의 출력은 28 x 28 x 64 이죠

334
00:39:14,184 --> 00:39:25,154
앞선 예시 처럼 28 x 28 x 256 을 입력인 대신에
28 x 28 x 64인 것입니다. .

335
00:39:25,154 --> 00:39:31,454
conv의 입력이 더 줄어든 셈입니다.

336
00:39:31,454 --> 00:39:40,499
3x3 뿐만 아니라 5x5 conv와 pooling layer에서도
동일한 작용을 하게됩니다.

337
00:39:41,562 --> 00:39:51,214
그렇다면 이제 전체 계산량을 계산해봅시다.
현재는 1x1 conv가 추가된 상태입니다.

338
00:39:51,214 --> 00:40:02,499
계산해보면 전체 358M번의 연산을 수행합니다.
기존의 854M보다 훨씬 줄어든 셈입니다.

339
00:40:02,499 --> 00:40:10,438
이를 통해 배울 수 있는 점은 1x1 conv를 이용하면
계산량을 조절할 수 있다는 사실입니다.

340
00:40:10,438 --> 00:40:12,118
질문있나요?

341
00:40:12,118 --> 00:40:15,535
[학생이 질문]

342
00:40:23,525 --> 00:40:30,979
질문은 "1 x 1 Conv를 수행하면 일부 정보손실이 
발생하지 않는지" 입니다.

343
00:40:30,979 --> 00:40:35,112
정보 손실이 발생할 수는 있습니다.

344
00:40:35,112 --> 00:40:46,013
그러나 동시에 redundancy가 있는 input features를 선형결합
한다고 볼 수 있습니다. 1x1 conv로 선형결합을 하고

345
00:40:47,623 --> 00:40:59,422
non-linearity를 추가하면(ReLU같은) 네트워크가 더 깊어지는
효과도 있습니다. 이것이 엄밀한 해석은 아닙니다. 하지만

346
00:40:59,422 --> 00:41:07,314
일반적으로 1x1 conv를 추가하면 여러모로 도움이 되고 더 잘 동작합니다.

347
00:41:07,314 --> 00:41:15,627
Inception Module에서 1 x 1 convs를 사용하는
기본적인 이유는 계산복잡도를 조절하기 위해서입니다.

348
00:41:15,627 --> 00:41:20,450
GooleNet은 Inception module들을 쌓아올려 구성합니다.

349
00:41:20,450 --> 00:41:22,827
가운데 그림이 완전한 모습의 inception architecture 입니다.

350
00:41:22,827 --> 00:41:32,773
GoogLeNet을 더 깊게 알아봅시다. 우선 슬라이드에
안맞아서 그림을 돌려놨습니다.

351
00:41:32,773 --> 00:41:41,867
GoogLeNet의 앞단(stem) 에는 일반적인 네트워크구조 입니다.
초기 6개의 레이어는 지금까지 봤던 일반적인 레이어들이죠

352
00:41:43,256 --> 00:41:48,570
처음에는 conv pool을 몇 번 반복합니다.

353
00:41:48,570 --> 00:41:54,911
이 후에는 Inception module에 쌓이는데 
모두 조금씩 다릅니다.

354
00:41:54,911 --> 00:41:58,433
그리고 마지막에는 classifier 결과를 출력합니다.

355
00:41:58,433 --> 00:42:08,982
GoogLeNet에서는 계산량이 많은 FC-layer를 대부분 걷어냈고
파라미터를가 줄어들어도 모델이 잘 동작함을 확인했습니다.

356
00:42:08,982 --> 00:42:17,098
그리고 여기 보시면 추가적인 줄기가 뻗어있는데
이들은 보조분류기(auxiliary classifier) 입니다.

357
00:42:18,866 --> 00:42:23,273
그리고 이것들은 또한 당신이 알고있는
단지 작은 미니 네트워크들입니다.

358
00:42:23,273 --> 00:42:29,217
Average pooling과 1x1 conv가 있고 
FC-layey도 몇개 붙습니다.

359
00:42:29,217 --> 00:42:35,702
그리고 SoftMax로 1000개의 ImageNet class를 구분합니다.

360
00:42:35,702 --> 00:42:41,350
그리고 실제로 이 곳에서도
ImageNet trainset loss를 계산합니다.

361
00:42:41,350 --> 00:42:51,752
네트워크의 끝에서 뿐만 아니라 이 두 곳에서도 Loss를
계산하는 이유는 네트워크가 깊기 때문입니다.

362
00:42:51,752 --> 00:43:02,140
보조분류기를 중간 레이어에 달아주면
추가적엔 그레디언트를 얻을 수 있고

363
00:43:02,140 --> 00:43:13,484
따라서 중간 레이어의 학습을 도울 수 있습니다.

364
00:43:13,484 --> 00:43:20,711
전체 아키텍쳐의 모습입니다. 
가중치를 가진 레이어는 총 22개입니다.

365
00:43:20,711 --> 00:43:29,474
각 Inception Module은 1x1/3x3/5x5 conv layer를
병렬적으로 가지고있습니다.

366
00:43:29,474 --> 00:43:44,128
GoogLeNet은 아주 신중하게 디자인된 모델입니다. 모델 디자인의
일부는 앞서 말씀드린 직관들에서 비롯된 것들이고,

367
00:43:44,128 --> 00:43:55,511
일부는 Google의 거대한 클러스터를 이용한 cross validation을
수행한 결과 나온 최적의 디자인이기도합니다.

368
00:43:55,511 --> 00:43:57,105
질문있나요?

369
00:43:57,105 --> 00:44:00,522
[학생이 질문]

370
00:44:24,442 --> 00:44:32,457
질문은 "보조분류기에서 나온 결과를 최종 분류결과에
이용할 수 있는지" 입니다.

371
00:44:32,457 --> 00:44:39,164
GoogLeNet 학습 시, 각 보조분류기의 Loss를 모두 합친
평균을 계산합니다. 아마도 도움일 될 것입니다.

372
00:44:39,164 --> 00:44:49,272
최종 아키텍쳐에서 보조분류기의 결과를 사용하는지 정확히 기억이 
나지 않습니다만 충분히 가능성 있습니다. 한번 확인해 보시기 바랍니다.

373
00:44:49,272 --> 00:44:52,689
[학생이 질문]

374
00:44:58,352 --> 00:45:10,219
질문은 "bottleneck layer를 구성할 때 1x1 conv 말고
다른 방법으로 차원을 축소시켜도 되는지" 입니다.

375
00:45:10,219 --> 00:45:17,138
여기에서 1x1 conv를 쓴 이유는 차원 축소의 효과도 있고 
다른 레이어들처럼 conv layer이기 때문입니다.

376
00:45:17,138 --> 00:45:26,180
차원 축소 과정에서 이전의 feature map와 연관이 있는지 학습하려면 
전체 네트워크를 Backprop으로 학습시킬 수 있어야 합니다.

377
00:45:28,601 --> 00:45:30,730
질문있나요?

378
00:45:30,730 --> 00:45:34,147
[학생이 질문]

379
00:45:35,807 --> 00:45:42,549
질문은 "각 레이어가 가중치를 공유하는지 아닌지" 입니다.

380
00:45:42,549 --> 00:45:45,542
네 모든 레이어를 가중치를 공유하지 않습니다.

381
00:45:45,542 --> 00:45:46,690
질문있나요?

382
00:45:46,690 --> 00:45:50,107
[학생이 질문]

383
00:45:56,784 --> 00:46:00,143
질문은 "왜 앞선 레이어에서 그레디언트를 
전달해야만 하는지" 입니다.

384
00:46:00,143 --> 00:46:07,785
네트워크의 맨 마지막에서부터 Chain rule을 이용해서
그레디언트가 전달된다고 생각해봅시다.

385
00:46:09,599 --> 00:46:21,178
네트워크가 엄청 깊은 경우에서는 그레디언트 신호가
점점 작아지게 되고 결국에는 0에 가깝게 될 수 있습니다.

386
00:46:21,178 --> 00:46:28,377
그렇기 때문에 보조 분류기를 이용해서 추가적인
그레디언트 신호를 흘려줍니다.

387
00:46:28,377 --> 00:46:32,667
[학생이 질문]

388
00:46:32,667 --> 00:46:35,853
질문은 "backprob을 각 보조분류기에서 별도로 여러번 수행하는지" 
입니다.

389
00:46:35,853 --> 00:46:41,446
아닙니다. backprob은 한번만 합니다.
어떤 식으로 생각하시면 되냐면

390
00:46:41,446 --> 00:46:48,075
computational graph 상에 서로 다른 출력이 있는 것인데

391
00:46:48,075 --> 00:46:54,004
각 출력에서의 gradient를 모두 계산한 다음
한번에 Backprop을 합니다.

392
00:46:54,004 --> 00:46:58,970
마치 computational graph 상에 다 함께 있는 것 처럼
생각하고 계산한다고 보시면 됩니다.

393
00:46:58,970 --> 00:47:05,423
진도가 많이 남았으므로 질문은 수업종료 후 
해주시기 바랍니다.

394
00:47:07,353 --> 00:47:10,520
GoogLeNet에는 기본적으로 22개의 레이어가 있습니다.

395
00:47:11,441 --> 00:47:15,983
아주 효율적인 Inception module이 있고 
FC Layer를 들어냈습니다

396
00:47:15,983 --> 00:47:22,026
AlexNet보다 12배 작은 파라미터를 가지고있고 
GoogLeNet은 ILSVRC 2014 clssification의 우승자이죠

397
00:47:25,228 --> 00:47:30,869
자 그럼 2015년도의 우승자를 살펴보겠습니다. 
ResNet이 바로 그 주인공입니다.

398
00:47:30,869 --> 00:47:38,339
ResNet은 혁명적으로 네트워크의 깊이가 깊어진 모델입니다. 
2014년부터 네트워크가 깊어지긴 했지만

399
00:47:38,339 --> 00:47:45,616
ResNet 아키텍쳐는 152 레이어로 엄청나게 더 깊어졌죠

400
00:47:45,616 --> 00:47:48,846
지금부터는 ResNet에 대해서 조금 더 알아보겠습니다.

401
00:47:48,846 --> 00:47:54,286
ResNet 아키텍쳐는 엄청나게 깊은 네트워크입니다. 
기존의 어떤 네트워크보다도 훨씬 더 깊습니다

402
00:47:54,286 --> 00:48:00,479
ResNet는 residual connections 라는 방법을 사용합니다. 
지금부터 논의해볼 주제입니다.

403
00:48:00,479 --> 00:48:04,158
ImageNet데이터를 분류하기 위해 ResNet은
152개의 레이어를 가지고 있습니다.

404
00:48:04,158 --> 00:48:07,969
그리고 ILSVRC'15에서 3.57% top5 error를 기록했습니다.

405
00:48:07,969 --> 00:48:18,114
그리고 아주 놀랍게도 이들은 ResNet 하나로 ImageNet과 COCO
classification/detection 대회를 모조리 휩쓸었습니다.

406
00:48:18,114 --> 00:48:23,546
그리고 다른 참가자들보다 월등히 우수했습니다.

407
00:48:25,055 --> 00:48:32,538
자 그럼 ResNet과 residual connection의 
기본 Motivation에 대해서 알아봅시다

408
00:48:32,538 --> 00:48:41,939
그들이 처음 시작한 질문은 바로 일반 CNN을
깊고 더 깊게 쌓게되면 어떤 일이 발생할지였습니다.

409
00:48:41,939 --> 00:48:53,874
가령 VGG에 conv pool 레이어를 깊게만 쌓는다고
과연 성능이 더 좋아지는 것일까요?

410
00:48:55,601 --> 00:48:58,421
대답은 NO 입니다.

411
00:48:58,421 --> 00:49:06,599
네트워크가 깊어지면 어떤 일이 발생하는지 살펴보겠습니다.
20레이어/56레이어의 네트워크를 한번 비교해보죠

412
00:49:09,498 --> 00:49:16,817
둘다 평범한 CNN입니다. 오른쪽의 test error를 보시면 
56 레이어가 20 레이어보다 안좋습니다.

413
00:49:16,817 --> 00:49:19,771
네트워크가 깊다고 다 좋은 것은 아니구나를 알수있죠

414
00:49:19,771 --> 00:49:29,680
하지만 Training Error가 조금 이상합니다.  다시한번
20/56 레이어를 한번 비교해보겠습니다

415
00:49:29,680 --> 00:49:40,271
여러분에게 56 레이어 네트워크가 있다면 당연히 엄청나게
많은 파라미터가 있으니 Orverfit하겠구나 를 예상하실 것입니다.

416
00:49:41,294 --> 00:49:48,985
그리고 overfit이 발생한다면 test error는 높더라도 
training error는 아주 낮아야 정상일 것입니다.

417
00:49:48,985 --> 00:49:55,511
그런데 56 레이어 네트워크의 traing error을 보자하니 
20레이어보다 안좋습니다.

418
00:49:56,833 --> 00:50:01,545
따라서 더 깊은 모델의 test 성능이 안좋은 원인은
over-fitting이 아니라는 것을 알 수 있습니다.

419
00:50:03,462 --> 00:50:10,253
ResNet 저자들이 내린 가설은 더 깊은 모델을 학습 시 
optimization에 문제가 생긴다는 것입니다.

420
00:50:10,253 --> 00:50:13,361
더 깊은 모델은 최적화하기가 더 어렵습니다.

421
00:50:13,361 --> 00:50:15,611
더 얕은 네트워크보다

422
00:50:16,835 --> 00:50:18,507
그리고 그 추론은 잘,

423
00:50:18,507 --> 00:50:20,679
더 깊은 모델은 적어도 수행 할 수 있어야합니다.

424
00:50:20,679 --> 00:50:23,263
더 얕은 모델 일 수 있습니다.

425
00:50:23,263 --> 00:50:25,428
당신은 건설을 통해 실제로 해결책을 가질 수 있습니다.

426
00:50:25,428 --> 00:50:27,835
여기서 학습 한 레이어를 가져갈 수 있습니다.

427
00:50:27,835 --> 00:50:29,676
너의 얕은 모델에서

428
00:50:29,676 --> 00:50:32,330
이들을 복사 한 다음 나머지 추가로 복사하십시오.

429
00:50:32,330 --> 00:50:35,192
더 깊은 계층에서는 ID 매핑을 추가하기 만하면됩니다.

430
00:50:35,192 --> 00:50:38,092
그래서 이것은 단지 잘 작동해야합니다.

431
00:50:36,803 --> 00:50:43,383


432
00:50:38,092 --> 00:50:39,533
더 얕은 층으로.

433
00:50:39,533 --> 00:50:42,277
그리고 제대로 배울 수 없었던 당신의 모델,

434
00:50:42,277 --> 00:50:46,295
최소한 이것을 배울 수 있어야합니다.

435
00:50:43,383 --> 00:50:50,723


436
00:50:46,295 --> 00:50:49,878
그리고 이것에 의해 그렇게 동기 부여 된 해결책은

437
00:50:51,608 --> 00:50:56,184
어떻게하면 우리 아키텍처를 어떻게
더 쉽게 만들 수 있을까요?

438
00:50:55,123 --> 00:50:59,169


439
00:50:56,184 --> 00:50:59,086
우리의 모델은 이러한 종류의 솔루션을 배우고,

440
00:50:59,086 --> 00:51:00,594
또는 적어도 이것과 같은 무엇인가?

441
00:51:00,594 --> 00:51:05,493
그래서 그들의 생각은 스태킹 대신에 잘됩니다.

442
00:51:02,702 --> 00:51:07,224


443
00:51:05,493 --> 00:51:07,865
이 모든 층들은 서로 겹쳐지며

444
00:51:07,865 --> 00:51:11,794
모든 계층에서 기본 매핑을 시도하고 배우십시오.

445
00:51:09,480 --> 00:51:15,861


446
00:51:11,794 --> 00:51:16,754
대신에 원하는 블록을 가질 수 있습니다.

447
00:51:16,754 --> 00:51:19,662
우리가 잔여 매핑을 시도하고 적합하게 만들 때,

448
00:51:19,662 --> 00:51:21,708
직접 매핑 대신.

449
00:51:21,708 --> 00:51:24,053
그래서이 모양이이 오른쪽에 있습니다.

450
00:51:24,053 --> 00:51:28,220
이 블록에 대한 입력은 들어오는 입력 일뿐입니다.

451
00:51:25,587 --> 00:51:29,809


452
00:51:29,818 --> 00:51:33,985
그리고 여기서 우리는 우리 편을 여기에서 사용하려고합니다.

453
00:51:35,960 --> 00:51:43,283


454
00:51:37,338 --> 00:51:40,241
우리는 시도하고 맞추기 위해 레이어를 사용하려고합니다.

455
00:51:40,241 --> 00:51:43,408
X의 H에 대한 우리의 욕망의 일부 잔여 물,

456
00:51:43,283 --> 00:51:49,956


457
00:51:44,332 --> 00:51:48,499
X의 원하는 함수 H 대신에 X를 뺀 값.

458
00:51:49,450 --> 00:51:53,004
그리고 기본적으로이 블록의 끝에서 우리는

459
00:51:49,956 --> 00:52:00,647


460
00:51:53,004 --> 00:51:55,827
이 오른쪽의 단계 연결,이 루프,

461
00:51:55,827 --> 00:51:58,977
우리가 입력을 받아들이면 그냥 통과시킵니다.

462
00:51:58,977 --> 00:52:02,536
정체성으로, 그래서 우리는 무게 레이어가 없다면

463
00:52:00,647 --> 00:52:07,993


464
00:52:02,536 --> 00:52:04,292
그 사이에 정체성이 될 것입니다.

465
00:52:04,292 --> 00:52:07,241
출력과 같은 것이지만, 이제는

466
00:52:07,241 --> 00:52:10,145
우리의 추가 무게 레이어는 델타를 배우고,

467
00:52:07,993 --> 00:52:17,638


468
00:52:10,145 --> 00:52:12,562
우리 X의 일부 잔여 물.

469
00:52:14,067 --> 00:52:15,794
그리고 지금은 이것의 출력이 될 것입니다.

470
00:52:15,794 --> 00:52:19,127
우리의 원래 R X와 약간의 잔여 물

471
00:52:17,638 --> 00:52:23,262


472
00:52:20,085 --> 00:52:21,193
우리는 그것을 호출하려고합니다.

473
00:52:21,193 --> 00:52:24,502
기본적으로 델타이므로 아이디어는

474
00:52:23,262 --> 00:52:28,970


475
00:52:24,502 --> 00:52:28,428
이제는 출력이 쉬워야합니다. 예를 들어,

476
00:52:28,428 --> 00:52:31,428
동일성이 이상적인 경우,

477
00:52:28,970 --> 00:52:35,832


478
00:52:32,510 --> 00:52:36,268
X의 F의 모든 가중치를 스쿼시하기 만하면됩니다.

479
00:52:36,268 --> 00:52:39,249
우리 체중 레이어에서 모든 제로로 설정

480
00:52:36,967 --> 00:52:41,134


481
00:52:39,249 --> 00:52:41,498
예를 들어, 우리는 정체성을 얻고 자합니다.

482
00:52:41,498 --> 00:52:43,509
출력으로, 우리는 뭔가를 얻을 수 있습니다,

483
00:52:43,509 --> 00:52:47,021
예를 들어,이 솔루션에 가까운 건설

484
00:52:47,021 --> 00:52:48,578
우리가 전에 가지고 있었던.

485
00:52:48,578 --> 00:52:50,504
맞아, 이것은 단지 네트워크 아키텍처 일뿐입니다.

486
00:52:50,504 --> 00:52:52,638
알았어, 이걸 시도해 보자.

487
00:52:52,638 --> 00:52:56,764
우리의 체중 레이어가 어떻게 잔류하는지
배우고, 무언가가 되십시오.

488
00:52:53,100 --> 00:53:00,435


489
00:52:56,764 --> 00:53:00,962
닫기를하면 X에 가까울 가능성이 커집니다.

490
00:53:00,962 --> 00:53:03,341
그것은 정확히 X를 수정하는 것입니다.

491
00:53:03,341 --> 00:53:05,388
무엇이 있어야하는지에 대한이 전체 맵핑.

492
00:53:05,388 --> 00:53:08,249
좋아, 이것에 대해 질문이 있으십니까?

493
00:53:06,317 --> 00:53:09,355


494
00:53:08,249 --> 00:53:09,189
[학생은 마이크에서 소리 쳤다]

495
00:53:09,189 --> 00:53:12,689
- 문제는 같은 차원입니까?

496
00:53:13,770 --> 00:53:17,603
그렇습니다.이 두 경로는 같은 차원입니다.

497
00:53:15,170 --> 00:53:21,862


498
00:53:18,752 --> 00:53:21,573
일반적으로 그것은 동일한 차원이거나,

499
00:53:21,573 --> 00:53:23,856
또는 그들이 실제로하는 것은 그들이

500
00:53:21,862 --> 00:53:26,975


501
00:53:23,856 --> 00:53:26,761
투영 및 바로 가기 및 다른 방법이 있습니다.

502
00:53:26,761 --> 00:53:30,980
같은 치수로 작동하도록 패딩 처리

503
00:53:26,975 --> 00:53:32,224


504
00:53:30,980 --> 00:53:32,288
깊이 현명한.

505
00:53:32,288 --> 00:53:33,395
예

506
00:53:33,395 --> 00:53:34,953
- [학생] 잔차라는 단어를 사용하면

507
00:53:34,953 --> 00:53:39,120
너는 마이크에 대해 이야기하고 있었다.

508
00:53:42,899 --> 00:53:46,827


509
00:53:45,857 --> 00:53:48,597
- 그래서 질문은 정확히 우리가 의미하는 바입니다.

510
00:53:46,827 --> 00:53:50,008


511
00:53:48,597 --> 00:53:52,158
이 변환 결과의 잔차

512
00:53:52,158 --> 00:53:53,638
잔여 물입니까?

513
00:53:53,638 --> 00:53:57,609
따라서 우리는 X의 F만큼이나 우리의
결과를 여기에서 생각할 수 있습니다.

514
00:53:54,468 --> 00:54:02,650


515
00:53:57,609 --> 00:54:01,899
더하기 X, 여기서 X의 F는 변환의 결과입니다.

516
00:54:01,899 --> 00:54:05,536
그리고 나서 X는 우리의 입력입니다.

517
00:54:02,650 --> 00:54:11,934


518
00:54:05,536 --> 00:54:06,650
정체성에 의해.

519
00:54:06,650 --> 00:54:09,795
그래서 평범한 레이어를 사용하고 싶습니다.

520
00:54:09,795 --> 00:54:12,044
우리가하려는 것은 뭔가를 배우는 것입니다.

521
00:54:12,044 --> 00:54:15,477
X의 H와 비슷하지만 이전에
보았던 것은 어렵다는 것입니다.

522
00:54:13,197 --> 00:54:16,436


523
00:54:15,477 --> 00:54:17,198
X의 H를 배우기.

524
00:54:17,198 --> 00:54:20,671
우리가 매우 깊은 네트워크를 갖게되면 X의 좋은 H입니다.

525
00:54:18,162 --> 00:54:21,577


526
00:54:20,671 --> 00:54:22,961
그래서 여기 아이디어는 시도하고
그것을 무너 뜨리는 것입니다.

527
00:54:21,577 --> 00:54:27,647


528
00:54:22,961 --> 00:54:26,491
X의 H가 X의 F와 같지 않고 대신에,

529
00:54:26,491 --> 00:54:29,438
그리고 X의 F를 배우려고하자.

530
00:54:27,647 --> 00:54:33,649


531
00:54:29,438 --> 00:54:32,838
그래서 직접적으로이 H의 X를 배우는 대신

532
00:54:32,838 --> 00:54:35,531
우리는 단지 우리가 추가해야 할
것이 무엇인지 배우고 싶습니다.

533
00:54:35,531 --> 00:54:39,741
또는 다음 단계로 넘어갈 때 입력 값을 뺍니다.

534
00:54:39,741 --> 00:54:43,350
따라서 여러분은 이것을이 입력을
수정하는 것으로 생각할 수 있습니다.

535
00:54:43,350 --> 00:54:44,659
의미에서 그 자리에.

536
00:54:44,659 --> 00:54:45,889
우리는 -

537
00:54:45,889 --> 00:54:49,121
[학생이 중얼 거려서 마이크에 중얼 거림]

538
00:54:46,651 --> 00:54:53,263


539
00:54:49,121 --> 00:54:50,844
- 문제는 우리가 잔차라는 말을 할 때입니다.

540
00:54:50,844 --> 00:54:52,438
우리는 F of X에 대해 이야기하고 있습니까?

541
00:54:52,438 --> 00:54:53,547
네.

542
00:54:53,547 --> 00:54:55,712
따라서 X의 F는 우리가 잔차라고 부르는 것입니다.

543
00:54:55,712 --> 00:54:58,129
그리고 그 의미가 있습니다.

544
00:54:58,873 --> 00:55:04,441


545
00:55:01,477 --> 00:55:03,941
네, 또 다른 질문입니다.

546
00:55:03,941 --> 00:55:07,441
[학생이 마이크에서 엉망으로]

547
00:55:04,441 --> 00:55:10,718


548
00:55:11,319 --> 00:55:13,524
- 문제는 실제적으로 우리가 합계하는 것입니다.

549
00:55:13,524 --> 00:55:16,670
X와 X의 F를 함께 사용하거나, 가중치를 배웁니다.

550
00:55:16,670 --> 00:55:20,145
조합을 선택하면 직접 합계를 계산할 수 있습니다.

551
00:55:17,327 --> 00:55:22,543


552
00:55:20,145 --> 00:55:22,789
왜냐하면 당신이 직접 합계를 할 때,

553
00:55:22,789 --> 00:55:26,059
이것은 내가 무엇인지 알게하는 아이디어입니다.

554
00:55:26,059 --> 00:55:28,809
나는 X에 더하거나 뺄 필요가있다.

555
00:55:26,322 --> 00:55:31,342


556
00:55:30,652 --> 00:55:34,463
이 모든 것이 주된 직관입니까?

557
00:55:31,342 --> 00:55:36,248


558
00:55:34,463 --> 00:55:35,361
문제.

559
00:55:35,361 --> 00:55:38,778
[학생은 마이크에서 소리 쳤다]

560
00:55:36,248 --> 00:55:39,820


561
00:55:40,721 --> 00:55:43,055
- 그래, 왜 그게 문제인지 분명하지 않다.

562
00:55:43,055 --> 00:55:45,013
잔류 물을 배우는 것이 더 쉬워야한다고

563
00:55:45,013 --> 00:55:47,099
직접 매핑을 배우는 것보다?

564
00:55:47,099 --> 00:55:49,799
그리고 이것은 단지 그들의 가설입니다.

565
00:55:47,493 --> 00:55:53,623


566
00:55:49,799 --> 00:55:54,371
가설은 우리가 잔차를 배우면

567
00:55:54,371 --> 00:55:58,747
X에 대한 델타가 무엇인지 배워야합니다.

568
00:55:58,747 --> 00:56:02,137
우리의 가설이 일반적으로

569
00:55:59,844 --> 00:56:04,404


570
00:56:02,137 --> 00:56:06,359
심지어 우리의 솔루션과 같은 건설,

571
00:56:04,404 --> 00:56:08,287


572
00:56:06,359 --> 00:56:09,880
우리는 몇몇 얕은 층들을 가지고있었습니다.

573
00:56:08,287 --> 00:56:13,436


574
00:56:09,880 --> 00:56:12,908
우리는이 모든 신원 매핑을 배웠습니다.

575
00:56:12,908 --> 00:56:16,101
상단에 이것이 있어야했던 해결책이었습니다.

576
00:56:13,436 --> 00:56:22,539


577
00:56:16,101 --> 00:56:19,409
좋은, 그리고 그래서 그것은 아마
많은 이들 층을 암시합니다.

578
00:56:19,409 --> 00:56:22,347
실제로 정체성에 가까운 것,

579
00:56:22,347 --> 00:56:23,985
좋은 층이 될 것입니다.

580
00:56:23,985 --> 00:56:26,282
그래서 그것 때문에, 우리는 이것을 공식화합니다.

581
00:56:26,282 --> 00:56:29,358
정체성을 배울 수있는 것으로서

582
00:56:29,358 --> 00:56:30,954
약간의 델타 플러스.

583
00:56:30,954 --> 00:56:34,315
정말로 정체성이 가장 좋은 경우

584
00:56:34,315 --> 00:56:36,763
F는 X 스쿼시 변환을 0으로 만듭니다.

585
00:56:36,763 --> 00:56:38,852
상대적으로,

586
00:56:38,852 --> 00:56:40,363
배우기가 더 쉬워 보일 수도 있지만,

587
00:56:40,363 --> 00:56:42,699
또한 우리는 가까운 것을 얻을 수 있습니다.

588
00:56:42,699 --> 00:56:44,784
신원 매핑.

589
00:56:43,626 --> 00:56:49,006


590
00:56:44,784 --> 00:56:47,117
그리고 다시 이것은 반드시 필연적 인 것이 아닙니다.

591
00:56:47,117 --> 00:56:50,966
입증 된 것 또는 무엇 이건
그것은 직관과 가설 일뿐입니다.

592
00:56:49,006 --> 00:56:54,404


593
00:56:50,966 --> 00:56:54,038
그리고 나서 우리는 나중에 사람들이

594
00:56:54,038 --> 00:56:55,723
실제로 이것에 도전하고 오 말할 수 있습니다.

595
00:56:54,404 --> 00:56:58,244


596
00:56:55,723 --> 00:56:58,708
그것은 실제로 필요한 잔차가 아니며,

597
00:56:58,708 --> 00:57:02,259
그러나 적어도 이것은이 논문의 가설이며,

598
00:57:02,259 --> 00:57:05,295
실제로이 모델을 사용하여,

599
00:57:03,211 --> 00:57:06,463


600
00:57:05,295 --> 00:57:07,507
그것은 아주 잘 할 수있었습니다.

601
00:57:06,463 --> 00:57:11,939


602
00:57:07,507 --> 00:57:08,810
문제.

603
00:57:08,810 --> 00:57:12,227
[학생은 마이크에서 소리 쳤다]

604
00:57:11,939 --> 00:57:16,207


605
00:57:16,207 --> 00:57:22,190


606
00:57:22,190 --> 00:57:27,961


607
00:57:27,961 --> 00:57:30,305


608
00:57:30,305 --> 00:57:39,000


609
00:57:39,000 --> 00:57:43,904


610
00:57:41,813 --> 00:57:44,584
- 그렇습니다. 그래서 사람들은
다른 방법을 시도해 보았습니다.

611
00:57:44,584 --> 00:57:49,128
이전 레이어의 입력 및 예

612
00:57:49,128 --> 00:57:51,746
그래서 이것은 기본적으로 매우 활발한 연구 분야입니다.

613
00:57:51,746 --> 00:57:54,208
어떻게 우리가이 모든 연결을 공식화하는지,

614
00:57:54,208 --> 00:57:56,747
그리고이 모든 구조에서 무엇이 연결되어 있는지

615
00:57:56,747 --> 00:57:59,364
그래서 우리는 다른 네트워크의 몇
가지 예를 보게 될 것입니다.

616
00:57:59,364 --> 00:58:03,695
잠시 후에 아키텍처가 활성화되지만
이는 활성화 된 영역입니다.

617
00:58:00,233 --> 00:58:06,241


618
00:58:03,695 --> 00:58:04,695
연구.

619
00:58:05,658 --> 00:58:09,435
좋습니다. 그래서 우리는 기본적으로
모든 잔차 블록을 가지고 있습니다.

620
00:58:06,241 --> 00:58:10,862


621
00:58:09,435 --> 00:58:12,093
서로 위에 쌓인

622
00:58:10,862 --> 00:58:16,447


623
00:58:12,093 --> 00:58:14,788
우리는 완전한 상주 아키텍처를 볼 수 있습니다.

624
00:58:14,788 --> 00:58:17,940
이 나머지 블록들 각각은 두 개씩 세 개씩 있습니다.

625
00:58:16,447 --> 00:58:25,656


626
00:58:17,940 --> 00:58:22,475
레이어를이 블록의 일부로 사용하면 작업이 완료됩니다.

627
00:58:22,475 --> 00:58:25,669
이것이 단지 좋은 구성이라고 말하는 것입니다.

628
00:58:25,669 --> 00:58:27,299
잘 작동합니다.

629
00:58:27,299 --> 00:58:29,828
우리는이 모든 블록들을 매우 깊이 쌓아 놓습니다.

630
00:58:29,828 --> 00:58:33,351
이 아주 깊은 건축물과 같은 또 다른 것

631
00:58:30,784 --> 00:58:35,007


632
00:58:33,351 --> 00:58:37,101
그것은 기본적으로 최대 150
개의 레이어를 가능하게합니다.

633
00:58:35,007 --> 00:58:37,319


634
00:58:37,319 --> 00:58:41,423


635
00:58:41,421 --> 00:58:45,171
이것의 깊이, 그리고 우리가하는
일은 우리가 쌓는 것입니다.

636
00:58:46,582 --> 00:58:48,915
이 모든 것들과 주기적으로 우리는 또한 두 배

637
00:58:48,915 --> 00:58:51,232
필터 수 및 공간적으로 샘플 다운

638
00:58:51,232 --> 00:58:53,982
우리가 그렇게 할 때 보폭 2를 사용합니다.

639
00:58:53,866 --> 00:59:01,007


640
00:58:55,856 --> 00:58:58,509
그리고 나서 우리는이 추가적인 [벙어리]

641
00:58:58,509 --> 00:59:02,025
우리 네트워크의 맨 처음에

642
00:59:01,007 --> 00:59:10,356


643
00:59:02,025 --> 00:59:02,858
그리고 결국 우리는 또한 듣습니다.

644
00:59:02,858 --> 00:59:03,867
완전히 연결된 레이어가 없다.

645
00:59:03,867 --> 00:59:06,034
전역 평균 풀링 레이어가 있습니다.

646
00:59:06,034 --> 00:59:08,641
그것은 공간적으로 모든 것을 평균 할 것입니다.

647
00:59:08,641 --> 00:59:12,808
마지막 1000 방법 분류에 입력하십시오.

648
00:59:10,356 --> 00:59:18,063


649
00:59:14,694 --> 00:59:16,991
이것이 전체 ResNet 아키텍처입니다.

650
00:59:16,991 --> 00:59:19,523
그리고 그것은 매우 단순하고 우아합니다.

651
00:59:18,063 --> 00:59:26,463


652
00:59:19,523 --> 00:59:21,935
이 모든 ResNet 블록은 서로의 위에 있습니다.

653
00:59:21,935 --> 00:59:26,222
그들은 총 깊이가 34, 50, 100,

654
00:59:26,222 --> 00:59:29,389
ImageNet에 대해 최대 152 개를 시도했습니다.

655
00:59:26,463 --> 00:59:32,411


656
00:59:32,411 --> 00:59:39,398


657
00:59:34,230 --> 00:59:38,559
좋아, 한 가지 더 알고 싶은 것은

658
00:59:38,559 --> 00:59:41,059
매우 깊은 네트워크의 경우, 더 많은 네트워크

659
00:59:39,398 --> 00:59:43,648


660
00:59:41,059 --> 00:59:43,964
깊이가 50 이상인 경우 병목 현상 레이어도 사용합니다.

661
00:59:43,964 --> 00:59:46,663
효율성을 높이기 위해 GoogleNet이
수행 한 것과 유사합니다.

662
00:59:46,663 --> 00:59:51,055
이제 각 블록 내에서 당신은 가고 있습니다.

663
00:59:51,055 --> 00:59:53,835
그들이 한 일은 하나의 전환 필터 (conv filter)

664
00:59:51,423 --> 00:59:57,599


665
00:59:53,835 --> 00:59:57,195
그것을 먼저 더 작은 깊이로 투영합니다.

666
00:59:57,195 --> 01:00:00,964
그래서 다시 우리가보고 있다면 28
시까 지 28 살이라고합시다.

667
00:59:57,599 --> 01:00:03,351


668
01:00:00,964 --> 01:00:04,277
256 임플란트로, 우리는 이것을 하나씩 처리합니다.

669
01:00:04,277 --> 01:00:06,116
그것은 깊이를 투사하고있다.

670
01:00:04,773 --> 01:00:06,777


671
01:00:06,116 --> 01:00:07,949
우리는 28에 의해 28만큼 64를 얻습니다.

672
01:00:06,777 --> 01:00:10,711


673
01:00:09,107 --> 01:00:11,410
이제 당신의 회선은 3 대 3의 conv,

674
01:00:11,410 --> 01:00:14,999
여기에 그들은 오직 하나만 가지고
있으며, 이것 이상으로 작동합니다.

675
01:00:13,458 --> 01:00:17,949


676
01:00:14,999 --> 01:00:18,486
축소 된 단계로 인해 비용이 적게 듭니다.

677
01:00:18,486 --> 01:00:21,071
그리고 나서 그들은 다른

678
01:00:21,071 --> 01:00:23,811
깊이있는 백업 프로젝트

679
01:00:23,811 --> 01:00:27,370
~ 256, 실제 블록입니다.

680
01:00:26,284 --> 01:00:32,790


681
01:00:27,370 --> 01:00:29,870
더 깊은 네트워크에서 보게 될 것입니다.

682
01:00:33,021 --> 01:00:37,808
따라서 실제로 ResNet은 일괄 정규화를 사용합니다.

683
01:00:37,808 --> 01:00:41,282
모든 전환 레이어 후에는 Xavier 초기화를 사용합니다.

684
01:00:41,282 --> 01:00:45,449
그들이 도움을 주었던 여분의 스케일링 요소

685
01:00:45,097 --> 01:00:50,179


686
01:00:46,411 --> 01:00:50,578
SGD + 모멘텀으로 훈련 된 초기화를 향상시킵니다.

687
01:00:50,179 --> 01:00:53,951


688
01:00:51,604 --> 01:00:53,812
그들의 학습 속도는 비슷한 학습 속도를 사용합니다.

689
01:00:53,812 --> 01:00:56,470
학습 속도를 떨어 뜨리는 일정 유형

690
01:00:56,470 --> 01:00:59,470
귀하의 유효성 오류가 평원 때.

691
01:01:01,751 --> 01:01:04,541
미니 배치 크기 256, 체중 감량 조금

692
01:01:04,541 --> 01:01:05,874
아무런 탈락도 없습니다.

693
01:01:07,645 --> 01:01:10,180
그리고 실험적으로 그들은 그들이

694
01:01:10,180 --> 01:01:11,900
이 매우 깊은 네트워크를 훈련 할 수 있었고,

695
01:01:11,900 --> 01:01:13,581
퇴화하지 않고.

696
01:01:12,470 --> 01:01:17,290


697
01:01:13,581 --> 01:01:16,772
그들은 기본적으로 좋은 그라디언트
흐름을 가질 수있었습니다.

698
01:01:16,772 --> 01:01:19,060
네트워크를 통해 모든 길로 되돌아옵니다.

699
01:01:17,290 --> 01:01:22,559


700
01:01:19,060 --> 01:01:22,625
그들은 ImageNet에서 최대 152 개의 레이어를 시도했으며,

701
01:01:22,625 --> 01:01:26,632
1200 Cifar에, 당신은 그것으로 놀았 어,

702
01:01:25,147 --> 01:01:32,201


703
01:01:26,632 --> 01:01:30,715
그러나 더 작은 데이터 세트 및
그들은 또한 지금 그것을 보았다.

704
01:01:31,994 --> 01:01:34,746
당신은 더 깊은 네트워크가 더
낮은 훈련을 이룰 수 있습니다.

705
01:01:32,201 --> 01:01:36,938


706
01:01:34,746 --> 01:01:36,303
예상대로 오류가 발생했습니다.

707
01:01:36,303 --> 01:01:39,418
그래서 당신은 똑같은 이상한 그림을 가지고 있지 않습니다.

708
01:01:36,938 --> 01:01:39,199


709
01:01:39,418 --> 01:01:43,031
우리가 이전에 본 행동

710
01:01:43,031 --> 01:01:44,543
잘못된 방향이었다.

711
01:01:44,543 --> 01:01:47,450
그래서 여기에서 그들은 1 위를 쓸어 버릴 수있었습니다.

712
01:01:46,404 --> 01:01:53,721


713
01:01:47,450 --> 01:01:49,534
모든 ILSVRC 대회에서,

714
01:01:49,534 --> 01:01:52,760
2015 년 모든 COCO 대회

715
01:01:52,760 --> 01:01:54,843
상당한 이윤으로

716
01:01:53,721 --> 01:01:58,860


717
01:01:56,152 --> 01:02:00,764
총 상위 5 개 오류는 분류에서 3.6 %였습니다

718
01:01:58,860 --> 01:02:05,241


719
01:02:00,764 --> 01:02:04,816
실제로 이것은 인간의 성능보다 낫습니다.

720
01:02:04,816 --> 01:02:06,649
ImageNet 논문에서.

721
01:02:05,241 --> 01:02:16,955


722
01:02:08,902 --> 01:02:12,054
또한 인간의 측정 기준이있었습니다.

723
01:02:12,054 --> 01:02:16,221
사실 우리 실험실 안드레 카파시가 썼다.

724
01:02:17,213 --> 01:02:21,130
일주일 내내 스스로 훈련하고 기본적으로

725
01:02:17,796 --> 01:02:21,721


726
01:02:22,317 --> 01:02:24,730
모두 다 했어.이 일을 혼자서 했어.

727
01:02:24,730 --> 01:02:28,531
그리고 나는 어딘가에 5-ish %를 생각했다.

728
01:02:25,481 --> 01:02:37,335


729
01:02:28,531 --> 01:02:30,774
그래서 저는 기본적으로 할 수있었습니다.

730
01:02:30,774 --> 01:02:34,191
적어도 인간보다.

731
01:02:36,175 --> 01:02:40,262
좋아요, 이것들은 주요 네트워크의 종류입니다.

732
01:02:37,335 --> 01:02:41,593


733
01:02:40,262 --> 01:02:42,069
최근에 사용 된

734
01:02:42,069 --> 01:02:44,401
우리는 처음에 AlexNet을 시작했고,

735
01:02:44,401 --> 01:02:48,004
VGG와 GoogleNet은 여전히 인기가 있지만,

736
01:02:48,004 --> 01:02:51,157
그러나 ResNet은 가장 최근에
가장 우수한 수행 모델입니다.

737
01:02:51,157 --> 01:02:55,290
새로운 네트워크를 훈련시키는 무언가를 찾고 있다면

738
01:02:51,449 --> 01:02:56,243


739
01:02:55,290 --> 01:02:56,885
ResNet을 사용할 수 있습니다.

740
01:02:56,885 --> 01:02:58,218
그것으로 작업.

741
01:03:00,154 --> 01:03:04,034
따라서이 중 일부를 더 빨리 살펴 보는 것이 더 좋습니다.

742
01:03:04,034 --> 01:03:06,403
복잡성에 대한 감각.

743
01:03:06,403 --> 01:03:08,288
여기에 우리는

744
01:03:08,288 --> 01:03:12,038
성능 그래서 이것이 여기에 최고 정확도입니다.

745
01:03:13,193 --> 01:03:15,275
그리고 더 높은 것이 좋습니다.

746
01:03:15,275 --> 01:03:17,077
그래서 우리는 이러한 모델을 많이 보게 될 것입니다.

747
01:03:17,077 --> 01:03:19,007
일부 다른 버전뿐만 아니라

748
01:03:19,007 --> 01:03:21,540
그 (것)들의 이렇게,이 GoogleNet 처음 것은,

749
01:03:21,540 --> 01:03:25,456
내 생각에 V2, V3 및 여기서는 최고라고 생각합니다.

750
01:03:25,456 --> 01:03:28,325
V4는 실제로 ResNet 더하기 시작입니다.

751
01:03:25,999 --> 01:03:32,229


752
01:03:28,325 --> 01:03:31,389
조합, 그래서 이들은 단지 종류의

753
01:03:31,389 --> 01:03:34,010
점진적이고 작은 변화

754
01:03:32,229 --> 01:03:36,420


755
01:03:34,010 --> 01:03:36,745
그 (것)들의 위에 건설 해, 이렇게 최상이다

756
01:03:36,745 --> 01:03:39,159
여기서 모델을 수행합니다.

757
01:03:39,159 --> 01:03:41,946
그리고 우리가 오른쪽을 보면, 그들의

758
01:03:41,946 --> 01:03:45,446
여기서 계산상의 복잡성은 정렬됩니다.

759
01:03:43,275 --> 01:03:53,720


760
01:03:47,686 --> 01:03:50,799
Y 축이 가장 정확합니다.

761
01:03:50,799 --> 01:03:52,313
그래서 더 높습니다.

762
01:03:52,313 --> 01:03:56,413
X 축은 사용자의 작업이므로 오른쪽으로 갈수록,

763
01:03:53,720 --> 01:04:02,635


764
01:03:56,413 --> 01:03:58,939
당신이하고있는 작전이 많을수록 더 계산적으로

765
01:03:58,939 --> 01:04:01,646
비싼 다음 큰 원,

766
01:04:01,646 --> 01:04:03,074
귀하의 서클은 귀하의 메모리 사용량,

767
01:04:03,074 --> 01:04:05,287
회색 원이 여기서 참조됩니다.

768
01:04:05,287 --> 01:04:07,251
그러나 원이 클수록 메모리 사용량이 많아집니다.

769
01:04:07,251 --> 01:04:11,014
그래서 여기서 우리는 VGG가
녹색의 것들임을 알 수 있습니다.

770
01:04:09,609 --> 01:04:19,321


771
01:04:11,014 --> 01:04:13,342
일종의 효율성이 가장 낮습니다.

772
01:04:13,342 --> 01:04:15,099
그들은 가장 큰 기억을 가지고 있으며,

773
01:04:15,099 --> 01:04:16,206
대부분의 작업,

774
01:04:16,206 --> 01:04:18,623
그러나 그들은 꽤 잘합니다.

775
01:04:19,838 --> 01:04:22,411
GoogleNet이 가장 효율적입니다.

776
01:04:22,411 --> 01:04:24,947
그것은 작전 측면에서 길이다.

777
01:04:24,947 --> 01:04:29,275
메모리 사용을위한 작은 작은 원이 있습니다.

778
01:04:28,121 --> 01:04:33,216


779
01:04:29,275 --> 01:04:33,249
이전 모델 인 AlexNet은 정확도가 가장 낮습니다.

780
01:04:33,249 --> 01:04:35,175
상대적으로 계산량이 적습니다.

781
01:04:35,175 --> 01:04:37,994
그것은 더 작은 네트워크이지만,
또한 특별히 그렇지 않습니다.

782
01:04:37,994 --> 01:04:39,411
메모리 효율적인.

783
01:04:38,761 --> 01:04:41,618


784
01:04:41,309 --> 01:04:46,216
그리고 ResNet 여기, 우리는
적당한 효율성을 가지고 있습니다.

785
01:04:41,618 --> 01:04:45,186


786
01:04:45,186 --> 01:04:54,640


787
01:04:46,216 --> 01:04:48,500
메모리의 측면에서 보면 중간에있는 것입니다.

788
01:04:48,500 --> 01:04:52,500
작업을 수행 할 수 있으며 정확도가 가장 높습니다.

789
01:04:54,640 --> 01:04:59,614


790
01:04:56,029 --> 01:04:58,028
여기에 몇 가지 추가 플롯도 있습니다.

791
01:04:58,028 --> 01:05:00,808
당신은 당신의 자신의 시간에 이것들을 더 볼 수 있습니다.

792
01:04:59,614 --> 01:05:06,949


793
01:05:00,808 --> 01:05:03,790
왼쪽의이 그림은 정방향 통과 시간을 보여줍니다.

794
01:05:03,790 --> 01:05:07,015
그래서 이것은 밀리 세컨드 (milliseconds)이며
맨 위에 올 수 있습니다.

795
01:05:07,015 --> 01:05:10,443
VGG 앞으로 약 200 밀리 초를 얻을 수 있습니다.

796
01:05:10,443 --> 01:05:12,656
이것으로 초당 5 프레임,

797
01:05:12,656 --> 01:05:14,868
이것은 순서대로 정렬됩니다.

798
01:05:14,868 --> 01:05:17,649
권력을 바라보고있는이 줄거리도있다.

799
01:05:17,649 --> 01:05:21,774
소비와 당신이 여기에서이 종이를 더 보는 경우에,

800
01:05:21,774 --> 01:05:24,883
이러한 종류의 계산에 대한 추가 분석이 있습니다.

801
01:05:24,883 --> 01:05:25,883
비교.

802
01:05:28,353 --> 01:05:37,814


803
01:05:30,604 --> 01:05:33,508
그래서 이것들은 여러분이해야 할 주요 아키텍처였습니다.

804
01:05:33,508 --> 01:05:35,966
정말로 깊이 알고 있고 익숙하다.

805
01:05:35,966 --> 01:05:38,750
적극적으로 사용하려고 생각하고 있습니다.

806
01:05:38,750 --> 01:05:40,468
하지만 이제는 간단히 살펴 보겠습니다.

807
01:05:40,468 --> 01:05:42,346
좋은 다른 아키텍쳐

808
01:05:42,346 --> 01:05:45,513
역사적 영감을 아는 것

809
01:05:44,260 --> 01:05:47,171


810
01:05:46,480 --> 01:05:49,230
또는 최근 연구 분야.

811
01:05:47,171 --> 01:05:51,201


812
01:05:50,716 --> 01:05:52,759
첫 번째 네트워크 인 Network in Network,

813
01:05:51,201 --> 01:05:59,985


814
01:05:52,759 --> 01:05:56,342
이것은 2014 년 이후이며이 배경의 아이디어입니다.

815
01:06:00,529 --> 01:06:05,524
이 바닐라 길쌈 레이어가 있다는 것입니다.

816
01:06:05,524 --> 01:06:09,451
그러나 우리는 이것들을 가지고 있습니다. 이것은

817
01:06:09,451 --> 01:06:12,683
그들이 마이크로 네트워크라고 부르는 MLP 전환 계층

818
01:06:10,054 --> 01:06:13,527


819
01:06:12,683 --> 01:06:14,687
또는 기본적으로 networth 내의 네트워크,

820
01:06:13,527 --> 01:06:15,131


821
01:06:14,687 --> 01:06:16,118
종이의 이름.

822
01:06:16,118 --> 01:06:20,278
각 전환 레이어 내에서 MLP를 스택하려는 위치

823
01:06:20,278 --> 01:06:23,152
맨 위에 두 개의 완전히 연결된 레이어가있는

824
01:06:23,152 --> 01:06:25,483
표준 전환과 계산이 가능하다.

825
01:06:25,483 --> 01:06:28,179
이 지역의 더 추상적 인 기능

826
01:06:28,179 --> 01:06:29,167
패치가 맞아.

827
01:06:29,167 --> 01:06:31,207
그래서 conv 필터를 미끄러지는 대신에,

828
01:06:31,207 --> 01:06:35,343
그것은 약간 더 복잡한 계층 구조로 미끄러지고있다.

829
01:06:35,343 --> 01:06:39,970
주변의 필터 세트를 사용하여

830
01:06:36,008 --> 01:06:39,931


831
01:06:39,970 --> 01:06:41,975
활성화지도.

832
01:06:41,975 --> 01:06:45,282
그리고 이렇게, 이것은 완전히 연결된,

833
01:06:45,282 --> 01:06:47,941
또는 기본적으로 하나의 전환 유형의 층으로 구성됩니다.

834
01:06:47,941 --> 01:06:50,191
그것들을 모두 쌓아 올릴 것입니다.

835
01:06:50,191 --> 01:06:52,891
우리가이 네트워크를 가지고있는 아래쪽 다이어그램

836
01:06:51,713 --> 01:07:02,550


837
01:06:52,891 --> 01:06:57,196
각 레이어에 쌓인 네트워크 내에서

838
01:06:57,196 --> 01:07:00,665
그리고 이것을 알아야하는 주된 이유는

839
01:07:00,665 --> 01:07:04,015
그것은 GoogleNet과 ResNet의 선구자였습니다.

840
01:07:02,550 --> 01:07:04,717


841
01:07:04,015 --> 01:07:07,774
병목 현상에 대한 아이디어가 담긴 2014 년

842
01:07:05,903 --> 01:07:09,953


843
01:07:07,774 --> 01:07:10,102
네가 거기에서 아주 많이 사용되는 것을 보았다.

844
01:07:10,102 --> 01:07:13,018
그리고 그것은 또한 약간의 철학적 영감을주었습니다.

845
01:07:13,018 --> 01:07:16,578
로컬 네트워크 유형학에 대한 GoogleNet의 아이디어

846
01:07:16,578 --> 01:07:19,153
네트워크에서 사용 된 네트워크,

847
01:07:19,153 --> 01:07:22,070
다른 종류의 구조.

848
01:07:24,238 --> 01:07:28,612
이제 일련의 작품에 대해 이야기하겠습니다.

849
01:07:25,626 --> 01:07:32,561


850
01:07:28,612 --> 01:07:31,315
대부분 설치되어있는 ResNet
이후에 작동하거나 작동합니다.

851
01:07:31,315 --> 01:07:33,845
resNet을 개선하기 위해

852
01:07:32,561 --> 01:07:38,679


853
01:07:33,845 --> 01:07:36,759
그 이후로 연구가 이루어졌습니다.

854
01:07:36,759 --> 01:07:38,113
나는이 꽤 빨리 갈 것이다,

855
01:07:38,113 --> 01:07:39,911
그래서 매우 높은 수준에 있습니다.

856
01:07:38,679 --> 01:07:46,285


857
01:07:39,911 --> 01:07:41,337
당신이 이것들에 관심이 있다면

858
01:07:41,337 --> 01:07:44,754
논문을보고 자세한 내용을 알아보십시오.

859
01:07:45,755 --> 01:07:49,876
그래서 ResNet의 저자는 잠시 후에

860
01:07:46,285 --> 01:07:50,884


861
01:07:49,876 --> 01:07:54,043
2016 년에는이 신문을 통해

862
01:07:50,884 --> 01:07:54,972


863
01:07:55,066 --> 01:07:56,742
ResNet 블록 설계.

864
01:07:56,742 --> 01:08:00,517
그래서 그들은 기본적으로 레이어가 무엇인지 조정했습니다.

865
01:08:00,517 --> 01:08:03,015
ResNet 블록 경로에 있던

866
01:08:03,015 --> 01:08:06,489
이 새로운 구조가

867
01:08:06,489 --> 01:08:10,214
정보 전달을위한보다 직접적인 경로

868
01:08:10,214 --> 01:08:13,901
네트워크 전반에 걸쳐, 당신은 좋은 것을 원한다.

869
01:08:12,707 --> 01:08:22,214


870
01:08:13,901 --> 01:08:16,073
경로를 통해 모든 방법으로 정보를 전파하고,

871
01:08:16,073 --> 01:08:18,861
그런 다음 다시 맨 아래로 백업하십시오.

872
01:08:18,861 --> 01:08:21,434
그래서 그들은이 새로운 블록이 더
좋았다는 것을 보여주었습니다.

873
01:08:21,435 --> 01:08:25,319
더 나은 성능을 제공 할 수있었습니다.

874
01:08:23,290 --> 01:08:27,924


875
01:08:25,319 --> 01:08:28,959
이 논문에서는 Wide
Residual 네트워크도 있습니다.

876
01:08:27,924 --> 01:08:32,402


877
01:08:28,959 --> 01:08:33,591
ResNets은 네트워크를 훨씬 더 깊게 만들었지 만

878
01:08:32,402 --> 01:08:36,405


879
01:08:33,591 --> 01:08:35,877
이러한 잔여 연결을 추가 할뿐만 아니라

880
01:08:35,877 --> 01:08:39,396
그들의 주장은 잔차가 실제로 있다는 것입니다.

881
01:08:36,405 --> 01:08:43,406


882
01:08:39,396 --> 01:08:40,228
중요한 요인.

883
01:08:40,228 --> 01:08:41,524
이 잔존물 구조를 가짐으로써,

884
01:08:41,525 --> 01:08:45,290
반드시 아주 깊은 네트워크가있는 것은 아닙니다.

885
01:08:43,406 --> 01:08:51,994


886
01:08:45,290 --> 01:08:49,467
그래서 그들은 더 넓은 잔차 블록을 사용했습니다.

887
01:08:49,467 --> 01:08:51,892
따라서 이것이 의미하는 바는 모든 필터가

888
01:08:51,892 --> 01:08:52,794
전환 층.

889
01:08:52,794 --> 01:08:55,996
따라서 레이어 당 F 필터를 사용하기 전에

890
01:08:55,997 --> 01:08:59,308
그들은 K의 이러한 인자들을 사용하여 우물을 말했고,

891
01:08:57,428 --> 01:09:05,912


892
01:08:59,308 --> 01:09:02,662
모든 레이어 그것은 F times K 필터가 될 것입니다.

893
01:09:02,663 --> 01:09:06,622
그래서이 넓은 층을 사용하여

894
01:09:06,622 --> 01:09:09,002
그들의 50 층 너비의 ResNet이
out-perform 할 수있었습니다.

895
01:09:06,927 --> 01:09:20,769


896
01:09:09,002 --> 01:09:11,502
152 레이어 원래 ResNet,

897
01:09:13,754 --> 01:09:16,289
그것은 또한 다음과 같은 추가적인 장점을 가지고있다.

898
01:09:16,289 --> 01:09:20,703
동일한 금액으로도이 금액이 증가합니다.

899
01:09:20,703 --> 01:09:23,035
매개 변수, 더 가슴의 계산 효율

900
01:09:21,606 --> 01:09:31,467


901
01:09:23,035 --> 01:09:25,777
이러한 작업을 병렬 처리 할 수 있기 때문에

902
01:09:25,777 --> 01:09:26,922
더 쉽게.

903
01:09:26,923 --> 01:09:30,333
더 많은 뉴런을 가진 바로 그 콘볼 루트

904
01:09:30,333 --> 01:09:32,750
더 많은 커널에 퍼져있다.

905
01:09:31,467 --> 01:09:36,547


906
01:09:32,750 --> 01:09:35,620
보다 순차적 인 깊이와는 달리,

907
01:09:35,620 --> 01:09:38,606
따라서 계산 효율이 향상됩니다.

908
01:09:36,548 --> 01:09:40,884


909
01:09:38,606 --> 01:09:39,546
너비.

910
01:09:39,546 --> 01:09:41,294
그래서 여기에서이 작품이 시작되는 것을 볼 수 있습니다.

911
01:09:41,294 --> 01:09:43,592
너비의 공헌을 이해하려고 노력하다.

912
01:09:43,593 --> 01:09:46,740
깊이와 잔여 연결부,

913
01:09:45,645 --> 01:09:51,142


914
01:09:46,740 --> 01:09:49,817
한 쪽에서 다른 쪽에서 몇 가지 주장하기.

915
01:09:49,817 --> 01:09:53,317
그리고이 다른 신문은 같은 시간에,

916
01:09:51,142 --> 01:09:56,447


917
01:09:55,064 --> 01:09:58,125
아마 조금 나중에, ResNeXt,

918
01:09:56,447 --> 01:09:59,447


919
01:09:58,125 --> 01:10:01,791
다시 이것은 ResNet의 제작자입니다.

920
01:10:00,367 --> 01:10:06,802


921
01:10:01,791 --> 01:10:04,383
계속해서 아키텍처를 추진하고 있습니다.

922
01:10:04,383 --> 01:10:08,300
그리고 여기에도 그들은 괜찮은 생각이 들었습니다.

923
01:10:06,802 --> 01:10:12,527


924
01:10:09,699 --> 01:10:12,932
참으로이 폭 너비를 단지 대신에 더 많이 다루겠습니다.

925
01:10:12,932 --> 01:10:15,019
이 잔여 블록의 너비를 늘린다.

926
01:10:15,019 --> 01:10:18,576
더 많은 필터를 통해 그들은 구조를 가지고 있습니다.

927
01:10:18,576 --> 01:10:22,525
그리고 각 나머지 블록 내에서, 다중 병렬

928
01:10:20,884 --> 01:10:24,906


929
01:10:22,525 --> 01:10:24,534
통로와 그들이 전화 할거야

930
01:10:24,534 --> 01:10:26,415
이 경로의 총 수는 카디널리티입니다.

931
01:10:24,906 --> 01:10:29,110


932
01:10:26,415 --> 01:10:30,498
그래서 기본적으로 하나의 ResNet
블록을 사용하고 있습니다.

933
01:10:29,110 --> 01:10:35,567


934
01:10:32,576 --> 01:10:35,531
병목 현상이 있고 상대적으로 더 얇은 것은

935
01:10:35,531 --> 01:10:38,395
그러나 이들 중 여러 개를 병렬로 수행해야합니다.

936
01:10:38,395 --> 01:10:42,563
그리고 여기에도이 두 가지가 있다는 것을 알 수 있습니다.

937
01:10:42,563 --> 01:10:44,452
광범위한 네트워크에 대한이 아이디어와의 관계,

938
01:10:44,452 --> 01:10:49,287
또한 시작 모듈에 약간의 연결이있다.

939
01:10:46,527 --> 01:10:56,046


940
01:10:49,287 --> 01:10:51,651
우리가이 평행선을 가지고있는 곳에서도 말이죠.

941
01:10:51,651 --> 01:10:54,023
이들 층은 병행하여 작동한다.

942
01:10:54,023 --> 01:10:58,190
그리고 이제이 ResNeXt에는 그 취향이 있습니다.

943
01:10:56,046 --> 01:10:57,922


944
01:10:57,922 --> 01:11:04,904


945
01:11:00,838 --> 01:11:04,843
ResNets을 개선하기위한 또 다른 접근법

946
01:11:04,843 --> 01:11:08,635
확률 적 깊이라고 불리는이
아이디어가이 작업에서 나왔습니다.

947
01:11:08,635 --> 01:11:11,747
동기 부여는 잘 보입니다.

948
01:11:09,506 --> 01:11:14,077


949
01:11:11,747 --> 01:11:13,878
이 깊이 문제에서.

950
01:11:13,878 --> 01:11:18,045
일반적인 문제를 더 깊고 깊게 느끼면

951
01:11:15,764 --> 01:11:22,868


952
01:11:19,575 --> 01:11:21,537
당신은 사라지는 그라디언트를 가지고있을 것입니다.

953
01:11:21,537 --> 01:11:26,064
당신은 할 수 없습니다. 당신의
그라디언트는 점점 작아 질 것입니다.

954
01:11:22,868 --> 01:11:30,927


955
01:11:26,064 --> 01:11:28,267
당신이 다시 전파하려고하는 동안 결국 사라집니다.

956
01:11:28,267 --> 01:11:32,071
그것들은 매우 긴 레이어들 또는
많은 수의 레이어들 위에 있습니다.

957
01:11:30,927 --> 01:11:42,784


958
01:11:32,071 --> 01:11:35,193
그래서 그들의 동기가 무엇인지 잘 알고 자 노력합니다.

959
01:11:35,193 --> 01:11:39,608
짧은 네트워크 동안 교육 및 그들은이 아이디어를 사용하여

960
01:11:39,608 --> 01:11:43,045
트레이닝 중에 레이어의 하위 집합을 삭제합니다.

961
01:11:43,045 --> 01:11:45,664
따라서 레이어의 하위 집합에 대해서는 그냥 사라집니다.

962
01:11:45,664 --> 01:11:48,436
무게와 그들은 정체성 연결로 설정했습니다.

963
01:11:48,436 --> 01:11:52,569
그리고 지금 당신이 얻는 것은이 짧은
네트워크를 가지고 있다는 것입니다.

964
01:11:49,028 --> 01:11:54,284


965
01:11:52,569 --> 01:11:54,615
교육을받는 동안

966
01:11:54,615 --> 01:11:56,126
그라디언트가 더 좋습니다.

967
01:11:56,126 --> 01:11:59,533
또한 조금 더 효율적입니다.

968
01:11:59,533 --> 01:12:01,535
일종의 드롭 아웃 권리처럼.

969
01:12:01,535 --> 01:12:04,074
그것은 전에 보았던 이런 종류의 향이 있습니다.

970
01:12:02,266 --> 01:12:06,906


971
01:12:04,074 --> 01:12:06,441
그리고 테스트 시간에 전체 깊은
네트워크를 사용하고자합니다.

972
01:12:06,441 --> 01:12:08,108
너는 훈련을 받았다.

973
01:12:06,906 --> 01:12:11,764


974
01:12:10,446 --> 01:12:12,781
그래서 이들은 다음을 보는 작품들입니다.

975
01:12:11,764 --> 01:12:18,404


976
01:12:12,781 --> 01:12:14,830
상주 아키텍처, 다른 것을 이해하려고 시도하다.

977
01:12:14,830 --> 01:12:19,038
ResNet 교육을 향상시키기 위해 노력하고 있습니다.

978
01:12:19,038 --> 01:12:22,431
그래서 현재 진행중인 작품도 있습니다.

979
01:12:22,431 --> 01:12:25,046
ResNet을 넘어서서,

980
01:12:22,661 --> 01:12:29,786


981
01:12:25,046 --> 01:12:29,306
어쩌면 더 잘 작동 할 수있는 ResNet 아키텍처,

982
01:12:29,306 --> 01:12:32,253
또는 ResNets과 비슷하거나 더 낫습니다.

983
01:12:29,786 --> 01:12:33,268


984
01:12:32,253 --> 01:12:35,769
그리고 한 가지 아이디어는 FractalNet입니다.

985
01:12:33,268 --> 01:12:39,044


986
01:12:35,769 --> 01:12:38,472
꽤 최근에, 그리고 FractalNet의 논쟁

987
01:12:38,472 --> 01:12:41,502
그 잔여 표현은 어쩌면

988
01:12:39,044 --> 01:12:48,005


989
01:12:41,502 --> 01:12:43,429
실제로는 필요하지 않으므로 다시 돌아갑니다.

990
01:12:43,429 --> 01:12:45,273
우리가 이전에 말한 것에.

991
01:12:45,273 --> 01:12:47,776
잔여 네트워크의 동기는 무엇입니까?

992
01:12:47,776 --> 01:12:49,946
말하자면, 알다시피,

993
01:12:48,005 --> 01:12:52,404


994
01:12:49,946 --> 01:12:52,645
이것이 왜 도움이되는지에 대한
좋은 이유가 아니라이 백서에서

995
01:12:52,645 --> 01:12:55,790
그들은 여기에 다른 아키텍처가 있다고 말하고 있습니다.

996
01:12:55,790 --> 01:12:58,407
우리가 소개하고있는 것은 잔여 표현이 아닙니다.

997
01:12:58,407 --> 01:13:01,074
우리는 그 열쇠가 전환에 더 많은 것이라고 생각합니다.

998
01:13:01,074 --> 01:13:03,898
효과적으로 얕은 네트워크에서 깊은 네트워크로,

999
01:13:02,244 --> 01:13:07,828


1000
01:13:03,898 --> 01:13:06,393
그래서 그들은이 프랙탈 아키텍처를 가지고 있습니다.

1001
01:13:06,393 --> 01:13:09,091
네가 여기 오른쪽을 보면,

1002
01:13:07,828 --> 01:13:19,124


1003
01:13:09,091 --> 01:13:13,258
그들이이 프랙탈 패션에서 그것을 구성하는 이들 층.

1004
01:13:14,769 --> 01:13:17,389
그래서 얕은 통로와 깊은 통로가 있습니다.

1005
01:13:17,389 --> 01:13:18,639
귀하의 산출물에

1006
01:13:20,045 --> 01:13:22,459
그래서 그들은 서로 다른 길이의 경로를 가지고 있습니다.

1007
01:13:22,459 --> 01:13:25,236
그들은 하위 경로를 버리고 훈련시키고,

1008
01:13:25,236 --> 01:13:29,568
그리고 다시 한번이 드롭 아웃 종류의 향이 있습니다.

1009
01:13:29,568 --> 01:13:32,341
테스트 시간에 그들은 전체 프랙탈
네트워크를 사용할 것입니다.

1010
01:13:30,065 --> 01:13:35,146


1011
01:13:32,341 --> 01:13:35,036
그들은 이것이 가능하다는 것을 보여줍니다.

1012
01:13:35,036 --> 01:13:37,203
아주 좋은 성능을 얻으십시오.

1013
01:13:39,047 --> 01:13:42,143
Densely Connected라고하는
또 다른 아이디어가 있습니다.

1014
01:13:42,143 --> 01:13:44,886
길쌈 네트워크, DenseNet 및이 아이디어

1015
01:13:43,443 --> 01:13:51,007


1016
01:13:44,886 --> 01:13:47,587
이제 우리는이 블럭들이 호출됩니다.

1017
01:13:47,587 --> 01:13:48,567
고밀도 블록.

1018
01:13:48,567 --> 01:13:50,778
그리고 각 블록 내에서 각 레이어는

1019
01:13:50,778 --> 01:13:54,428
그 후에 모든 다른 레이어에 연결되어,

1020
01:13:51,007 --> 01:13:52,465


1021
01:13:52,465 --> 01:13:58,164


1022
01:13:54,428 --> 01:13:55,940
이 피드 포워드 방식으로

1023
01:13:55,940 --> 01:13:57,502
따라서이 블록 내에서 블록에 대한 입력

1024
01:13:57,502 --> 01:14:00,362
다른 모든 전환 레이어에 대한 입력이기도합니다.

1025
01:13:58,164 --> 01:14:06,687


1026
01:14:00,362 --> 01:14:03,954
그리고 당신이 각각의 conv 출력을 계산할 때,

1027
01:14:03,954 --> 01:14:05,799
그 출력은 이제 모든

1028
01:14:05,799 --> 01:14:08,779
후에 레이어를 연결 한 다음 모두 연결합니다.

1029
01:14:06,687 --> 01:14:10,226


1030
01:14:08,779 --> 01:14:11,690
전환 레이어에 대한 입력으로

1031
01:14:10,226 --> 01:14:16,468


1032
01:14:11,690 --> 01:14:15,452
그들에게는 줄이기위한 다른 과정들이있다.

1033
01:14:15,452 --> 01:14:18,643
차원과 효율적인 유지.

1034
01:14:16,468 --> 01:14:20,484


1035
01:14:18,643 --> 01:14:22,519
그리고 이것으로부터 그들의 주요 테이크 아웃은,

1036
01:14:20,484 --> 01:14:24,628


1037
01:14:22,519 --> 01:14:26,033
그들은 이것이 완화되고 있다고 주장한다.

1038
01:14:24,628 --> 01:14:30,645


1039
01:14:26,033 --> 01:14:28,980
이 모든 것이 있기 때문에 사라지는 그라디언트 문제

1040
01:14:28,980 --> 01:14:30,863
매우 조밀 한 연결.

1041
01:14:30,863 --> 01:14:34,912
그것은 특징 전파를 강화하고 또한 격려한다.

1042
01:14:34,912 --> 01:14:37,324
미래의 사용 권리는 너무 많기 때문에

1043
01:14:37,324 --> 01:14:40,997
학습하고있는 각 기능지도를 연결합니다.

1044
01:14:39,364 --> 01:14:41,986


1045
01:14:40,997 --> 01:14:43,820
나중에 여러 개의 레이어에 입력되고

1046
01:14:41,986 --> 01:14:46,548


1047
01:14:43,820 --> 01:14:45,487
여러 번 사용되었습니다.

1048
01:14:46,548 --> 01:14:50,607


1049
01:14:47,906 --> 01:14:50,003
그래서 이것들은 단지 몇 가지 아이디어 일뿐입니다.

1050
01:14:50,003 --> 01:14:54,215
당신은 대안을 알고 있습니다. 그렇지
않으면 우리가 할 수있는 것이 없습니다.

1051
01:14:50,607 --> 01:14:53,600


1052
01:14:54,215 --> 01:14:57,026
ResNets과 아직 수행 중입니다

1053
01:14:57,026 --> 01:14:59,974
ResNets과 비슷하거나 더 낫습니다.

1054
01:14:57,786 --> 01:15:01,622


1055
01:14:59,974 --> 01:15:03,006
현재 연구 중 또 다른 매우 활발한 분야.

1056
01:15:01,622 --> 01:15:03,626


1057
01:15:03,006 --> 01:15:04,691
이 많은 것들이보고 있다는 것을 알 수 있습니다.

1058
01:15:03,626 --> 01:15:08,504


1059
01:15:04,691 --> 01:15:08,247
서로 다른 레이어가 서로 연결되는 방식으로

1060
01:15:08,247 --> 01:15:11,830
그리고 이러한 네트워크에서 깊이가
관리되는 방법에 대해 설명합니다.

1061
01:15:08,504 --> 01:15:11,508


1062
01:15:11,508 --> 01:15:17,007


1063
01:15:13,528 --> 01:15:14,998
그리고 내가 언급하고 싶은 마지막 한가지

1064
01:15:14,998 --> 01:15:17,991
빠르게 효율적인 네트워크 일뿐입니다.

1065
01:15:17,991 --> 01:15:21,021
효율성에 대한 아이디어와 GoogleNet을 보았습니다.

1066
01:15:21,021 --> 01:15:23,406
이 방향을 연구 한 작품이었습니다.

1067
01:15:23,406 --> 01:15:26,430
우리가 어떻게 중요한 네트워크를 효율적으로 만들 수 있는지

1068
01:15:26,430 --> 01:15:29,450
당신은 실용적인 사용법을 많이 알고 있습니다.

1069
01:15:29,450 --> 01:15:33,994
특히 배포와 마찬가지로

1070
01:15:33,994 --> 01:15:37,927
SqueezeNet이라고하는 또 다른 최근 네트워크

1071
01:15:37,927 --> 01:15:39,934
매우 효율적인 네트워크를 찾고 있습니다.

1072
01:15:39,934 --> 01:15:41,618
그들은 화재 모듈 (fire modules)

1073
01:15:41,618 --> 01:15:44,285
많은 스퀴즈 레이어로 구성되어 있습니다.

1074
01:15:42,724 --> 01:15:48,607


1075
01:15:44,285 --> 01:15:46,864
하나씩 필터를 적용한 다음 피드를

1076
01:15:46,864 --> 01:15:49,645
하나씩 및 3 x 3 필터를 갖는 확장 레이어,

1077
01:15:48,607 --> 01:15:52,951


1078
01:15:49,645 --> 01:15:52,680
그리고 그들은 이런 종류의 건축물로
그것을 보여주고 있습니다.

1079
01:15:52,680 --> 01:15:57,010
그들은 ImageNet에서 AlexNet
수준의 정확성을 얻을 수 있으며,

1080
01:15:53,786 --> 01:15:56,442


1081
01:15:57,010 --> 01:15:59,220
그러나 50 배 적은 매개 변수로,

1082
01:15:59,220 --> 01:16:02,330
그런 다음 네트워크 압축을 추가로 수행 할 수 있습니다.

1083
01:16:02,330 --> 01:16:06,093
AlexNet보다 500 배나 더 작아 지도록

1084
01:16:06,093 --> 01:16:10,095
전체 네트워크가 0.5 메가가되도록하십시오.

1085
01:16:07,042 --> 01:16:12,962


1086
01:16:10,095 --> 01:16:12,881
그래서 이것은 우리가 어떻게 할 것인가의 방향입니다.

1087
01:16:12,881 --> 01:16:14,762
효율적인 네트워크 모델 압축

1088
01:16:14,762 --> 01:16:17,145
나중에 강의에서 더 자세히 다루겠습니다.

1089
01:16:17,145 --> 01:16:20,062
그러나 당신에게 그 힌트를주는 것뿐입니다.

1090
01:16:21,856 --> 01:16:25,541
오늘 우리는 여러 종류의 이야기를했습니다.

1091
01:16:25,541 --> 01:16:26,809
CNN 아키텍처의

1092
01:16:26,809 --> 01:16:29,758
주요 아키텍처 중 네 가지를 자세히 살펴 보았습니다.

1093
01:16:29,758 --> 01:16:31,555
넓은 사용법에서 볼 수 있습니다.

1094
01:16:31,555 --> 01:16:35,553
초기 인기 네트워크 중 하나 인 AlexNet.

1095
01:16:32,141 --> 01:16:36,900


1096
01:16:35,553 --> 01:16:38,832
VGG 및 GoogleNet과 같이 널리 사용됩니다.

1097
01:16:38,832 --> 01:16:42,885
그러나 ResNet은 일종의 인계 역할을합니다.

1098
01:16:42,885 --> 01:16:45,906
당신이 할 수있을 때 가장 많이보고 있어야합니다.

1099
01:16:45,906 --> 01:16:47,133
우리는 또한이 다른 네트워크들을 보았습니다.

1100
01:16:47,133 --> 01:16:49,337
간단히 조금 더 깊이 들어

1101
01:16:49,337 --> 01:16:50,587
레벨 개요.

1102
01:16:51,921 --> 01:16:54,505
그리고이 테이크 아웃을 사용할 수있는 이러한 모델

1103
01:16:52,700 --> 01:16:58,039


1104
01:16:54,505 --> 01:16:57,164
그들은 당신이 그들을 사용할 수 있도록
많은 [mumbles]에 있습니다.

1105
01:16:57,164 --> 01:16:58,228
필요할 때.

1106
01:16:58,228 --> 01:17:00,275
극도로 깊은 네트워크 경향이 있습니다.

1107
01:17:00,275 --> 01:17:03,994
그러나 또한 중요한 연구가 주변에있다.

1108
01:17:03,994 --> 01:17:06,827
우리가 어떻게 레이어를 연결하는지에 대한 디자인,

1109
01:17:04,397 --> 01:17:12,380


1110
01:17:06,827 --> 01:17:09,975
연결을 건너 뛰고, 무엇이 무엇에 연결되어 있는지,

1111
01:17:09,975 --> 01:17:13,533
또한 이들을 사용하여 아키텍처를 설계하십시오.

1112
01:17:12,380 --> 01:17:14,999


1113
01:17:13,533 --> 01:17:15,419
그래디언트 흐름을 개선합니다.

1114
01:17:15,419 --> 01:17:18,203
최근의 추세에 대한 검토가 있습니다.

1115
01:17:18,203 --> 01:17:21,436
깊이 대 너비의 필요성은 무엇입니까?

1116
01:17:20,423 --> 01:17:24,839


1117
01:17:21,436 --> 01:17:22,748
잔여 연결.

1118
01:17:22,748 --> 01:17:24,548
트레이드 오프, 실제로 문제를 돕는 것,

1119
01:17:24,548 --> 01:17:27,546
최근의 많은 작품들이 있습니다.

1120
01:17:24,839 --> 01:17:29,159


1121
01:17:27,546 --> 01:17:29,343
이 방향으로 볼 수있다.

1122
01:17:29,343 --> 01:17:31,380
당신이 관심이 있다면 내가 지적한 것들 중 일부.

1123
01:17:31,380 --> 01:17:33,597
그리고 다음에 우리는 재발 신경
네트워크에 대해서 이야기 할 것입니다.

1124
01:17:33,597 --> -00:00:00,810
감사.

1125
01:17:33,900 --> 01:17:40,460


1126
01:17:40,460 --> 01:17:46,583


1127
01:17:46,583 --> 01:17:51,235


1128
01:17:52,359 --> 01:17:54,881

