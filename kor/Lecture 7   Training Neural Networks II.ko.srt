1
00:00:08,840 --> 00:00:11,842
자 12시가 지났군요 시작합니다.

2
00:00:15,129 --> 00:00:17,904
우선 지난시간에 배운 내용을 복습하겠습니다.

3
00:00:17,904 --> 00:00:23,885
지난시간에 Nerural networks를 학습 시킬 때 필요한 여러가지
중요한 것들을 배웠습니다.

4
00:00:23,885 --> 00:00:30,924
지난 시간에 배운 내용을 다시 한번 살펴 본 뒤에 
몇 가지 더 배워보겠습니다.

5
00:00:30,924 --> 00:00:35,192
우선 공지사항을 몇 가지 전달합니다.

6
00:00:35,192 --> 00:00:40,130
우선 과제 1의 마감기한이 종료되었습니다.
여러분 모두 잘 제출했길 바랍니다.

7
00:00:40,130 --> 00:00:57,807
다들 잘 했나요?
과제 1은 체점 중에 있습니다.

8
00:00:57,807 --> 00:01:04,606
그리고 두 번째는 프로젝트 제안서 제출은
금일 11:59 PM 까지 입니다.

9
00:01:05,444 --> 00:01:09,559
오늘 까지 꼭 제출 해 주시기 바랍니다. 
자세한 사항은 web과 Piazza에서 확인바랍니다.

10
00:01:09,559 --> 00:01:15,754
그리고 과제 2가 나왔습니다. 
제출까지 1주일정도의 기간이 남았습니다.

11
00:01:15,754 --> 00:01:26,345
과제 2는 우리 수업에서 가장 오래 걸리는 과제 입니다.
그러니 서둘러 진행 하는 것을 추천드립니다.

12
00:01:27,607 --> 00:01:32,969
그리고 과제 2를 하실때 팁을 드리자면 
현재 대다수가 Googld Cloud를 쓰고 계신데

13
00:01:32,969 --> 00:01:39,071
Googld Cloud에서 instances를 사용하지 않을 시에는
반드시 종료시켜 줘야 합니다. 아니면 요금이 계속 지불됩니다.

14
00:01:39,071 --> 00:01:43,384
여러분들이 쿠폰을 너무 많이 씁니다.

15
00:01:43,384 --> 00:01:52,708
instance에서 아무 작업도 하지 않더라도 켜져 있으면
요금이 부과됩니다.

16
00:01:52,708 --> 00:01:57,603
그러니 사용하지 않으면 반드시 종료시켜 줘야 합니다.

17
00:01:57,603 --> 00:02:05,455
여기 제 대쉬보드 화면인데, 이런 식으로 확실하게
STOP 버튼을 눌러줘야 합니다.

18
00:02:05,455 --> 00:02:09,129
매일 매일 이렇게 종료 시켜 주시기 바랍니다.

19
00:02:09,966 --> 00:02:21,338
그리고 Googld Cloud를 사용할 때 아셔하 할 것은
GPU를 쓰는 것이 CPU보다 더 비싸다는 것입니다.

20
00:02:21,338 --> 00:02:28,807
GPU를 쓰시면 어림잡아 시간당 거의 1 달러를 사용하게 됩니다. 
상당히 가격이 높습니다.

21
00:02:28,807 --> 00:02:40,224
CPU instances는 조금 더 쌉니다. 추천 드리는 방법은 GPU 
instance는 GPU가 필요할 때만 쓰는 것입니다.

22
00:02:40,224 --> 00:02:47,862
가령 과제 2를 하실 때 대게는 CPU만 있으면 됩니다. 따라서
CPU instance만 사용하면 됩니다.

23
00:02:47,862 --> 00:02:53,475
 하지만 과제2의 마지막 문제를 풀때 사용하는
Tensorflow와 PyTorch는 GPU가 필요합니다.

24
00:02:53,475 --> 00:02:59,382
CPU와 GPU instance를 각각 만들고 정말 필요할 때만 
GPU instance를 사용하시길 권장드립니다.

25
00:02:59,382 --> 00:03:04,792
그러니 비용이 얼마나 지불되는 지를 모니터링 하면서 
지나치게 지불되는 것을 주의하시기 바랍니다.

26
00:03:04,792 --> 00:03:08,233
공지사항 관련한 질문사항 있나요?

27
00:03:11,665 --> 00:03:12,667
문제.

28
00:03:12,667 --> 00:03:14,387
[학생이 질문]

29
00:03:14,387 --> 00:03:16,618
RAM을 얼마나 사용해야 하냐고 하셨는데

30
00:03:16,618 --> 00:03:22,348
과제용으로는 8~16기가면 충분합니다.

31
00:03:22,348 --> 00:03:27,599
CPU나 RAM을 너무 많이 추가하면 돈이 많이 듭니다.

32
00:03:27,599 --> 00:03:35,027
CPU는 2~4개를 쓰고 RAM은 8~16 기가를 쓰시면 
과제 하실 때 충분한 사양입니다.

33
00:03:37,121 --> 00:03:40,902
지난 시간 내용을 복습해 보면 
Activation function을 배웠습니다.

34
00:03:40,902 --> 00:03:45,447
지난 시간에 다양한 Activation Function과 각각의 
특성을 배웠습니다.

35
00:03:45,447 --> 00:04:00,221
10년 전에는 sigmoid가 아주 유명했습니다. 하지만 Vanishing
gradients가 생기는 문제가 있었죠 tanh도 마찬가지 이구요

36
00:04:00,221 --> 00:04:09,715
그래서 요즘은 대부분 ReLU를 씁니다. 일반적인 
네트워크에서 가장 잘 동작하는 녀석입니다.

37
00:04:09,715 --> 00:04:17,305
그래서 가중치 초기화에 대해서도 배웠습니다.

38
00:04:17,305 --> 00:04:24,272
가중치가 지나치게 작으면 activatiom이 사라집니다.

39
00:04:24,273 --> 00:04:30,068
작은 값이 여러 번 곱해지기 때문에 점점 
0이 되는 것이었습니다.

40
00:04:30,068 --> 00:04:33,557
결국 모든 값이 0이 되고 학습은 일어나지 않습니다.

41
00:04:33,557 --> 00:04:41,693
반면에 가중치가 너무 큰 값으로 초기화되면 그 값이 또
계속 곱해질 것이고 결국은 터져버릴 것입니다(explode).

42
00:04:41,693 --> 00:04:45,874
이 경우에도 학습이 일어나지 않을 것입니다.

43
00:04:45,874 --> 00:04:59,016
Xavie/MSRA(HE) Initialzation 같은 방법으로 초기화를 
잘 시켜주면 Activation의 분포를 좋게 유지시킬 수 있습니다.

44
00:04:59,016 --> 00:05:04,813
명심해야 할 점은 위의 것들이 Network가 깊어지면 깊어질
수록 더 중요하다는 것입니다.

45
00:05:04,813 --> 00:05:12,105
Network가 깊어지면 깊어질수록 가중치를 더 많이
곱하게 되기 때문입니다.

46
00:05:12,105 --> 00:05:24,151
그리고 데이터 전처리에 대해서도 배웠습니다. 그리고 CNN에서 
흔히들 zero-mean과 noamalization을 한다고 배웠습니다.

47
00:05:24,151 --> 00:05:30,453
제가 여러분에게 왜 그걸 해야하는 지를 좀 
더 직관적으로 말씀 드리겠습니다.

48
00:05:30,453 --> 00:05:40,017
자 여기에서 우리는 Binary classification 문제를 풉니다. 
빨간/파란 점들을 나누는 것입니다.

49
00:05:40,017 --> 00:05:47,433
왼쪽의 경우 not normalized/centered 데이터 입니다.

50
00:05:47,433 --> 00:05:55,492
이 경우에도 물론 classification이 가능하지만 선이 조금만
움직여도 classification이 잘 되지 않습니다.

51
00:05:55,492 --> 00:06:06,477
왼쪽의 예시가 의미하는 것은 손심 함수가 아주 약간의 
가중치 변화에도 엄청 예민하다는 것입니다.

52
00:06:07,800 --> 00:06:15,039
왼쪽의 경우 동일한 함수를 쓰더라도 학습 시키기 아주 어렵습니다.
재차 말씀드리지만 Loss가 파라미터에 너무 민감하기 때문입니다.

53
00:06:15,039 --> 00:06:25,836
반면 오른쪽은 데이터의 중심을 원점에 맞추고(zero-center), 
Unit variance로 만들어 준 경우 입니다.

54
00:06:25,836 --> 00:06:36,008
오른쪽에서의 선이 조금씩 흔들리는 경우는 보면 
손실 함수는 이런 가중치의 변동에 덜 민감함을 알 수 있습니다.

55
00:06:36,008 --> 00:06:41,549
이 경우가 최적화가 더 쉽습니다.
학습이 더 잘되는 것입니다.

56
00:06:41,549 --> 00:06:47,024
그리고 이는 Linear classification의 경우에만
국한되는 것이 아닙니다.

57
00:06:47,024 --> 00:06:58,241
Neural network 내부에도 다수의(interleavings) 
linear classifer가 있다고 생각할 수 있습니다.

58
00:06:59,563 --> 00:07:06,172
이 경우에도 Neural network의 입력이 zero-centered가
아니고 Unit variance가 아닌 경우라면

59
00:07:06,172 --> 00:07:16,117
레이어의 Weight matrix가 아주 조금만 변해도 출력은 엄청
심하게 변하게 됩니다.  이는 학습을 어렵게 합니다.

60
00:07:16,117 --> 00:07:20,966
이를 통해 왜 nomalization가 중요한지를 좀 더 
직관적으로 이해하실 수 있을 것입니다.

61
00:07:22,349 --> 00:07:27,347
자 우리는 Normalization이 엄청 중요하다는 것을 알고
있기 때문에 batch normalization도 배웠습니다.

62
00:07:27,347 --> 00:07:36,515
이는 actications이 zero mean과 unit variance가
될 수 있도록 레이어를 하나 추가하는 방법이었습니다.

63
00:07:36,515 --> 00:07:41,950
제가 이 슬라이드에 batch normalization 수식을 
다시한번 적어 봤습니다.

64
00:07:41,950 --> 00:07:45,657
이 슬라이드를 보면 과제 2를 하실 때 수월할 것입니다.

65
00:07:45,657 --> 00:07:59,739
BN에서는 forward pass 시에 미니배치에서의 평균과 표준편차를
계산해서 Normalization을 수행했습니다.

66
00:07:59,739 --> 00:08:06,126
그리고 레이어의 유연한 표현성(expressivity)을 위해서 
scale, shift 파라미터를 추가했습니다.

67
00:08:06,126 --> 00:08:10,475
과제 2를 하실때 이를 참고하시기 바랍니다.

68
00:08:10,475 --> 00:08:18,631
그리고 학습 과정을 다루는 방법도 배웠습니다. 학습 도중
Loss curve가 어떻게 보여야 하는지도 말이죠

69
00:08:18,631 --> 00:08:27,168
여기 예제 네트워크가 있습니다. 주말동안 한번 돌려봤습니다.
저는 학습을 진행할 때 이런 식으로 진행합니다.

70
00:08:27,168 --> 00:08:36,280
가운데 그래프는 시간에 따른 Loss 값을 나타냅니다. 
네트워크가 Loss를 줄이고 있으면 잘 하고 있는 것이죠

71
00:08:36,280 --> 00:08:48,949
맨 오른쪽 그래프를 보면 X는 시간축이고 Y는 성능 지표입니다. 
Training/Validation set의 성능지표를 나타냅니다.

72
00:08:48,950 --> 00:08:59,165
이를 해석해보면 Training set의 성능을 계속 올라가죠 Loss도 
계속 내려갑니다. 하지만 validation은 침체하고 있습니다.

73
00:08:59,165 --> 00:09:05,551
이런 경우는 overfititing이라고 할 수 있겠습니다.
추가적인 regularization이 필요한 것입니다.

74
00:09:06,802 --> 00:09:09,989
그리고 이전 강의에서 hyperparameter search도 배웠습니다.

75
00:09:09,989 --> 00:09:15,283
네트워크에는 무수히 많은 하이퍼파라미터가 존재합니다. 
이를 올바르게 잘 선택하는 것은 상당히 중요합니다.

76
00:09:15,283 --> 00:09:21,210
그리고 grid search와 random search를 배웠습니다. 
이론상 random search가 더 좋았습니다.

77
00:09:21,210 --> 00:09:31,154
왜냐하면 성능이 특정 하이퍼파라미터에 의해 크게 좌우될 때 그 
파라미터를 좀 더 넓은 범위로 탐색할 수 있기 때문입니다.

78
00:09:31,154 --> 00:09:37,490
그리고 하이퍼파라미터 최적화 시에 
coarse search 이후 fine search를 한다고 배웠습니다.

79
00:09:37,490 --> 00:09:43,893
처음에는 하이퍼파라미터를 조금 더 넒은 범위에서 찾습니다.
Interation도 작게 줘서 학습시켜봅니다.(coarse)

80
00:09:43,893 --> 00:09:48,458
그리고 결과가 좋은 범위로 좁히는 것입니다.

81
00:09:48,458 --> 00:09:52,151
그리고 iterations를 조금 더 돌면서 더 작은 범위를
다시 탐색합니다. (fine search)

82
00:09:52,151 --> 00:09:57,193
적절한 하이퍼파라미터를 찾을 때 까지 
이 과정을 반복합니다.

83
00:09:57,193 --> 00:10:04,940
가장 중요한 점은 coarse range를 설정할 때 가능한
최대한 넓은 범위를 설정해 줘야 한다는 것입니다.

84
00:10:04,940 --> 00:10:14,231
그 범위가 하이퍼파라미터 범위의 끝에서 끝까지 다 살펴볼 수
있도록 할수록 좋습니다. 충분위 넓은 범위를 사용해야 합니다.

85
00:10:17,947 --> 00:10:18,780
질문 있나요?

86
00:10:20,529 --> 00:10:27,157
[학생이 질문]

87
00:10:32,325 --> 00:10:35,039
질문은 보통 하이퍼파라미터를 몇 개씩 선택하는지 입니다.

88
00:10:35,039 --> 00:10:38,729
여기 예제에서는 2개 인데 보통은 2개보단 많습니다.

89
00:10:38,729 --> 00:10:45,927
그 선택은 모델에 따라 다릅니다. 선택한 하이퍼파라미터의 수가
많을 수록 기하급수적으로 경우의 수가 늘어납니다.

90
00:10:45,927 --> 00:10:48,497
때문에 한번에 너무 많이 할 수 는 없습니다.

91
00:10:48,497 --> 00:10:52,222
또 이는 여러분이 얼마나 많은 자원을
학습에 사용할 수 있는지도 중요합니다.

92
00:10:52,222 --> 00:10:56,230
이는 사람마다 바르고 실험마다 다릅니다.

93
00:10:56,230 --> 00:11:05,838
저같은 경우 두세개 정도를 고르는 편이고 많아도 네개 정도만
선택합니다. 그 이상이면 out of control이 되버립니다.

94
00:11:05,838 --> 00:11:10,891
일반적으로 Learning rate가 가장 중요합니다. 
제일 먼저 못막아 놔야(nail) 합니다.

95
00:11:10,891 --> 00:11:20,027
regularization, learning rate decay,
model size 같은 것들은 LR보단 덜 중요합니다.

96
00:11:20,027 --> 00:11:23,208
Block Coordinate Descent(BCD) 
같은 방법을 쓸 수도 있습니다.

97
00:11:23,208 --> 00:11:27,944
가령, 우선 Learning rate를 정해놓은 다음에 다양한 
모델 사이즈를 시도해 보는 것입니다.

98
00:11:27,944 --> 00:11:31,244
이 방법을 쓰면 기하급수적으로 늘어나는 Search space를
조금은 줄일 수 있습니다.

99
00:11:31,244 --> 00:11:35,855
하지만 이 방법은 정확히 어떤 순서로 어떻게 찾아야 
할지 정해야 하는 것이 가장 큰 문제입니다.

100
00:11:36,738 --> 00:11:38,605
다른 질문 있으신가요?

101
00:11:38,605 --> 00:11:57,526
[학생이 질문]

102
00:11:57,526 --> 00:12:05,022
질문은 우리가 어떤 하이퍼파리미터 값을 변경할 시에 다른 하이퍼파라미터
의 최적 값이 변해버리는 경우가 빈번하냐는 것입니다.

103
00:12:05,022 --> 00:12:11,824
그런 일이 가끔 발생하긴 합니다.  Learning rates가 이런 문제에 
덜 민감함에도 실제로 이런 일이 발생하곤 합니다.

104
00:12:11,824 --> 00:12:18,615
Leraning rates가 좋은 범위 내에 속했으면 하지만 보통은 
optimal 보다는 작은 값이고 학습 속도가 길어지곤 합니다.

105
00:12:18,615 --> 00:12:31,776
이런 경우에는, 오늘 배우게 될, 더 좋은(fancier) 최적화 방법을
사용하면 모델이 learning rate에 덜 민감하도록 할 수 있습니다.

106
00:12:31,776 --> 00:12:33,447
다른 질문 있으신가요?

107
00:12:33,447 --> 00:12:37,793
[학생이 질문]

108
00:12:37,793 --> 00:12:41,777
질문은 Learning rate를 작게 하고 Epoch을 늘리면
어떤 일이 발생하는지 입니다.

109
00:12:41,777 --> 00:12:45,624
그렇게 되면 엄청 오래 걸리겠죠

110
00:12:45,624 --> 00:12:48,868
[학생이 질문]

111
00:12:48,868 --> 00:12:55,338
직관적으로 Learning rates를 낮게 주고 오랫동안 학습시키면
이론적으로는 동작합니다.

112
00:12:55,338 --> 00:13:00,976
하지만 실제로는 Learning rate가 0.01이냐 0.001이냐는
상당히 중요한 문제입니다.

113
00:13:00,976 --> 00:13:04,416
여러분이 적절한 Leraning rate를 찾으면 6시간, 12시간 또는
하루면 학습을 다 시킬 수 있을 텐데

114
00:13:04,416 --> 00:13:12,396
 그런데 여러분이 엄청 조심스러워서 Learning rate를 
10배, 100배 줄여 버라면 하루면 끝나는 것이 100일이 걸릴수 있습니다.

115
00:13:12,396 --> 00:13:16,885
그럼 세 달이 걸리게 되는 것입니다. 좋지 않은 경우입니다.

116
00:13:16,885 --> 00:13:21,153
보통 컴퓨터과학 분야를 배울때 이런 constants(10배 100배)를 
중요하게 다루지 않는 경향이 있는데

117
00:13:21,153 --> 00:13:25,929
당신은 실제로 훈련하는 것에 대해 생각하고 있습니다.
그 constants은 많은 문제가됩니다.

118
00:13:25,929 --> 00:13:27,346
다른 질문 있나요?

119
00:13:28,362 --> 00:13:33,870
[학생이 질문]

120
00:13:33,870 --> 00:13:38,292
질문은 Low learning rate를 주게되면 
local optima에 빠질 수 있지 않냐는 것입니다.

121
00:13:38,292 --> 00:13:43,086
이는 직관적으로는 그럴 수 있겠지만 실제로
그런 일은 많이 발생하지는 않습니다.

122
00:13:43,086 --> 00:13:47,515
이에 관한 내용을 오늘 잠시 뒤에 다시 배울 것입니다.

123
00:13:47,515 --> 00:13:53,636
오늘은 Neural networks를 학습시킬 때 필요한 흥미롭고 
아주 중요한 문제를 몇 가지 배워보도록 하겠습니다.

124
00:13:53,636 --> 00:14:00,140
제가 이전에 더 강력한 최적화 알고리즘이 있다고
앞서 몇 번 언급한 적이 있었습니다.

125
00:14:00,140 --> 00:14:07,552
오늘은 사람들이 많이 사용하는 그 강력한 알고리즘들에 대해서
좀 더 자세히 알아보는 시간이 되겠습니다.

126
00:14:07,552 --> 00:14:10,849
지난 강의에 Regularization에 대해서도 조금 배웠었죠

127
00:14:10,849 --> 00:14:16,291
네트워크의 Train/Test Error 간의 격차를 줄이고자
사용하는 추가적인 기법입니다.

128
00:14:16,291 --> 00:14:22,628
Neural Network에서 실제로 사람들이 사용하고 있는 
Regularization 전략에 대해서 다뤄보도록 하겠습니다.

129
00:14:22,628 --> 00:14:26,886
그리고 Transfer learning에 대해서도 배울 것입니다.

130
00:14:26,886 --> 00:14:31,975
원하는 양 보다 더 적은 데이터만을 가지고 있을때
사용할 수 있는 방법입니다.

131
00:14:33,306 --> 00:14:40,370
지난 강의를 돌이켜보면 Neural network에서 가장 중요한 것은
바로 최적화 문제 였다는 것을 알 수 있습니다.

132
00:14:40,370 --> 00:14:51,467
Nerwork의 가중치에 대해서 손실 함수를 정의해 놓으면 이 
손심 함수는 그 가중치가 얼마나 좋은지 나쁜지를 알려줍니다.

133
00:14:51,467 --> 00:14:56,993
그리고 우리는 손심 함수가 가중치에 대한
"산(landscape)"이라고 상상해 볼 수 있을 것입니다.

134
00:14:56,993 --> 00:15:04,627
여기 맨 오른쪽 사진에 간단한 예제를 가져왔습니다. 
X/Y축은 두 개의 가중치를 의미합니다.

135
00:15:04,627 --> 00:15:08,469
그리고 각 색은 Loss의 값을 나타냅니다.

136
00:15:08,469 --> 00:15:15,680
이 오른쪽의 2차원의 문제를  두개의 가중치
W_1과 W_2를 최적화 시키는 문제라고 생각해 봅시다.

137
00:15:15,680 --> 00:15:23,688
우리의 목적은 가장 붉은색인 지점을 찾는 것입니다. 
즉 가장 낮은 Loss를 가진 가중치를 찾는 것이죠.

138
00:15:23,688 --> 00:15:29,584
자 지금까지 배운 것을 생각해 봅시다. 가장 간단한 최적화 알고리즘은
바로 Stochastic Gradient Descent 입니다.

139
00:15:29,584 --> 00:15:32,878
이 세 줄로 된 엄청 간단한 알고리즘이죠

140
00:15:32,878 --> 00:15:39,664
우선 미니 배치 안의 데이터에서 Loss를 계산합니다.

141
00:15:39,664 --> 00:15:45,141
그리고 'Gradient 의 반대 방향" 을 이용해서 
파라미터 벡터를 업데이트합니다.

142
00:15:45,141 --> 00:15:49,283
반대 방향인 이유는 손실 함수를 내려가는
방향 이어야 하기 때문입니다.

143
00:15:49,283 --> 00:15:56,767
이 단계를 계속 반복하면 결국 붉은색 지역으로 수렴할 것이고 
Loss가 낮을 것입니다.

144
00:15:56,767 --> 00:16:05,947
하지만 이 심플한 알고리즘을 실제로 사용하게 되면
몇 가지 문제에 봉착하고 맙니다.

145
00:16:05,947 --> 00:16:09,198
SGD의 문제점 중 하나는,

146
00:16:09,198 --> 00:16:19,454
가령 우리의 손실함수가 이런 식으로 생겼다고 생각해 봅시다.
여기에 똑같이 W_1과 W_2가 있다고 해봅시다.

147
00:16:19,454 --> 00:16:23,957
둘중 어떤 하나는 업데이트를 해도 손실 함수가
아주 느리게 변합니다.

148
00:16:23,957 --> 00:16:27,172
즉, 수평 축의 가중치는 변해도 Loss가 아주 천천히 줄어듭니다.

149
00:16:28,637 --> 00:16:35,415
다시 말해 Loss는 수직 방향의 가중치 변화에 훨씬 더
민감하게 반응하는 것입니다.

150
00:16:35,415 --> 00:16:41,242
다시 말해 현재 지점에서 Loss는 bad condition number를
지니고 있다고 말할 수 있을 것입니다.

151
00:16:41,242 --> 00:16:46,535
이 지점의 Hessian maxrix의 최대/최소 singular values
값의 비율이 매우 안좋다는 뜻입니다.

152
00:16:46,535 --> 00:16:50,982
이를 좀 더 직관적으로 이해해보면 우선 Loss가 
 taco shell 같은 모양입니다.

153
00:16:50,982 --> 00:16:54,878
한 방향으로는 엄청나게 민감한 반면에 다른 방향으로는
덜 민감한 상태에 있는 것입니다.

154
00:16:54,878 --> 00:17:01,118
그렇다면 질문입니다. 이런 상황에서 SGD으로 학습이 되는
과정은 어떤 모습일까요?

155
00:17:05,795 --> 00:17:12,681
손실 함수가 이런 식으로 생긴 환경에서 SGD를 수행하면, 
이런 특이한 지그재그 형태를 볼 수 있을 것입니다.

156
00:17:12,682 --> 00:17:22,596
왜냐하면 이런 함수에서는 gradient의 방향이
고르지 못하기 때문입니다.

157
00:17:22,597 --> 00:17:29,820
Gradient를 계산하고 업데이트 하게 되면 line을 
넘나들면서  왔다갔다 하게 됩니다.

158
00:17:29,820 --> 00:17:36,480
Loss에 영향을 덜 주는 수평방향 차원의 가중치는
업데이트가 아주 느리게 진행됩니다.

159
00:17:36,480 --> 00:17:42,036
이렇게 빠르게 변하는 수직 차원을 가로지르면서
지그지그로 아주 지저분하게(nasty) 움직이게 됩니다.

160
00:17:42,036 --> 00:17:50,624
아주 바람직하지 않은 행동입니다. 그리고 이 문제는 고차원
공간에서 훨씬 더 빈번하게 발생합니다.

161
00:17:51,671 --> 00:18:01,102
우리가 본 예시 그림은 2차원 밖에 되지 않습니다만 실제로는
가중치가 수천 수억개 일 수 있을 것입니다.

162
00:18:01,102 --> 00:18:14,706
이 경우 수억개의 방향으로 움직일 수 있습니다. 이런 수억개의 방향중에
불균형한 방향이 존재한다면 SGD는 잘 동장하지 않을 것입니다.

163
00:18:14,706 --> 00:18:21,058
수억개의 파라미터가 있다고 했을때 이런 불균형의 
발생 비율은 상당히 높습니다.

164
00:18:21,058 --> 00:18:26,883
고차원 공간에서 발생할 수 있는 이런 문제는
실제로도 정말 큰 문제가 됩니다.

165
00:18:28,278 --> 00:18:34,049
SGD에서 발생하는 또 다른 문제는 
local minima 와 saddle points와 관련된 문제입니다.

166
00:18:34,049 --> 00:18:44,488
그림을 좀 바꿨습니다. 이제는 X축은 어떤 하나의 가중치를 나타내고
Y축은 Loss를 나타내고 있습니다.

167
00:18:44,488 --> 00:18:52,068
우선 위의 그래프를 보면, 이 휘어진 손실함수는
중간에 "valley"가 하나 있습니다.

168
00:18:52,068 --> 00:18:55,521
이런 상황에서 SGD는 어떻게 움직일까요?

169
00:18:55,521 --> 00:18:57,516
[학생이 대답]

170
00:18:57,516 --> 00:19:04,939
이 경우 SGD는 멈춰버립니다. 왜냐하면 gradient가 0이 
되기 때문이죠. locally falt 합니다.

171
00:19:04,939 --> 00:19:09,679
SGD의 동작을 생각해보면 gradient를 계산하고 
그 반대방향으로 이동하는 것 이었습니다.

172
00:19:09,679 --> 00:19:16,347
저 위치에서는 "opposite gradient" 도 0 이 되며, 
따라서 학습이 멈춰버리게 됩니다.

173
00:19:16,347 --> 00:19:19,891
또 다른 문제는 saddle points에 관한 것입니다.

174
00:19:19,891 --> 00:19:26,625
local minima는 아니지만, 한쪽 방향으로는 증가하고 있고 
다른 한쪽 방향으로는 감소하고 있는 지역을 생각해 볼 수 있습니다.

175
00:19:26,625 --> 00:19:29,438
이런 곳에서도 gradient는 0이 됩니다.(아래 그림)

176
00:19:29,438 --> 00:19:36,384
saddle point에서도 gradient = 0 이므로 멈추게 됩니다.

177
00:19:36,384 --> 00:19:48,607
비록 이처럼 1차원의 예제만 봐서는 local minima가 엄청 심각하고
saddle point는 좀 덜 심각해 보이지만,

178
00:19:48,607 --> 00:19:57,656
고차원 공간에서는 그 반대입니다. 여러분에게 1억 차원의 공간이
있다고 생각해 봅시다. 여기에서 saddle point는 무엇일까요?

179
00:19:57,656 --> 00:20:03,620
Saddle point가 의미하는 것이 어떤 방향은 Loss가 증가하고
몇몇 방향은 Loss가 감소하고 있는 곳을 생각해 볼 수 있습니다.

180
00:20:03,620 --> 00:20:10,076
1억개의 차원에서 생각해보면 이는 정말 빈번하게 발생합니다. 
사실 거의 모든 곳에서 발생한다고 할 수 있습니다.

181
00:20:10,076 --> 00:20:17,229
Local minima를 생각해보면 1억 개의 방향을 계산했는데 그 방향이
전부 Loss가 상승하는 방향이라는 것입니다.

182
00:20:17,229 --> 00:20:22,801
고차원 공간을 생각하면 그런 일이 발생하는 것은
매우 드문 경우입니다.

183
00:20:23,755 --> 00:20:33,768
지난 몇 년간 알려진 사실은 very large neural network가 
local minima 보다는 saddle point에 취약하다는 것입니다.

184
00:20:33,768 --> 00:20:40,625
그리고 또한  saddle point 뿐만 아니라  saddle point의
근처에서도 문제는 발생합니다.

185
00:20:40,625 --> 00:20:48,420
아래 예시를 보면 saddle point 근처에서 gradient가 0 은 
아니지만 기울기가 아주 작습니다.

186
00:20:48,420 --> 00:20:54,096
그것이 의미하는 바는 gradient를 계산해서 업데이트를 해도
기울기가 아주 작기 때문에

187
00:20:54,096 --> 00:21:02,357
가중치가 saddle point 근처에 있을때면 
업데이트가 아주 느리게 진행됩니다.

188
00:21:02,357 --> 00:21:10,600
이는 아주 큰 문제입니다. 
그리고 SGD의 또 다른 문제가 있습니다.

189
00:21:10,600 --> 00:21:14,006
SGD는 Stochastic gradient descent라는
것을 다시한번 명심하시기 바랍니다.

190
00:21:14,006 --> 00:21:21,071
사실 손실함수를 계산할 때는 엄청 엄청 많은 Traing set
각각의  loss를 전부 계산해야 합니다.

191
00:21:21,071 --> 00:21:26,604
이 예시의 N이 전체 training set일 경우에 
이 값은 "n = 백만"이 될 수도 있습니다.

192
00:21:26,604 --> 00:21:29,832
Loss를 계산할 때 마다 매번 전부를 계산하는 것은 어렵습니다.

193
00:21:29,832 --> 00:21:37,442
그래서 실제로는 미니배치의 데이터들만 가지고
실제 Loss를 추정하기만 합니다.

194
00:21:37,442 --> 00:21:42,633
이는 매번 정확한 gradient를 얻을 수가 
없다는 것을 의미합니다.

195
00:21:42,633 --> 00:21:47,258
대신에 gradient의 부정확한 
추정값(noisy estimate) 만을 구할 뿐입니다.

196
00:21:47,258 --> 00:21:51,060
오른쪽에 보이는 것은 제가 좀 과장해서 그린 것입니다.

197
00:21:51,060 --> 00:22:00,412
각 지점의 gradient에 random uniform noise를 추가하고
SGD를 수행하게 만들었습니다.

198
00:22:00,412 --> 00:22:08,472
따라서 실제로 SGD가 이런식으로 동작하진 않지만 이 예제로
gradient에 noise가 들어가면 어떻게 되는지 알 수 있습니다.

199
00:22:08,472 --> 00:22:14,521
손실함수 공간을 이런식으로 비틀거리면서 돌아다니게 되면 
minima까지 도달하는데 시간이 더 오래 걸릴 것입니다.

200
00:22:16,208 --> 00:22:19,451
지금까지 SGD에 어떤 문제가 있는지에 대해서
이야기해 보았습니다.

201
00:22:19,451 --> 00:22:21,441
질문 있나요?

202
00:22:21,441 --> 00:22:25,608
[학생이 질문]

203
00:22:29,584 --> 00:22:34,920
질문은 바로 SGD를 쓰지 않고 그냥 GD를 쓰면
이런 문제가 전부 해결되는지 입니다.

204
00:22:35,766 --> 00:22:44,591
자 이전의 taco shell에서의 문제를 다시한번 살펴보면 
full batch gradient descent에서도 같은 문제가 발생합니다

205
00:22:44,591 --> 00:22:54,605
Noise의 문제도 한번 볼까요. Noise는 미니배치이기 때문에서만이
아니라 네트워크의  explicit stochasticity 로도 발생합니다.

206
00:22:54,605 --> 00:22:58,221
이는 나중에 더 살펴 볼 것이지만 
이는 여전히 문제가 됩니다.

207
00:22:58,221 --> 00:23:05,586
Saddle points 또한 full batch GD에서 문제가 됩니다.
전체 데이터를 사용한다고 해도 여전히 나타날 수 있겠죠

208
00:23:05,586 --> 00:23:10,734
기본적으로 full batch gradient descent를 사용한다
하더라도 이런 문제들이 해결되지는 않습니다.

209
00:23:10,734 --> 00:23:17,089
이러한 위험요소들을 다루기 위해서는 더 좋은
최적화 알고리즘이 필요합니다.

210
00:23:17,089 --> 00:23:22,451
이번 문제들의 대다수를 해결할 수 있는 아주
간단한 방법이 하나 있습니다.

211
00:23:22,451 --> 00:23:27,463
SGD에 momentum term을 추가하는 것이죠

212
00:23:27,463 --> 00:23:33,408
왼쪽은 classic 버전의 SGD입니다. 
오로지 gradient 방향으로만 움직이는 녀석이죠

213
00:23:33,408 --> 00:23:43,547
반면 오른쪽은 SGD + momentum 입니다. 엄청 조금의 변화만
있습니다. 2개의 수식과 5라인의 코드를 보실 수 있죠

214
00:23:43,547 --> 00:23:51,816
아이디어는 아주 간단합니다. 그저 velocity를 유지하는 것입니다. 
gradient 를 계산할 때 velocity를 이용합니다.

215
00:23:51,816 --> 00:23:58,296
현재 미니배치의 gradient 방향만 고려하는 것이 아니라 
velocity를 같이 고려하는 것입니다.

216
00:23:58,296 --> 00:24:05,310
여기에는 하이퍼 파라미터 rho가 추가되었습니다. 
momemtum의 비율을 나타냅니다.

217
00:24:06,410 --> 00:24:17,333
velocity의 영향력을 rho의 비율로 맞춰주는데
보통 0.9와 같은 높은 값으로 맞춰줍니다.

218
00:24:17,333 --> 00:24:21,658
velocity에 일정 비율 rho를 곱해주고 
현재 gradient를 더합니다.

219
00:24:21,658 --> 00:24:27,484
이를 통해서 우리는 이제 gradient vector 그대로의 방향이 아닌
velocity vector의 방향으로 나아가게 됩니다.

220
00:24:28,812 --> 00:24:35,033
엄청 간단한 방법이지만 지금까지 말했던 문제들을
해결하는데 많은 도움을 줄 수 있습니다.

221
00:24:35,033 --> 00:24:45,294
local minima와 saddle points문제를 생각해보면 물리적으로
공이 굴러내려오는 것을 상상해 볼 수 있습니다.

222
00:24:45,294 --> 00:24:48,700
이 공은 떨어지면 속도가 점점 빨라집니다.

223
00:24:48,700 --> 00:24:57,407
이 공은 local minima에 도달해도 여전히 velocity를 가지고
있기 때문에 gradient = 0 이라도 움직일 수 있습니다.

224
00:24:57,407 --> 00:25:01,524
때문에 local minima를 극복할 수 있게 되고 
계속해서 내려갈 수 있습니다.

225
00:25:01,524 --> 00:25:04,294
 saddle points에서도 비슷한 일이 일어나겠죠

226
00:25:04,294 --> 00:25:11,219
saddle point 주변의 gradient가 작더라도, 굴러내려오는 속도가
있기 때문에 velocity를 가지게 됩니다.

227
00:25:11,219 --> 00:25:16,947
때문에 saddle point를 잘 극복해 내고 계속
밑으로 내려올 수 있는 것입니다.

228
00:25:16,947 --> 00:25:22,434
업데이트가 잘 안되는 경우(poor conditioning) 를 다시
한번 살펴보겠습니다.

229
00:25:22,434 --> 00:25:31,590
아래와 같이 지그재그로 움직이는 상황이라면 
momentum이 이 변동을 서로 상쇄시켜 버립니다.

230
00:25:31,590 --> 00:25:46,491
이를 통해서 loss에 만감한 수직 방향의 변동은 줄여주고
수평방향의 움직임은 점차 가속화 될 것입니다.

231
00:25:46,491 --> 00:25:51,829
momentum을 추가하게 되면 high condition number 
problem을 해결하는 데 도움이 되는 것입니다.

232
00:25:51,829 --> 00:26:05,692
오른쪽을 보면 검은색이 일반 SGD입니다. 지그지그로 움직이죠.
파란색이 Momentum SGD입니다.

233
00:26:05,692 --> 00:26:13,129
Momentum을 추가해서 velocity가 생기면
결국 noise가 평균화되버립니다.

234
00:26:13,129 --> 00:26:20,822
보통의 SGD가 구불구불 움직이는 것에 비해서 momemum은 
minima를 향해서 더 부드럽게 움직입니다.

235
00:26:20,822 --> 00:26:22,017
질문 있나요?

236
00:26:22,017 --> 00:26:26,184
[학생이 질문]

237
00:26:35,261 --> 00:26:40,950
문제는 어떻게 SGD Momemtum이 poorly conditioned 
coordinate 문제를 해결할 수 있는지 입니다.

238
00:26:40,950 --> 00:26:49,610
우선 velocity estimation term에서 velocity가 어떻게
계산되는지를 보면 gradient를 계속해서 더해갑니다.

239
00:26:49,610 --> 00:26:57,088
이는 하이퍼파라미터인 rho에 영향을 받습니다.
그리고 현재 gradient가 상대적으로 작은 값이고

240
00:26:57,088 --> 00:26:59,739
그리고 이 상황에서 rho가 잘 동작한다면

241
00:26:59,739 --> 00:27:05,997
그러면 velocity가 실제 gradient보다 더 커지는
지점 까지 조금씩 증가할 것입니다.

242
00:27:05,997 --> 00:27:10,862
이는  poorly conditioned dimension에서 더 빨리
학습될 수 있도록 도와줍니다.

243
00:27:13,054 --> 00:27:18,505
SGD momentum를 연상할 때 유용한 그림이 하나 있습니다.

244
00:27:18,505 --> 00:27:20,758
빨간 점이 현재 지점입니다.

245
00:27:20,758 --> 00:27:30,560
Red Vector는 현재 지점에서의 gradient의 방향을 나타냅니다.
Green vector는 Velocity vector입니다.

246
00:27:30,560 --> 00:27:36,802
실제 업데이트는(autual step) 이 둘의 가중평균으로
구할 수 있습니다.

247
00:27:36,802 --> 00:27:40,534
이는 gradient의 noise를 극복할 수 있게 해줍니다.

248
00:27:40,534 --> 00:27:48,209
Momentum의 변형이 있습니다. Nesterov accelerated
gradient 인데, Nesterov momentum 라고도 합니다.

249
00:27:48,209 --> 00:27:52,222
계산하는 순서를 조금 바꾼 것입니다.

250
00:27:52,222 --> 00:28:00,770
기본 SGD momentum은 "현재 지점" 에서의 gradient를
계산한 뒤에 velocity와 섞어 줍니다.

251
00:28:00,770 --> 00:28:04,714
Nesterov를 사용하면 방법이 조금 다릅니다.

252
00:28:04,714 --> 00:28:11,250
빨간 점에서 시작해서 우선은 Velocity방향으로 움직입니다.

253
00:28:11,250 --> 00:28:19,217
그리고 그 지점에서의 gradient를 계산합니다. 그리고 다시
원점으로 돌아가서 둘을 합치는 것입니다.

254
00:28:19,217 --> 00:28:21,304


255
00:28:21,304 --> 00:28:23,127
그러나 당신은 당신이 친절하다고 상상할 수 있습니다.

256
00:28:23,127 --> 00:28:26,164
정보를 조금 더 섞는다.

257
00:28:26,164 --> 00:28:28,089
속도 방향이 실제로

258
00:28:28,089 --> 00:28:29,857
약간 잘못 됐어.

259
00:28:29,857 --> 00:28:31,774
조금 더 큰 그라디언트 정보

260
00:28:31,774 --> 00:28:35,187
객관적인 풍경의 일부.

261
00:28:35,187 --> 00:28:37,369
이것은 또한 정말 좋은 이론적 속성을 가지고 있습니다.

262
00:28:37,369 --> 00:28:39,836
볼록 최적화에 관해서는,

263
00:28:39,836 --> 00:28:42,211
그러나 그 보장은 조금 간다.

264
00:28:42,211 --> 00:28:44,295
비 볼록 (non-convex)
문제에 관해서는 창 밖으로 나가라.

265
00:28:44,295 --> 00:28:46,431
신경 네트워크와 유사합니다.

266
00:28:46,431 --> 00:28:48,553
방정식에 그것을 써서, 네 스테 로프 운동량

267
00:28:48,553 --> 00:28:51,546
다음과 같이 보입니다.

268
00:28:51,546 --> 00:28:54,379
우리의 속도를 업데이트하기 위해,

269
00:28:54,379 --> 00:28:56,230
이전의 속도에 따라

270
00:28:56,230 --> 00:28:57,640
저기 그 그라디언트.

271
00:28:57,640 --> 00:29:00,223
이제 우리가 다음 단계를 밟을 때,

272
00:29:01,388 --> 00:29:03,144
우리는 실제로 우리의 속도의 방향으로 나아 간다.

273
00:29:03,144 --> 00:29:04,457
정보를 통합하는 것입니다.

274
00:29:04,457 --> 00:29:06,707
이 여러 점.

275
00:29:06,707 --> 00:29:07,540
질문 있나요?

276
00:29:08,922 --> 00:29:12,842
[학생이 질문]

277
00:29:12,842 --> 00:29:15,228
질문은 velocity의 초기값을 구하는 좋은 방법이 있는지 입니다.

278
00:29:15,228 --> 00:29:17,483
velocity의 초기값은 항상 0입니다.

279
00:29:17,483 --> 00:29:20,581
이는 하이퍼파라미터가 아닙니다. 그저 0으로 둡니다.

280
00:29:20,581 --> 00:29:21,800
다른 질문 있나요?

281
00:29:21,800 --> 00:29:25,967
[학생이 질문]

282
00:29:32,477 --> 00:29:35,817
- 직관적으로 속도는 일종의 가중치 합입니다.

283
00:29:35,817 --> 00:29:38,553
시간 경과에 따라 보았던 그라디언트를

284
00:29:38,553 --> 00:29:41,951
[학생이 질문]

285
00:29:41,951 --> 00:29:44,512
- 최근의 그라데이션이 더 무겁게 적용됩니다.

286
00:29:44,512 --> 00:29:46,947
매 단계마다, 우리는 우리의 오래된 속도를 취합니다.

287
00:29:46,947 --> 00:29:48,559
우리는 마찰로 쇠퇴하고 우리는

288
00:29:48,559 --> 00:29:50,201
우리의 현재 그라디언트.

289
00:29:50,201 --> 00:29:51,417
당신은 이런 생각을 할 수 있습니다.

290
00:29:51,417 --> 00:29:55,147
최근 그라디언트의 부드러운 이동 평균

291
00:29:55,147 --> 00:29:57,511
기하 급수적으로 감퇴하는 종류의

292
00:29:57,511 --> 00:30:00,594
당신의 그라디언트가 시간을 거슬러 올라갈 때.

293
00:30:03,112 --> 00:30:05,359
이 네 스테 로프 배합은 조금 성가시다.

294
00:30:05,359 --> 00:30:07,837
당신이 이것을 보게된다면, 보통 당신이

295
00:30:07,837 --> 00:30:09,933
당신의 손실 기능을 가지고, 당신이 평가하고 싶다.

296
00:30:09,933 --> 00:30:12,117
당신의 손실과 같은 점에서의 그라디언트.

297
00:30:12,117 --> 00:30:14,645
네 스테 로프 (Nesterov)

298
00:30:14,645 --> 00:30:17,259
함께 일하는 것이 약간 짜증납니다.

299
00:30:17,259 --> 00:30:18,852
고맙게도, 귀여운 변수 변경이 있습니다.

300
00:30:18,852 --> 00:30:19,768
넌 할 수있어.

301
00:30:19,768 --> 00:30:20,918
변수를 변경하면

302
00:30:20,918 --> 00:30:22,868
약간 개편하고 개조하면

303
00:30:22,868 --> 00:30:25,230
약간 다른 방식으로 네 스테 로프 추진력을 써라.

304
00:30:25,230 --> 00:30:27,243
이제 다시, 손실을 평가할 수 있습니다.

305
00:30:27,243 --> 00:30:29,877
같은 점에서 항상 그래디언트가 적용됩니다.

306
00:30:29,877 --> 00:30:31,857
이 변수를 변경하면,

307
00:30:31,857 --> 00:30:34,578
당신은 네 스테 로프에 대한 좋은 해석을 얻습니다.

308
00:30:34,578 --> 00:30:36,880
그것은 첫 번째 단계에서 여기있는 것입니다.

309
00:30:36,880 --> 00:30:39,371
이것은 속도를 업데이트하는 것과 정확히 같습니다.

310
00:30:39,371 --> 00:30:42,224
바닐라 SGD 모멘텀 사례에서 우리는

311
00:30:42,224 --> 00:30:44,493
우리의 현재 속도를 가지고, 우리는 그라디언트를 평가한다.

312
00:30:44,493 --> 00:30:47,081
현재 시점에서

313
00:30:47,081 --> 00:30:48,663
부패하는 방식으로.

314
00:30:48,663 --> 00:30:50,977
이제 두 번째 업데이트에서 이제 실제로

315
00:30:50,977 --> 00:30:52,436
우리의 매개 변수 벡터를 업데이트합니다.

316
00:30:52,436 --> 00:30:55,199
두 번째 방정식에서 우리는 현재의

317
00:30:55,199 --> 00:30:58,077
현재 속도 플러스

318
00:30:58,077 --> 00:31:00,245
우리의 현재 속도

319
00:31:00,245 --> 00:31:01,939
우리의 이전 속도.

320
00:31:01,939 --> 00:31:06,051
여기, 네 스테 로프 추진력은 일종의 통합입니다.

321
00:31:06,051 --> 00:31:07,673
어떤 종류의 오류 수정 기간

322
00:31:07,673 --> 00:31:11,756
현재 속도와 이전 속도.

323
00:31:13,514 --> 00:31:15,211
SGD, SGD 모멘텀을 살펴보면

324
00:31:15,211 --> 00:31:17,230
이런 종류의 단순한 네 스테 로프 추진력

325
00:31:17,230 --> 00:31:20,730
문제는 SGD와 비교할 때

326
00:31:21,827 --> 00:31:24,145
SGD 종류는 이걸 취하고 SGD는 검정색입니다.

327
00:31:24,145 --> 00:31:26,831
이 느린 진보를 극한쪽으로 가져가는 종류.

328
00:31:26,831 --> 00:31:28,734
파란색과 녹색은 기세를 보여줍니다.

329
00:31:28,734 --> 00:31:30,083
네 스테 로프.

330
00:31:30,083 --> 00:31:32,235
이것들은 오버 슈트 (overshooting)와
같은 이런 행동을한다.

331
00:31:32,235 --> 00:31:35,492
그들이 속도를 키우고있는 최소한의 원인

332
00:31:35,492 --> 00:31:37,288
최소값을 지나서

333
00:31:37,288 --> 00:31:39,302
스스로 고쳐서 돌아 오는

334
00:31:39,302 --> 00:31:40,334
최소쪽으로.

335
00:31:40,334 --> 00:31:41,167
문제?

336
00:31:42,508 --> 00:31:46,675
- [학생] [듣기에는 너무 낮게 말하기]

337
00:31:52,509 --> 00:31:54,849
- 문제는이 사진이 좋아 보인다는 것입니다.

338
00:31:54,849 --> 00:31:56,470
하지만 미니 마가 전화하면 어떻게 될까요?

339
00:31:56,470 --> 00:31:58,535
이 좁은 대야에있다?

340
00:31:58,535 --> 00:32:00,019
속도가 당신의 원인일까요?

341
00:32:00,019 --> 00:32:02,012
그 미니 마를 건너 뛰는거야?

342
00:32:02,012 --> 00:32:03,461
사실 그것은 정말로 흥미로운 점입니다.

343
00:32:03,461 --> 00:32:05,717
그리고 최근의 이론적 연구의 주제는,

344
00:32:05,717 --> 00:32:07,969
하지만 그 아이디어는 아마

345
00:32:07,969 --> 00:32:09,556
날카로운 미니 마는 사실 나쁜 미니 마입니다.

346
00:32:09,556 --> 00:32:11,778
우리는 그것들에 착륙하기를 원하지 않는다.

347
00:32:11,778 --> 00:32:13,660
왜냐하면 그 아이디어는 아마도

348
00:32:13,660 --> 00:32:15,525
매우 날카로운 최소치.

349
00:32:15,525 --> 00:32:18,086
더 많이 어울리는 미니 마라.

350
00:32:18,086 --> 00:32:20,356
우리가 훈련 세트를 두 배로 늘렸다 고 상상한다면,

351
00:32:20,356 --> 00:32:22,511
전체 최적화 환경이 바뀔 것입니다.

352
00:32:22,511 --> 00:32:24,083
그리고 아마도 매우 민감한 미니 마

353
00:32:24,083 --> 00:32:25,970
우리가 수집한다면 실제로 사라질거야.

354
00:32:25,970 --> 00:32:27,905
더 많은 훈련 데이터.

355
00:32:27,905 --> 00:32:29,279
우리는 이런 직관을 가지고 있습니다.

356
00:32:29,279 --> 00:32:31,674
어쩌면 매우 평평한 최소치에 착륙하기를 원할지도 모른다.

357
00:32:31,674 --> 00:32:33,904
왜냐하면 그 매우 평평한 최소치는 아마

358
00:32:33,904 --> 00:32:36,418
교육 자료를 변경함에 따라 더욱 강력 해졌습니다.

359
00:32:36,418 --> 00:32:38,354
평평한 최소치는 실제로 일반화 될 수 있습니다.

360
00:32:38,354 --> 00:32:40,938
데이터를 테스트하는 것이 좋습니다.

361
00:32:40,938 --> 00:32:42,453
이것은 다시 아주 일종의

362
00:32:42,453 --> 00:32:44,907
이론적 인 작업이지만 실제로는

363
00:32:44,907 --> 00:32:46,769
네가 가져다주는 정말 좋은 점이야.

364
00:32:46,769 --> 00:32:48,869
어떤 의미에서는 실제로는 기능입니다.

365
00:32:48,869 --> 00:32:51,922
실제로 SGD의 추진력이 아닌 버그

366
00:32:51,922 --> 00:32:54,839
그 매우 날카로운 미니 마를 건너 뜁니다.

367
00:32:56,464 --> 00:33:00,464
그것은 실제로 좋은 것입니다, 믿거 나 말거나.

368
00:33:01,310 --> 00:33:02,717
당신이 볼 수있는 또 다른 일은 당신이 보는 것이다.

369
00:33:02,717 --> 00:33:04,801
기세와 네 스테 로프의 차이점에서,

370
00:33:04,801 --> 00:33:06,832
너는 그것 때문에 볼 수있다.

371
00:33:06,832 --> 00:33:09,247
Nesterov의 수정 계수, 아마도 그렇지 않습니다.

372
00:33:09,247 --> 00:33:10,783
상당히 크게 오버 슈팅 (overshooting)

373
00:33:10,783 --> 00:33:13,200
바닐라 운동량에 비해.

374
00:33:15,168 --> 00:33:17,714
일반적인 최적화의 또 다른 종류가 있습니다.

375
00:33:17,714 --> 00:33:20,553
전략은 AdaGrad라고하는이 알고리즘입니다.

376
00:33:20,553 --> 00:33:23,172
이 교수 인 존 듀치 (John Duchi)

377
00:33:23,172 --> 00:33:25,777
박사 과정에서 일했다.

378
00:33:25,777 --> 00:33:28,860
AdaGrad의 아이디어는 당신과 마찬가지로,

379
00:33:30,766 --> 00:33:32,453
최적화 과정 중에,

380
00:33:32,453 --> 00:33:35,371
너는 계속 견적을 유지하려고한다.

381
00:33:35,371 --> 00:33:37,690
또는 모든 제곱 된 그래디언트의 합계

382
00:33:37,690 --> 00:33:40,054
당신이 훈련 도중 보는 것을.

383
00:33:40,054 --> 00:33:42,039
이제는 속도 용어를 갖는 것보다는,

384
00:33:42,039 --> 00:33:44,442
대신에 우리는이 2 학기의 학기를 가지고 있습니다.

385
00:33:44,442 --> 00:33:46,544
교육을하는 동안 계속해서

386
00:33:46,544 --> 00:33:49,684
이 제곱 된 기울기의 제곱 된 기울기.

387
00:33:49,684 --> 00:33:52,378
이제 매개 변수 벡터를 업데이트 할 때,

388
00:33:52,378 --> 00:33:55,545
우리는이 2 학기 제곱 학기로 나눌거야.

389
00:33:57,430 --> 00:33:59,819
우리가 업데이트 단계를 수행 할 때.

390
00:33:59,819 --> 00:34:02,688
문제는 이러한 종류의 스케일링이 무엇인가하는 것입니다.

391
00:34:02,688 --> 00:34:05,521
우리가 가진이 상황에서해라.

392
00:34:06,653 --> 00:34:08,878
매우 높은 조건 번호?

393
00:34:08,878 --> 00:34:13,045
- [학생] [듣기에는 너무 낮게 말하기]

394
00:34:16,741 --> 00:34:18,791
- 우리가 두 좌표를 가지고 있다면,

395
00:34:18,791 --> 00:34:21,214
항상 매우 높은 그라디언트를 가진

396
00:34:21,214 --> 00:34:23,389
그리고 항상 아주 작은 그라디언트를 가진 하나,

397
00:34:23,389 --> 00:34:25,007
그 다음 우리가 제곱의 합을 더할 때

398
00:34:25,007 --> 00:34:27,126
작은 그라디언트의 경우, 우리는

399
00:34:27,127 --> 00:34:30,697
적은 수로, 우리는 운동을 가속화 할 것입니다.

400
00:34:30,697 --> 00:34:33,609
느린 차원을 따라,

401
00:34:33,610 --> 00:34:35,666
한 차원을 따라

402
00:34:35,666 --> 00:34:37,812
그런 다음 다른 차원을 따라, 그라디언트

403
00:34:37,812 --> 00:34:40,728
매우 큰 경향이 있습니다. 그렇다면 우리는

404
00:34:40,728 --> 00:34:42,926
큰 숫자로, 그래서 우리는 좀 천천히

405
00:34:42,927 --> 00:34:46,409
흔들 거리는 차원에 따른 우리의 진보.

406
00:34:46,409 --> 00:34:48,514
그러나 여기에는 문제가 있습니다.

407
00:34:48,514 --> 00:34:50,942
이것이 AdaGrad가 어떻게되는지에 대한 질문입니다.

408
00:34:50,943 --> 00:34:52,976
훈련 과정 동안, t

409
00:34:52,976 --> 00:34:56,579
크고 크고 커지면?

410
00:34:56,579 --> 00:34:58,876
- [학생] [듣기에는 너무 낮게 말하기]

411
00:34:58,876 --> 00:35:00,891
- AdaGrad를 사용하면 단계가 실제로 작아집니다.

412
00:35:00,891 --> 00:35:02,724
작고 작아서 우리는 단지

413
00:35:02,724 --> 00:35:04,501
이 추정치를 계속 업데이트하십시오.

414
00:35:04,501 --> 00:35:06,645
시간 경과에 따른 제곱 된 구배이므로이 추정치

415
00:35:06,645 --> 00:35:08,896
단조롭게 성장하고 단조롭게 성장하다

416
00:35:08,896 --> 00:35:10,380
훈련 과정에서.

417
00:35:10,380 --> 00:35:12,962
이제 이것이 우리의 계단 크기를 더 작게 만듭니다.

418
00:35:12,962 --> 00:35:15,844
시간이 지남에 따라 점점 작아집니다.

419
00:35:15,844 --> 00:35:18,038
다시 볼록한 경우에는

420
00:35:18,038 --> 00:35:20,819
이것이 실제로 있다는 것을 보여주는 정말 좋은 이론입니다.

421
00:35:20,819 --> 00:35:23,965
볼록 케이스에서 정말 좋은 원인,

422
00:35:23,965 --> 00:35:25,812
당신이 최소한 접근하면, 당신은 원하는 것입니다.

423
00:35:25,812 --> 00:35:28,610
속도를 늦추어 실제로 수렴합니다.

424
00:35:28,610 --> 00:35:30,404
그것은 실제로 일종의 기능입니다.

425
00:35:30,404 --> 00:35:31,677
볼록한 경우.

426
00:35:31,677 --> 00:35:33,865
그러나 볼록하지 않은 경우에는 약간 그렇습니다.

427
00:35:33,865 --> 00:35:36,562
문제가되는 이유는

428
00:35:36,562 --> 00:35:38,694
안장 점, 당신은 AdaGrad와 붙어 있을지도 모릅니다.

429
00:35:38,694 --> 00:35:42,492
그러면 더 이상 진전을 이루지 못할 것입니다.

430
00:35:42,492 --> 00:35:44,773
AdaGrad에는 약간의 변형이 있습니다.

431
00:35:44,773 --> 00:35:47,370
실제로 주소를 지정하는 RMSProp

432
00:35:47,370 --> 00:35:49,163
이 우려는 조금 있습니다.

433
00:35:49,163 --> 00:35:52,006
이제 RMSProp을 사용하여이 견적을 계속 유지합니다.

434
00:35:52,006 --> 00:35:53,875
제곱 된 그라데이션 대신에

435
00:35:53,875 --> 00:35:55,428
그 제곱 된 견적을 그냥 내버려둬.

436
00:35:55,428 --> 00:35:57,595
훈련 기간 동안 지속적으로 축적되며,

437
00:35:57,595 --> 00:36:01,570
대신에 우리는 제곱 된 추정치가 실제로 붕괴되도록합니다.

438
00:36:01,570 --> 00:36:03,969
이것은 기세 갱신과 같은 종류를 보는 것을 끝내고,

439
00:36:03,969 --> 00:36:06,057
우리가 기세의 종류를 가지고있는 것을 제외하고는

440
00:36:06,057 --> 00:36:08,114
운동량보다는 제곱 된 구배

441
00:36:08,114 --> 00:36:09,825
실제 그라디언트 이상.

442
00:36:09,825 --> 00:36:12,872
이제 RMSProp을 사용하여 그라디언트를 계산 한 후,

443
00:36:12,872 --> 00:36:15,354
우리는 grad square의 현재 추정치를 취합니다.

444
00:36:15,354 --> 00:36:16,983
우리는이 붕괴율로 곱합니다.

445
00:36:16,983 --> 00:36:20,846
일반적으로 .9 또는 .99와 비슷합니다.

446
00:36:20,846 --> 00:36:24,477
그런 다음이 값에서 감쇠율을 뺀 값을 더합니다.

447
00:36:24,477 --> 00:36:27,086
현재 제곱 된 그라디언트의

448
00:36:27,086 --> 00:36:30,866
이제 시간이 지남에 따라 상상할 수 있습니다.

449
00:36:30,866 --> 00:36:32,814
그런 다음 다시 한 번 우리가 한 발짝 내딛 으면

450
00:36:32,814 --> 00:36:36,080
AdaGrad와 정확히 똑같아 보입니다.

451
00:36:36,080 --> 00:36:37,678
여기서 우리는 제곱 된 구배로 나눕니다.

452
00:36:37,678 --> 00:36:39,592
이 좋은 재산을 다시 갖기위한 단계에서

453
00:36:39,592 --> 00:36:42,080
하나의 차원을 따라 가속 이동의,

454
00:36:42,080 --> 00:36:44,555
다른 차원을 따라 움직임을 느리게합니다.

455
00:36:44,555 --> 00:36:46,609
그러나 이제 RMSProp을 사용하면

456
00:36:46,609 --> 00:36:49,614
새어 나오면 문제가 해결됩니다.

457
00:36:49,614 --> 00:36:51,396
어쩌면 항상 너를 둔화시키는

458
00:36:51,396 --> 00:36:52,896
원하지 않을 수도 있습니다.

459
00:36:56,940 --> 00:36:58,964
여기서 다시 우리는 우리가 가장
좋아하는 것을 보여주고 있습니다.

460
00:36:58,964 --> 00:37:01,817
SGD의 장난감 문제, SGD 모멘텀

461
00:37:01,817 --> 00:37:04,658
파란색으로 표시하고 RMSProp을 빨간색으로 표시합니다.

462
00:37:04,658 --> 00:37:07,807
RMSProp 및 SGD 모멘텀을 확인할 수 있습니다.

463
00:37:07,807 --> 00:37:09,965
SGD보다 훨씬 잘하고 있습니다.

464
00:37:09,965 --> 00:37:12,748
그러나 그들의 질적 행동은 조금 다릅니다.

465
00:37:12,748 --> 00:37:16,005
SGD 모멘텀으로 인해 오버 슛이 발생했습니다.

466
00:37:16,005 --> 00:37:17,973
최소값은 되돌아 오는 반면,

467
00:37:17,973 --> 00:37:21,061
RMSProp, 일종의 조정입니다.

468
00:37:21,061 --> 00:37:22,914
우리가 만들고있는 것과 같은 궤적

469
00:37:22,914 --> 00:37:24,549
사이에 거의 같은 진보

470
00:37:24,549 --> 00:37:26,877
모든 차원.

471
00:37:26,877 --> 00:37:28,544
그건 그렇고, 당신은 실제로 말할 수 없습니다.

472
00:37:28,544 --> 00:37:32,967
하지만이 음모는 녹색으로
AdaGrad를 보여주고 있습니다.

473
00:37:32,967 --> 00:37:34,897
같은 학습 속도로,하지만 그냥

474
00:37:34,897 --> 00:37:37,244
끊임없이이 문제로 인해 붙어 다닙니다.

475
00:37:37,244 --> 00:37:39,091
쇠퇴하는 학습 속도.

476
00:37:39,091 --> 00:37:41,354
실제로 AdaGrad는 그렇게
일반적이지 않을 수 있습니다.

477
00:37:41,354 --> 00:37:43,437
이 많은 것들을 위해.

478
00:37:44,276 --> 00:37:45,874
그것은 약간의 불공정 한 비교입니다.

479
00:37:45,874 --> 00:37:46,877
AdaGrad.

480
00:37:46,877 --> 00:37:48,715
아마 당신은 학습 속도를 증가시켜야 할 것입니다.

481
00:37:48,715 --> 00:37:50,495
AdaGrad와 만나고, 그러면 결국

482
00:37:50,495 --> 00:37:53,043
이 경우에는 RMSProp과 비슷합니다.

483
00:37:53,043 --> 00:37:55,639
그러나 일반적으로 AdaGrad를 사용하지 않는 경향이 있습니다.

484
00:37:55,639 --> 00:37:57,633
순전히 신경 네트워크를 훈련 할 때.

485
00:37:57,633 --> 00:37:58,466
문제?

486
00:37:58,466 --> 00:38:00,281
- [학생] [듣기에는 너무 낮게 말하기]

487
00:38:00,281 --> 00:38:03,455
- 대답은 예입니다.이 문제는 볼록합니다.

488
00:38:03,455 --> 00:38:04,872
그러나이 경우,

489
00:38:07,631 --> 00:38:09,128
그것은 불공평 한 비교의 조금이다.

490
00:38:09,128 --> 00:38:11,315
학습 속도가 그렇게 비슷하지 않기 때문에

491
00:38:11,315 --> 00:38:12,488
방법 중.

492
00:38:12,488 --> 00:38:14,114
나는 AdaGrad에게 조금 불공평하다.

493
00:38:14,114 --> 00:38:15,892
이 시각화에서 같은 것을 보여줌으로써

494
00:38:15,892 --> 00:38:17,775
서로 다른 알고리즘 간의 학습 속도,

495
00:38:17,775 --> 00:38:20,284
아마도 당신이 별도로해야 할 때

496
00:38:20,284 --> 00:38:23,617
알고리즘 당 학습 률을 돌 렸습니다.

497
00:38:28,455 --> 00:38:29,921
우리는 기세로 보았고, 우리는이 아이디어를 가지고있었습니다.

498
00:38:29,921 --> 00:38:32,203
우리가 속도를 키우고있는 속도의

499
00:38:32,203 --> 00:38:34,249
그라디언트를 추가 한 다음 스테핑

500
00:38:34,249 --> 00:38:35,888
속도 방향으로.

501
00:38:35,888 --> 00:38:38,200
AdaGrad와 RMSProp을 통해 우리는

502
00:38:38,200 --> 00:38:40,279
견적을 세우는 다른 아이디어가 있습니다.

503
00:38:40,279 --> 00:38:42,296
제곱 된 그라디언트를

504
00:38:42,296 --> 00:38:44,229
제곱 된 그라디언트에 의해.

505
00:38:44,229 --> 00:38:46,252
그럼이 두 가지 모두 좋은 아이디어처럼 보입니다.

506
00:38:46,252 --> 00:38:47,143
자신에.

507
00:38:47,143 --> 00:38:48,439
왜 우리는 함께 붙지 않는거야?

508
00:38:48,439 --> 00:38:49,465
둘 다 사용합니까?

509
00:38:49,465 --> 00:38:51,383
어쩌면 그게 더 나아질거야.

510
00:38:51,383 --> 00:38:53,796
이것은 Adam이라는 알고리즘에 우리를 데려옵니다.

511
00:38:53,796 --> 00:38:57,226
오히려 우리를 아담과 매우 가깝게 이끌 것입니다.

512
00:38:57,226 --> 00:38:59,812
우리는 몇 장의 슬라이드에서 약간의

513
00:38:59,812 --> 00:39:01,604
우리가 여기서 교정 할 필요가 있습니다.

514
00:39:01,604 --> 00:39:03,914
여기 아담과 함께 우리는 견적을 유지합니다.

515
00:39:03,914 --> 00:39:07,373
첫 번째 순간과 두 번째 순간.

516
00:39:07,373 --> 00:39:10,706
이제 빨간색으로, 우리는이 견적을 만든다.

517
00:39:10,706 --> 00:39:13,654
첫 번째 순간의 무게로

518
00:39:13,654 --> 00:39:15,152
우리 그라디언트의.

519
00:39:15,152 --> 00:39:18,385
우리는 두 번째 순간에 대한이
움직이는 추정을 가지고 있습니다.

520
00:39:18,385 --> 00:39:20,904
AdaGrad 및 RMSProp과 유사합니다.

521
00:39:20,904 --> 00:39:23,226
우리의 제곱 된 그라디언트의 움직이는 추정치.

522
00:39:23,226 --> 00:39:27,032
이제 업데이트 단계를 수행 할 때

523
00:39:27,032 --> 00:39:29,106
일종의 첫 번째 순간을 사용합니다.

524
00:39:29,106 --> 00:39:31,680
속도, 또한 두 번째 순간으로 나눕니다.

525
00:39:31,680 --> 00:39:34,766
또는 오히려 두 번째 순간의 제곱근,

526
00:39:34,766 --> 00:39:37,766
이 제곱 된 기울기 용어입니다.

527
00:39:38,613 --> 00:39:40,302
아담에 대한이 생각은 조금만 끝나 봅니다.

528
00:39:40,302 --> 00:39:43,064
RMSProp 플러스 기세처럼, 또는 결국

529
00:39:43,064 --> 00:39:46,754
운동량 플러스 두 번째 제곱 된 그라디언트처럼 보입니다.

530
00:39:46,754 --> 00:39:50,304
그것은 두 종류의 좋은 특성을 통합합니다.

531
00:39:50,304 --> 00:39:52,474
그러나 여기에 약간의 문제가 있습니다.

532
00:39:52,474 --> 00:39:55,227
그것이 무슨 일이 일어나는가의 문제입니다.

533
00:39:55,227 --> 00:39:57,560
처음부터?

534
00:40:00,683 --> 00:40:02,492
맨 처음 단계에서 볼 수 있습니다.

535
00:40:02,492 --> 00:40:04,536
처음에는 우리가 초기화했습니다.

536
00:40:04,536 --> 00:40:06,619
우리의 두 번째 순간은 0입니다.

537
00:40:06,619 --> 00:40:10,119
이제 두 번째 순간을 한 번 업데이트 한 후,

538
00:40:11,551 --> 00:40:14,007
일반적으로이 베타 2, 두 번째 순간

539
00:40:14,007 --> 00:40:17,038
붕괴 속도는 .9 또는 .99와 비슷하지만,

540
00:40:17,038 --> 00:40:18,720
뭔가 하나에 아주 가깝습니다.

541
00:40:18,720 --> 00:40:21,556
한 번의 업데이트 후 두 번째 순간은 여전히

542
00:40:21,556 --> 00:40:23,352
매우 0에 매우 가깝습니다.

543
00:40:23,352 --> 00:40:25,751
이제 우리가 여기서 업데이트 단계를 수행 할 때

544
00:40:25,751 --> 00:40:27,906
우리는 두 번째 순간으로 나눕니다.

545
00:40:27,906 --> 00:40:30,224
이제 우리는 아주 작은 숫자로 나누고 있습니다.

546
00:40:30,224 --> 00:40:31,681
우리는 매우 큰 단계를 만들고 있습니다.

547
00:40:31,681 --> 00:40:32,862
처음에는

548
00:40:32,862 --> 00:40:35,189
처음에는이 매우 큰 단계였습니다.

549
00:40:35,189 --> 00:40:38,253
문제의 기하학 때문이 아닙니다.

550
00:40:38,253 --> 00:40:40,189
우리가 우리가

551
00:40:40,189 --> 00:40:43,907
초기화 된 두 번째 모멘트 추정치는 0입니다.

552
00:40:43,907 --> 00:40:44,807
문제?

553
00:40:44,807 --> 00:40:48,974
- [학생] [듣기에는 너무 낮게 말하기]

554
00:40:53,317 --> 00:40:55,165
- 사실이에요.

555
00:40:55,165 --> 00:40:56,573
의견은 만약 당신의 첫 번째 순간

556
00:40:56,573 --> 00:40:58,973
또한 매우 작습니다, 그렇다면 당신은

557
00:40:58,973 --> 00:41:00,850
작아서 제곱근으로 나눕니다.

558
00:41:00,850 --> 00:41:03,391
작은 제곱의, 그래서 무슨 일이 일어날 것인가?

559
00:41:03,391 --> 00:41:06,231
그들은 서로를 취소 할 수도 있고 괜찮을 수도 있습니다.

560
00:41:06,231 --> 00:41:07,738
사실입니다.

561
00:41:07,738 --> 00:41:09,366
때때로 이들은 서로를 취소합니다.

562
00:41:09,366 --> 00:41:11,616
그리고 너는 괜찮아.하지만 때로는이게 끝난다.

563
00:41:11,616 --> 00:41:14,117
처음부터 매우 큰 단계를 밟았습니다.

564
00:41:14,117 --> 00:41:16,730
그것은 아주 나쁠 수 있습니다.

565
00:41:16,730 --> 00:41:18,762
어쩌면 당신은 조금 제대로 초기화하지 않을 것입니다.

566
00:41:18,762 --> 00:41:20,018
당신은 매우 큰 걸음을 내딛습니다.

567
00:41:20,018 --> 00:41:21,917
이제 초기화가 완전히 엉망이되었습니다.

568
00:41:21,917 --> 00:41:23,171
그리고 너는 아주 나쁜 부분에있어.

569
00:41:23,171 --> 00:41:24,500
객관적 풍경의

570
00:41:24,500 --> 00:41:26,630
거기에서 수렴.

571
00:41:26,630 --> 00:41:27,650
문제?

572
00:41:27,650 --> 00:41:31,400
- [학생] [듣기에는 너무 낮게 말하기]

573
00:41:31,400 --> 00:41:32,779
- 생각은이게 뭐야?

574
00:41:32,779 --> 00:41:35,616
마지막 방정식에서 마이너스 7 항에?

575
00:41:35,616 --> 00:41:37,016
실제로 AdaGrad에 나타났습니다.

576
00:41:37,016 --> 00:41:38,332
RMSProp과 Adam.

577
00:41:38,332 --> 00:41:40,728
아이디어는 우리가 무언가로 나누고 있다는 것입니다.

578
00:41:40,728 --> 00:41:42,672
우리는 우리가 0으로 나누지 않고
있는지 확인하기를 원합니다.

579
00:41:42,672 --> 00:41:44,510
그래서 우리는 항상 작은 양의 상수

580
00:41:44,510 --> 00:41:46,058
분모에게

581
00:41:46,058 --> 00:41:49,094
우리는 0으로 나눈 것이 아닙니다.

582
00:41:49,094 --> 00:41:50,094
그것은 기술적으로 하이퍼 매개 변수입니다.

583
00:41:50,094 --> 00:41:51,807
그러나 그것은별로 중요하지 않은 경향이 있습니다.

584
00:41:51,807 --> 00:41:53,262
10을 마이너스 7로 설정하면,

585
00:41:53,262 --> 00:41:54,914
10에서 8까지, 그런 식으로,

586
00:41:54,914 --> 00:41:56,497
잘 작동하는 경향이있다.

587
00:41:58,452 --> 00:42:00,713
Adam과 함께 우리가 방금 이야기 한 것을 기억하십시오.

588
00:42:00,713 --> 00:42:02,585
첫 번째 몇 단계에서의이 아이디어는

589
00:42:02,585 --> 00:42:03,791
그것은 매우 커지고, 우리는

590
00:42:03,791 --> 00:42:05,996
매우 큰 계단과 혼란.

591
00:42:05,996 --> 00:42:08,378
Adam은이 편향 보정 용어도 추가합니다.

592
00:42:08,378 --> 00:42:11,074
매우 큰 단계를 취하는이 문제를 피하기 위해

593
00:42:11,074 --> 00:42:12,995
처음에는

594
00:42:12,995 --> 00:42:15,129
처음으로 업데이트 한 것을 볼 수 있습니다.

595
00:42:15,129 --> 00:42:17,775
그리고 두 번째 순간, 우리는 비 편향 추정

596
00:42:17,775 --> 00:42:21,196
통합하여 첫 번째와 두 번째 순간의

597
00:42:21,196 --> 00:42:23,104
현재 시간 스텝, t.

598
00:42:23,104 --> 00:42:24,662
이제 우리는 실제로 이것을 사용하여 단계를 밟습니다.

599
00:42:24,662 --> 00:42:27,749
원래의 추정치보다 편견없는 추정치

600
00:42:27,749 --> 00:42:30,035
1 차 및 2 차 추정치.

601
00:42:30,035 --> 00:42:33,652
이것은 우리에게 완전한 형태의 아담을줍니다.

602
00:42:33,652 --> 00:42:37,106
그건 그렇고, Adam은 정말로, [웃음] 정말 좋습니다.

603
00:42:37,106 --> 00:42:39,177
최적화 알고리즘, 그리고 그것은 정말 잘 작동합니다.

604
00:42:39,177 --> 00:42:41,342
다른 문제가 많아서 일종의

605
00:42:41,342 --> 00:42:43,708
그냥 내 기본 최적화 알고리즘에 대한

606
00:42:43,708 --> 00:42:46,035
새로운 문제.

607
00:42:46,035 --> 00:42:48,445
특히 베타 1을 .9와 같게 설정하면,

608
00:42:48,445 --> 00:42:51,599
베타 2는 .999와 같음, 학습 속도는 1 e

609
00:42:51,599 --> 00:42:53,573
마이너스 3 또는 5 전자 마이너스 4,

610
00:42:53,573 --> 00:42:55,426
바로 그 점에 대한 큰 시선입니다.

611
00:42:55,426 --> 00:42:59,282
내가 사용 해본 모든 아키텍처.

612
00:42:59,282 --> 00:43:00,473
시도해 봐.

613
00:43:00,473 --> 00:43:04,003
일반적으로 시작할 수있는 좋은 장소입니다.

614
00:43:04,003 --> 00:43:06,434
[웃음]

615
00:43:06,434 --> 00:43:07,798
우리가 실제로 이런 것들을 계획한다면

616
00:43:07,798 --> 00:43:09,780
SGD, SGD 모멘텀,

617
00:43:09,780 --> 00:43:12,119
RMSProp과 Adam은 같은 문제에 대해,

618
00:43:12,119 --> 00:43:14,355
당신은 보라색의 아담이 여기에 있음을 볼 수 있습니다,

619
00:43:14,355 --> 00:43:16,869
SGD 모멘텀의 요소를 결합한 것

620
00:43:16,869 --> 00:43:18,579
및 RMSProp.

621
00:43:18,579 --> 00:43:20,467
최소한의 Adam 종류의 오버 슛

622
00:43:20,467 --> 00:43:23,430
SGD 추진력과 조금 비슷하지만

623
00:43:23,430 --> 00:43:25,660
운동량만큼 오버 슛.

624
00:43:25,660 --> 00:43:27,610
아담도 이와 비슷한 행동을합니다.

625
00:43:27,610 --> 00:43:30,007
커브하려고하는 종류의 RMSProp

626
00:43:30,007 --> 00:43:33,753
모든 차원에서 동등한 진전을 이룰 수 있습니다.

627
00:43:33,753 --> 00:43:35,519
어쩌면이 작은 2 차원 예제에서,

628
00:43:35,519 --> 00:43:38,191
Adam은 다른 것과 비슷하게 수렴했습니다.

629
00:43:38,191 --> 00:43:39,483
하지만 질적으로 볼 수 있습니다.

630
00:43:39,483 --> 00:43:41,567
그것은 두 종류의 행동을 결합시키는 종류입니다.

631
00:43:41,567 --> 00:43:43,317
운동량 및 RMSProp.

632
00:43:45,527 --> 00:43:49,194
최적화 알고리즘에 대한 질문이 있으십니까?

633
00:43:50,533 --> 00:43:52,830
- [학생] [듣기에는 너무 낮게 말하기]

634
00:43:52,830 --> 00:43:54,657
그들은 여전히 훈련하는데 아주 오랜 시간이 걸립니다.

635
00:43:54,657 --> 00:43:57,091
[듣기에는 너무 낮게 말한다]

636
00:43:57,091 --> 00:43:59,289
- 아담이 고치지 않는 것은 무엇인가?

637
00:43:59,289 --> 00:44:00,595
이 신경 회로망은 여전히 커질 것인가?

638
00:44:00,595 --> 00:44:03,678
그들은 여전히 훈련하는데 오랜 시간이 걸린다.

639
00:44:05,229 --> 00:44:07,583
여전히 문제가있을 수 있습니다.

640
00:44:07,583 --> 00:44:09,464
이 풍경이있는이 그림에서

641
00:44:09,464 --> 00:44:12,464
당신이 상상한다면 타원처럼 보이는 것들을

642
00:44:12,464 --> 00:44:15,659
우리는

643
00:44:15,659 --> 00:44:18,004
각 차원은 독립적으로 우리를 허용합니다.

644
00:44:18,004 --> 00:44:19,704
다른 속도에 따라 속도를 높이거나 낮추는 것

645
00:44:19,704 --> 00:44:22,389
좌표 축이 있지만 한 가지 문제는

646
00:44:22,389 --> 00:44:24,641
그 타코 쉘이 일종의 기울이면

647
00:44:24,641 --> 00:44:27,061
축에 정렬되어 있지 않으면 우리는 여전히

648
00:44:27,061 --> 00:44:29,205
개별 축을 따라 견적을내는 것만

649
00:44:29,205 --> 00:44:30,372
독립적으로

650
00:44:31,420 --> 00:44:33,602
너의 회전시킨 것에 상응한다.

651
00:44:33,602 --> 00:44:35,605
타코 껍데기와 수평으로 그것을 squishing

652
00:44:35,605 --> 00:44:38,616
세로로 표시 할 수 있지만 실제로 회전 할 수는 없습니다.

653
00:44:38,616 --> 00:44:41,347
이런 종류의 경우

654
00:44:41,347 --> 00:44:44,091
불량 컨디셔닝,

655
00:44:44,091 --> 00:44:45,884
그 다음 Adam 또는 기타 알고리즘

656
00:44:45,884 --> 00:44:49,217
정말로 그 우려를 해결할 수는 없습니다.

657
00:44:51,841 --> 00:44:54,046
우리가 보아 왔던 또 다른 것

658
00:44:54,046 --> 00:44:56,229
이러한 최적화 알고리즘은 학습 속도입니다.

659
00:44:56,229 --> 00:44:58,191
하이퍼 파라미터로.

660
00:44:58,191 --> 00:45:00,111
우리는 두 번 전에이 그림을 보았습니다.

661
00:45:00,111 --> 00:45:02,313
당신이 다른 학습 속도를 사용할 때,

662
00:45:02,313 --> 00:45:04,298
때로는 너무 높으면 폭발 할 수도 있습니다.

663
00:45:04,298 --> 00:45:05,582
노란색으로.

664
00:45:05,582 --> 00:45:08,077
매우 낮은 학습률 인 경우 파란색으로,

665
00:45:08,077 --> 00:45:10,114
수렴하는 데 오랜 시간이 걸릴 수 있습니다.

666
00:45:10,114 --> 00:45:11,251
오른쪽을 선택하는 것은 까다로운 일입니다.

667
00:45:11,251 --> 00:45:12,418
학습 속도.

668
00:45:14,197 --> 00:45:15,571
이것은 약간의 트릭 질문입니다.

669
00:45:15,571 --> 00:45:17,066
우리는 실제로 붙들어서는 안되기 때문에

670
00:45:17,066 --> 00:45:18,540
코스 전체에 하나의 학습 속도

671
00:45:18,540 --> 00:45:19,793
훈련.

672
00:45:19,793 --> 00:45:21,832
때로는 사람들이 학습 률을 떨어 뜨리는 것을 보게됩니다.

673
00:45:21,832 --> 00:45:24,546
우리가 결합 할 수있는 시간이 지남에

674
00:45:24,546 --> 00:45:27,323
이 다른 곡선의 효과

675
00:45:27,323 --> 00:45:30,190
왼쪽에, 그리고 각각의 좋은 속성을 얻을.

676
00:45:30,190 --> 00:45:31,907
때로는 더 높은 학습 률로 시작할 것입니다.

677
00:45:31,907 --> 00:45:34,395
훈련 시작 근처, 그리고 나서 붕괴

678
00:45:34,395 --> 00:45:35,588
학습 속도와 그것을 더 작게 만든다.

679
00:45:35,588 --> 00:45:39,851
훈련 과정 전반에 걸쳐 작아진다.

680
00:45:39,851 --> 00:45:42,691
이것들에 대한 몇 가지 전략은 단계적 부식이 될 것이며,

681
00:45:42,691 --> 00:45:45,483
100,000 번째 반복에서, 당신은 단지 부패합니다.

682
00:45:45,483 --> 00:45:47,280
어떤 요인에 의해 당신은 계속 간다.

683
00:45:47,280 --> 00:45:48,594
기하 급수적 인 감소를 볼 수도 있습니다.

684
00:45:48,594 --> 00:45:53,064
훈련 중 계속 부패하는 곳.

685
00:45:53,064 --> 00:45:54,607
다양한 유사 광고가 표시 될 수 있습니다.

686
00:45:54,607 --> 00:45:56,345
학습 속도를 지속적으로 낮추는 것

687
00:45:56,345 --> 00:45:58,083
훈련 도중.

688
00:45:58,083 --> 00:46:00,855
당신이 논문, 특히 공명 된 논문을 보면,

689
00:46:00,855 --> 00:46:03,232
당신은 종종 이런 종류의 플롯을 보지만,

690
00:46:03,232 --> 00:46:04,832
그 손실은 일종의 추락입니다.

691
00:46:04,832 --> 00:46:07,265
그 다음에 떨어 뜨리고 다시 평평하게하고,

692
00:46:07,265 --> 00:46:08,383
다시 떨어 뜨린다.

693
00:46:08,383 --> 00:46:09,791
이 플롯에서 진행되는 것은

694
00:46:09,791 --> 00:46:11,797
그들은 단계 붕괴 학습 속도를 사용하고 있으며,

695
00:46:11,797 --> 00:46:14,039
어디에서 그들이 평평한이 부분에

696
00:46:14,039 --> 00:46:15,599
갑자기 다시 떨어지면

697
00:46:15,599 --> 00:46:17,371
학습 속도를 떨어 뜨린 반복

698
00:46:17,371 --> 00:46:18,886
어떤 요인.

699
00:46:18,886 --> 00:46:22,828
학습 속도를 떨어 뜨리는이 아이디어는,

700
00:46:22,828 --> 00:46:24,090
당신은 그것이 가까이에 있다고 상상할 수 있습니다.

701
00:46:24,090 --> 00:46:26,728
일부 좋은 지역,하지만 지금은 그라디언트가 작아지고,

702
00:46:26,728 --> 00:46:28,551
그것은 너무 많이 튀는 종류입니다.

703
00:46:28,551 --> 00:46:29,728
그런 다음 학습률을 떨어 뜨리면

704
00:46:29,728 --> 00:46:31,066
속도를 늦추고 계속할 수 있습니다.

705
00:46:31,066 --> 00:46:33,230
경관을 진전시키는 것.

706
00:46:33,230 --> 00:46:36,960
이것은 때때로 실제로 도움이되는 경향이 있습니다.

707
00:46:36,960 --> 00:46:38,909
지적해야 할 점은

708
00:46:38,909 --> 00:46:40,912
학습 붕괴 속도는 조금 더 일반적입니다.

709
00:46:40,912 --> 00:46:44,084
SGD의 모멘텀과 약간의 공통점

710
00:46:44,084 --> 00:46:45,458
아담처럼.

711
00:46:45,458 --> 00:46:47,707
내가 지적하고자하는 또 다른 점은

712
00:46:47,707 --> 00:46:49,699
학습률 감퇴는 일종의

713
00:46:49,699 --> 00:46:50,943
2 차 하이퍼 매개 변수.

714
00:46:50,943 --> 00:46:52,247
일반적으로 최적화하지 말아야합니다.

715
00:46:52,247 --> 00:46:53,809
처음부터 이걸로.

716
00:46:53,809 --> 00:46:55,271
보통 당신이 네트워크를 얻는 종류 일 때

717
00:46:55,271 --> 00:46:58,589
처음에 일하면서

718
00:46:58,589 --> 00:47:00,097
학습 률 저하없이 좋은 학습 률

719
00:47:00,097 --> 00:47:01,362
출발점에서.

720
00:47:01,362 --> 00:47:02,714
공동으로 상호 유효성 검사를 시도하는 중입니다.

721
00:47:02,714 --> 00:47:04,629
학습 속도 감소 및 초기 학습 속도

722
00:47:04,629 --> 00:47:06,553
그리고 다른 것들, 당신은 혼란 스러울 것입니다.

723
00:47:06,553 --> 00:47:08,259
학습 속도 감퇴를 설정하기 위해 무엇을합니까?

724
00:47:08,259 --> 00:47:11,066
부패가 없으면 어떻게되는지보십시오.

725
00:47:11,066 --> 00:47:12,829
그러면 안구의 종류가 손실 곡선을 보게됩니다.

726
00:47:12,829 --> 00:47:15,912
거기서 당신은 붕괴가 필요하다고 생각합니다.

727
00:47:17,345 --> 00:47:19,006
잠시 언급하고 싶은 또 다른 점

728
00:47:19,006 --> 00:47:22,039
이 모든 알고리즘에 대한 아이디어입니다.

729
00:47:22,039 --> 00:47:22,998
우리가 얘기 한

730
00:47:22,998 --> 00:47:25,433
1 차 최적화 알고리즘입니다.

731
00:47:25,433 --> 00:47:27,992
이 그림에서,이 1 차원 그림에서,

732
00:47:27,992 --> 00:47:32,238
우리는 이런 종류의 매력적인 목적 함수를 가지고 있습니다.

733
00:47:32,238 --> 00:47:33,549
우리의 현재 시점에서 빨간색으로.

734
00:47:33,549 --> 00:47:34,995
우리가 기본적으로하는 일은 컴퓨팅

735
00:47:34,995 --> 00:47:36,542
그 시점의 구배

736
00:47:36,542 --> 00:47:38,619
그라디언트 정보를 사용하여

737
00:47:38,619 --> 00:47:41,207
함수에 대한 선형 근사,

738
00:47:41,207 --> 00:47:43,568
1 차 테일러 근사법의 일종이다.

739
00:47:43,568 --> 00:47:44,693
우리의 기능에.

740
00:47:44,693 --> 00:47:47,416
이제 우리는 1 차 근사

741
00:47:47,416 --> 00:47:49,778
우리의 실제 기능이며, 우리는 한 걸음 내딛습니다.

742
00:47:49,778 --> 00:47:52,299
근사치를 최소화하려고합니다.

743
00:47:52,299 --> 00:47:54,535
그러나이 근사치는 유지되지 않습니다.

744
00:47:54,535 --> 00:47:56,377
매우 큰 지역의 경우, 우리는 단계를 밟을 수 없습니다.

745
00:47:56,377 --> 00:47:57,838
그 방향으로 너무 멀리.

746
00:47:57,838 --> 00:47:59,521
하지만 실제로, 여기에있는 아이디어는 우리가 단지

747
00:47:59,521 --> 00:48:01,129
첫 번째 정보

748
00:48:01,129 --> 00:48:03,002
함수의 미분.

749
00:48:03,002 --> 00:48:04,994
당신은 실제로 조금 더 좋아 할 수 있습니다.

750
00:48:04,994 --> 00:48:07,433
2 차 근사법에 대한 아이디어가 있는데,

751
00:48:07,433 --> 00:48:09,716
여기서 우리는 1 차 미분

752
00:48:09,716 --> 00:48:11,733
및 2 차 미분 정보.

753
00:48:11,733 --> 00:48:14,812
이제 우리는 2 차 테일러 근사를 만듭니다.

754
00:48:14,812 --> 00:48:17,194
우리의 기능과 종류에 근사치

755
00:48:17,194 --> 00:48:18,934
우리의 함수는 2 차 함수입니다.

756
00:48:18,934 --> 00:48:20,498
이제 2 차 방정식을 사용하면 바로 나아갈 수 있습니다.

757
00:48:20,498 --> 00:48:22,766
최소한으로, 당신은 정말로 행복합니다.

758
00:48:22,766 --> 00:48:26,254
이것은 2 차 최적화에 대한 아이디어입니다.

759
00:48:26,254 --> 00:48:28,687
이것을 여러 차원으로 일반화하면,

760
00:48:28,687 --> 00:48:30,974
당신은 뉴턴 단계라는 것을 얻습니다.

761
00:48:30,974 --> 00:48:33,137
이 헤 시안 행렬을 계산하면,

762
00:48:33,137 --> 00:48:35,551
2 차 미분의 행렬이며,

763
00:48:35,551 --> 00:48:37,753
그리고 당신은이 헤 시안 행렬을 뒤집어 쓰게됩니다.

764
00:48:37,753 --> 00:48:39,759
직접 최소 단계로 이동하려면

765
00:48:39,759 --> 00:48:44,174
함수에 대한이 2 차 근사값.

766
00:48:44,174 --> 00:48:45,869
누구나 뭔가 다른 것을 발견합니까?

767
00:48:45,869 --> 00:48:47,872
이 업데이트 규칙에 대해 다른 규칙과 비교

768
00:48:47,872 --> 00:48:49,395
우리가 본거야?

769
00:48:49,395 --> 00:48:51,592
- [학생] [듣기에는 너무 낮게 말하기]

770
00:48:51,592 --> 00:48:53,313
- 이것은 학습 속도가 없습니다.

771
00:48:53,313 --> 00:48:54,813
그건 멋집니다.

772
00:48:56,948 --> 00:48:58,463
우리는이 2 차 근사를 만들고 있습니다.

773
00:48:58,463 --> 00:48:59,638
그리고 우리는 최소한으로 밟고 있습니다.

774
00:48:59,638 --> 00:49:01,149
2 차항의

775
00:49:01,149 --> 00:49:03,755
적어도 뉴톤의 방법의이 바닐라 버전에서는,

776
00:49:03,755 --> 00:49:05,166
실제로는 학습 속도가 필요하지 않습니다.

777
00:49:05,166 --> 00:49:06,405
너는 항상 최소한으로 나아 간다.

778
00:49:06,405 --> 00:49:08,334
매 단계마다.

779
00:49:08,334 --> 00:49:09,988
그러나, 실제로, 당신은 끝낼지도 모르지만,

780
00:49:09,988 --> 00:49:11,492
어쨌든 학습 속도가 있기 때문에, 다시,

781
00:49:11,492 --> 00:49:13,750
그 2 차 근사는 완벽하지 않을 수도 있습니다.

782
00:49:13,750 --> 00:49:15,405
그래서 당신은 방향으로 나아갈 수 있습니다.

783
00:49:15,405 --> 00:49:17,101
실제로는 아니고 최소한으로

784
00:49:17,101 --> 00:49:18,670
적어도 최소한으로 족답

785
00:49:18,670 --> 00:49:19,790
이 바닐라 버전에서는

786
00:49:19,790 --> 00:49:21,540
학습 속도가 있습니다.

787
00:49:24,479 --> 00:49:25,685
하지만 불행히도 이것은 아마도

788
00:49:25,685 --> 00:49:27,851
깊은 학습을위한 약간의 비실용적 인

789
00:49:27,851 --> 00:49:30,061
이 헤 시안 행렬

790
00:49:30,061 --> 00:49:33,413
N × N이며, N은 매개 변수의 수입니다.

791
00:49:33,413 --> 00:49:35,004
귀하의 네트워크에서.

792
00:49:35,004 --> 00:49:37,829
N이 1 억이면 1 억 제곱

793
00:49:37,829 --> 00:49:38,983
너무 큽니다.

794
00:49:38,983 --> 00:49:40,419
당신은 분명히 그것을 메모리에 저장할 수 없습니다.

795
00:49:40,419 --> 00:49:42,531
그리고 당신은 그것을 반전 할 수 없습니다.

796
00:49:42,531 --> 00:49:44,796
실제로, 사람들은 때때로 이것을 사용합니다.

797
00:49:44,796 --> 00:49:46,971
준 뉴튼 방법.

798
00:49:46,971 --> 00:49:48,275
전체 헤 시안과 반전

799
00:49:48,275 --> 00:49:50,983
전체 헤 시안, 근사치로 작업합니다.

800
00:49:50,983 --> 00:49:53,210
낮은 순위의 근사치가 일반적입니다.

801
00:49:53,210 --> 00:49:57,577
때로는 몇 가지 문제에 대해이를 볼 수 있습니다.

802
00:49:57,577 --> 00:50:00,242
L-BFGS는 특정 2 차 옵티 마이저입니다.

803
00:50:00,242 --> 00:50:02,647
이 대략적인 초를 가지고,

804
00:50:02,647 --> 00:50:03,972
헤 시안의 근사치를 유지한다.

805
00:50:03,972 --> 00:50:06,458
당신이 때때로 보게 될 것이지만 실제로는,

806
00:50:06,458 --> 00:50:08,418
많은 사람들에게 너무 잘 작동하지 않는다.

807
00:50:08,418 --> 00:50:11,690
깊은 학습 문제가 있기 때문에 이러한
approximations,

808
00:50:11,690 --> 00:50:13,774
이 2 차 근사값은 정말로

809
00:50:13,774 --> 00:50:15,772
확률론적인 사례를 아주 많이 다루십시오.

810
00:50:15,772 --> 00:50:16,895
아주 잘.

811
00:50:16,895 --> 00:50:18,772
그들은 또한 잘 작동하지 않는 경향이 있습니다.

812
00:50:18,772 --> 00:50:21,101
비 볼록 문제.

813
00:50:21,101 --> 00:50:23,627
나는 지금 당장 그것에 너무 많이 들어가고 싶지 않다.

814
00:50:23,627 --> 00:50:24,963
실제로, 당신이 정말로해야 할 일

815
00:50:24,963 --> 00:50:27,435
아마 아담은 정말 좋은 선택입니다

816
00:50:27,435 --> 00:50:29,507
많은 다른 신경 네트워크 일들에 대해,

817
00:50:29,507 --> 00:50:31,737
하지만 네가 상황에

818
00:50:31,737 --> 00:50:33,557
전체 일괄 업데이트를 할 여력이 있습니다.

819
00:50:33,557 --> 00:50:34,956
당신은 당신의 문제가

820
00:50:34,956 --> 00:50:37,377
정말로 어떤 확률이라도, L-BFGS

821
00:50:37,377 --> 00:50:39,459
좋은 선택입니다.

822
00:50:39,459 --> 00:50:41,370
L-BFGS는 실제로 훈련에 사용되지 않습니다.

823
00:50:41,370 --> 00:50:43,666
신경망이 너무 많이 있지만, 우리가 보게 될 것입니다.

824
00:50:43,666 --> 00:50:45,208
몇 가지 강의에서 때때로

825
00:50:45,208 --> 00:50:47,736
스타일 이전,

826
00:50:47,736 --> 00:50:49,848
실제로 덜 확률론이있는 곳

827
00:50:49,848 --> 00:50:52,091
매개 변수가 적지 만 여전히 원하는 경우

828
00:50:52,091 --> 00:50:54,841
최적화 문제를 해결할 수 있습니다.

829
00:50:56,319 --> 00:50:58,060
우리가 이야기 한이 모든 전략

830
00:50:58,060 --> 00:51:01,477
지금까지는 훈련 오류를 줄이려고합니다.

831
00:51:02,829 --> 00:51:04,439
이 모든 최적화 알고리즘은 실제로

832
00:51:04,439 --> 00:51:05,927
훈련 오류를 몰아내는 것에 대해

833
00:51:05,927 --> 00:51:07,937
목표 기능을 최소화하고,

834
00:51:07,937 --> 00:51:09,123
하지만 우리는 정말로 신경 쓰지 않아.

835
00:51:09,123 --> 00:51:10,888
그다지 훈련 오류.

836
00:51:10,888 --> 00:51:12,521
대신 Google은 실적에 대해

837
00:51:12,521 --> 00:51:13,688
보이지 않는 데이터.

838
00:51:13,688 --> 00:51:15,456
우리는이 격차를 줄이는 데 정말로 신경을 씁니다.

839
00:51:15,456 --> 00:51:17,302
열차와 시험 오류 사이.

840
00:51:17,302 --> 00:51:19,716
문제는 일단 우리가 이미

841
00:51:19,716 --> 00:51:21,713
목적 함수를 최적화하는 데 능숙하고,

842
00:51:21,713 --> 00:51:23,533
이 격차를 줄이기 위해 우리가 할 수있는 일

843
00:51:23,533 --> 00:51:24,770
우리 모델이 더 잘 수행되도록

844
00:51:24,770 --> 00:51:26,020
보이지 않는 데이터에?

845
00:51:28,982 --> 00:51:30,733
정말 빠르고 쉽고 쉬운 일

846
00:51:30,733 --> 00:51:34,102
시도하는 것은 모델 앙상블에 대한이 아이디어이다.

847
00:51:34,102 --> 00:51:35,984
때로는 여러 영역에서 작동합니다.

848
00:51:35,984 --> 00:51:37,252
기계 학습에서.

849
00:51:37,252 --> 00:51:38,674
아이디어는 꽤 간단합니다.

850
00:51:38,674 --> 00:51:40,337
하나의 모델 만 가지고있는 것이 아니라,

851
00:51:40,337 --> 00:51:42,638
우리는 10 가지 모델을 독립적으로 교육 할 것입니다.

852
00:51:42,638 --> 00:51:45,073
다른 초기 랜덤 재시작으로부터.

853
00:51:45,073 --> 00:51:47,086
이제 테스트 시간에 데이터를 실행합니다.

854
00:51:47,086 --> 00:51:48,901
10 모델 모두 평균을 통해

855
00:51:48,901 --> 00:51:51,818
그 10 가지 모델의 예측.

856
00:51:54,047 --> 00:51:55,677
이 여러 모델을 함께 추가

857
00:51:55,677 --> 00:51:57,769
과핑을 조금 줄이는 경향이있다.

858
00:51:57,769 --> 00:52:00,163
성능을 약간 향상시키는 경향이 있습니다.

859
00:52:00,163 --> 00:52:02,040
일반적으로 2 % 정도.

860
00:52:02,040 --> 00:52:04,204
이것은 일반적으로 크게 개선되지는 않지만,

861
00:52:04,204 --> 00:52:05,787
그러나 그것은 일관된 개선이다.

862
00:52:05,787 --> 00:52:07,498
대회에서 볼 수 있습니다.

863
00:52:07,498 --> 00:52:09,783
ImageNet과 같은 다른 것들,

864
00:52:09,783 --> 00:52:11,498
모델 앙상블 사용은 매우 일반적입니다.

865
00:52:11,498 --> 00:52:13,748
최대한의 성능을 얻으십시오.

866
00:52:14,973 --> 00:52:17,065
당신은 실제로 이것으로 조금 창의적이 될 수 있습니다.

867
00:52:17,065 --> 00:52:19,128
때로는 별도의 모델을 교육하는 것이 아니라

868
00:52:19,128 --> 00:52:20,967
독립적으로, 당신은 단지 여러개를 유지할 수 있습니다.

869
00:52:20,967 --> 00:52:22,595
과정에서 모델의 스냅 샷

870
00:52:22,595 --> 00:52:24,863
교육을받은 후 다음을 사용하십시오.

871
00:52:24,863 --> 00:52:26,413
너 앙상블처럼.

872
00:52:26,413 --> 00:52:28,262
그럼 당신은 여전히, 테스트 시간에, 평균해야합니다.

873
00:52:28,262 --> 00:52:30,289
이러한 여러 스냅 샷의 예측,

874
00:52:30,289 --> 00:52:31,812
그러나 스냅 샷을 수집하는 동안

875
00:52:31,812 --> 00:52:33,729
훈련 과정.

876
00:52:34,618 --> 00:52:36,441
사실 훌륭한 종이가 제시되고 있습니다.

877
00:52:36,441 --> 00:52:39,178
이번 주 ICLR에서

878
00:52:39,178 --> 00:52:42,092
우리가 사용하는이 아이디어의 멋진 버전

879
00:52:42,092 --> 00:52:43,695
미친 학습 속도 일정,

880
00:52:43,695 --> 00:52:45,800
우리의 학습 속도가 매우 느린 곳에서,

881
00:52:45,800 --> 00:52:48,481
그 다음 매우 빠르다, 그 다음 매우
느리게, 그 다음 매우 빠르다.

882
00:52:48,481 --> 00:52:49,756
아이디어는이 미친 듯이

883
00:52:49,756 --> 00:52:51,817
학습 일정, 그 다음 과정

884
00:52:51,817 --> 00:52:53,705
훈련의 경우 모델이 수렴 할 수 있습니다.

885
00:52:53,705 --> 00:52:55,699
객관적인 조경에있는 다른 지구에

886
00:52:55,699 --> 00:52:58,116
모두 합리적으로 좋다.

887
00:52:59,202 --> 00:53:00,373
이것들에 앙상블을하면

888
00:53:00,373 --> 00:53:02,256
다른 스냅 샷, 그럼 당신은 향상시킬 수 있습니다

889
00:53:02,256 --> 00:53:03,738
당신의 공연은 꽤 멋지 네요.

890
00:53:03,738 --> 00:53:06,017
모델을 한 번만 교육하는 경우에도 마찬가지입니다.

891
00:53:06,017 --> 00:53:07,516
질문이 있으십니까?

892
00:53:07,516 --> 00:53:11,683
- [학생] [듣기에는 너무 낮게 말하기]

893
00:53:25,873 --> 00:53:28,273
- 문제는

894
00:53:28,273 --> 00:53:29,558
오류의 원인이 큰 차이가 있습니다.

895
00:53:29,558 --> 00:53:30,737
그건 너 지나치다는 뜻이지.

896
00:53:30,737 --> 00:53:33,898
아무런 격차가 없다면, 그것은 또한 어쩌면 나쁠 것입니까?

897
00:53:33,898 --> 00:53:36,131
우리는 실제로 작은, 최적의 갭을 원합니까?

898
00:53:36,131 --> 00:53:37,931
둘 사이?

899
00:53:37,931 --> 00:53:39,617
우리는 그 격차를별로 신경 쓰지 않습니다.

900
00:53:39,617 --> 00:53:41,496
우리가 정말로 신경 쓰는 부분은 최대화입니다.

901
00:53:41,496 --> 00:53:44,504
유효성 검사 집합의 성능

902
00:53:44,504 --> 00:53:46,369
일어날 수있는 일은 당신이

903
00:53:46,369 --> 00:53:48,871
격차가 보이지 않으면 개선 할 수있다.

904
00:53:48,871 --> 00:53:52,288
당신의 절대적인 성과는, 많은 경우에,

905
00:53:53,709 --> 00:53:55,480
조금 더 overfitting.

906
00:53:55,480 --> 00:53:56,931
이 이상한 상관 관계가 있습니다.

907
00:53:56,931 --> 00:53:58,698
유효성 검사 집합의 절대 성능

908
00:53:58,698 --> 00:54:00,280
그 간격의 크기.

909
00:54:00,280 --> 00:54:03,205
우리는 절대적인 성과만을 염려합니다.

910
00:54:03,205 --> 00:54:04,220
뒤에서 질문 하나?

911
00:54:04,220 --> 00:54:05,769
- [학생] 하이퍼 매개 변수가 같은가요?

912
00:54:05,769 --> 00:54:07,489
앙상블을 위해서?

913
00:54:07,489 --> 00:54:08,739
- 하이퍼 매개 변수가 동일합니까?

914
00:54:08,739 --> 00:54:10,013
앙상블 때문에?

915
00:54:10,013 --> 00:54:11,135
그것은 좋은 질문입니다.

916
00:54:11,135 --> 00:54:12,719
때로는 그렇지 않습니다.

917
00:54:12,719 --> 00:54:15,891
모델의 다른 크기를 시도해 볼 수도 있습니다.

918
00:54:15,891 --> 00:54:16,998
다른 학습 속도, 다른

919
00:54:16,998 --> 00:54:19,144
정규화 전략과 앙상블

920
00:54:19,144 --> 00:54:20,099
이 다른 것들.

921
00:54:20,099 --> 00:54:23,099
그것은 실제로 때때로 일어납니다.

922
00:54:23,981 --> 00:54:25,615
때때로 할 수있는 또 다른 작은 트릭

923
00:54:25,615 --> 00:54:27,832
그 동안 훈련 중에, 당신은 실제로

924
00:54:27,832 --> 00:54:29,472
기하 급수적 인 평균

925
00:54:29,472 --> 00:54:32,254
귀하의 매개 변수 벡터 자체의 종류에

926
00:54:32,254 --> 00:54:34,365
너의 자신의 네트워크의 부드러운 앙상블

927
00:54:34,365 --> 00:54:36,263
훈련 도중.

928
00:54:36,263 --> 00:54:38,083
그런 다음 부드럽게 감퇴하는 평균을 사용하십시오.

929
00:54:38,083 --> 00:54:39,902
매개 변수 벡터 대신

930
00:54:39,902 --> 00:54:42,134
실제 검사 점 자체.

931
00:54:42,134 --> 00:54:43,457
이를 폴리 랙 평균 (Polyak averaging)이라고하며,

932
00:54:43,457 --> 00:54:45,747
때때로 도움이됩니다.

933
00:54:45,747 --> 00:54:47,163
이 작은 트릭 중 하나 일뿐입니다.

934
00:54:47,163 --> 00:54:48,819
때때로 추가 할 수는 있지만 어쩌면 그렇지 않을 수도 있습니다.

935
00:54:48,819 --> 00:54:51,323
실제로 너무 일반적입니다.

936
00:54:51,323 --> 00:54:53,295
당신이 가질 수있는 또 다른 질문은

937
00:54:53,295 --> 00:54:54,846
실제로 어떻게 성능을 향상시킬 수 있습니까?

938
00:54:54,846 --> 00:54:56,263
단일 모델의?

939
00:54:57,714 --> 00:54:59,518
우리가 앙상블을 가질 때, 우리는 여전히 달릴 필요가있다.

940
00:54:59,518 --> 00:55:01,224
테스트 시간에는 10 가지 모델이 있습니다.

941
00:55:01,224 --> 00:55:02,988
그렇게 좋지는 않습니다.

942
00:55:02,988 --> 00:55:04,638
우리는 몇 가지 전략을 개선하기를 정말로 원합니다.

943
00:55:04,638 --> 00:55:06,704
우리의 단일 모델의 성능.

944
00:55:06,704 --> 00:55:08,722
이것이 바로 정규화에 대한이 아이디어입니다.

945
00:55:08,722 --> 00:55:10,702
모델에 무언가를 추가합니다.

946
00:55:10,702 --> 00:55:12,439
훈련 데이터를 피팅하지 못하게한다.

947
00:55:12,439 --> 00:55:14,877
더 잘 수행하려는 시도에서 너무 잘

948
00:55:14,877 --> 00:55:16,688
보이지 않는 데이터.

949
00:55:16,688 --> 00:55:18,789
우리는 몇 가지 아이디어, 몇 가지 방법을 보았습니다.

950
00:55:18,789 --> 00:55:20,730
이미 우리가 추가하는 정규화를 위해

951
00:55:20,730 --> 00:55:24,000
손실에 대한 명시적인 추가 용어.

952
00:55:24,000 --> 00:55:25,911
우리가 모델을 말하고있는이 한 단어

953
00:55:25,911 --> 00:55:27,999
데이터에 맞게, 그리고 다른 용어

954
00:55:27,999 --> 00:55:30,223
그것은 정규화 용어입니다.

955
00:55:30,223 --> 00:55:32,017
우리가 사용한 숙제 하나에서 이걸 봤어.

956
00:55:32,017 --> 00:55:33,517
L2 정규화.

957
00:55:35,289 --> 00:55:37,533
우리가 강의에서 이야기했던 것처럼

958
00:55:37,533 --> 00:55:39,853
강의 이전에,이 L2 정규화는

959
00:55:39,853 --> 00:55:41,903
문맥에서 어쩌면 많은 의미를 가질 수 있습니다.

960
00:55:41,903 --> 00:55:43,486
신경 네트워크.

961
00:55:44,407 --> 00:55:48,467
때때로 우리는 신경망을 위해 다른 것들을 사용합니다.

962
00:55:48,467 --> 00:55:50,321
한 가지 정규화 전략은 수퍼,

963
00:55:50,321 --> 00:55:51,812
신경 네트워크를위한 슈퍼 공통

964
00:55:51,812 --> 00:55:53,861
이 탈락의 생각입니다.

965
00:55:53,861 --> 00:55:55,565
드롭 아웃은 매우 간단합니다.

966
00:55:55,565 --> 00:55:57,074
앞으로 나아갈 때마다

967
00:55:57,074 --> 00:55:59,530
네트워크는 모든 계층에서

968
00:55:59,530 --> 00:56:02,749
임의로 일부 뉴런을 0으로 설정하십시오.

969
00:56:02,749 --> 00:56:04,027
우리가 앞으로 전달할 때마다,

970
00:56:04,027 --> 00:56:05,510
다른 임의의 부분 집합을 설정합니다.

971
00:56:05,510 --> 00:56:07,135
뉴런의 0으로.

972
00:56:07,135 --> 00:56:09,173
이러한 종류의 수익은 한 번에 한 계층 씩 진행됩니다.

973
00:56:09,173 --> 00:56:11,145
우리는 하나의 레이어를 실행하고,

974
00:56:11,145 --> 00:56:12,873
레이어의 값, 무작위로 설정

975
00:56:12,873 --> 00:56:14,433
그들 중 일부는 제로로, 그리고 우리는 계속

976
00:56:14,433 --> 00:56:15,678
네트워크를 통해.

977
00:56:15,678 --> 00:56:17,893
이제 완전히 연결된 네트워크를 보면

978
00:56:17,893 --> 00:56:21,090
드롭 아웃 버전 대 왼쪽

979
00:56:21,090 --> 00:56:22,930
오른쪽에있는 동일한 네트워크의

980
00:56:22,930 --> 00:56:25,988
우리가 탈락 한 후에는 외모가 좋아

981
00:56:25,988 --> 00:56:28,308
같은 네트워크의 더 작은 버전처럼,

982
00:56:28,308 --> 00:56:30,885
여기서 우리는 뉴런의 일부 서브
세트만을 사용하고 있습니다.

983
00:56:30,885 --> 00:56:34,774
우리가 사용하는이 부분 집합은 각 반복마다 다르며,

984
00:56:34,774 --> 00:56:36,231
각각의 정방향 패스에서.

985
00:56:36,231 --> 00:56:37,217
문제?

986
00:56:37,217 --> 00:56:41,384
- [학생] [듣기에는 너무 낮게 말하기]

987
00:56:44,179 --> 00:56:45,717
- 문제는 우리가 무엇을 0으로 놓고있는 것입니까?

988
00:56:45,717 --> 00:56:46,860
그것은 활성화입니다.

989
00:56:46,860 --> 00:56:49,082
각 계층은 이전 활성화를 계산 중입니다.

990
00:56:49,082 --> 00:56:50,305
가중치 행렬이 당신에게주는 시간

991
00:56:50,305 --> 00:56:52,216
우리의 다음 활성화.

992
00:56:52,216 --> 00:56:53,881
그런 다음 그 활성화를 취하십시오.

993
00:56:53,881 --> 00:56:55,759
그 중 일부를 0으로 설정 한 다음

994
00:56:55,759 --> 00:56:59,024
당신의 다음 계층은 부분적으로
활성화가 제로화 될 것입니다.

995
00:56:59,024 --> 00:57:02,077
다른 매트릭스가 당신에게 당신의 다음
활성화를주는 시간을 곱하십시오.

996
00:57:02,077 --> 00:57:03,640
문제?

997
00:57:03,640 --> 00:57:07,187
- [학생] [듣기에는 너무 낮게 말하기]

998
00:57:07,187 --> 00:57:09,236
- 어떤 층에서이 작업을 수행합니까?

999
00:57:09,236 --> 00:57:11,708
완전히 연결된 레이어에서 더 일반적입니다.

1000
00:57:11,708 --> 00:57:14,939
하지만 가끔씩 길쌈 계층에서도 이것을 볼 수 있습니다.

1001
00:57:14,939 --> 00:57:16,386
컨볼 루션 레이어에서 작업 할 때,

1002
00:57:16,386 --> 00:57:18,521
때로는 떨어 뜨리는 대신에

1003
00:57:18,521 --> 00:57:20,575
무작위로 각 활성화, 대신 때때로

1004
00:57:20,575 --> 00:57:23,908
무작위로 전체 기능 맵을 삭제할 수 있습니다.

1005
00:57:24,940 --> 00:57:26,731
컨볼 루션에서는 채널 크기가

1006
00:57:26,731 --> 00:57:28,269
전체 채널을 삭제할 수도 있습니다.

1007
00:57:28,269 --> 00:57:30,602
임의의 요소가 아닌

1008
00:57:32,544 --> 00:57:34,405
탈락은 실제로 아주 간단합니다.

1009
00:57:34,405 --> 00:57:37,022
두 줄만 추가하면되고,

1010
00:57:37,022 --> 00:57:38,965
드롭 아웃 전화 당 한 줄.

1011
00:57:38,965 --> 00:57:40,884
여기에는 3 층 신경망이 있습니다.

1012
00:57:40,884 --> 00:57:42,057
드롭 아웃이 추가되었습니다.

1013
00:57:42,057 --> 00:57:44,375
당신은 우리가해야 할 모든 것을 볼 수 있습니다.

1014
00:57:44,375 --> 00:57:46,149
이 여분의 줄을 무작위로 추가했습니다.

1015
00:57:46,149 --> 00:57:47,641
어떤 것을 0으로 설정하십시오.

1016
00:57:47,641 --> 00:57:49,945
이것은 구현하기 쉽습니다.

1017
00:57:49,945 --> 00:57:52,623
그러나 질문은 이것이 왜 심지어 좋은 생각인지?

1018
00:57:52,623 --> 00:57:54,481
우리는 네트워크를 심각하게 망치고 있습니다.

1019
00:57:54,481 --> 00:57:56,537
무리를 지어 훈련 시간에

1020
00:57:56,537 --> 00:57:58,552
값을 0으로 설정합니다.

1021
00:57:58,552 --> 00:58:01,473
어떻게하면이 말이 가능할까요?

1022
00:58:01,473 --> 00:58:04,390
약간 손이 물결 모양 아이디어의 한 종류

1023
00:58:05,480 --> 00:58:07,474
사람들은 드롭 아웃이 예방에 도움이된다는

1024
00:58:07,474 --> 00:58:10,107
특징의 공동 적응.

1025
00:58:10,107 --> 00:58:11,377
어쩌면 우리가 시도하고 있다고 상상한다면

1026
00:58:11,377 --> 00:58:14,281
어쩌면 어떤 우주에서는 고양이를 분류하고,

1027
00:58:14,281 --> 00:58:16,338
네트워크는 하나의 뉴런을 학습 할 수있다.

1028
00:58:16,338 --> 00:58:18,602
귀에는 꼬리가있는 하나의 뉴런이 있고,

1029
00:58:18,602 --> 00:58:21,551
입력에 대한 하나의 뉴런은 모피입니다.

1030
00:58:21,551 --> 00:58:23,284
그런 다음 이런 것들을 함께 결합합니다.

1031
00:58:23,284 --> 00:58:25,236
그것이 고양이인지 아닌지를 결정합니다.

1032
00:58:25,236 --> 00:58:27,391
하지만 이제 우리가 중퇴하면,

1033
00:58:27,391 --> 00:58:30,427
고양이애에 관한 최종 결정, 네트워크

1034
00:58:30,427 --> 00:58:32,176
이것들에 너무 많이 의존해서는 안된다.

1035
00:58:32,176 --> 00:58:33,316
하나의 특징.

1036
00:58:33,316 --> 00:58:34,929
대신, 배포 할 필요가 있습니다.

1037
00:58:34,929 --> 00:58:38,210
많은 다른 특징에 걸쳐 catness의 그것의 생각.

1038
00:58:38,210 --> 00:58:42,690
이것은 어떻게 든 overfitting을
방지하는 데 도움이 될 수 있습니다.

1039
00:58:42,690 --> 00:58:44,687
드롭 아웃의 또 다른 해석

1040
00:58:44,687 --> 00:58:46,682
최근에 조금 나왔다.

1041
00:58:46,682 --> 00:58:48,999
그것은 마치 모델 앙상블을하는 것과 같습니다.

1042
00:58:48,999 --> 00:58:50,832
단일 모델 내에서

1043
00:58:52,175 --> 00:58:53,689
왼쪽 그림을 보면,

1044
00:58:53,689 --> 00:58:55,546
드롭 아웃을 네트워크에 적용한 후,

1045
00:58:55,546 --> 00:58:57,245
우리는이 서브 네트워크를 계산하고 있습니다.

1046
00:58:57,245 --> 00:58:59,230
뉴런의 일부 하위 집합을 사용합니다.

1047
00:58:59,230 --> 00:59:01,625
이제 모든 다른 잠재적 인 드롭 아웃 마스크

1048
00:59:01,625 --> 00:59:03,876
다른 잠재적 인 서브 네트워크로 연결됩니다.

1049
00:59:03,876 --> 00:59:06,419
이제 드롭 아웃은 전체적인 앙상블을 배우는 것과 같습니다.

1050
00:59:06,419 --> 00:59:08,115
동시에 모든 네트워크가 동시에

1051
00:59:08,115 --> 00:59:09,630
매개 변수를 공유하십시오.

1052
00:59:09,630 --> 00:59:12,077
그건 그렇고, 잠재력의 수 때문에

1053
00:59:12,077 --> 00:59:14,275
드롭 아웃 마스크는 수에서 기하 급수적으로 커집니다.

1054
00:59:14,275 --> 00:59:15,970
뉴런의

1055
00:59:15,970 --> 00:59:17,637
이 모든 것들.

1056
00:59:18,574 --> 00:59:21,106
이것은 정말로 거대한, 거대한 앙상블입니다.

1057
00:59:21,106 --> 00:59:25,273
동시에 훈련을받는 네트워크의

1058
00:59:26,107 --> 00:59:29,613
그러면 문제는 테스트 시간에 어떻게됩니까?

1059
00:59:29,613 --> 00:59:31,733
일단 우리가 중도 탈락하자마자 근본적으로

1060
00:59:31,733 --> 00:59:34,643
신경망의 작동을 바 꾸었습니다.

1061
00:59:34,643 --> 00:59:37,642
이전에는 신경 네트워크 인 f를 사용했습니다.

1062
00:59:37,642 --> 00:59:39,201
가중치의 함수, w,

1063
00:59:39,201 --> 00:59:42,190
와 입력 x를 곱한 다음 생성합니다.

1064
00:59:42,190 --> 00:59:43,335
출력 y.

1065
00:59:43,335 --> 00:59:45,141
하지만 이제 우리 네트워크는

1066
00:59:45,141 --> 00:59:47,314
이 추가 입력 z는

1067
00:59:47,314 --> 00:59:48,753
무작위 탈락 마스크.

1068
00:59:48,753 --> 00:59:50,490
그 z는 무작위입니다.

1069
00:59:50,490 --> 00:59:53,217
테스트 시간에 임의성을 갖는 것은
어쩌면 좋지 않을 수 있습니다.

1070
00:59:53,217 --> 00:59:55,130
페이스 북에서 일한다고 상상해보십시오.

1071
00:59:55,130 --> 00:59:56,674
그리고 당신은 이미지를 분류하고 싶다.

1072
00:59:56,674 --> 00:59:57,929
사람들이 업로드하고 있습니다.

1073
00:59:57,929 --> 01:00:00,397
그리고 오늘, 당신의 이미지는 고양이로 분류됩니다.

1074
01:00:00,397 --> 01:00:01,530
내일은 그렇지 않습니다.

1075
01:00:01,530 --> 01:00:03,577
그것은 정말로 이상하고 정말로 나쁠 것입니다.

1076
01:00:03,577 --> 01:00:05,964
아마 이것을 제거하고 싶을 것입니다.

1077
01:00:05,964 --> 01:00:08,240
시험 시간에 확률론

1078
01:00:08,240 --> 01:00:09,808
이미 훈련을 받았다.

1079
01:00:09,808 --> 01:00:11,318
그럼 우리는 평균을 내고 싶다.

1080
01:00:11,318 --> 01:00:12,578
이 임의성.

1081
01:00:12,578 --> 01:00:14,790
이것을 쓰면 상상할 수 있습니다.

1082
01:00:14,790 --> 01:00:16,757
실제로이 임의성을 무시해 버린다.

1083
01:00:16,757 --> 01:00:18,616
일부 필수 요소가 있지만 실제로는

1084
01:00:18,616 --> 01:00:20,738
이 통합은 완전히 다루기가 어렵습니다.

1085
01:00:20,738 --> 01:00:23,086
우리는이 것을 평가하는 방법을 모른다.

1086
01:00:23,086 --> 01:00:24,853
너는 형체가 나쁘다.

1087
01:00:24,853 --> 01:00:26,224
네가 상상할 수있는 한가지

1088
01:00:26,224 --> 01:00:28,558
샘플링을 통해이 적분을 근사하고 있으며,

1089
01:00:28,558 --> 01:00:30,278
여기서 z의 여러 샘플을 그립니다.

1090
01:00:30,278 --> 01:00:31,969
테스트 시간에 평균을 낸 다음,

1091
01:00:31,969 --> 01:00:34,326
그러나 이것은 아직도 약간의 무작위성을 도입 할 것이고,

1092
01:00:34,326 --> 01:00:36,525
그것은 조금 나쁘다.

1093
01:00:36,525 --> 01:00:38,029
고맙게도, 중도 이탈의 경우 우리는

1094
01:00:38,029 --> 01:00:39,520
실제로이 적분에 근사하다.

1095
01:00:39,520 --> 01:00:41,908
현지에서 값싼 방법으로.

1096
01:00:41,908 --> 01:00:44,427
우리가 하나의 뉴런을 고려한다면, 출력은 a이다.

1097
01:00:44,427 --> 01:00:46,147
입력은 x와 y이고, 두 가중치,

1098
01:00:46,147 --> 01:00:47,713
하나, 둘.

1099
01:00:47,713 --> 01:00:51,107
그런 다음 테스트 시간에, 우리의 가치는 단지

1100
01:00:51,107 --> 01:00:53,107
w 1 x x w 2 y.

1101
01:00:54,075 --> 01:00:56,510
이제 우리가이 네트워크에 대해 훈련했다고 상상해보십시오.

1102
01:00:56,510 --> 01:00:58,997
훈련 도중, 우리는 확률로 탈락을 사용했다.

1103
01:00:58,997 --> 01:01:01,130
우리 뉴런을 떨어 뜨리는 것의 1/2.

1104
01:01:01,130 --> 01:01:03,853
이제 훈련 중 예상되는 가치,

1105
01:01:03,853 --> 01:01:05,135
우리는 분석적으로 계산할 수 있습니다.

1106
01:01:05,135 --> 01:01:06,802
이 작은 경우.

1107
01:01:08,197 --> 01:01:09,824
4 개의 드롭 아웃 마스크가 있습니다.

1108
01:01:09,824 --> 01:01:11,007
우리는 값을 평균화하려고합니다.

1109
01:01:11,007 --> 01:01:12,734
이 4 개의 가면을 가로 질러

1110
01:01:12,734 --> 01:01:14,522
우리는 예상되는

1111
01:01:14,522 --> 01:01:18,689
훈련 도중 1/2 w 1 x x w 2 y입니다.

1112
01:01:19,560 --> 01:01:22,211
이 사이에 분리가 있습니다.

1113
01:01:22,211 --> 01:01:24,842
이 평균값 w 1 x와 w 2 y

1114
01:01:24,842 --> 01:01:26,567
시험 시간 및 훈련 시간에,

1115
01:01:26,567 --> 01:01:29,485
평균값은 단지 1/2 정도입니다.

1116
01:01:29,485 --> 01:01:31,593
우리가 할 수있는 한 가지 싼 것은

1117
01:01:31,593 --> 01:01:35,368
테스트 시간에, 우리는 어떤 확률도 없다.

1118
01:01:35,368 --> 01:01:37,195
대신이 출력을 곱하면됩니다.

1119
01:01:37,195 --> 01:01:38,772
드롭 아웃 확률로

1120
01:01:38,772 --> 01:01:41,221
이제이 예상 값은 같습니다.

1121
01:01:41,221 --> 01:01:43,745
이것은 지역 저렴한 근사치와 비슷합니다.

1122
01:01:43,745 --> 01:01:45,218
이 복잡한 적분에

1123
01:01:45,218 --> 01:01:46,978
이것은 사람들이 실제로하는 일입니다.

1124
01:01:46,978 --> 01:01:49,061
실제로 중도 탈락.

1125
01:01:50,200 --> 01:01:51,922
드롭 아웃에서 우리는이 예측 기능을 가지고 있습니다.

1126
01:01:51,922 --> 01:01:53,722
레이어의 출력을 곱하면됩니다.

1127
01:01:53,722 --> 01:01:56,754
드롭 아웃 확률로

1128
01:01:56,754 --> 01:01:58,853
드롭 아웃의 요약은 정말 간단하다는 것입니다.

1129
01:01:58,853 --> 01:01:59,878
앞으로 패스에.

1130
01:01:59,878 --> 01:02:02,178
구현에 두 줄만 추가하면됩니다.

1131
01:02:02,178 --> 01:02:04,292
어떤 노드를 무작위로 제로로 바꾼다.

1132
01:02:04,292 --> 01:02:06,694
그런 다음, 테스트 시간 예측 기능에서,

1133
01:02:06,694 --> 01:02:09,375
당신은 방금 하나의 작은 곱셈을 추가했습니다.

1134
01:02:09,375 --> 01:02:10,694
당신의 확률로.

1135
01:02:10,694 --> 01:02:11,814
드롭 아웃은 매우 간단합니다.

1136
01:02:11,814 --> 01:02:14,881
때로는 잘 작동하는 경향이있다.

1137
01:02:14,881 --> 01:02:17,098
신경망을 규칙 화하기 위해.

1138
01:02:17,098 --> 01:02:19,189
그건 그렇고 가끔 보시는 한 가지 일반적인 트릭입니다.

1139
01:02:19,189 --> 01:02:21,939
거꾸로 떨어지는 생각입니다.

1140
01:02:23,150 --> 01:02:25,489
어쩌면 테스트 시간에, 당신은
효율성에 대해 더 신경을 쓰고,

1141
01:02:25,489 --> 01:02:27,500
그래서 당신은 여분의 곱셈을 제거하고 싶습니다.

1142
01:02:27,500 --> 01:02:29,220
시험 시간에 p.

1143
01:02:29,220 --> 01:02:31,248
그럼 당신이 할 수있는 것은, 시험 시간에,

1144
01:02:31,248 --> 01:02:33,410
전체 무게 행렬을 사용하지만 지금은

1145
01:02:33,410 --> 01:02:35,838
훈련 시간에 대신 p로 나눕니다.

1146
01:02:35,838 --> 01:02:38,162
GPU에서 교육이 진행되고 있기 때문입니다.

1147
01:02:38,162 --> 01:02:39,198
네가 한 일이라도 상관 없다.

1148
01:02:39,198 --> 01:02:41,259
여분의 훈련 시간에,하지만 그때

1149
01:02:41,259 --> 01:02:42,527
시험 시간에 너는 이걸 원해.

1150
01:02:42,527 --> 01:02:45,218
가능한 한 효율적이다.

1151
01:02:45,218 --> 01:02:46,051
문제?

1152
01:02:46,901 --> 01:02:51,068
- [학생] [듣기에는 너무 낮게 말하기]

1153
01:02:53,095 --> 01:02:57,262
이제 그라디언트 [듣기에는 너무 낮습니다.]

1154
01:02:58,163 --> 01:02:59,710
- 질문은 그라디언트가 어떻게되는지에 대한 것입니다.

1155
01:02:59,710 --> 01:03:02,697
탈락과 훈련 도중?

1156
01:03:02,697 --> 01:03:03,530
당신 말이 맞아요.

1157
01:03:03,530 --> 01:03:04,532
우리는 단지 그라디언트를 전파하게됩니다.

1158
01:03:04,532 --> 01:03:07,068
떨어 뜨리지 않은 노드를 통해

1159
01:03:07,068 --> 01:03:09,494
이것은

1160
01:03:09,494 --> 01:03:10,851
당신이 중퇴로 훈련 할 때,

1161
01:03:10,851 --> 01:03:12,711
일반적으로 교육은

1162
01:03:12,711 --> 01:03:14,034
각 단계에서 업데이트 만하고 있습니다.

1163
01:03:14,034 --> 01:03:15,841
네트워크의 일부 하위 항목.

1164
01:03:15,841 --> 01:03:17,047
드롭 아웃을 사용하는 경우 일반적으로

1165
01:03:17,047 --> 01:03:19,022
훈련하는 데 시간이 오래 걸리지 만

1166
01:03:19,022 --> 01:03:22,772
그것이 수렴 된 후에 더 나은 일반화.

1167
01:03:24,894 --> 01:03:27,578
탈락, 우리는 이런 종류의 것이 었습니다.

1168
01:03:27,578 --> 01:03:28,859
구체적인 인스턴스화.

1169
01:03:28,859 --> 01:03:30,394
좀 더 일반적인 전략이 있습니다.

1170
01:03:30,394 --> 01:03:33,295
훈련 도중 정규화를 위해

1171
01:03:33,295 --> 01:03:35,570
우리는 네트워크에 어떤 종류의 임의성을 추가합니다.

1172
01:03:35,570 --> 01:03:37,967
트레이닝 데이터를 잘 맞추지 못하게합니다.

1173
01:03:37,967 --> 01:03:39,619
그걸 엉망으로 만들지 마라.

1174
01:03:39,619 --> 01:03:41,522
훈련 데이터를 완벽하게 피팅하는 것.

1175
01:03:41,522 --> 01:03:43,010
이제 테스트 시간에 평균을 내고 싶습니다.

1176
01:03:43,010 --> 01:03:45,076
희망을 갖고 개선 할 모든 무작위성

1177
01:03:45,076 --> 01:03:46,645
우리 일반화.

1178
01:03:46,645 --> 01:03:48,506
드롭 아웃이 가장 일반적인 예일 것입니다

1179
01:03:48,506 --> 01:03:50,734
이 전략 유형의

1180
01:03:50,734 --> 01:03:54,412
일괄 정규화 일종의 아이디어도이 아이디어에 부합한다.

1181
01:03:54,412 --> 01:03:57,218
배치 정상화, 훈련 중 기억,

1182
01:03:57,218 --> 01:03:59,481
하나의 데이터 요소가 다른 미니
배치에 나타날 수 있습니다.

1183
01:03:59,481 --> 01:04:01,240
다른 다른 데이터 포인트로

1184
01:04:01,240 --> 01:04:02,798
존경심과 관련하여 약간의 확률이 있습니다.

1185
01:04:02,798 --> 01:04:04,912
얼마나 정확히 하나의 데이터 포인트로

1186
01:04:04,912 --> 01:04:07,685
그 시점은 훈련 중에 정상화됩니다.

1187
01:04:07,685 --> 01:04:09,792
하지만 이제 테스트 시간에, 우리는 평균 아웃

1188
01:04:09,792 --> 01:04:11,531
이 확률론은

1189
01:04:11,531 --> 01:04:13,272
글로벌 추정치가

1190
01:04:13,272 --> 01:04:15,220
미니 배치 당 추정치.

1191
01:04:15,220 --> 01:04:17,021
사실 일괄 정규화는

1192
01:04:17,021 --> 01:04:18,910
비슷한 규칙 효과의 종류

1193
01:04:18,910 --> 01:04:20,708
그들이 모두 소개하기 때문에 중퇴로

1194
01:04:20,708 --> 01:04:22,707
어떤 종류의 확률이나 소음

1195
01:04:22,707 --> 01:04:24,736
훈련 시간에, 그러나 그 후에 그것을 밖으로 평균

1196
01:04:24,736 --> 01:04:25,963
시험 시간에.

1197
01:04:25,963 --> 01:04:28,473
사실, 당신이 네트워크를 훈련 할 때

1198
01:04:28,473 --> 01:04:30,485
일괄 정규화, 때때로 당신은 사용하지 않는다.

1199
01:04:30,485 --> 01:04:32,467
드롭 아웃 및 배치 일괄 정규화

1200
01:04:32,467 --> 01:04:34,244
충분히 정규화 효과를 더한다.

1201
01:04:34,244 --> 01:04:36,229
귀하의 네트워크에.

1202
01:04:36,229 --> 01:04:37,697
당신이 할 수 있기 때문에 dropout은 다소 멋지다.

1203
01:04:37,697 --> 01:04:39,341
실제로 정규화 강도를 조정한다.

1204
01:04:39,341 --> 01:04:41,481
그 매개 변수 p를 변화시킴으로써

1205
01:04:41,481 --> 01:04:44,318
배치 정규화에서 제어.

1206
01:04:44,318 --> 01:04:46,357
다른 종류의 전략

1207
01:04:46,357 --> 01:04:49,413
이 패러다임은 데이터 증가에 대한 아이디어입니다.

1208
01:04:49,413 --> 01:04:51,494
훈련 중, 바닐라 버전

1209
01:04:51,494 --> 01:04:53,816
교육을 위해 데이터가 있으며 레이블이 있습니다.

1210
01:04:53,816 --> 01:04:57,563
우리는 매 시간 단계마다 CNN을
업데이트하기 위해이 정보를 사용합니다.

1211
01:04:57,563 --> 01:04:59,166
하지만 대신 우리가 할 수있는 일은 무작위 적입니다.

1212
01:04:59,166 --> 01:05:02,233
훈련 도중 어떤 식 으로든 이미지를 변형시킨다.

1213
01:05:02,233 --> 01:05:04,040
라벨이 보존되도록한다.

1214
01:05:04,040 --> 01:05:06,572
이제 이러한 무작위 변환에 대해 교육합니다.

1215
01:05:06,572 --> 01:05:09,903
원본 이미지가 아닌 이미지의

1216
01:05:09,903 --> 01:05:12,464
때로는 임의의 수평 반전이 나타날 수 있습니다.

1217
01:05:12,464 --> 01:05:14,055
고양이를 가져 와서 뒤집으면

1218
01:05:14,055 --> 01:05:16,638
가로로, 그것은 여전히 고양이입니다.

1219
01:05:18,175 --> 01:05:20,309
크기가 다른 작물을 무작위로 샘플링합니다.

1220
01:05:20,309 --> 01:05:22,081
무작위로 자르기 때문에 이미지에서

1221
01:05:22,081 --> 01:05:24,248
고양이의 고양이는 여전히 고양이입니다.

1222
01:05:25,673 --> 01:05:27,677
그런 다음 테스트하는 동안 평범한

1223
01:05:27,677 --> 01:05:30,802
이 확률은

1224
01:05:30,802 --> 01:05:32,993
고정 된 작물 세트, 종종 네 모퉁이

1225
01:05:32,993 --> 01:05:34,794
그리고 중간과 그들의 플립.

1226
01:05:34,794 --> 01:05:36,443
아주 흔한 것은 당신이 읽을 때,

1227
01:05:36,443 --> 01:05:38,526
예를 들어, ImageNet에 관한 논문은

1228
01:05:38,526 --> 01:05:40,452
해당 모델의 단일 작물 성능,

1229
01:05:40,452 --> 01:05:41,919
이것은 전체 이미지와 같습니다.

1230
01:05:41,919 --> 01:05:43,560
그리고 그들의 모델에 대한 10 가지 작물 성능,

1231
01:05:43,560 --> 01:05:46,376
이 5 가지 표준 작물입니다.

1232
01:05:46,376 --> 01:05:47,793
플립 플러스.

1233
01:05:48,723 --> 01:05:50,891
또한 데이터 증가와 함께, 당신은 때때로

1234
01:05:50,891 --> 01:05:53,104
색상 지터 사용, 임의로

1235
01:05:53,104 --> 01:05:55,361
이미지의 대비 또는 밝기를 변경하십시오.

1236
01:05:55,361 --> 01:05:56,830
훈련 도중.

1237
01:05:56,830 --> 01:05:58,168
좀 더 복잡해질 수 있어요.

1238
01:05:58,168 --> 01:05:59,998
색상 지터를 사용하여

1239
01:05:59,998 --> 01:06:01,910
어쩌면 색상 불안감을 줄 수 있습니다.

1240
01:06:01,910 --> 01:06:05,127
데이터 공간의 PCA 방향 또는 기타

1241
01:06:05,127 --> 01:06:07,168
색상 지터를하는 곳

1242
01:06:07,168 --> 01:06:10,024
일부 데이터에 의존하는 방식으로

1243
01:06:10,024 --> 01:06:11,941
조금 덜 일반적.

1244
01:06:12,977 --> 01:06:15,135
일반적으로 데이터 증가는 실제로

1245
01:06:15,135 --> 01:06:16,402
신청할 수있는 일반적인 사항

1246
01:06:16,402 --> 01:06:18,522
단지 어떤 문제라도.

1247
01:06:18,522 --> 01:06:20,206
당신이 해결하려고하는 모든 문제,

1248
01:06:20,206 --> 01:06:22,054
너는 어떤 방법에 대해 생각하는거야?

1249
01:06:22,054 --> 01:06:24,146
내 데이터를 변환 할 수있는

1250
01:06:24,146 --> 01:06:25,425
라벨을 바꿨나요?

1251
01:06:25,425 --> 01:06:26,721
이제 교육 기간 중에는

1252
01:06:26,721 --> 01:06:29,630
이러한 무작위 변환은 입력 데이터에 적용됩니다.

1253
01:06:29,630 --> 01:06:31,703
이런 종류의 정규화 효과가 있습니다.

1254
01:06:31,703 --> 01:06:33,683
다시 말하면 네트워크에

1255
01:06:33,683 --> 01:06:35,856
훈련 중 어떤 종류의 확률론,

1256
01:06:35,856 --> 01:06:39,439
시험 시간에 그것을 밖으로 marginalize.

1257
01:06:40,540 --> 01:06:43,131
이제 우리는이 패턴의 세 가지 예를 보았습니다.

1258
01:06:43,131 --> 01:06:45,717
드롭 아웃, 일괄 정규화, 데이터 증가,

1259
01:06:45,717 --> 01:06:47,639
그러나 다른 많은 예들도 있습니다.

1260
01:06:47,639 --> 01:06:49,456
일단 당신이 당신의 마음에이 패턴을 가지고 있다면,

1261
01:06:49,456 --> 01:06:51,067
너는 이걸 알아볼거야.

1262
01:06:51,067 --> 01:06:53,534
때때로 다른 신문을 읽으면서.

1263
01:06:53,534 --> 01:06:55,520
탈락과 관련된 또 다른 종류의 아이디어가 있습니다.

1264
01:06:55,520 --> 01:06:57,207
DropConnect라고합니다.

1265
01:06:57,207 --> 01:06:59,339
DropConnect를 사용하면 같은 생각입니다.

1266
01:06:59,339 --> 01:07:01,978
그러나 활성화를 제로로하기보다는

1267
01:07:01,978 --> 01:07:03,952
모든 전진 패스에서 대신 무작위로

1268
01:07:03,952 --> 01:07:06,750
대신에 가중치 행렬의 일부 값을 제로화하십시오.

1269
01:07:06,750 --> 01:07:10,137
다시 말하지만, 비슷한 종류의 풍미가 있습니다.

1270
01:07:10,137 --> 01:07:13,367
내가 좋아하는 멋진 아이디어의 또 다른 종류,

1271
01:07:13,367 --> 01:07:14,913
이건 너무 일반적으로 사용되지는 않지만 나는

1272
01:07:14,913 --> 01:07:16,766
그냥 정말 멋진 생각이라고 생각해.

1273
01:07:16,766 --> 01:07:19,885
분수 맥스 풀링에 대한 아이디어입니다.

1274
01:07:19,885 --> 01:07:22,057
일반적으로 2 x 2 최대 풀링을 할 때,

1275
01:07:22,057 --> 01:07:24,294
이 고정 된 2 x 2 영역이 있습니다.

1276
01:07:24,294 --> 01:07:26,642
당신이 앞으로 나아가는 동안,

1277
01:07:26,642 --> 01:07:29,552
그러나 이제 분수 최대 풀링과 함께,

1278
01:07:29,552 --> 01:07:32,185
우리가 풀링 레이어를 가질 때마다,

1279
01:07:32,185 --> 01:07:33,872
우리는 수영장을 정확히 무작위로 선정하려고합니다.

1280
01:07:33,872 --> 01:07:36,336
우리가 넘는 지역.

1281
01:07:36,336 --> 01:07:38,197
여기 오른쪽의 예에서,

1282
01:07:38,197 --> 01:07:39,971
나는 세 가지 세트를 보여 줬어.

1283
01:07:39,971 --> 01:07:41,954
보이는 임의의 풀링 영역 중

1284
01:07:41,954 --> 01:07:43,555
훈련 도중.

1285
01:07:43,555 --> 01:07:46,720
이제 테스트 시간 동안, 당신은 평범한 분이 십니다.

1286
01:07:46,720 --> 01:07:49,342
많은 다른 것을 시도함으로써 확률론,

1287
01:07:49,342 --> 01:07:52,882
풀링 영역의 일부 고정 세트에 고정하거나

1288
01:07:52,882 --> 01:07:55,189
많은 샘플을 그려 평균화합니다.

1289
01:07:55,189 --> 01:07:56,519
그것은 멋진 생각입니다.

1290
01:07:56,519 --> 01:07:59,512
그것은 그렇게 일반적으로 사용되지 않습니다.

1291
01:07:59,512 --> 01:08:01,793
정말 놀라운 또 다른 종이

1292
01:08:01,793 --> 01:08:04,878
실제로 나온이 패러다임에서

1293
01:08:04,878 --> 01:08:06,375
지난 1 년 동안

1294
01:08:06,375 --> 01:08:08,612
우리가 수업을 마지막으로 가르쳤을 때,이 생각입니다.

1295
01:08:08,612 --> 01:08:10,396
확률 적 깊이의

1296
01:08:10,396 --> 01:08:13,087
여기에 우리는 왼쪽에 네트워크가 있습니다.

1297
01:08:13,087 --> 01:08:15,975
아이디어는 우리가 매우 깊은
네트워크를 가지고 있다는 것입니다.

1298
01:08:15,975 --> 01:08:17,760
네트워크에서 무작위로 레이어를 삭제하려고합니다.

1299
01:08:17,760 --> 01:08:19,015
훈련 도중.

1300
01:08:19,015 --> 01:08:21,459
훈련 도중, 우리는

1301
01:08:21,459 --> 01:08:23,046
일부 레이어 및 일부 하위 집합 만 사용

1302
01:08:23,046 --> 01:08:24,598
훈련 중 층의

1303
01:08:24,599 --> 01:08:27,339
이제 테스트 시간 동안 우리는
전체 네트워크를 사용할 것입니다.

1304
01:08:27,339 --> 01:08:28,747
이것은 일종의 미친 짓이다.

1305
01:08:28,747 --> 01:08:30,736
이 작품이 놀랍습니다.

1306
01:08:30,736 --> 01:08:32,337
그러나 이것은 비슷한 종류의 경향이있다.

1307
01:08:32,337 --> 01:08:33,955
드롭 아웃으로 정규화 효과

1308
01:08:33,955 --> 01:08:35,795
그리고이 다른 전략들.

1309
01:08:35,795 --> 01:08:38,470
그러나 다시 말하지만, 이것은
슈퍼하고 최첨단의 연구입니다.

1310
01:08:38,470 --> 01:08:40,693
이것은 실제적으로 일반적으로 많이 사용되는 것이 아니며,

1311
01:08:40,694 --> 01:08:42,527
하지만 멋진 아이디어입니다.

1312
01:08:45,179 --> 01:08:49,096
정규화에 관한 막판 질문이 있습니까?

1313
01:08:50,429 --> 01:08:52,201
아니? 그걸 써. 좋은 생각입니다.

1314
01:08:52,201 --> 01:08:53,158
네?

1315
01:08:53,158 --> 01:08:57,531
- [학생] [듣기에는 너무 낮게 말하기]

1316
01:08:57,531 --> 01:08:58,669
- 문제는 보통 당신이 사용하는 것입니까?

1317
01:08:58,669 --> 01:09:01,669
하나 이상의 정규화 방법?

1318
01:09:04,810 --> 01:09:07,065
일반적으로 배치 정규화를 사용해야합니다.

1319
01:09:07,066 --> 01:09:08,515
가지고있는 좋은 일종의

1320
01:09:08,515 --> 01:09:10,237
요즘 대부분의 네트워크에서

1321
01:09:10,237 --> 01:09:13,135
특히 매우 깊은 것들에 대해 수렴하는 데 도움이됩니다.

1322
01:09:13,135 --> 01:09:15,399
많은 경우, 일괄 정규화 만

1323
01:09:15,399 --> 01:09:18,228
경향이 있지만, 때로는 때로는

1324
01:09:18,228 --> 01:09:20,608
일괄 정규화만으로 충분하지 않다면,

1325
01:09:20,608 --> 01:09:22,227
드롭 아웃 추가를 고려해 볼 수 있습니다.

1326
01:09:22,227 --> 01:09:25,689
또는 다른 것들을 일단 네트워크
overfitting을 참조하십시오.

1327
01:09:25,689 --> 01:09:27,882
일반적으로 시각 장애인 교차 검증을 수행하지 않습니다.

1328
01:09:27,883 --> 01:09:29,011
이 일들에.

1329
01:09:29,011 --> 01:09:31,009
대신, 당신은 그들을 타겟 방식으로 추가합니다.

1330
01:09:31,010 --> 01:09:34,427
네트워크가 지나치게 적합하다는 것을 알게되면

1331
01:09:36,885 --> 01:09:39,466
하나의 빠른 것은, 그것은 전송 학습의 아이디어입니다.

1332
01:09:39,466 --> 01:09:41,225
우리는 정규화와 함께 보았습니다.

1333
01:09:41,225 --> 01:09:43,004
우리는 사이의 간격을 줄이는 데 도움이 될 수 있습니다.

1334
01:09:43,004 --> 01:09:45,336
열차와 시험 오류가 다른

1335
01:09:45,336 --> 01:09:47,503
정규화 전략.

1336
01:09:49,388 --> 01:09:51,576
overfitting의 한 가지 문제점은 때때로 있습니다.

1337
01:09:51,576 --> 01:09:53,497
당신은 충분한 데이터가 없다는 이유로 과장됩니다.

1338
01:09:53,497 --> 01:09:55,117
크고 강력한 모델을 사용하고 싶습니다.

1339
01:09:55,117 --> 01:09:57,400
그 크고 강력한 모델은

1340
01:09:57,400 --> 01:10:00,929
소규모 데이터 세트에 너무 많이 적용됩니다.

1341
01:10:00,929 --> 01:10:03,316
정규화는 그것을 해결하는 한 가지 방법이며,

1342
01:10:03,316 --> 01:10:06,394
다른 방법은 이전 학습을 사용하는 것입니다.

1343
01:10:06,394 --> 01:10:08,667
이 신화를 흉상의 종류를 옮기십시오 옮기십시오

1344
01:10:08,667 --> 01:10:11,180
방대한 양의 데이터가 필요 없다는 점

1345
01:10:11,180 --> 01:10:13,215
CNN을 훈련시키기 위해서.

1346
01:10:13,215 --> 01:10:15,785
아이디어는 정말 간단합니다.

1347
01:10:15,785 --> 01:10:18,296
당신은 아마도 먼저 CNN을 가져갈 것입니다.

1348
01:10:18,296 --> 01:10:21,283
다음은 VGG 스타일 아키텍처의 일종입니다.

1349
01:10:21,283 --> 01:10:23,078
너 CNN 가져가, 너 훈련 할거야.

1350
01:10:23,078 --> 01:10:25,516
ImageNet과 같은 매우 큰 데이터 세트에서,

1351
01:10:25,516 --> 01:10:26,709
실제로 충분한 데이터가있는 곳

1352
01:10:26,709 --> 01:10:28,524
전체 네트워크를 훈련시키는 것.

1353
01:10:28,524 --> 01:10:30,488
이제 아이디어는 당신이

1354
01:10:30,488 --> 01:10:32,715
이 데이터 세트에서 일부 기능에 이르기까지

1355
01:10:32,715 --> 01:10:35,081
작은 데이터 세트.

1356
01:10:35,081 --> 01:10:37,078
아마도 1,000 명을 분류하는 대신

1357
01:10:37,078 --> 01:10:39,627
ImageNet 카테고리, 이제 분류하고 싶습니다.

1358
01:10:39,627 --> 01:10:41,775
10 마리의 개 유형 또는 그런 것.

1359
01:10:41,775 --> 01:10:43,349
작은 데이터 세트 만 있습니다.

1360
01:10:43,349 --> 01:10:46,402
여기에 우리의 작은 데이터
세트에는 C 클래스 만 있습니다.

1361
01:10:46,402 --> 01:10:48,900
그러면 당신이 일반적으로 할 일은이 마지막 일입니다.

1362
01:10:48,900 --> 01:10:51,421
모든 연결 계층

1363
01:10:51,421 --> 01:10:54,453
마지막 층은 최종 학급 점수를 특징으로하며,

1364
01:10:54,453 --> 01:10:58,620
이제이 행렬을 무작위로 다시 초기화해야합니다.

1365
01:11:00,136 --> 01:11:02,191
ImageNet의 경우 4,096 x 1,000

1366
01:11:02,191 --> 01:11:03,437
3 차원 행렬.

1367
01:11:03,437 --> 01:11:06,542
이제 새로운 수업을 위해

1368
01:11:06,542 --> 01:11:09,667
4,096-by-C 또는 10 또는 어떤 것이 든 될 수 있습니다.

1369
01:11:09,667 --> 01:11:12,154
이 마지막 행렬을 무작위로 다시 초기화하면,

1370
01:11:12,154 --> 01:11:14,470
이전의 모든 레이어의 가중치 고정

1371
01:11:14,470 --> 01:11:17,406
이제는 기본적으로 선형 분류자를 학습합니다.

1372
01:11:17,406 --> 01:11:19,682
이 마지막 레이어의 매개 변수 만 트레이닝합니다.

1373
01:11:19,682 --> 01:11:22,432
귀하의 데이터에 수렴되도록하십시오.

1374
01:11:24,273 --> 01:11:25,750
이것은 꽤 잘 작동하는 경향이 있습니다.

1375
01:11:25,750 --> 01:11:29,241
함께 사용할 매우 작은 데이터 세트가 있어야합니다.

1376
01:11:29,241 --> 01:11:31,373
이제 조금 더 많은 데이터가 있다면,

1377
01:11:31,373 --> 01:11:32,874
시도 할 수있는 또 다른 사항은 실제로

1378
01:11:32,874 --> 01:11:35,651
전체 네트워크 미세 조정.

1379
01:11:35,651 --> 01:11:37,910
상위 레이어가 수렴 한 후

1380
01:11:37,910 --> 01:11:40,051
당신은 당신의 데이터에 대한 마지막 레이어를 배우고,

1381
01:11:40,051 --> 01:11:42,683
실제로 업데이트를 시도 할 수 있습니다.

1382
01:11:42,683 --> 01:11:45,420
뿐만 아니라 전체 네트워크.

1383
01:11:45,420 --> 01:11:47,532
더 많은 데이터가 있다면

1384
01:11:47,532 --> 01:11:49,919
네트워크의 더 큰 부분을 업데이트합니다.

1385
01:11:49,919 --> 01:11:52,304
일반적인 전략은 다음과 같습니다.

1386
01:11:52,304 --> 01:11:54,461
당신은 네트워크를 업데이트하고 있습니다.

1387
01:11:54,461 --> 01:11:56,628
초기 학습 률로부터 학습 률

1388
01:11:56,628 --> 01:11:59,972
아마 원래의 매개 변수들

1389
01:11:59,972 --> 01:12:02,129
ImageNet에 수렴 된이 네트워크에서

1390
01:12:02,129 --> 01:12:03,458
아마 꽤 잘 일반적으로 일했습니다,

1391
01:12:03,458 --> 01:12:05,222
그리고 당신은 그것들을 아주 작은
양으로 바꾸고 싶을뿐입니다.

1392
01:12:05,222 --> 01:12:09,090
데이터 세트의 성능을 조정할 수 있습니다.

1393
01:12:09,090 --> 01:12:10,708
그런 다음 이전 학습을 할 때,

1394
01:12:10,708 --> 01:12:12,728
이 2x2 격자를 상상해보세요.

1395
01:12:12,728 --> 01:12:15,975
시나리오의 한쪽에서

1396
01:12:15,975 --> 01:12:17,605
어쩌면 아주 작은 양의 데이터를

1397
01:12:17,605 --> 01:12:19,181
데이터 세트 또는 매우 많은 양의 데이터

1398
01:12:19,181 --> 01:12:20,598
귀하의 데이터 세트.

1399
01:12:21,673 --> 01:12:24,786
그러면 데이터가 이미지와 매우 유사 할 수도 있습니다.

1400
01:12:24,786 --> 01:12:27,023
마찬가지로 ImageNet에는
동물의 그림이 많이 있습니다.

1401
01:12:27,023 --> 01:12:29,265
식물과 그런 것들.

1402
01:12:29,265 --> 01:12:31,043
다른 유형을 분류하려는 경우

1403
01:12:31,043 --> 01:12:33,046
동식물 및 다른 유형의 이미지

1404
01:12:33,046 --> 01:12:35,820
그런 식으로, 당신은 꽤 좋은 모양입니다.

1405
01:12:35,820 --> 01:12:38,025
그런 다음 일반적으로 데이터가

1406
01:12:38,025 --> 01:12:42,233
ImageNet과 매우 비슷합니다.

1407
01:12:42,233 --> 01:12:43,820
데이터 양이 매우 적은 경우,

1408
01:12:43,820 --> 01:12:46,026
당신은 기본적으로 선형 분류자를 훈련시킬 수 있습니다.

1409
01:12:46,026 --> 01:12:47,685
기능 맨 위에있는

1410
01:12:47,685 --> 01:12:49,346
ImageNet 모델.

1411
01:12:49,346 --> 01:12:52,121
함께 작업 할 데이터가 조금 있으면,

1412
01:12:52,121 --> 01:12:55,271
그러면 데이터를 미세 조정할 수 있습니다.

1413
01:12:55,271 --> 01:12:56,730
그러나 때때로 문제가 생깁니다.

1414
01:12:56,730 --> 01:12:59,240
귀하의 데이터가 ImageNet과
매우 다른 것처럼 보이는 경우.

1415
01:12:59,240 --> 01:13:01,108
어쩌면 당신이 어쩌면

1416
01:13:01,108 --> 01:13:03,635
엑스레이 또는 CAT 스캔 인 의료 영상

1417
01:13:03,635 --> 01:13:05,258
매우 다른 것처럼 보이는

1418
01:13:05,258 --> 01:13:07,266
ImageNet의 이미지에서, 그 경우,

1419
01:13:07,266 --> 01:13:09,557
당신은 조금 더 창조적 일 필요가있을 것입니다.

1420
01:13:09,557 --> 01:13:11,332
때때로 그것은 아직도 여기에서 잘 작동합니다,

1421
01:13:11,332 --> 01:13:13,634
그 마지막 레이어 피처는

1422
01:13:13,634 --> 01:13:14,893
너무 유익하다.

1423
01:13:14,893 --> 01:13:17,174
더 큰 부품을 다시 초기화하는 것을 고려해보십시오.

1424
01:13:17,174 --> 01:13:18,305
네트워크 및 약간의 정보 얻기

1425
01:13:18,305 --> 01:13:21,992
더 창의적이고 더 많은 실험을 시도해보십시오.

1426
01:13:21,992 --> 01:13:23,417
당신이 가진다면 다소 완화됩니다.

1427
01:13:23,417 --> 01:13:25,759
매우 다른 데이터 세트의 많은 양의 데이터

1428
01:13:25,759 --> 01:13:26,966
그런 다음 실제로 조정할 수 있습니다.

1429
01:13:26,966 --> 01:13:29,500
네트워크의 더 큰 부분.

1430
01:13:29,500 --> 01:13:31,193
이 아이디어는 제가 만들고 싶습니다.

1431
01:13:31,193 --> 01:13:33,072
전달 학습은 매우 보편적이다.

1432
01:13:33,072 --> 01:13:36,145
실제로는 예외가 아니라 표준입니다.

1433
01:13:36,145 --> 01:13:37,677
컴퓨터 비전 보고서를 읽으면서,

1434
01:13:37,677 --> 01:13:39,523
당신은 종종 이런 시스템 다이어그램을 볼 것입니다.

1435
01:13:39,523 --> 01:13:41,047
다른 작업.

1436
01:13:41,047 --> 01:13:42,587
왼쪽에서 우리는 물체 감지 작업을하고 있습니다.

1437
01:13:42,587 --> 01:13:45,191
오른쪽에서 이미지 캡션 작업을하고 있습니다.

1438
01:13:45,191 --> 01:13:46,764
이 두 모델 모두 CNN

1439
01:13:46,764 --> 01:13:48,872
그것은 이미지 처리의 일종입니다.

1440
01:13:48,872 --> 01:13:51,118
컴퓨터 비전의 거의 모든 응용 프로그램에서

1441
01:13:51,118 --> 01:13:53,131
요즘 대부분의 사람들은 훈련을받지 않습니다.

1442
01:13:53,131 --> 01:13:54,398
이러한 것들을 처음부터.

1443
01:13:54,398 --> 01:13:56,344
거의 항상 CNN이 사전 교육을 받게 될 것입니다.

1444
01:13:56,344 --> 01:13:58,273
ImageNet에서 확인한 다음 미세 조정할 수 있습니다.

1445
01:13:58,273 --> 01:14:00,458
가까이에있는 작업을 위해.

1446
01:14:00,458 --> 01:14:03,098
또한 자막 의미에서 가끔은

1447
01:14:03,098 --> 01:14:05,872
실제로 일부 단어 벡터 사전 연습

1448
01:14:05,872 --> 01:14:07,574
또한 언어와 관련되어 있습니다.

1449
01:14:07,574 --> 01:14:09,847
CNN을 ImageNet에서 미리 연습 할 수도 있지만,

1450
01:14:09,847 --> 01:14:11,214
큰 단어에 몇 개의 단어 벡터를 프리 트레인하다.

1451
01:14:11,214 --> 01:14:12,954
텍스트 코퍼스를 선택하고 전체를 미세하게 조정하십시오.

1452
01:14:12,954 --> 01:14:14,628
귀하의 데이터 세트.

1453
01:14:14,628 --> 01:14:16,188
캡션의 경우에는

1454
01:14:16,188 --> 01:14:17,927
단어 벡터를 이용한이 사전 학습은

1455
01:14:17,927 --> 01:14:19,392
조금 덜 공통적이다.

1456
01:14:19,392 --> 01:14:22,763
조금 덜 중요합니다.

1457
01:14:22,763 --> 01:14:24,384
귀하의 프로젝트를위한 테이크 아웃,

1458
01:14:24,384 --> 01:14:26,554
더 일반적으로 다른 모델에서 작업 할 때,

1459
01:14:26,554 --> 01:14:29,988
큰 데이터 세트가있을 때마다,

1460
01:14:29,988 --> 01:14:31,105
언제든지 문제가 생길 때마다

1461
01:14:31,105 --> 01:14:33,710
태클하고 싶지만 대용량 데이터 세트가 없습니다.

1462
01:14:33,710 --> 01:14:37,601
그럼 당신이 일반적으로해야 할 일은 다운로드

1463
01:14:37,601 --> 01:14:39,679
상대적으로 가까운 일부 사전 훈련 된 모델

1464
01:14:39,679 --> 01:14:42,158
관심있는 작업에 연결 한 다음

1465
01:14:42,158 --> 01:14:43,859
그 모델의 일부분을 다시 초기화하거나 미세 조정

1466
01:14:43,859 --> 01:14:45,344
데이터 모델.

1467
01:14:45,344 --> 01:14:48,404
네가 가지고 있더라도 꽤 잘하는 경향이있어.

1468
01:14:48,404 --> 01:14:49,693
적당한 양의 훈련 데이터

1469
01:14:49,693 --> 01:14:51,161
함께 일해.

1470
01:14:51,161 --> 01:14:52,998
이것이 일반적인 전략이기 때문에,

1471
01:14:52,998 --> 01:14:54,167
모든 다른 깊은 학습

1472
01:14:54,167 --> 01:14:56,314
거기 밖으로 소프트웨어 패키지 제공

1473
01:14:56,314 --> 01:14:58,223
방금 다운로드 할 수있는 모델 동물원

1474
01:14:58,223 --> 01:15:01,584
다양한 모델의 사전 훈련 된 버전.

1475
01:15:01,584 --> 01:15:04,009
오늘 요약하면, 우리는 최적화,

1476
01:15:04,009 --> 01:15:06,528
이는 교육 손실을 개선하는 방법에 관한 것입니다.

1477
01:15:06,528 --> 01:15:08,568
우리는 정규화에 대해 이야기했습니다.

1478
01:15:08,568 --> 01:15:11,369
테스트 데이터에 대한 귀하의 실적.

1479
01:15:11,369 --> 01:15:13,323
거기에 맞는 앙상블을 모델로합니다.

1480
01:15:13,323 --> 01:15:14,724
또한 우리는 이전 학습에 대해서도 이야기했습니다.

1481
01:15:14,724 --> 01:15:16,223
당신이 실제로 더 잘할 수있는 방법입니다.

1482
01:15:16,223 --> 01:15:17,925
데이터가 적습니다.

1483
01:15:17,925 --> 01:15:19,551
이것들은 모두 매우 유용한 전략입니다.

1484
01:15:19,551 --> 01:15:22,425
프로젝트에서나 그 이상에서 사용해야합니다.

1485
01:15:22,425 --> 01:15:24,717
다음 번에는 더 구체적으로 이야기하겠습니다.

1486
01:15:24,717 --> 01:15:25,723
다른 깊은 학습의 일부

1487
01:15:25,723 --> 00:00:00,000
거기 밖으로 소프트웨어 패키지.

