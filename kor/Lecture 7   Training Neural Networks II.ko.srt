1
-00:00:00,485 --> 00:00:05,515
Translated by visionNoob, KNU
https://github.com/insurgent92/CS231N_17_KOR_SUB

2
00:00:08,355 --> 00:00:11,357
자 12시가 지났군요 시작합니다.

3
00:00:14,644 --> 00:00:17,419
우선 지난시간에 배운 내용을 복습하겠습니다.

4
00:00:17,419 --> 00:00:23,400
지난시간에 Nerural networks를 학습 시킬 때 필요한 여러가지
중요한 것들을 배웠습니다.

5
00:00:23,400 --> 00:00:30,439
지난 시간에 배운 내용을 다시 한번 살펴 본 뒤에
몇 가지 더 배워보겠습니다.

6
00:00:30,439 --> 00:00:34,707
우선 공지사항을 몇 가지 전달합니다.

7
00:00:34,707 --> 00:00:39,645
우선 과제 1의 마감기한이 종료되었습니다.
여러분 모두 잘 제출했길 바랍니다.

8
00:00:39,645 --> 00:00:57,322
다들 잘 했나요?
과제 1은 체점 중에 있습니다.

9
00:00:57,322 --> 00:01:04,121
그리고 두 번째는 프로젝트 제안서 제출은
금일 11:59 PM 까지 입니다.

10
00:01:04,959 --> 00:01:09,074
오늘 까지 꼭 제출 해 주시기 바랍니다.
자세한 사항은 web과 Piazza에서 확인바랍니다.

11
00:01:09,074 --> 00:01:15,269
그리고 과제 2가 나왔습니다.
제출까지 1주일정도의 기간이 남았습니다.

12
00:01:15,269 --> 00:01:25,860
과제 2는 우리 수업에서 가장 오래 걸리는 과제 입니다.
그러니 서둘러 진행 하는 것을 추천드립니다.

13
00:01:27,122 --> 00:01:32,484
그리고 과제 2를 하실때 팁을 드리자면
현재 대다수가 Googld Cloud를 쓰고 계신데

14
00:01:32,484 --> 00:01:38,586
Googld Cloud에서 instances를 사용하지 않을 시에는
반드시 종료시켜 줘야 합니다. 아니면 요금이 계속 지불됩니다.

15
00:01:38,586 --> 00:01:42,899
여러분들이 쿠폰을 너무 많이 씁니다.

16
00:01:42,899 --> 00:01:52,223
instance에서 아무 작업도 하지 않더라도 켜져 있으면
요금이 부과됩니다.

17
00:01:52,223 --> 00:01:57,118
그러니 사용하지 않으면 반드시 종료시켜 줘야 합니다.

18
00:01:57,118 --> 00:02:04,970
여기 제 대쉬보드 화면인데, 이런 식으로 확실하게
STOP 버튼을 눌러줘야 합니다.

19
00:02:04,970 --> 00:02:08,644
매일 매일 이렇게 종료 시켜 주시기 바랍니다.

20
00:02:09,481 --> 00:02:20,853
그리고 Googld Cloud를 사용할 때 아셔하 할 것은
GPU를 쓰는 것이 CPU보다 더 비싸다는 것입니다.

21
00:02:20,853 --> 00:02:28,322
GPU를 쓰시면 어림잡아 시간당 거의 1 달러를 사용하게 됩니다.
상당히 가격이 높습니다.

22
00:02:28,322 --> 00:02:39,739
CPU instances는 조금 더 쌉니다. 추천 드리는 방법은 GPU
instance는 GPU가 필요할 때만 쓰는 것입니다.

23
00:02:39,739 --> 00:02:47,377
가령 과제 2를 하실 때 대게는 CPU만 있으면 됩니다. 따라서
CPU instance만 사용하면 됩니다.

24
00:02:47,377 --> 00:02:52,990
 하지만 과제2의 마지막 문제를 풀때 사용하는
Tensorflow와 PyTorch는 GPU가 필요합니다.

25
00:02:52,990 --> 00:02:58,897
CPU와 GPU instance를 각각 만들고 정말 필요할 때만
GPU instance를 사용하시길 권장드립니다.

26
00:02:58,897 --> 00:03:04,307
그러니 비용이 얼마나 지불되는 지를 모니터링 하면서
지나치게 지불되는 것을 주의하시기 바랍니다.

27
00:03:04,307 --> 00:03:07,748
공지사항 관련한 질문사항 있나요?

28
00:03:11,180 --> 00:03:12,182
문제.

29
00:03:12,182 --> 00:03:13,902
[학생이 질문]

30
00:03:13,902 --> 00:03:16,133
RAM을 얼마나 사용해야 하냐고 하셨는데

31
00:03:16,133 --> 00:03:21,863
과제용으로는 8~16기가면 충분합니다.

32
00:03:21,863 --> 00:03:27,114
CPU나 RAM을 너무 많이 추가하면 돈이 많이 듭니다.

33
00:03:27,114 --> 00:03:34,542
CPU는 2~4개를 쓰고 RAM은 8~16 기가를 쓰시면
과제 하실 때 충분한 사양입니다.

34
00:03:36,636 --> 00:03:40,417
지난 시간 내용을 복습해 보면
Activation function을 배웠습니다.

35
00:03:40,417 --> 00:03:44,962
지난 시간에 다양한 Activation Function과 각각의
특성을 배웠습니다.

36
00:03:44,962 --> 00:03:59,736
10년 전에는 sigmoid가 아주 유명했습니다. 하지만 Vanishing
gradients가 생기는 문제가 있었죠 tanh도 마찬가지 이구요

37
00:03:59,736 --> 00:04:09,230
그래서 요즘은 대부분 ReLU를 씁니다. 일반적인
네트워크에서 가장 잘 동작하는 녀석입니다.

38
00:04:09,230 --> 00:04:16,820
그래서 가중치 초기화에 대해서도 배웠습니다.

39
00:04:16,820 --> 00:04:23,787
가중치가 지나치게 작으면 activatiom이 사라집니다.

40
00:04:23,788 --> 00:04:29,583
작은 값이 여러 번 곱해지기 때문에 점점
0이 되는 것이었습니다.

41
00:04:29,583 --> 00:04:33,072
결국 모든 값이 0이 되고 학습은 일어나지 않습니다.

42
00:04:33,072 --> 00:04:41,208
반면에 가중치가 너무 큰 값으로 초기화되면 그 값이 또
계속 곱해질 것이고 결국은 터져버릴 것입니다(explode).

43
00:04:41,208 --> 00:04:45,389
이 경우에도 학습이 일어나지 않을 것입니다.

44
00:04:45,389 --> 00:04:58,531
Xavie/MSRA(HE) Initialzation 같은 방법으로 초기화를
잘 시켜주면 Activation의 분포를 좋게 유지시킬 수 있습니다.

45
00:04:58,531 --> 00:05:04,328
명심해야 할 점은 위의 것들이 Network가 깊어지면 깊어질
수록 더 중요하다는 것입니다.

46
00:05:04,328 --> 00:05:11,620
Network가 깊어지면 깊어질수록 가중치를 더 많이
곱하게 되기 때문입니다.

47
00:05:11,620 --> 00:05:23,666
그리고 데이터 전처리에 대해서도 배웠습니다. 그리고 CNN에서
흔히들 zero-mean과 noamalization을 한다고 배웠습니다.

48
00:05:23,666 --> 00:05:29,968
제가 여러분에게 왜 그걸 해야하는 지를 좀
더 직관적으로 말씀 드리겠습니다.

49
00:05:29,968 --> 00:05:39,532
자 여기에서 우리는 Binary classification 문제를 풉니다.
빨간/파란 점들을 나누는 것입니다.

50
00:05:39,532 --> 00:05:46,948
왼쪽의 경우 not normalized/centered 데이터 입니다.

51
00:05:46,948 --> 00:05:55,007
이 경우에도 물론 classification이 가능하지만 선이 조금만
움직여도 classification이 잘 되지 않습니다.

52
00:05:55,007 --> 00:06:05,992
왼쪽의 예시가 의미하는 것은 손심 함수가 아주 약간의
가중치 변화에도 엄청 예민하다는 것입니다.

53
00:06:07,315 --> 00:06:14,554
왼쪽의 경우 동일한 함수를 쓰더라도 학습 시키기 아주 어렵습니다.
재차 말씀드리지만 Loss가 파라미터에 너무 민감하기 때문입니다.

54
00:06:14,554 --> 00:06:25,351
반면 오른쪽은 데이터의 중심을 원점에 맞추고(zero-center),
Unit variance로 만들어 준 경우 입니다.

55
00:06:25,351 --> 00:06:35,523
오른쪽에서의 선이 조금씩 흔들리는 경우는 보면
손실 함수는 이런 가중치의 변동에 덜 민감함을 알 수 있습니다.

56
00:06:35,523 --> 00:06:41,064
이 경우가 최적화가 더 쉽습니다.
학습이 더 잘되는 것입니다.

57
00:06:41,064 --> 00:06:46,539
그리고 이는 Linear classification의 경우에만
국한되는 것이 아닙니다.

58
00:06:46,539 --> 00:06:57,756
Neural network 내부에도 다수의(interleavings)
linear classifer가 있다고 생각할 수 있습니다.

59
00:06:59,078 --> 00:07:05,687
이 경우에도 Neural network의 입력이 zero-centered가
아니고 Unit variance가 아닌 경우라면

60
00:07:05,687 --> 00:07:15,632
레이어의 Weight matrix가 아주 조금만 변해도 출력은 엄청
심하게 변하게 됩니다.  이는 학습을 어렵게 합니다.

61
00:07:15,632 --> 00:07:20,481
이를 통해 왜 nomalization가 중요한지를 좀 더
직관적으로 이해하실 수 있을 것입니다.

62
00:07:21,864 --> 00:07:26,862
자 우리는 Normalization이 엄청 중요하다는 것을 알고
있기 때문에 batch normalization도 배웠습니다.

63
00:07:26,862 --> 00:07:36,030
이는 actications이 zero mean과 unit variance가
될 수 있도록 레이어를 하나 추가하는 방법이었습니다.

64
00:07:36,030 --> 00:07:41,465
제가 이 슬라이드에 batch normalization 수식을
다시한번 적어 봤습니다.

65
00:07:41,465 --> 00:07:45,172
이 슬라이드를 보면 과제 2를 하실 때 수월할 것입니다.

66
00:07:45,172 --> 00:07:59,254
BN에서는 forward pass 시에 미니배치에서의 평균과 표준편차를
계산해서 Normalization을 수행했습니다.

67
00:07:59,254 --> 00:08:05,641
그리고 레이어의 유연한 표현성(expressivity)을 위해서
scale, shift 파라미터를 추가했습니다.

68
00:08:05,641 --> 00:08:09,990
과제 2를 하실때 이를 참고하시기 바랍니다.

69
00:08:09,990 --> 00:08:18,146
그리고 학습 과정을 다루는 방법도 배웠습니다. 학습 도중
Loss curve가 어떻게 보여야 하는지도 말이죠

70
00:08:18,146 --> 00:08:26,683
여기 예제 네트워크가 있습니다. 주말동안 한번 돌려봤습니다.
저는 학습을 진행할 때 이런 식으로 진행합니다.

71
00:08:26,683 --> 00:08:35,795
가운데 그래프는 시간에 따른 Loss 값을 나타냅니다.
네트워크가 Loss를 줄이고 있으면 잘 하고 있는 것이죠

72
00:08:35,795 --> 00:08:48,464
맨 오른쪽 그래프를 보면 X는 시간축이고 Y는 성능 지표입니다.
Training/Validation set의 성능지표를 나타냅니다.

73
00:08:48,465 --> 00:08:58,680
이를 해석해보면 Training set의 성능을 계속 올라가죠 Loss도
계속 내려갑니다. 하지만 validation은 침체하고 있습니다.

74
00:08:58,680 --> 00:09:05,066
이런 경우는 overfititing이라고 할 수 있겠습니다.
추가적인 regularization이 필요한 것입니다.

75
00:09:06,317 --> 00:09:09,504
그리고 이전 강의에서 hyperparameter search도 배웠습니다.

76
00:09:09,504 --> 00:09:14,798
네트워크에는 무수히 많은 하이퍼파라미터가 존재합니다.
이를 올바르게 잘 선택하는 것은 상당히 중요합니다.

77
00:09:14,798 --> 00:09:20,725
그리고 grid search와 random search를 배웠습니다.
이론상 random search가 더 좋았습니다.

78
00:09:20,725 --> 00:09:30,669
왜냐하면 성능이 특정 하이퍼파라미터에 의해 크게 좌우될 때 그
파라미터를 좀 더 넓은 범위로 탐색할 수 있기 때문입니다.

79
00:09:30,669 --> 00:09:37,005
그리고 하이퍼파라미터 최적화 시에
coarse search 이후 fine search를 한다고 배웠습니다.

80
00:09:37,005 --> 00:09:43,408
처음에는 하이퍼파라미터를 조금 더 넒은 범위에서 찾습니다.
Interation도 작게 줘서 학습시켜봅니다.(coarse)

81
00:09:43,408 --> 00:09:47,973
그리고 결과가 좋은 범위로 좁히는 것입니다.

82
00:09:47,973 --> 00:09:51,666
그리고 iterations를 조금 더 돌면서 더 작은 범위를
다시 탐색합니다. (fine search)

83
00:09:51,666 --> 00:09:56,708
적절한 하이퍼파라미터를 찾을 때 까지
이 과정을 반복합니다.

84
00:09:56,708 --> 00:10:04,455
가장 중요한 점은 coarse range를 설정할 때 가능한
최대한 넓은 범위를 설정해 줘야 한다는 것입니다.

85
00:10:04,455 --> 00:10:13,746
그 범위가 하이퍼파라미터 범위의 끝에서 끝까지 다 살펴볼 수
있도록 할수록 좋습니다. 충분위 넓은 범위를 사용해야 합니다.

86
00:10:17,462 --> 00:10:18,295
질문 있나요?

87
00:10:20,044 --> 00:10:26,672
[학생이 질문]

88
00:10:31,840 --> 00:10:34,554
질문은 보통 하이퍼파라미터를 몇 개씩 선택하는지 입니다.

89
00:10:34,554 --> 00:10:38,244
여기 예제에서는 2개 인데 보통은 2개보단 많습니다.

90
00:10:38,244 --> 00:10:45,442
그 선택은 모델에 따라 다릅니다. 선택한 하이퍼파라미터의 수가
많을 수록 기하급수적으로 경우의 수가 늘어납니다.

91
00:10:45,442 --> 00:10:48,012
때문에 한번에 너무 많이 할 수 는 없습니다.

92
00:10:48,012 --> 00:10:51,737
또 이는 여러분이 얼마나 많은 자원을
학습에 사용할 수 있는지도 중요합니다.

93
00:10:51,737 --> 00:10:55,745
이는 사람마다 바르고 실험마다 다릅니다.

94
00:10:55,745 --> 00:11:05,353
저같은 경우 두세개 정도를 고르는 편이고 많아도 네개 정도만
선택합니다. 그 이상이면 out of control이 되버립니다.

95
00:11:05,353 --> 00:11:10,406
일반적으로 Learning rate가 가장 중요합니다.
제일 먼저 못막아 놔야(nail) 합니다.

96
00:11:10,406 --> 00:11:19,542
regularization, learning rate decay,
model size 같은 것들은 LR보단 덜 중요합니다.

97
00:11:19,542 --> 00:11:22,723
Block Coordinate Descent(BCD)
같은 방법을 쓸 수도 있습니다.

98
00:11:22,723 --> 00:11:27,459
가령, 우선 Learning rate를 정해놓은 다음에 다양한
모델 사이즈를 시도해 보는 것입니다.

99
00:11:27,459 --> 00:11:30,759
이 방법을 쓰면 기하급수적으로 늘어나는 Search space를
조금은 줄일 수 있습니다.

100
00:11:30,759 --> 00:11:35,370
하지만 이 방법은 정확히 어떤 순서로 어떻게 찾아야
할지 정해야 하는 것이 가장 큰 문제입니다.

101
00:11:36,253 --> 00:11:38,120
다른 질문 있으신가요?

102
00:11:38,120 --> 00:11:57,041
[학생이 질문]

103
00:11:57,041 --> 00:12:04,537
질문은 우리가 어떤 하이퍼파리미터 값을 변경할 시에 다른 하이퍼파라미터
의 최적 값이 변해버리는 경우가 빈번하냐는 것입니다.

104
00:12:04,537 --> 00:12:11,339
그런 일이 가끔 발생하긴 합니다.  Learning rates가 이런 문제에
덜 민감함에도 실제로 이런 일이 발생하곤 합니다.

105
00:12:11,339 --> 00:12:18,130
Leraning rates가 좋은 범위 내에 속했으면 하지만 보통은
optimal 보다는 작은 값이고 학습 속도가 길어지곤 합니다.

106
00:12:18,130 --> 00:12:31,291
이런 경우에는, 오늘 배우게 될, 더 좋은(fancier) 최적화 방법을
사용하면 모델이 learning rate에 덜 민감하도록 할 수 있습니다.

107
00:12:31,291 --> 00:12:32,962
다른 질문 있으신가요?

108
00:12:32,962 --> 00:12:37,308
[학생이 질문]

109
00:12:37,308 --> 00:12:41,292
질문은 Learning rate를 작게 하고 Epoch을 늘리면
어떤 일이 발생하는지 입니다.

110
00:12:41,292 --> 00:12:45,139
그렇게 되면 엄청 오래 걸리겠죠

111
00:12:45,139 --> 00:12:48,383
[학생이 질문]

112
00:12:48,383 --> 00:12:54,853
직관적으로 Learning rates를 낮게 주고 오랫동안 학습시키면
이론적으로는 동작합니다.

113
00:12:54,853 --> 00:13:00,491
하지만 실제로는 Learning rate가 0.01이냐 0.001이냐는
상당히 중요한 문제입니다.

114
00:13:00,491 --> 00:13:03,931
여러분이 적절한 Leraning rate를 찾으면 6시간, 12시간 또는
하루면 학습을 다 시킬 수 있을 텐데

115
00:13:03,931 --> 00:13:11,911
 그런데 여러분이 엄청 조심스러워서 Learning rate를
10배, 100배 줄여 버라면 하루면 끝나는 것이 100일이 걸릴수 있습니다.

116
00:13:11,911 --> 00:13:16,400
그럼 세 달이 걸리게 되는 것입니다. 좋지 않은 경우입니다.

117
00:13:16,400 --> 00:13:20,668
보통 컴퓨터과학 분야를 배울때 이런 constants(10배 100배)를
중요하게 다루지 않는 경향이 있는데

118
00:13:20,668 --> 00:13:25,444
당신은 실제로 훈련하는 것에 대해 생각하고 있습니다.
그 constants은 많은 문제가됩니다.

119
00:13:25,444 --> 00:13:26,861
다른 질문 있나요?

120
00:13:27,877 --> 00:13:33,385
[학생이 질문]

121
00:13:33,385 --> 00:13:37,807
질문은 Low learning rate를 주게되면
local optima에 빠질 수 있지 않냐는 것입니다.

122
00:13:37,807 --> 00:13:42,601
이는 직관적으로는 그럴 수 있겠지만 실제로
그런 일은 많이 발생하지는 않습니다.

123
00:13:42,601 --> 00:13:47,030
이에 관한 내용을 오늘 잠시 뒤에 다시 배울 것입니다.

124
00:13:47,030 --> 00:13:53,151
오늘은 Neural networks를 학습시킬 때 필요한 흥미롭고
아주 중요한 문제를 몇 가지 배워보도록 하겠습니다.

125
00:13:53,151 --> 00:13:59,655
제가 이전에 더 강력한 최적화 알고리즘이 있다고
앞서 몇 번 언급한 적이 있었습니다.

126
00:13:59,655 --> 00:14:07,067
오늘은 사람들이 많이 사용하는 그 강력한 알고리즘들에 대해서
좀 더 자세히 알아보는 시간이 되겠습니다.

127
00:14:07,067 --> 00:14:10,364
지난 강의에 Regularization에 대해서도 조금 배웠었죠

128
00:14:10,364 --> 00:14:15,806
네트워크의 Train/Test Error 간의 격차를 줄이고자
사용하는 추가적인 기법입니다.

129
00:14:15,806 --> 00:14:22,143
Neural Network에서 실제로 사람들이 사용하고 있는
Regularization 전략에 대해서 다뤄보도록 하겠습니다.

130
00:14:22,143 --> 00:14:26,401
그리고 Transfer learning에 대해서도 배울 것입니다.

131
00:14:26,401 --> 00:14:31,490
원하는 양 보다 더 적은 데이터만을 가지고 있을때
사용할 수 있는 방법입니다.

132
00:14:32,821 --> 00:14:39,885
지난 강의를 돌이켜보면 Neural network에서 가장 중요한 것은
바로 최적화 문제 였다는 것을 알 수 있습니다.

133
00:14:39,885 --> 00:14:50,982
Nerwork의 가중치에 대해서 손실 함수를 정의해 놓으면 이
손심 함수는 그 가중치가 얼마나 좋은지 나쁜지를 알려줍니다.

134
00:14:50,982 --> 00:14:56,508
그리고 우리는 손심 함수가 가중치에 대한
"산(landscape)"이라고 상상해 볼 수 있을 것입니다.

135
00:14:56,508 --> 00:15:04,142
여기 맨 오른쪽 사진에 간단한 예제를 가져왔습니다.
X/Y축은 두 개의 가중치를 의미합니다.

136
00:15:04,142 --> 00:15:07,984
그리고 각 색은 Loss의 값을 나타냅니다.

137
00:15:07,984 --> 00:15:15,195
이 오른쪽의 2차원의 문제를  두개의 가중치
W_1과 W_2를 최적화 시키는 문제라고 생각해 봅시다.

138
00:15:15,195 --> 00:15:23,203
우리의 목적은 가장 붉은색인 지점을 찾는 것입니다.
즉 가장 낮은 Loss를 가진 가중치를 찾는 것이죠.

139
00:15:23,203 --> 00:15:29,099
자 지금까지 배운 것을 생각해 봅시다. 가장 간단한 최적화 알고리즘은
바로 Stochastic Gradient Descent 입니다.

140
00:15:29,099 --> 00:15:32,393
이 세 줄로 된 엄청 간단한 알고리즘이죠

141
00:15:32,393 --> 00:15:39,179
우선 미니 배치 안의 데이터에서 Loss를 계산합니다.

142
00:15:39,179 --> 00:15:44,656
그리고 'Gradient 의 반대 방향" 을 이용해서
파라미터 벡터를 업데이트합니다.

143
00:15:44,656 --> 00:15:48,798
반대 방향인 이유는 손실 함수를 내려가는
방향 이어야 하기 때문입니다.

144
00:15:48,798 --> 00:15:56,282
이 단계를 계속 반복하면 결국 붉은색 지역으로 수렴할 것이고
Loss가 낮을 것입니다.

145
00:15:56,282 --> 00:16:05,462
하지만 이 심플한 알고리즘을 실제로 사용하게 되면
몇 가지 문제에 봉착하고 맙니다.

146
00:16:05,462 --> 00:16:08,713
SGD의 문제점 중 하나는,

147
00:16:08,713 --> 00:16:18,969
가령 우리의 손실함수가 이런 식으로 생겼다고 생각해 봅시다.
여기에 똑같이 W_1과 W_2가 있다고 해봅시다.

148
00:16:18,969 --> 00:16:23,472
둘중 어떤 하나는 업데이트를 해도 손실 함수가
아주 느리게 변합니다.

149
00:16:23,472 --> 00:16:26,687
즉, 수평 축의 가중치는 변해도 Loss가 아주 천천히 줄어듭니다.

150
00:16:28,152 --> 00:16:34,930
다시 말해 Loss는 수직 방향의 가중치 변화에 훨씬 더
민감하게 반응하는 것입니다.

151
00:16:34,930 --> 00:16:40,757
다시 말해 현재 지점에서 Loss는 bad condition number를
지니고 있다고 말할 수 있을 것입니다.

152
00:16:40,757 --> 00:16:46,050
이 지점의 Hessian maxrix의 최대/최소 singular values
값의 비율이 매우 안좋다는 뜻입니다.

153
00:16:46,050 --> 00:16:50,497
이를 좀 더 직관적으로 이해해보면 우선 Loss가
 taco shell 같은 모양입니다.

154
00:16:50,497 --> 00:16:54,393
한 방향으로는 엄청나게 민감한 반면에 다른 방향으로는
덜 민감한 상태에 있는 것입니다.

155
00:16:54,393 --> 00:17:00,633
그렇다면 질문입니다. 이런 상황에서 SGD으로 학습이 되는
과정은 어떤 모습일까요?

156
00:17:05,310 --> 00:17:12,196
손실 함수가 이런 식으로 생긴 환경에서 SGD를 수행하면,
이런 특이한 지그재그 형태를 볼 수 있을 것입니다.

157
00:17:12,197 --> 00:17:22,111
왜냐하면 이런 함수에서는 gradient의 방향이
고르지 못하기 때문입니다.

158
00:17:22,112 --> 00:17:29,335
Gradient를 계산하고 업데이트 하게 되면 line을
넘나들면서  왔다갔다 하게 됩니다.

159
00:17:29,335 --> 00:17:35,995
Loss에 영향을 덜 주는 수평방향 차원의 가중치는
업데이트가 아주 느리게 진행됩니다.

160
00:17:35,995 --> 00:17:41,551
이렇게 빠르게 변하는 수직 차원을 가로지르면서
지그지그로 아주 지저분하게(nasty) 움직이게 됩니다.

161
00:17:41,551 --> 00:17:50,139
아주 바람직하지 않은 행동입니다. 그리고 이 문제는 고차원
공간에서 훨씬 더 빈번하게 발생합니다.

162
00:17:51,186 --> 00:18:00,617
우리가 본 예시 그림은 2차원 밖에 되지 않습니다만 실제로는
가중치가 수천 수억개 일 수 있을 것입니다.

163
00:18:00,617 --> 00:18:14,221
이 경우 수억개의 방향으로 움직일 수 있습니다. 이런 수억개의 방향중에
불균형한 방향이 존재한다면 SGD는 잘 동장하지 않을 것입니다.

164
00:18:14,221 --> 00:18:20,573
수억개의 파라미터가 있다고 했을때 이런 불균형의
발생 비율은 상당히 높습니다.

165
00:18:20,573 --> 00:18:26,398
고차원 공간에서 발생할 수 있는 이런 문제는
실제로도 정말 큰 문제가 됩니다.

166
00:18:27,793 --> 00:18:33,564
SGD에서 발생하는 또 다른 문제는
local minima 와 saddle points와 관련된 문제입니다.

167
00:18:33,564 --> 00:18:44,003
그림을 좀 바꿨습니다. 이제는 X축은 어떤 하나의 가중치를 나타내고
Y축은 Loss를 나타내고 있습니다.

168
00:18:44,003 --> 00:18:51,583
우선 위의 그래프를 보면, 이 휘어진 손실함수는
중간에 "valley"가 하나 있습니다.

169
00:18:51,583 --> 00:18:55,036
이런 상황에서 SGD는 어떻게 움직일까요?

170
00:18:55,036 --> 00:18:57,031
[학생이 대답]

171
00:18:57,031 --> 00:19:04,454
이 경우 SGD는 멈춰버립니다. 왜냐하면 gradient가 0이
되기 때문이죠. locally falt 합니다.

172
00:19:04,454 --> 00:19:09,194
SGD의 동작을 생각해보면 gradient를 계산하고
그 반대방향으로 이동하는 것 이었습니다.

173
00:19:09,194 --> 00:19:15,862
저 위치에서는 "opposite gradient" 도 0 이 되며,
따라서 학습이 멈춰버리게 됩니다.

174
00:19:15,862 --> 00:19:19,406
또 다른 문제는 saddle points에 관한 것입니다.

175
00:19:19,406 --> 00:19:26,140
local minima는 아니지만, 한쪽 방향으로는 증가하고 있고
다른 한쪽 방향으로는 감소하고 있는 지역을 생각해 볼 수 있습니다.

176
00:19:26,140 --> 00:19:28,953
이런 곳에서도 gradient는 0이 됩니다.(아래 그림)

177
00:19:28,953 --> 00:19:35,899
saddle point에서도 gradient = 0 이므로 멈추게 됩니다.

178
00:19:35,899 --> 00:19:48,122
비록 이처럼 1차원의 예제만 봐서는 local minima가 엄청 심각하고
saddle point는 좀 덜 심각해 보이지만,

179
00:19:48,122 --> 00:19:57,171
고차원 공간에서는 그 반대입니다. 여러분에게 1억 차원의 공간이
있다고 생각해 봅시다. 여기에서 saddle point는 무엇일까요?

180
00:19:57,171 --> 00:20:03,135
Saddle point가 의미하는 것이 어떤 방향은 Loss가 증가하고
몇몇 방향은 Loss가 감소하고 있는 곳을 생각해 볼 수 있습니다.

181
00:20:03,135 --> 00:20:09,591
1억개의 차원에서 생각해보면 이는 정말 빈번하게 발생합니다.
사실 거의 모든 곳에서 발생한다고 할 수 있습니다.

182
00:20:09,591 --> 00:20:16,744
Local minima를 생각해보면 1억 개의 방향을 계산했는데 그 방향이
전부 Loss가 상승하는 방향이라는 것입니다.

183
00:20:16,744 --> 00:20:22,316
고차원 공간을 생각하면 그런 일이 발생하는 것은
매우 드문 경우입니다.

184
00:20:23,270 --> 00:20:33,283
지난 몇 년간 알려진 사실은 very large neural network가
local minima 보다는 saddle point에 취약하다는 것입니다.

185
00:20:33,283 --> 00:20:40,140
그리고 또한  saddle point 뿐만 아니라  saddle point의
근처에서도 문제는 발생합니다.

186
00:20:40,140 --> 00:20:47,935
아래 예시를 보면 saddle point 근처에서 gradient가 0 은
아니지만 기울기가 아주 작습니다.

187
00:20:47,935 --> 00:20:53,611
그것이 의미하는 바는 gradient를 계산해서 업데이트를 해도
기울기가 아주 작기 때문에

188
00:20:53,611 --> 00:21:01,872
가중치가 saddle point 근처에 있을때면
업데이트가 아주 느리게 진행됩니다.

189
00:21:01,872 --> 00:21:10,115
이는 아주 큰 문제입니다.
그리고 SGD의 또 다른 문제가 있습니다.

190
00:21:10,115 --> 00:21:13,521
SGD는 Stochastic gradient descent라는
것을 다시한번 명심하시기 바랍니다.

191
00:21:13,521 --> 00:21:20,586
사실 손실함수를 계산할 때는 엄청 엄청 많은 Traing set
각각의  loss를 전부 계산해야 합니다.

192
00:21:20,586 --> 00:21:26,119
이 예시의 N이 전체 training set일 경우에
이 값은 "n = 백만"이 될 수도 있습니다.

193
00:21:26,119 --> 00:21:29,347
Loss를 계산할 때 마다 매번 전부를 계산하는 것은 어렵습니다.

194
00:21:29,347 --> 00:21:36,957
그래서 실제로는 미니배치의 데이터들만 가지고
실제 Loss를 추정하기만 합니다.

195
00:21:36,957 --> 00:21:42,148
이는 매번 정확한 gradient를 얻을 수가
없다는 것을 의미합니다.

196
00:21:42,148 --> 00:21:46,773
대신에 gradient의 부정확한
추정값(noisy estimate) 만을 구할 뿐입니다.

197
00:21:46,773 --> 00:21:50,575
오른쪽에 보이는 것은 제가 좀 과장해서 그린 것입니다.

198
00:21:50,575 --> 00:21:59,927
각 지점의 gradient에 random uniform noise를 추가하고
SGD를 수행하게 만들었습니다.

199
00:21:59,927 --> 00:22:07,987
따라서 실제로 SGD가 이런식으로 동작하진 않지만 이 예제로
gradient에 noise가 들어가면 어떻게 되는지 알 수 있습니다.

200
00:22:07,987 --> 00:22:14,036
손실함수 공간을 이런식으로 비틀거리면서 돌아다니게 되면
minima까지 도달하는데 시간이 더 오래 걸릴 것입니다.

201
00:22:15,723 --> 00:22:18,966
지금까지 SGD에 어떤 문제가 있는지에 대해서
이야기해 보았습니다.

202
00:22:18,966 --> 00:22:20,956
질문 있나요?

203
00:22:20,956 --> 00:22:25,123
[학생이 질문]

204
00:22:29,099 --> 00:22:34,435
질문은 바로 SGD를 쓰지 않고 그냥 GD를 쓰면
이런 문제가 전부 해결되는지 입니다.

205
00:22:35,281 --> 00:22:44,106
자 이전의 taco shell에서의 문제를 다시한번 살펴보면
full batch gradient descent에서도 같은 문제가 발생합니다

206
00:22:44,106 --> 00:22:54,120
Noise의 문제도 한번 볼까요. Noise는 미니배치이기 때문에서만이
아니라 네트워크의  explicit stochasticity 로도 발생합니다.

207
00:22:54,120 --> 00:22:57,736
이는 나중에 더 살펴 볼 것이지만
이는 여전히 문제가 됩니다.

208
00:22:57,736 --> 00:23:05,101
Saddle points 또한 full batch GD에서 문제가 됩니다.
전체 데이터를 사용한다고 해도 여전히 나타날 수 있겠죠

209
00:23:05,101 --> 00:23:10,249
기본적으로 full batch gradient descent를 사용한다
하더라도 이런 문제들이 해결되지는 않습니다.

210
00:23:10,249 --> 00:23:16,604
이러한 위험요소들을 다루기 위해서는 더 좋은
최적화 알고리즘이 필요합니다.

211
00:23:16,604 --> 00:23:21,966
이번 문제들의 대다수를 해결할 수 있는 아주
간단한 방법이 하나 있습니다.

212
00:23:21,966 --> 00:23:26,978
SGD에 momentum term을 추가하는 것이죠

213
00:23:26,978 --> 00:23:32,923
왼쪽은 classic 버전의 SGD입니다.
오로지 gradient 방향으로만 움직이는 녀석이죠

214
00:23:32,923 --> 00:23:43,062
반면 오른쪽은 SGD + momentum 입니다. 엄청 조금의 변화만
있습니다. 2개의 수식과 5라인의 코드를 보실 수 있죠

215
00:23:43,062 --> 00:23:51,331
아이디어는 아주 간단합니다. 그저 velocity를 유지하는 것입니다.
gradient 를 계산할 때 velocity를 이용합니다.

216
00:23:51,331 --> 00:23:57,811
현재 미니배치의 gradient 방향만 고려하는 것이 아니라
velocity를 같이 고려하는 것입니다.

217
00:23:57,811 --> 00:24:04,825
여기에는 하이퍼 파라미터 rho가 추가되었습니다.
momemtum의 비율을 나타냅니다.

218
00:24:05,925 --> 00:24:16,848
velocity의 영향력을 rho의 비율로 맞춰주는데
보통 0.9와 같은 높은 값으로 맞춰줍니다.

219
00:24:16,848 --> 00:24:21,173
velocity에 일정 비율 rho를 곱해주고
현재 gradient를 더합니다.

220
00:24:21,173 --> 00:24:26,999
이를 통해서 우리는 이제 gradient vector 그대로의 방향이 아닌
velocity vector의 방향으로 나아가게 됩니다.

221
00:24:28,327 --> 00:24:34,548
엄청 간단한 방법이지만 지금까지 말했던 문제들을
해결하는데 많은 도움을 줄 수 있습니다.

222
00:24:34,548 --> 00:24:44,809
local minima와 saddle points문제를 생각해보면 물리적으로
공이 굴러내려오는 것을 상상해 볼 수 있습니다.

223
00:24:44,809 --> 00:24:48,215
이 공은 떨어지면 속도가 점점 빨라집니다.

224
00:24:48,215 --> 00:24:56,922
이 공은 local minima에 도달해도 여전히 velocity를 가지고
있기 때문에 gradient = 0 이라도 움직일 수 있습니다.

225
00:24:56,922 --> 00:25:01,039
때문에 local minima를 극복할 수 있게 되고
계속해서 내려갈 수 있습니다.

226
00:25:01,039 --> 00:25:03,809
 saddle points에서도 비슷한 일이 일어나겠죠

227
00:25:03,809 --> 00:25:10,734
saddle point 주변의 gradient가 작더라도, 굴러내려오는 속도가
있기 때문에 velocity를 가지게 됩니다.

228
00:25:10,734 --> 00:25:16,462
때문에 saddle point를 잘 극복해 내고 계속
밑으로 내려올 수 있는 것입니다.

229
00:25:16,462 --> 00:25:21,949
업데이트가 잘 안되는 경우(poor conditioning) 를 다시
한번 살펴보겠습니다.

230
00:25:21,949 --> 00:25:31,105
아래와 같이 지그재그로 움직이는 상황이라면
momentum이 이 변동을 서로 상쇄시켜 버립니다.

231
00:25:31,105 --> 00:25:46,006
이를 통해서 loss에 만감한 수직 방향의 변동은 줄여주고
수평방향의 움직임은 점차 가속화 될 것입니다.

232
00:25:46,006 --> 00:25:51,344
momentum을 추가하게 되면 high condition number
problem을 해결하는 데 도움이 되는 것입니다.

233
00:25:51,344 --> 00:26:05,207
오른쪽을 보면 검은색이 일반 SGD입니다. 지그지그로 움직이죠.
파란색이 Momentum SGD입니다.

234
00:26:05,207 --> 00:26:12,644
Momentum을 추가해서 velocity가 생기면
결국 noise가 평균화되버립니다.

235
00:26:12,644 --> 00:26:20,337
보통의 SGD가 구불구불 움직이는 것에 비해서 momemum은
minima를 향해서 더 부드럽게 움직입니다.

236
00:26:20,337 --> 00:26:21,532
질문 있나요?

237
00:26:21,532 --> 00:26:25,699
[학생이 질문]

238
00:26:34,776 --> 00:26:40,465
문제는 어떻게 SGD Momemtum이 poorly conditioned
coordinate 문제를 해결할 수 있는지 입니다.

239
00:26:40,465 --> 00:26:49,125
우선 velocity estimation term에서 velocity가 어떻게
계산되는지를 보면 gradient를 계속해서 더해갑니다.

240
00:26:49,125 --> 00:26:56,603
이는 하이퍼파라미터인 rho에 영향을 받습니다.
그리고 현재 gradient가 상대적으로 작은 값이고

241
00:26:56,603 --> 00:26:59,254
그리고 이 상황에서 rho가 적절한 값으로
잘 동작한다고 하면

242
00:26:59,254 --> 00:27:05,512
그러면 velocity가 실제 gradient보다 더 커지는
지점 까지 조금씩 증가할 것입니다.

243
00:27:05,512 --> 00:27:10,377
이는  poorly conditioned dimension에서 더 빨리
학습될 수 있도록 도와줍니다.

244
00:27:12,569 --> 00:27:18,020
SGD momentum를 연상할 때 유용한 그림이 있습니다.

245
00:27:18,020 --> 00:27:20,273
빨간 점이 현재 지점입니다.

246
00:27:20,273 --> 00:27:30,075
Red Vector는 현재 지점에서의 gradient의 방향을 나타냅니다.
Green vector는 Velocity vector입니다.

247
00:27:30,075 --> 00:27:36,317
실제 업데이트는(autual step) 이 둘의 가중평균으로
구할 수 있습니다.

248
00:27:36,317 --> 00:27:40,049
이는 gradient의 noise를 극복할 수 있게 해줍니다.

249
00:27:40,049 --> 00:27:47,724
Momentum의 변형이 있습니다. Nesterov accelerated
gradient 인데, Nesterov momentum 라고도 합니다.

250
00:27:47,724 --> 00:27:51,737
계산하는 순서를 조금 바꾼 것입니다.

251
00:27:51,737 --> 00:28:00,285
기본 SGD momentum은 "현재 지점" 에서의 gradient를
계산한 뒤에 velocity와 섞어 줍니다.

252
00:28:00,285 --> 00:28:04,229
Nesterov를 사용하면 방법이 조금 다릅니다.

253
00:28:04,229 --> 00:28:10,765
빨간 점에서 시작해서 우선은 Velocity방향으로 움직입니다.

254
00:28:10,765 --> 00:28:18,732
그리고 그 지점에서의 gradient를 계산합니다. 그리고 다시
원점으로 돌아가서 둘을 합치는 것입니다.

255
00:28:18,732 --> 00:28:25,679
완벽한 설명은 아니지만 두 정보를 약간 더 섞어준다고
생각해 볼 수 있을 것입니다.

256
00:28:25,679 --> 00:28:34,702
velocity의 방향이 잘못되었을 경우에 현재 gradient의 방향을
좀 더 활용할 수 있도록 해줍니다.

257
00:28:34,702 --> 00:28:39,351
Nesterov는 Convex optimization 문제에서는
뛰어난 성능을 보이지만

258
00:28:39,351 --> 00:28:45,946
하지만 Neural network와 같은 non-convex problem
에서는 성능이 보장되지는 않습니다.

259
00:28:45,946 --> 00:28:51,061
Nesterov의 수식은 다음과 같습니다.

260
00:28:51,061 --> 00:28:57,155
velocity를 업데이트하기 위해서 이전의 velocity와
(x + pv)에서의 gradient를 계산합니다.

261
00:28:57,155 --> 00:29:06,222
그리고 step update는 앞서 계산한 velocity를
이용해서 구해줍니다.

262
00:29:06,222 --> 00:29:07,055
질문 있나요?

263
00:29:08,437 --> 00:29:12,357
[학생이 질문]

264
00:29:12,357 --> 00:29:14,743
질문은 velocity의 초기값을 구하는 좋은 방법이 있는지 입니다.

265
00:29:14,743 --> 00:29:16,998
velocity의 초기값은 항상 0입니다.

266
00:29:16,998 --> 00:29:20,096
이는 하이퍼파라미터가 아닙니다. 그저 0으로 둡니다.

267
00:29:20,096 --> 00:29:21,315
다른 질문 있나요?

268
00:29:21,315 --> 00:29:25,482
[학생이 질문]

269
00:29:31,992 --> 00:29:38,068
직관적으로 보면 velocity은 이전 gradients의
weighted sum입니다.

270
00:29:38,068 --> 00:29:41,466
[학생이 질문]

271
00:29:41,466 --> 00:29:44,027
그리고 더 최근의 gradients에 가중치가 더 크게 부여됩니다.

272
00:29:44,027 --> 00:29:49,716
매 스텝마다 이전 velocity에  rho(0.9 or 0.00) 를 곱하고
현재 gradient를 더해줍니다.

273
00:29:49,716 --> 00:29:54,662
그리고 이를 moving average라고 볼 수 있습니다.
(exponentially weighted moving average)

274
00:29:54,662 --> 00:30:00,109
그리고 시간이 지날수록 이전의 gradient들은
exponentially하게 감소합니다.

275
00:30:02,627 --> 00:30:11,632
Nesterov의 공식을 보시게 되면 다소 까다롭게 생겼습니다. 기존에는
Loss와 Gradient를 같은 점(x_t)에서 구했었습니다.

276
00:30:11,632 --> 00:30:19,283
Nesterov는 이 규칙을 조금 비틀어 버렸습니다. 이는 상당히 성가시지만
다행이도 쉽게 해결할 수 있는 변형 공식이 있습니다.

277
00:30:19,283 --> 00:30:29,392
변수들을 적절히 잘 바꿔주면 Nesterov를 조금 다르게 표현할 수 있으며
Loss와 Gradient를 같은 점에서 계산할 수 있게 됩니다.

278
00:30:29,392 --> 00:30:34,093
그리고 수정된 수식을 통해서 우리는 Nesterov를
새롭게 이해해 볼 수 있습니다.

279
00:30:34,093 --> 00:30:41,739
첫 번째 수식은 기존의 momentum과 동일합니다.

280
00:30:41,739 --> 00:30:48,178
기존과 동일하게 velocity 와 계산한 gradient를
일정 비율로 섞어주는 역할을 합니다.

281
00:30:48,178 --> 00:30:51,951
그리고 두 번째로 맨 밑의 수식을 보시기 바랍니다.

282
00:30:51,951 --> 00:30:57,592
우선 현재 점과 velocity를 더해줍니다.
여기 까지는 기존과 동일하죠

283
00:30:57,592 --> 00:31:01,454
그리고 여기에 "현재 velocity - 이전 velocity" 를 계산해서
일정 비율(rho)을 곱하고 더해줍니다.

284
00:31:01,454 --> 00:31:11,271
Nesterov momentum는 현재/이전의 velocity간의
에러보정(error-correcting term)이 추가됐습니다.

285
00:31:13,029 --> 00:31:25,249
SGD, Momentum, Nesterov의 예시를 한번 살펴봅시다.
기본 SGD은 검정색인데 세월아 네월아 내려가고 있습니다.

286
00:31:26,346 --> 00:31:29,598
파란색과 초록색이 momentum과 Nesterov입니다.

287
00:31:29,598 --> 00:31:36,803
minimum들은 minima를 그냥 지나쳐 버리는 경향이 있 습니다.
이전의 velocity의 영향을 받기 때문입니다.

288
00:31:36,803 --> 00:31:39,849
하지만 스스로 경로를 수정하고는 minima로 수렴합니다.

289
00:31:39,849 --> 00:31:40,682
질문 있나요?

290
00:31:42,023 --> 00:31:46,190
[학생이 질문]

291
00:31:52,024 --> 00:31:58,050
질문은 이 예시만 보면 momentum이 엄청 좋아 보이는데 만일
minima가 엄청 좁고 깊은 곳이라면 어떻게 되는지 입니다.

292
00:31:58,050 --> 00:32:01,527
momentum의 velocity가 오히려 minima를 건너 뛰는
현상도 발생할 수 있지 않을까요?

293
00:32:01,527 --> 00:32:05,232
그 부분은 상당히 흥미롭습니다. 그리고 이에 관련한
최근의 연구들이 주목하는 주제이기도 합니다.

294
00:32:05,232 --> 00:32:09,071
하지만 사실은 그렇게 좁고 깊은(sharp) minima는
좋은 minima가 아닙니다.

295
00:32:09,071 --> 00:32:17,601
그리고 사실 우리는 그런 곳에 도달하는 것도 원하지 않습니다.
좁고 깊은 minima는 훨씬 더 심한 overfits을 불러오게 됩니다.

296
00:32:17,601 --> 00:32:22,026
가령 Training set을 두배 늘어났다고 생각해보면, 최적화 시키는
산의 지형(landscape)자체가 바뀌게 될 것입니다.

297
00:32:22,026 --> 00:32:27,420
Training data이 더 많이 모이면 그런
민감한 minima는 점점 사라집니다.

298
00:32:27,420 --> 00:32:31,189
여기에서 얻을 수 있는 직관은 우리가 원하는 minima는
아주 평평한 minima 라는 것입니다.

299
00:32:31,189 --> 00:32:35,933
왜냐하면 "아주 평평한 minima"는 Training data의 변화에
좀 더 강인할 지도 모르기 때문입니다.

300
00:32:35,933 --> 00:32:40,453
결국 평평한 minima가 더 일반화를 잘 할 수도 있으며,
 Testing data에도 더 좋은 결과를 얻을 수 있을 것입니다.

301
00:32:40,453 --> 00:32:46,284
다시한번 말씀드리지만 이는 아주 최근에 연구되고 있는 분야입니다.
아주 좋은 질문을 해 주셨습니다.

302
00:32:46,284 --> 00:32:54,354
그러므로 momentum이 좁고 깊은 minima를 무시해 버리는 것은
버그가 아니라 momentum의 특징이라고 할 수 있습니다.

303
00:32:55,979 --> 00:32:59,979
아직 확실하진 않지만(믿거나 말거나)
이는 momentum의 좋은 점이라고 할 수 있겠습니다.

304
00:33:00,825 --> 00:33:04,316
자 그리고 일반 momentum와 Nesterov이 조금 다르게
움직이는 것을 볼 수 있습니다.

305
00:33:04,316 --> 00:33:12,715
이는 Nesterov에서 추가된 수식 때문입니다.  일반 momentum에
비해서 overshooting이 덜 한 것을 알 수 있습니다.

306
00:33:14,683 --> 00:33:20,068
최적화 방법 중에 AdaGrad도 있습니다.

307
00:33:20,068 --> 00:33:25,292
현재 Stanford에 계시는 John Duchi 교수님께서
Ph.D 시절에 제안하신 방법입니다.

308
00:33:25,292 --> 00:33:37,663
AdaGrad는 훈련도중 계산되는 gradients를 활용하는 방법입니다.

309
00:33:39,569 --> 00:33:43,957
Adagrad는 velocity term 대신에
grad squared term을 이용합니다.

310
00:33:43,957 --> 00:33:49,199
그리고 학습 도중에 계산되는 gradient에
제곱을 해서 계속 더해줍니다.

311
00:33:49,199 --> 00:33:57,449
그리고 Update를 할때 Update term을
앞서 계산한 gradient 제곱 항으로 나눠줍니다.

312
00:33:59,334 --> 00:34:07,261
Condition number인 경우 빨간색 박스 안의 값은 어떨까요?

313
00:34:08,393 --> 00:34:12,560
[학생이 대답]

314
00:34:16,256 --> 00:34:22,904
2차원 좌표가 있다고 해 봅시다. 그 중 한 차원은 항상 gradient가
높은 차원입니다. 그리고 다른 하나는 항상 작은 gradient를 가집니다.

315
00:34:22,904 --> 00:34:35,181
Small dimension에서는 gradient의 제곱 값 합이 작습니다.
이 작은 값이 나눠지므로 가속도가 붙게 됩니다.

316
00:34:35,181 --> 00:34:45,924
Large dimension에서는 gradient가 큰 값 이므로 큰 값이
나눠지게 되겠죠. 그러므로 속도가 점점 줄어듭니다.

317
00:34:45,924 --> 00:34:56,093
하지만 AdaGrad에는 문제가 하나 있습니다. 학습이 계속 진행되면
어떻게 될까요? 학습 횟수 t가 계속 늘어나는 것입니다.

318
00:34:56,094 --> 00:34:58,391
[학생이 대답]

319
00:34:58,391 --> 00:35:02,239
AdaGrad는 step을 진행할수록 값이 점점 작아집니다.

320
00:35:02,239 --> 00:35:09,895
update 동안 gradient의 제곱이 계속해서 더해집니다. 때문에
이 값(estimate)은 서서히(monotonically) 증가하게 됩니다.

321
00:35:09,895 --> 00:35:15,359
이는 Step size를 점점 더 작은 값이 되게 합니다.

322
00:35:15,359 --> 00:35:20,334
손실함수가 convex한 경우에 점점 작아지는 것은
정말 좋은 특징이 될 수 있습니다.

323
00:35:20,334 --> 00:35:28,125
convex case에서는 minimum에 근접하면 서서히
속도를 줄여서 수렴할 수 있게 하면 좋겠죠

324
00:35:28,125 --> 00:35:31,192
Convex case에서는 좋은 특징이라 할 수 있습니다.

325
00:35:31,192 --> 00:35:42,007
하지만 non-convex case에서는 문제가 될 수 있습니다. 가령
saddle point에 걸려버렸을 때 AdaGrad는 멈춰버릴 수 있습니다.

326
00:35:42,007 --> 00:35:48,678
AdaGrad의 변형이 RMSProp입니다.
앞서 언급한 문제를 개선시킨 방법입니다.

327
00:35:48,678 --> 00:35:53,390
RMSProp에서는 AdaGrad의
gradient  제곱 항을 그대로 사용합니다.

328
00:35:53,390 --> 00:36:01,085
하지만 이 값들을 그저 누적만 시키는 것이 아니라
기존의 누적 값에 decay_rate를 곱해줍니다.

329
00:36:01,085 --> 00:36:09,340
이 값은 기존의 momentum 수식과 유사하게 생겼습니다. 다만
gradients의 제곱을 계속해서 누적해 나갑니다.

330
00:36:09,340 --> 00:36:20,361
RMSProp에서는 gradient 제곱 항에 쓰는 decay rate는
보통 0.9 또는 0.99정도를 자주 사용합니다.

331
00:36:20,361 --> 00:36:26,601
그리고 '현재 gradient의 제곱'은 (1 - decay rate) 를
곱해줘서 더해줍니다.

332
00:36:26,601 --> 00:36:37,193
RMSProp은 gradient 제곱을 계속 나눠준다는 점에서
AdaGrad와 유사합니다.

333
00:36:37,193 --> 00:36:44,070
이를 통해 step의 속도를 가속/감속 시킬 수 있습니다.

334
00:36:44,070 --> 00:36:52,411
하지만 RMSProp의 경우에는 위와 같은 특징으로
점점 속도가 줄어드는 문제를 해결할 수 있었습니다.

335
00:36:56,455 --> 00:37:04,173
이 예제에서 SGD는 검정색이고 momentum은 파란색
그리고 RMSProp은 빨간색입니다.

336
00:37:04,173 --> 00:37:12,263
RMSProp이나 momentum은 기본 SGD보다는 훨씬 더 좋습니다.
하지만 이 둘의 행동양상은 조금 다릅니다.

337
00:37:12,263 --> 00:37:17,488
momentum의 경우에는 overshoots한 뒤에
다시 minima로 돌아옵니다.

338
00:37:17,488 --> 00:37:26,392
RMSProp은 각 차원마다의 상황에 맞도록 적절하게
궤적(trajectory)을 수정시킵니다.

339
00:37:26,392 --> 00:37:34,412
여러분들한테는 안보이시겠지만 지금 이 그림에는 초록색으로
AdaGrad도 있습니다. 동일한 Learning rate로 말입니다.

340
00:37:34,412 --> 00:37:38,606
하지만 Learning rates가 점차 감소하기 때문에 RMSProp에
가려서 보이지 않습니다.

341
00:37:38,606 --> 00:37:45,553
실제로 AdaGrad는 잘 쓰이지는 않습니다. 그리고 이런 식의
비교는 사실 AdaGrad에게는 불공정합니다.

342
00:37:46,392 --> 00:37:52,558
아마도 AdaGrad의 Learning rate를 늘리게 되면
RMSProp과 비슷한 동작을 할 것입니다.

343
00:37:52,558 --> 00:37:57,148
하지만 일반적으로 Nerural Network를 학습시킬 때
AdaGrad를 잘 사용하지는 않습니다.

344
00:37:57,148 --> 00:37:57,981
질문 있나요?

345
00:37:57,981 --> 00:37:59,796
[학생이 질문]

346
00:37:59,796 --> 00:38:04,387
이 예시에서의 문제는 convex case입니다.
(convex인데 왜 Adagrad에게 불리한지)

347
00:38:07,146 --> 00:38:12,003
하지만 learning rates가 서로 상이하기 때문입니다.

348
00:38:12,003 --> 00:38:17,290
때문에 여러 알고리즘 간에 '같은 Learning rates' 를 가지고
AdaGrad 를 visualization하는 것은 공정하지 못합니다.

349
00:38:17,290 --> 00:38:23,132
visualization을 하고자 한다면 알고리즘 별로
learning rates를 조정하는 것이 좋습니다.

350
00:38:27,970 --> 00:38:35,403
자 우리는 momentum이라는 방법을 알아봤습니다.
velocity를 이용해서 step을 조절하는 방법이었습니다.

351
00:38:35,403 --> 00:38:43,744
AdaGrad와 RMSProp은 momentum과 다른 방식을 사용하죠.
gradients의 제곱을 나눠주는 방식으로 step을 조절했습니다.

352
00:38:43,744 --> 00:38:46,658
이 둘은(momentum 계열 vs Ada계열)
모두 괜찮은 아이디어인 것 같습니다.

353
00:38:46,658 --> 00:38:50,898
그렇다면 그럼 그 둘은 합쳐서 둘 다 사용해보면 어떨까요?
아마 훨씬 더 좋은 방법이 될 것 같습니다.

354
00:38:50,898 --> 00:38:56,741
여기 Adam 이라는 알고리즘이 있습니다. 여기 수식은
Adam과 "유사한" 알고리즘이라고 보시면 됩니다.

355
00:38:56,741 --> 00:39:01,119
앞으로 몇 장의 슬라이드를 통해서 '진짜' Adam
이 되도록 조금씩 바꿔 볼 것입니다.

356
00:39:01,119 --> 00:39:06,888
Adam은 first moment와 second moment을 이용해서
이전의 정보(estimate)를 유지시킵니다.

357
00:39:06,888 --> 00:39:14,667
빨간색 first moment는 gradient의 가중 합입니다.

358
00:39:14,667 --> 00:39:22,741
그리고 second moment는 AdaGrad이나 RMSProp처럼
gradients의 제곱을 이용하는 방법입니다.

359
00:39:22,741 --> 00:39:28,621
Adam으로 Update를 진행하게 되면
우선 first moment는 velocity를 담당합니다.

360
00:39:28,621 --> 00:39:37,281
그리고  sqrt(second moment)를 나눠주는데
second moment는 gradient의 제곱 항입니다.

361
00:39:38,128 --> 00:39:46,269
Adam은 마치 RMSProp + momentum 같아 보입니다. 즉
momentum +  second squared gradients 이죠

362
00:39:46,269 --> 00:39:51,989
이 두 종류의 유용한 특징을 모두 이용하는 것입니다.
하지만 여기에도 문제가 하나 있습니다.

363
00:39:51,989 --> 00:40:06,134
초기 step 에서는 어떤 일이 발생할까요?
초기에 second moment를 0으로 초기화합니다.

364
00:40:06,134 --> 00:40:16,803
second moment를 1회 Update 하고 난 후를 생각해봅시다.
beta2는 decay_rate로 .9또는 .99로 1에 가까운 값입니다.

365
00:40:18,235 --> 00:40:22,867
그렇기 때문에 1회 업데이트 이후에도 second moment는
여전히 0에 가깝습니다.

366
00:40:22,867 --> 00:40:32,377
update step에서 second moment로 나누게 되는데 나눠주는
값이 크기 때문에 초기 step이 엄청나게 커지게 됩니다.

367
00:40:32,377 --> 00:40:37,768
중요한 것은 이 커진 step이 실제로 손실함수가
가파르기 때문(geometry)이 아니라는 것입니다.

368
00:40:37,768 --> 00:40:43,422
이 값은 second moment을 0으로 초기화 시켰기 때문에
발생하는 '인공적인' 현상입니다.

369
00:40:43,422 --> 00:40:44,322
질문 있나요?

370
00:40:44,322 --> 00:40:48,489
[학생이 질문]

371
00:40:52,832 --> 00:41:00,365
first moment도 second moment처럼 초기에
엄청 작은 값일 것입니다. 따라서

372
00:41:00,365 --> 00:41:02,906
learning rate에 '엄청 작은 값'을 곱하고(first moment)
'엄청 작은 값의 제곱근(second)'을 나누면 어떻게 될까요?

373
00:41:02,906 --> 00:41:05,746
어쩌면 서로 상쇄시킬 수 있고. 상쇄됀다면
문제는 해결되는 것이죠.

374
00:41:05,746 --> 00:41:13,632
예 사실입니다. 경우에 따라서 서로 상쇄될 수도 있습니다. 하지만
간혹 엄청 큰 step이 발생하는 경우도 생길 수 있습니다.

375
00:41:13,632 --> 00:41:16,245
하지만 한번 발생하면 정말 나쁜 상황일 것입니다.

376
00:41:16,245 --> 00:41:19,533
가령 여러분이 제대로 초기화를 해주지 않아서
아주 큰 step이 발생했다면

377
00:41:19,533 --> 00:41:26,145
초기화가 엉망이 될 것이고 아주 엉뚱한 곳으로 이동할 수도
있습니다. 결국 수렴할 수 없게 될 수도 있습니다.

378
00:41:26,145 --> 00:41:27,165
질문 있나요?

379
00:41:27,165 --> 00:41:30,915
[학생이 질문]

380
00:41:30,915 --> 00:41:37,847
질문은 수식의 10^-7이 무엇인지 입니다. 저 값은
AdaGrad, RMSProp, Adam에서 등장합니다.

381
00:41:37,847 --> 00:41:42,187
우리는 현재 어떤 값으로 '나누기' 를 하고 있습니다.
우리는 그 값이 항상 0이 아니라는 것을 모릅니다.

382
00:41:42,187 --> 00:41:48,609
따라서 분모에 작은 양수 값을 더해줘서 0이 되는 것을
사전에 방지할 수 있습니다.

383
00:41:48,609 --> 00:41:56,012
이 또한 하이퍼파라미터 이긴 하지만 큰 영향력은 없습니다.
보통 10^-7이나 10^-8정도를 사용하면 잘 동작합니다.

384
00:41:57,967 --> 00:42:05,511
Adam은 초기 Step이 엄청 커져 버릴 수 있고
이로 인해 잘못될 수도 있다고 했습니다.

385
00:42:05,511 --> 00:42:12,510
Adam을 이를 해결하기 위해 보정하는 항을 추가합니다.
(bias correction term )

386
00:42:12,510 --> 00:42:22,619
first/second moments를 Update하고 난 후 현재 Step에
맞는 적절한 unbiased term 을 계산해 줍니다.

387
00:42:22,619 --> 00:42:29,550
실제 Adam은 first/second moment만 계산하는 것이 아니라 
이렇게 unbiased term을 넣어줘야 합니다.

388
00:42:29,550 --> 00:42:33,167
지금 보시는 부분이 '완전한 Adam' 이라 할 수 있겠습니다.

389
00:42:33,167 --> 00:42:45,550
그리고 Adam은 엄청 좋습니다. 다양한 문제들에도 정말 잘 동작합니다.
저는 어떤 문제에서도 기본 알고리즘으로 Adam을 사용하곤 합니다.

390
00:42:45,550 --> 00:42:53,088
특히나 beta_1 = 0.9, beta_2 = 0.999로 설정하고
Learning rate를 E-3나 E-4 정도로만 설정해 놓으면

391
00:42:53,088 --> 00:42:58,797
거의 모든 아키텍쳐에서 잘 동작하는 
기본 설정으로 아주 제격입니다.

392
00:42:58,797 --> 00:43:03,518
일반적으로 Adam으로 시작해 보는 것은 정말 좋습니다.

393
00:43:03,518 --> 00:43:05,949
[웃음]

394
00:43:05,949 --> 00:43:11,634
자 동일한 환경에서 SGD, Momentum, RMSProp,
Adam을 한번 비교해 봅시다.

395
00:43:11,634 --> 00:43:18,094
보라색 Adam을 한번 봅시다. momentum과 RMSProp을
짬뽕시켜 놓은듯한 모습입니다.

396
00:43:18,094 --> 00:43:25,175
Adam이 momentum처럼 overshoots하긴 하지만 
momentum만큼 엄청 심하지는 않습니다.

397
00:43:25,175 --> 00:43:33,268
그리고 Adam은 RMSProp같은 특징도 가지고 있습니다. 각 차원의
상황을 따로 고려해서 Step을 이동합니다.

398
00:43:33,268 --> 00:43:37,706
여기 단순한 2차원 예제에서 수렴하는 것을 보면
그놈이 그놈같아 보일 수는 있지만

399
00:43:37,706 --> 00:43:42,832
확실한 것은 Adam이 momentum스러우면서도 
RMSProp스럽다는 것입니다.

400
00:43:45,042 --> 00:43:48,709
Optimization 알고리즘에 대한 질문 있으신가요?

401
00:43:50,048 --> 00:43:56,606
[학생이 질문]

402
00:43:56,606 --> 00:44:03,193
질문은 Adam이 해결하지 못하는 것은 무엇인지 입니다. 
Neural networks는 여전이 엄청 크고 학습은 오래걸립니다.

403
00:44:04,744 --> 00:44:07,098
Adam을 쓰더라도 여전히 문제점들이 있습니다.

404
00:44:07,098 --> 00:44:11,979
가령 손실함수가 타원형일 경우를 생각해 봅시다.

405
00:44:11,979 --> 00:44:19,219
Adam을 이용하면 각 차원마다 적절하게 속도를 높히고
줄이면서 '독립적으로' step을 조절할 것입니다.

406
00:44:19,219 --> 00:44:26,576
하지만 이 타원이 축 방향으로 정렬되어 있지 않고 
기울어져 있다고 생각해 봅시다.

407
00:44:26,576 --> 00:44:29,887
이 경우에도 Adam은 차원에 해당하는 축 만을
조절할 수 있습니다.

408
00:44:30,935 --> 00:44:38,131
이는 차원을 회전시킨 다음에 수평/수직 축으로만 늘렸다 줄였다 
하는 것입니다. 회전을 시킬수는 없죠

409
00:44:38,131 --> 00:44:48,732
이러한 회전된 타원(poor conditioning) 문제는 Adam을 비롯한
다른 여러 알고리즘들도 다룰 수 없는 문제입니다.

410
00:44:51,356 --> 00:44:57,706
지금까지의 모든 Opimization 알고리즘은
learning rate 이라는 하이퍼파라미터를 가지고 있었습니다.

411
00:44:57,706 --> 00:45:01,828
지난 시간이 이 그림을 본 적 있을 것입니다.

412
00:45:01,828 --> 00:45:05,097
Learning rate가 지나치게 높으면 
노란색 처럼 솟구치게 되겠죠

413
00:45:05,097 --> 00:45:09,629
파란색 처럼 너무 낮으면 
수렴하는데 너무 오래걸립니다.

414
00:45:09,629 --> 00:45:11,933
하지만 learning rate를 잘 고르는 것은 상당히 까다롭습니다.

415
00:45:13,712 --> 00:45:19,308
우리는 학습 과정에서 learning rate 하나를 정해놓고
시작해야 하기 때문에 잘 고르는게 참 어렵습니다.

416
00:45:19,308 --> 00:45:29,705
 learning rates dacay 전략을 사용해 볼 수 있습니다.
각각의 learning rates의 특성을 적절히 이용하는 것이죠

417
00:45:29,705 --> 00:45:39,366
처음에는 learning rates를 높게 설정한 다음에 학습이 진행될수록 
learning rates를 점점 낮추는 것입니다.

418
00:45:39,366 --> 00:45:46,795
다양한 전략을 시도해 볼 수 있지만 가령 100,000 iter에서 
learning rates를 낮추고 학습시키는 것입니다.(step decay)

419
00:45:46,795 --> 00:45:52,579
혹은 exponential decay 처럼 학습과정 동안에 
꾸준히 learning rate를 낮출 수도 있습니다.

420
00:45:52,579 --> 00:45:57,598
꾸준히 learning rate를 감소시키는 것에는 
다양한 전략이 있을 수 있습니다.

421
00:45:57,598 --> 00:46:04,347
오른쪽의 그림을 보시기 바랍니다. Resnet 논문에 있는
그림입니다. Loss가 계속 내려가고 있습니다.

422
00:46:04,347 --> 00:46:07,898
그리고 어느순간 평평해 지는 듯 하더니 다시 또 내려가는
것을 반복하는 것을 알 수 있습니다.

423
00:46:07,898 --> 00:46:11,312
Resnet 논문에서는 step decay learning rate 전략을
사용한 것입니다.

424
00:46:11,312 --> 00:46:18,401
평평해지다가 갑자기 내려가는 구간은 
earning rate를 낮추는 구간입니다.

425
00:46:18,401 --> 00:46:26,243
learning rate를 언제 낮춰야 하는지를 생각해보면, 현재 수렴을
잘 하고 있는 상황에서 gradient가 점점 작아지고 있는 것입니다.

426
00:46:26,243 --> 00:46:28,066
learning rate가 너무 높아서 더 깊게 들어가지 못합니다. 
(bouncing around too much)

427
00:46:28,066 --> 00:46:32,745
이 상황에서  learning rate를 낮추게 되면 속도가 줄어들 것이고 
지속해서 Loss를 내려갈 수 있을 것입니다.

428
00:46:32,745 --> 00:46:36,475
실제로 상당히 도움이 되는 방법입니다.

429
00:46:36,475 --> 00:46:44,973
그리고 한가지 말씀드릴 것은 learning rate decay는
Adam 보다는 SGD Momentum을 사용할 때 자주 씁니다.

430
00:46:44,973 --> 00:46:50,458
또 한가지 말씀드릴 것은 learning rate decay는
부차적인 (second-order ) 하이퍼파라미터 라는 것입니다.

431
00:46:50,458 --> 00:46:53,324
일반적으로 learning-rate decay를 학습 초기부터
고려하지는 않습니다.

432
00:46:53,324 --> 00:47:00,877
보통 학습 초기에는 learning rate decay가 없다고 생각하고
learning rate를 잘 선택하는 것이 중요합니다.

433
00:47:00,877 --> 00:47:06,068
learning rate와 dacay 등을 cross-validate 하려고
한다면 문제가 너무 복잡해 집니다.

434
00:47:06,068 --> 00:47:10,581
learning rate decay를 설정하는 순서는 
우선 decay 없이 학습을 시켜 봅니다.

435
00:47:10,581 --> 00:47:15,427
그리고 Loss curve를 잘 살피고 있다가 decay가 필요한 곳이
어디인지 고려해 보는 것이 좋습니다.

436
00:47:16,860 --> 00:47:24,948
지금까지 배운 Optimization 알고리즘들을 모두
1차미분을 활용한(first-order) 방법이었습니다.

437
00:47:24,948 --> 00:47:33,064
그림처럼 1차원의 손실함수가 있다고 생각해 봅시다. 
우리는 지금 빨간색 점에 있는 것입니다.

438
00:47:33,064 --> 00:47:36,057
이 점에서 gradient 를 계산하겠죠

439
00:47:36,057 --> 00:47:40,722
이 gradient 정보를 이용해서 우리는 손실함수를
선형함수로 근사시킵니다.

440
00:47:40,722 --> 00:47:44,208
이는 일종의 1차 테일러 근사입니다. 
(first-order Taylor apporximation)

441
00:47:44,208 --> 00:47:51,814
우리는 이 1차 근사함수를 실제 손실함수라고 가정하고
Step을 내려갈 것입니다.

442
00:47:51,814 --> 00:47:57,353
하지만 이 근사함수로는 멀리갈 수 없습니다.

443
00:47:57,353 --> 00:48:04,509
현재 우리가 사용하는 정보는 1차 미분값일 뿐입니다.
그리고 우리는 조금 더 괜찮은 방법을 생각해 볼 수도 있을 것입니다.

444
00:48:04,509 --> 00:48:11,248
2차 근사 (second-order approximation)의 정보를
추가적으로 활용하는 방법이 있습니다.

445
00:48:11,248 --> 00:48:18,449
이는 2차 테일러 근사 함수가 될 것이고 이 함수는
2차함수의 모양입니다.

446
00:48:18,449 --> 00:48:22,281
2차 근사를 이용하면 minima에 더 잘 근접할 수 있습니다.
you're really happy

447
00:48:22,281 --> 00:48:25,769
이것이 바로 2nd-order optimization의 기본 아이디어입니다.

448
00:48:25,769 --> 00:48:30,489
방금 예시는 다차원으로 확장시켜보면 이를
'Newton step' 이라고 합니다.

449
00:48:30,489 --> 00:48:35,066
Hessian matrix를 계산합니다. 2차 미분값들로 된 행렬입니다.

450
00:48:35,066 --> 00:48:43,689
 Hessian matrix의 역행렬을  이용하게 되면 실제 손실함수의
2차 근사를 이용해 minima로 곧장 이동할 수 있을 것입니다.

451
00:48:43,689 --> 00:48:48,910
혹시 이 알고리즘이 다른 Optimization 알고리즘에 비해
특이한 것이 무엇인지 아시겠습니까?

452
00:48:48,910 --> 00:48:51,107
[학생이 대답]

453
00:48:51,107 --> 00:48:54,328
네 맞습니다. Learning rate가 없습니다.
That's kind of cool.

454
00:48:56,463 --> 00:49:00,664
지금 우리는 2차근사 함수를 만들었고 이 2차근사 함수의
minima로 이동하는 것입니다.

455
00:49:00,664 --> 00:49:04,681
적어도 "기본적인 Newton's method" 에서는 
learning rate는 불필요합니다.

456
00:49:04,681 --> 00:49:07,849
매 step마다 항상 minma를 향해 이동합니다.

457
00:49:07,849 --> 00:49:13,265
하지만 실제로는 learning rate가 필요합니다.
왜냐하면 2차 근사도 사실상 완벽하지 않기 때문이죠

458
00:49:13,265 --> 00:49:21,055
minima로 이동하는게 아니라 'minima의 방향'으로 이동하기 때문이죠
어쨌든 기본버전에는 learning rate가 없습니다.

459
00:49:23,994 --> 00:49:27,366
하지만 불행하게도 Deep learning에서는 사용할 수 없습니다.

460
00:49:27,366 --> 00:49:34,519
왜냐하면 Hessian matrix는 N x N 행렬입니다. 
N은 Network의 파라미터 수입니다.

461
00:49:34,519 --> 00:49:38,498
N이 1억이면 1억의 제곱만큼 존재할 것입니다.

462
00:49:38,498 --> 00:49:42,046
이를 메모리에 저장할 방법은 없으며 
또한 역행렬계산도 불가능 할 것입니다.

463
00:49:42,046 --> 00:49:46,486
그래서 실제로는 'quasi-Newton methods' 를 이용합니다.

464
00:49:46,486 --> 00:49:52,725
Full Hessian을 그대로 사용하기 보다 근사시킵니다. 
Low-rank approximations 하는 방법입니다.

465
00:49:52,725 --> 00:49:57,092
여러분도 앞으로 간혹 접하게 될 것입니다.

466
00:49:57,092 --> 00:50:03,487
L-BFGS도 second-order optimizer입니다. 이 방법도 
Hassian을 근사시켜서 사용하는 방법입니다.

467
00:50:03,487 --> 00:50:11,205
L-BFGS도 여러분이 가끔씩 보게될텐데 사실상 
DNN에서는 잘 사용하지 않습니다.

468
00:50:11,205 --> 00:50:16,410
왜냐하면 L-BFGS에서 2차근사가 stochastic case에서
잘 동작하지는 않기 때문입니다.

469
00:50:16,410 --> 00:50:20,616
그리고 L-BFGS는 non-convex problems에도
적합하지 않습니다.

470
00:50:20,616 --> 00:50:23,142
더 깊게는 들어가지 않겠습니다.

471
00:50:23,142 --> 00:50:29,022
실제로는 Adam을 제일 많이 씁니다.

472
00:50:29,022 --> 00:50:38,974
하지만 full batch update가 가능하고 stochasticity이
적은 경우라면  L-BFGS가 좋은 선택이 될 수 있습니다.

473
00:50:38,974 --> 00:50:43,181
L-BFGS가 Neural network를 학습시키는데 그렇게
많이 사용되지는 않지만

474
00:50:43,181 --> 00:50:47,251
앞으로 보게될 Sytle tranfer와 같은 알고리즘에
L-BFGS을 종종 사용할 수 있습니다.

475
00:50:47,251 --> 00:50:54,356
Sytle tranfer같은 stochasticity와 파라미터가
적은 경우에서 Optimization을 해야할 경우에 말이죠

476
00:50:55,834 --> 00:51:00,992
지금까지 이야기했던 모든 것들을 전부 training error를
줄이기 위한 방법들이었습니다.

477
00:51:02,344 --> 00:51:07,452
optimization 알고리즘들은 training error를 줄이고 
손실함수를 최소화시키기 위한 역할을 수행합니다.

478
00:51:07,452 --> 00:51:10,403
하지만 사실 우리는 Training error에 크게 신경쓰지 않습니다.

479
00:51:10,403 --> 00:51:13,203
대신에 "한번도 보지 못한 데이터"에 대한 성능이 더 중요하죠

480
00:51:13,203 --> 00:51:16,817
우리가 원하는 것은 train/test error의 격차를
줄이는 것입니다.

481
00:51:16,817 --> 00:51:19,231


482
00:51:19,231 --> 00:51:21,228
목적 함수를 최적화하는 데 능숙하고,

483
00:51:21,228 --> 00:51:23,048
이 격차를 줄이기 위해 우리가 할 수있는 일

484
00:51:23,048 --> 00:51:24,285
우리 모델이 더 잘 수행되도록

485
00:51:24,285 --> 00:51:25,535
보이지 않는 데이터에?

486
00:51:28,497 --> 00:51:30,248
정말 빠르고 쉽고 쉬운 일

487
00:51:30,248 --> 00:51:33,617
시도하는 것은 모델 앙상블에 대한이 아이디어이다.

488
00:51:33,617 --> 00:51:35,499
때로는 여러 영역에서 작동합니다.

489
00:51:35,499 --> 00:51:36,767
기계 학습에서.

490
00:51:36,767 --> 00:51:38,189
아이디어는 꽤 간단합니다.

491
00:51:38,189 --> 00:51:39,852
하나의 모델 만 가지고있는 것이 아니라,

492
00:51:39,852 --> 00:51:42,153
우리는 10 가지 모델을 독립적으로 교육 할 것입니다.

493
00:51:42,153 --> 00:51:44,588
다른 초기 랜덤 재시작으로부터.

494
00:51:44,588 --> 00:51:46,601
이제 테스트 시간에 데이터를 실행합니다.

495
00:51:46,601 --> 00:51:48,416
10 모델 모두 평균을 통해

496
00:51:48,416 --> 00:51:51,333
그 10 가지 모델의 예측.

497
00:51:53,562 --> 00:51:55,192
이 여러 모델을 함께 추가

498
00:51:55,192 --> 00:51:57,284
과핑을 조금 줄이는 경향이있다.

499
00:51:57,284 --> 00:51:59,678
성능을 약간 향상시키는 경향이 있습니다.

500
00:51:59,678 --> 00:52:01,555
일반적으로 2 % 정도.

501
00:52:01,555 --> 00:52:03,719
이것은 일반적으로 크게 개선되지는 않지만,

502
00:52:03,719 --> 00:52:05,302
그러나 그것은 일관된 개선이다.

503
00:52:05,302 --> 00:52:07,013
대회에서 볼 수 있습니다.

504
00:52:07,013 --> 00:52:09,298
ImageNet과 같은 다른 것들,

505
00:52:09,298 --> 00:52:11,013
모델 앙상블 사용은 매우 일반적입니다.

506
00:52:11,013 --> 00:52:13,263
최대한의 성능을 얻으십시오.

507
00:52:14,488 --> 00:52:16,580
당신은 실제로 이것으로 조금 창의적이 될 수 있습니다.

508
00:52:16,580 --> 00:52:18,643
때로는 별도의 모델을 교육하는 것이 아니라

509
00:52:18,643 --> 00:52:20,482
독립적으로, 당신은 단지 여러개를 유지할 수 있습니다.

510
00:52:20,482 --> 00:52:22,110
과정에서 모델의 스냅 샷

511
00:52:22,110 --> 00:52:24,378
교육을받은 후 다음을 사용하십시오.

512
00:52:24,378 --> 00:52:25,928
너 앙상블처럼.

513
00:52:25,928 --> 00:52:27,777
그럼 당신은 여전히, 테스트 시간에, 평균해야합니다.

514
00:52:27,777 --> 00:52:29,804
이러한 여러 스냅 샷의 예측,

515
00:52:29,804 --> 00:52:31,327
그러나 스냅 샷을 수집하는 동안

516
00:52:31,327 --> 00:52:33,244
훈련 과정.

517
00:52:34,133 --> 00:52:35,956
사실 훌륭한 종이가 제시되고 있습니다.

518
00:52:35,956 --> 00:52:38,693
이번 주 ICLR에서

519
00:52:38,693 --> 00:52:41,607
우리가 사용하는이 아이디어의 멋진 버전

520
00:52:41,607 --> 00:52:43,210
미친 학습 속도 일정,

521
00:52:43,210 --> 00:52:45,315
우리의 학습 속도가 매우 느린 곳에서,

522
00:52:45,315 --> 00:52:47,996
그 다음 매우 빠르다, 그 다음 매우
느리게, 그 다음 매우 빠르다.

523
00:52:47,996 --> 00:52:49,271
아이디어는이 미친 듯이

524
00:52:49,271 --> 00:52:51,332
학습 일정, 그 다음 과정

525
00:52:51,332 --> 00:52:53,220
훈련의 경우 모델이 수렴 할 수 있습니다.

526
00:52:53,220 --> 00:52:55,214
객관적인 조경에있는 다른 지구에

527
00:52:55,214 --> 00:52:57,631
모두 합리적으로 좋다.

528
00:52:58,717 --> 00:52:59,888
이것들에 앙상블을하면

529
00:52:59,888 --> 00:53:01,771
다른 스냅 샷, 그럼 당신은 향상시킬 수 있습니다

530
00:53:01,771 --> 00:53:03,253
당신의 공연은 꽤 멋지 네요.

531
00:53:03,253 --> 00:53:05,532
모델을 한 번만 교육하는 경우에도 마찬가지입니다.

532
00:53:05,532 --> 00:53:07,031
질문이 있으십니까?

533
00:53:07,031 --> 00:53:11,198
- [학생] [듣기에는 너무 낮게 말하기]

534
00:53:25,388 --> 00:53:27,788
- 문제는

535
00:53:27,788 --> 00:53:29,073
오류의 원인이 큰 차이가 있습니다.

536
00:53:29,073 --> 00:53:30,252
그건 너 지나치다는 뜻이지.

537
00:53:30,252 --> 00:53:33,413
아무런 격차가 없다면, 그것은 또한 어쩌면 나쁠 것입니까?

538
00:53:33,413 --> 00:53:35,646
우리는 실제로 작은, 최적의 갭을 원합니까?

539
00:53:35,646 --> 00:53:37,446
둘 사이?

540
00:53:37,446 --> 00:53:39,132
우리는 그 격차를별로 신경 쓰지 않습니다.

541
00:53:39,132 --> 00:53:41,011
우리가 정말로 신경 쓰는 부분은 최대화입니다.

542
00:53:41,011 --> 00:53:44,019
유효성 검사 집합의 성능

543
00:53:44,019 --> 00:53:45,884
일어날 수있는 일은 당신이

544
00:53:45,884 --> 00:53:48,386
격차가 보이지 않으면 개선 할 수있다.

545
00:53:48,386 --> 00:53:51,803
당신의 절대적인 성과는, 많은 경우에,

546
00:53:53,224 --> 00:53:54,995
조금 더 overfitting.

547
00:53:54,995 --> 00:53:56,446
이 이상한 상관 관계가 있습니다.

548
00:53:56,446 --> 00:53:58,213
유효성 검사 집합의 절대 성능

549
00:53:58,213 --> 00:53:59,795
그 간격의 크기.

550
00:53:59,795 --> 00:54:02,720
우리는 절대적인 성과만을 염려합니다.

551
00:54:02,720 --> 00:54:03,735
뒤에서 질문 하나?

552
00:54:03,735 --> 00:54:05,284
- [학생] 하이퍼 매개 변수가 같은가요?

553
00:54:05,284 --> 00:54:07,004
앙상블을 위해서?

554
00:54:07,004 --> 00:54:08,254
- 하이퍼 매개 변수가 동일합니까?

555
00:54:08,254 --> 00:54:09,528
앙상블 때문에?

556
00:54:09,528 --> 00:54:10,650
그것은 좋은 질문입니다.

557
00:54:10,650 --> 00:54:12,234
때로는 그렇지 않습니다.

558
00:54:12,234 --> 00:54:15,406
모델의 다른 크기를 시도해 볼 수도 있습니다.

559
00:54:15,406 --> 00:54:16,513
다른 학습 속도, 다른

560
00:54:16,513 --> 00:54:18,659
정규화 전략과 앙상블

561
00:54:18,659 --> 00:54:19,614
이 다른 것들.

562
00:54:19,614 --> 00:54:22,614
그것은 실제로 때때로 일어납니다.

563
00:54:23,496 --> 00:54:25,130
때때로 할 수있는 또 다른 작은 트릭

564
00:54:25,130 --> 00:54:27,347
그 동안 훈련 중에, 당신은 실제로

565
00:54:27,347 --> 00:54:28,987
기하 급수적 인 평균

566
00:54:28,987 --> 00:54:31,769
귀하의 매개 변수 벡터 자체의 종류에

567
00:54:31,769 --> 00:54:33,880
너의 자신의 네트워크의 부드러운 앙상블

568
00:54:33,880 --> 00:54:35,778
훈련 도중.

569
00:54:35,778 --> 00:54:37,598
그런 다음 부드럽게 감퇴하는 평균을 사용하십시오.

570
00:54:37,598 --> 00:54:39,417
매개 변수 벡터 대신

571
00:54:39,417 --> 00:54:41,649
실제 검사 점 자체.

572
00:54:41,649 --> 00:54:42,972
이를 폴리 랙 평균 (Polyak averaging)이라고하며,

573
00:54:42,972 --> 00:54:45,262
때때로 도움이됩니다.

574
00:54:45,262 --> 00:54:46,678
이 작은 트릭 중 하나 일뿐입니다.

575
00:54:46,678 --> 00:54:48,334
때때로 추가 할 수는 있지만 어쩌면 그렇지 않을 수도 있습니다.

576
00:54:48,334 --> 00:54:50,838
실제로 너무 일반적입니다.

577
00:54:50,838 --> 00:54:52,810
당신이 가질 수있는 또 다른 질문은

578
00:54:52,810 --> 00:54:54,361
실제로 어떻게 성능을 향상시킬 수 있습니까?

579
00:54:54,361 --> 00:54:55,778
단일 모델의?

580
00:54:57,229 --> 00:54:59,033
우리가 앙상블을 가질 때, 우리는 여전히 달릴 필요가있다.

581
00:54:59,033 --> 00:55:00,739
테스트 시간에는 10 가지 모델이 있습니다.

582
00:55:00,739 --> 00:55:02,503
그렇게 좋지는 않습니다.

583
00:55:02,503 --> 00:55:04,153
우리는 몇 가지 전략을 개선하기를 정말로 원합니다.

584
00:55:04,153 --> 00:55:06,219
우리의 단일 모델의 성능.

585
00:55:06,219 --> 00:55:08,237
이것이 바로 정규화에 대한이 아이디어입니다.

586
00:55:08,237 --> 00:55:10,217
모델에 무언가를 추가합니다.

587
00:55:10,217 --> 00:55:11,954
훈련 데이터를 피팅하지 못하게한다.

588
00:55:11,954 --> 00:55:14,392
더 잘 수행하려는 시도에서 너무 잘

589
00:55:14,392 --> 00:55:16,203
보이지 않는 데이터.

590
00:55:16,203 --> 00:55:18,304
우리는 몇 가지 아이디어, 몇 가지 방법을 보았습니다.

591
00:55:18,304 --> 00:55:20,245
이미 우리가 추가하는 정규화를 위해

592
00:55:20,245 --> 00:55:23,515
손실에 대한 명시적인 추가 용어.

593
00:55:23,515 --> 00:55:25,426
우리가 모델을 말하고있는이 한 단어

594
00:55:25,426 --> 00:55:27,514
데이터에 맞게, 그리고 다른 용어

595
00:55:27,514 --> 00:55:29,738
그것은 정규화 용어입니다.

596
00:55:29,738 --> 00:55:31,532
우리가 사용한 숙제 하나에서 이걸 봤어.

597
00:55:31,532 --> 00:55:33,032
L2 정규화.

598
00:55:34,804 --> 00:55:37,048
우리가 강의에서 이야기했던 것처럼

599
00:55:37,048 --> 00:55:39,368
강의 이전에,이 L2 정규화는

600
00:55:39,368 --> 00:55:41,418
문맥에서 어쩌면 많은 의미를 가질 수 있습니다.

601
00:55:41,418 --> 00:55:43,001
신경 네트워크.

602
00:55:43,922 --> 00:55:47,982
때때로 우리는 신경망을 위해 다른 것들을 사용합니다.

603
00:55:47,982 --> 00:55:49,836
한 가지 정규화 전략은 수퍼,

604
00:55:49,836 --> 00:55:51,327
신경 네트워크를위한 슈퍼 공통

605
00:55:51,327 --> 00:55:53,376
이 탈락의 생각입니다.

606
00:55:53,376 --> 00:55:55,080
드롭 아웃은 매우 간단합니다.

607
00:55:55,080 --> 00:55:56,589
앞으로 나아갈 때마다

608
00:55:56,589 --> 00:55:59,045
네트워크는 모든 계층에서

609
00:55:59,045 --> 00:56:02,264
임의로 일부 뉴런을 0으로 설정하십시오.

610
00:56:02,264 --> 00:56:03,542
우리가 앞으로 전달할 때마다,

611
00:56:03,542 --> 00:56:05,025
다른 임의의 부분 집합을 설정합니다.

612
00:56:05,025 --> 00:56:06,650
뉴런의 0으로.

613
00:56:06,650 --> 00:56:08,688
이러한 종류의 수익은 한 번에 한 계층 씩 진행됩니다.

614
00:56:08,688 --> 00:56:10,660
우리는 하나의 레이어를 실행하고,

615
00:56:10,660 --> 00:56:12,388
레이어의 값, 무작위로 설정

616
00:56:12,388 --> 00:56:13,948
그들 중 일부는 제로로, 그리고 우리는 계속

617
00:56:13,948 --> 00:56:15,193
네트워크를 통해.

618
00:56:15,193 --> 00:56:17,408
이제 완전히 연결된 네트워크를 보면

619
00:56:17,408 --> 00:56:20,605
드롭 아웃 버전 대 왼쪽

620
00:56:20,605 --> 00:56:22,445
오른쪽에있는 동일한 네트워크의

621
00:56:22,445 --> 00:56:25,503
우리가 탈락 한 후에는 외모가 좋아

622
00:56:25,503 --> 00:56:27,823
같은 네트워크의 더 작은 버전처럼,

623
00:56:27,823 --> 00:56:30,400
여기서 우리는 뉴런의 일부 서브
세트만을 사용하고 있습니다.

624
00:56:30,400 --> 00:56:34,289
우리가 사용하는이 부분 집합은 각 반복마다 다르며,

625
00:56:34,289 --> 00:56:35,746
각각의 정방향 패스에서.

626
00:56:35,746 --> 00:56:36,732
문제?

627
00:56:36,732 --> 00:56:40,899
- [학생] [듣기에는 너무 낮게 말하기]

628
00:56:43,694 --> 00:56:45,232
- 문제는 우리가 무엇을 0으로 놓고있는 것입니까?

629
00:56:45,232 --> 00:56:46,375
그것은 활성화입니다.

630
00:56:46,375 --> 00:56:48,597
각 계층은 이전 활성화를 계산 중입니다.

631
00:56:48,597 --> 00:56:49,820
가중치 행렬이 당신에게주는 시간

632
00:56:49,820 --> 00:56:51,731
우리의 다음 활성화.

633
00:56:51,731 --> 00:56:53,396
그런 다음 그 활성화를 취하십시오.

634
00:56:53,396 --> 00:56:55,274
그 중 일부를 0으로 설정 한 다음

635
00:56:55,274 --> 00:56:58,539
당신의 다음 계층은 부분적으로
활성화가 제로화 될 것입니다.

636
00:56:58,539 --> 00:57:01,592
다른 매트릭스가 당신에게 당신의 다음
활성화를주는 시간을 곱하십시오.

637
00:57:01,592 --> 00:57:03,155
문제?

638
00:57:03,155 --> 00:57:06,702
- [학생] [듣기에는 너무 낮게 말하기]

639
00:57:06,702 --> 00:57:08,751
- 어떤 층에서이 작업을 수행합니까?

640
00:57:08,751 --> 00:57:11,223
완전히 연결된 레이어에서 더 일반적입니다.

641
00:57:11,223 --> 00:57:14,454
하지만 가끔씩 길쌈 계층에서도 이것을 볼 수 있습니다.

642
00:57:14,454 --> 00:57:15,901
컨볼 루션 레이어에서 작업 할 때,

643
00:57:15,901 --> 00:57:18,036
때로는 떨어 뜨리는 대신에

644
00:57:18,036 --> 00:57:20,090
무작위로 각 활성화, 대신 때때로

645
00:57:20,090 --> 00:57:23,423
무작위로 전체 기능 맵을 삭제할 수 있습니다.

646
00:57:24,455 --> 00:57:26,246
컨볼 루션에서는 채널 크기가

647
00:57:26,246 --> 00:57:27,784
전체 채널을 삭제할 수도 있습니다.

648
00:57:27,784 --> 00:57:30,117
임의의 요소가 아닌

649
00:57:32,059 --> 00:57:33,920
탈락은 실제로 아주 간단합니다.

650
00:57:33,920 --> 00:57:36,537
두 줄만 추가하면되고,

651
00:57:36,537 --> 00:57:38,480
드롭 아웃 전화 당 한 줄.

652
00:57:38,480 --> 00:57:40,399
여기에는 3 층 신경망이 있습니다.

653
00:57:40,399 --> 00:57:41,572
드롭 아웃이 추가되었습니다.

654
00:57:41,572 --> 00:57:43,890
당신은 우리가해야 할 모든 것을 볼 수 있습니다.

655
00:57:43,890 --> 00:57:45,664
이 여분의 줄을 무작위로 추가했습니다.

656
00:57:45,664 --> 00:57:47,156
어떤 것을 0으로 설정하십시오.

657
00:57:47,156 --> 00:57:49,460
이것은 구현하기 쉽습니다.

658
00:57:49,460 --> 00:57:52,138
그러나 질문은 이것이 왜 심지어 좋은 생각인지?

659
00:57:52,138 --> 00:57:53,996
우리는 네트워크를 심각하게 망치고 있습니다.

660
00:57:53,996 --> 00:57:56,052
무리를 지어 훈련 시간에

661
00:57:56,052 --> 00:57:58,067
값을 0으로 설정합니다.

662
00:57:58,067 --> 00:58:00,988
어떻게하면이 말이 가능할까요?

663
00:58:00,988 --> 00:58:03,905
약간 손이 물결 모양 아이디어의 한 종류

664
00:58:04,995 --> 00:58:06,989
사람들은 드롭 아웃이 예방에 도움이된다는

665
00:58:06,989 --> 00:58:09,622
특징의 공동 적응.

666
00:58:09,622 --> 00:58:10,892
어쩌면 우리가 시도하고 있다고 상상한다면

667
00:58:10,892 --> 00:58:13,796
어쩌면 어떤 우주에서는 고양이를 분류하고,

668
00:58:13,796 --> 00:58:15,853
네트워크는 하나의 뉴런을 학습 할 수있다.

669
00:58:15,853 --> 00:58:18,117
귀에는 꼬리가있는 하나의 뉴런이 있고,

670
00:58:18,117 --> 00:58:21,066
입력에 대한 하나의 뉴런은 모피입니다.

671
00:58:21,066 --> 00:58:22,799
그런 다음 이런 것들을 함께 결합합니다.

672
00:58:22,799 --> 00:58:24,751
그것이 고양이인지 아닌지를 결정합니다.

673
00:58:24,751 --> 00:58:26,906
하지만 이제 우리가 중퇴하면,

674
00:58:26,906 --> 00:58:29,942
고양이애에 관한 최종 결정, 네트워크

675
00:58:29,942 --> 00:58:31,691
이것들에 너무 많이 의존해서는 안된다.

676
00:58:31,691 --> 00:58:32,831
하나의 특징.

677
00:58:32,831 --> 00:58:34,444
대신, 배포 할 필요가 있습니다.

678
00:58:34,444 --> 00:58:37,725
많은 다른 특징에 걸쳐 catness의 그것의 생각.

679
00:58:37,725 --> 00:58:42,205
이것은 어떻게 든 overfitting을
방지하는 데 도움이 될 수 있습니다.

680
00:58:42,205 --> 00:58:44,202
드롭 아웃의 또 다른 해석

681
00:58:44,202 --> 00:58:46,197
최근에 조금 나왔다.

682
00:58:46,197 --> 00:58:48,514
그것은 마치 모델 앙상블을하는 것과 같습니다.

683
00:58:48,514 --> 00:58:50,347
단일 모델 내에서

684
00:58:51,690 --> 00:58:53,204
왼쪽 그림을 보면,

685
00:58:53,204 --> 00:58:55,061
드롭 아웃을 네트워크에 적용한 후,

686
00:58:55,061 --> 00:58:56,760
우리는이 서브 네트워크를 계산하고 있습니다.

687
00:58:56,760 --> 00:58:58,745
뉴런의 일부 하위 집합을 사용합니다.

688
00:58:58,745 --> 00:59:01,140
이제 모든 다른 잠재적 인 드롭 아웃 마스크

689
00:59:01,140 --> 00:59:03,391
다른 잠재적 인 서브 네트워크로 연결됩니다.

690
00:59:03,391 --> 00:59:05,934
이제 드롭 아웃은 전체적인 앙상블을 배우는 것과 같습니다.

691
00:59:05,934 --> 00:59:07,630
동시에 모든 네트워크가 동시에

692
00:59:07,630 --> 00:59:09,145
매개 변수를 공유하십시오.

693
00:59:09,145 --> 00:59:11,592
그건 그렇고, 잠재력의 수 때문에

694
00:59:11,592 --> 00:59:13,790
드롭 아웃 마스크는 수에서 기하 급수적으로 커집니다.

695
00:59:13,790 --> 00:59:15,485
뉴런의

696
00:59:15,485 --> 00:59:17,152
이 모든 것들.

697
00:59:18,089 --> 00:59:20,621
이것은 정말로 거대한, 거대한 앙상블입니다.

698
00:59:20,621 --> 00:59:24,788
동시에 훈련을받는 네트워크의

699
00:59:25,622 --> 00:59:29,128
그러면 문제는 테스트 시간에 어떻게됩니까?

700
00:59:29,128 --> 00:59:31,248
일단 우리가 중도 탈락하자마자 근본적으로

701
00:59:31,248 --> 00:59:34,158
신경망의 작동을 바 꾸었습니다.

702
00:59:34,158 --> 00:59:37,157
이전에는 신경 네트워크 인 f를 사용했습니다.

703
00:59:37,157 --> 00:59:38,716
가중치의 함수, w,

704
00:59:38,716 --> 00:59:41,705
와 입력 x를 곱한 다음 생성합니다.

705
00:59:41,705 --> 00:59:42,850
출력 y.

706
00:59:42,850 --> 00:59:44,656
하지만 이제 우리 네트워크는

707
00:59:44,656 --> 00:59:46,829
이 추가 입력 z는

708
00:59:46,829 --> 00:59:48,268
무작위 탈락 마스크.

709
00:59:48,268 --> 00:59:50,005
그 z는 무작위입니다.

710
00:59:50,005 --> 00:59:52,732
테스트 시간에 임의성을 갖는 것은
어쩌면 좋지 않을 수 있습니다.

711
00:59:52,732 --> 00:59:54,645
페이스 북에서 일한다고 상상해보십시오.

712
00:59:54,645 --> 00:59:56,189
그리고 당신은 이미지를 분류하고 싶다.

713
00:59:56,189 --> 00:59:57,444
사람들이 업로드하고 있습니다.

714
00:59:57,444 --> 00:59:59,912
그리고 오늘, 당신의 이미지는 고양이로 분류됩니다.

715
00:59:59,912 --> 01:00:01,045
내일은 그렇지 않습니다.

716
01:00:01,045 --> 01:00:03,092
그것은 정말로 이상하고 정말로 나쁠 것입니다.

717
01:00:03,092 --> 01:00:05,479
아마 이것을 제거하고 싶을 것입니다.

718
01:00:05,479 --> 01:00:07,755
시험 시간에 확률론

719
01:00:07,755 --> 01:00:09,323
이미 훈련을 받았다.

720
01:00:09,323 --> 01:00:10,833
그럼 우리는 평균을 내고 싶다.

721
01:00:10,833 --> 01:00:12,093
이 임의성.

722
01:00:12,093 --> 01:00:14,305
이것을 쓰면 상상할 수 있습니다.

723
01:00:14,305 --> 01:00:16,272
실제로이 임의성을 무시해 버린다.

724
01:00:16,272 --> 01:00:18,131
일부 필수 요소가 있지만 실제로는

725
01:00:18,131 --> 01:00:20,253
이 통합은 완전히 다루기가 어렵습니다.

726
01:00:20,253 --> 01:00:22,601
우리는이 것을 평가하는 방법을 모른다.

727
01:00:22,601 --> 01:00:24,368
너는 형체가 나쁘다.

728
01:00:24,368 --> 01:00:25,739
네가 상상할 수있는 한가지

729
01:00:25,739 --> 01:00:28,073
샘플링을 통해이 적분을 근사하고 있으며,

730
01:00:28,073 --> 01:00:29,793
여기서 z의 여러 샘플을 그립니다.

731
01:00:29,793 --> 01:00:31,484
테스트 시간에 평균을 낸 다음,

732
01:00:31,484 --> 01:00:33,841
그러나 이것은 아직도 약간의 무작위성을 도입 할 것이고,

733
01:00:33,841 --> 01:00:36,040
그것은 조금 나쁘다.

734
01:00:36,040 --> 01:00:37,544
고맙게도, 중도 이탈의 경우 우리는

735
01:00:37,544 --> 01:00:39,035
실제로이 적분에 근사하다.

736
01:00:39,035 --> 01:00:41,423
현지에서 값싼 방법으로.

737
01:00:41,423 --> 01:00:43,942
우리가 하나의 뉴런을 고려한다면, 출력은 a이다.

738
01:00:43,942 --> 01:00:45,662
입력은 x와 y이고, 두 가중치,

739
01:00:45,662 --> 01:00:47,228
하나, 둘.

740
01:00:47,228 --> 01:00:50,622
그런 다음 테스트 시간에, 우리의 가치는 단지

741
01:00:50,622 --> 01:00:52,622
w 1 x x w 2 y.

742
01:00:53,590 --> 01:00:56,025
이제 우리가이 네트워크에 대해 훈련했다고 상상해보십시오.

743
01:00:56,025 --> 01:00:58,512
훈련 도중, 우리는 확률로 탈락을 사용했다.

744
01:00:58,512 --> 01:01:00,645
우리 뉴런을 떨어 뜨리는 것의 1/2.

745
01:01:00,645 --> 01:01:03,368
이제 훈련 중 예상되는 가치,

746
01:01:03,368 --> 01:01:04,650
우리는 분석적으로 계산할 수 있습니다.

747
01:01:04,650 --> 01:01:06,317
이 작은 경우.

748
01:01:07,712 --> 01:01:09,339
4 개의 드롭 아웃 마스크가 있습니다.

749
01:01:09,339 --> 01:01:10,522
우리는 값을 평균화하려고합니다.

750
01:01:10,522 --> 01:01:12,249
이 4 개의 가면을 가로 질러

751
01:01:12,249 --> 01:01:14,037
우리는 예상되는

752
01:01:14,037 --> 01:01:18,204
훈련 도중 1/2 w 1 x x w 2 y입니다.

753
01:01:19,075 --> 01:01:21,726
이 사이에 분리가 있습니다.

754
01:01:21,726 --> 01:01:24,357
이 평균값 w 1 x와 w 2 y

755
01:01:24,357 --> 01:01:26,082
시험 시간 및 훈련 시간에,

756
01:01:26,082 --> 01:01:29,000
평균값은 단지 1/2 정도입니다.

757
01:01:29,000 --> 01:01:31,108
우리가 할 수있는 한 가지 싼 것은

758
01:01:31,108 --> 01:01:34,883
테스트 시간에, 우리는 어떤 확률도 없다.

759
01:01:34,883 --> 01:01:36,710
대신이 출력을 곱하면됩니다.

760
01:01:36,710 --> 01:01:38,287
드롭 아웃 확률로

761
01:01:38,287 --> 01:01:40,736
이제이 예상 값은 같습니다.

762
01:01:40,736 --> 01:01:43,260
이것은 지역 저렴한 근사치와 비슷합니다.

763
01:01:43,260 --> 01:01:44,733
이 복잡한 적분에

764
01:01:44,733 --> 01:01:46,493
이것은 사람들이 실제로하는 일입니다.

765
01:01:46,493 --> 01:01:48,576
실제로 중도 탈락.

766
01:01:49,715 --> 01:01:51,437
드롭 아웃에서 우리는이 예측 기능을 가지고 있습니다.

767
01:01:51,437 --> 01:01:53,237
레이어의 출력을 곱하면됩니다.

768
01:01:53,237 --> 01:01:56,269
드롭 아웃 확률로

769
01:01:56,269 --> 01:01:58,368
드롭 아웃의 요약은 정말 간단하다는 것입니다.

770
01:01:58,368 --> 01:01:59,393
앞으로 패스에.

771
01:01:59,393 --> 01:02:01,693
구현에 두 줄만 추가하면됩니다.

772
01:02:01,693 --> 01:02:03,807
어떤 노드를 무작위로 제로로 바꾼다.

773
01:02:03,807 --> 01:02:06,209
그런 다음, 테스트 시간 예측 기능에서,

774
01:02:06,209 --> 01:02:08,890
당신은 방금 하나의 작은 곱셈을 추가했습니다.

775
01:02:08,890 --> 01:02:10,209
당신의 확률로.

776
01:02:10,209 --> 01:02:11,329
드롭 아웃은 매우 간단합니다.

777
01:02:11,329 --> 01:02:14,396
때로는 잘 작동하는 경향이있다.

778
01:02:14,396 --> 01:02:16,613
신경망을 규칙 화하기 위해.

779
01:02:16,613 --> 01:02:18,704
그건 그렇고 가끔 보시는 한 가지 일반적인 트릭입니다.

780
01:02:18,704 --> 01:02:21,454
거꾸로 떨어지는 생각입니다.

781
01:02:22,665 --> 01:02:25,004
어쩌면 테스트 시간에, 당신은
효율성에 대해 더 신경을 쓰고,

782
01:02:25,004 --> 01:02:27,015
그래서 당신은 여분의 곱셈을 제거하고 싶습니다.

783
01:02:27,015 --> 01:02:28,735
시험 시간에 p.

784
01:02:28,735 --> 01:02:30,763
그럼 당신이 할 수있는 것은, 시험 시간에,

785
01:02:30,763 --> 01:02:32,925
전체 무게 행렬을 사용하지만 지금은

786
01:02:32,925 --> 01:02:35,353
훈련 시간에 대신 p로 나눕니다.

787
01:02:35,353 --> 01:02:37,677
GPU에서 교육이 진행되고 있기 때문입니다.

788
01:02:37,677 --> 01:02:38,713
네가 한 일이라도 상관 없다.

789
01:02:38,713 --> 01:02:40,774
여분의 훈련 시간에,하지만 그때

790
01:02:40,774 --> 01:02:42,042
시험 시간에 너는 이걸 원해.

791
01:02:42,042 --> 01:02:44,733
가능한 한 효율적이다.

792
01:02:44,733 --> 01:02:45,566
문제?

793
01:02:46,416 --> 01:02:50,583
- [학생] [듣기에는 너무 낮게 말하기]

794
01:02:52,610 --> 01:02:56,777
이제 그라디언트 [듣기에는 너무 낮습니다.]

795
01:02:57,678 --> 01:02:59,225
- 질문은 그라디언트가 어떻게되는지에 대한 것입니다.

796
01:02:59,225 --> 01:03:02,212
탈락과 훈련 도중?

797
01:03:02,212 --> 01:03:03,045
당신 말이 맞아요.

798
01:03:03,045 --> 01:03:04,047
우리는 단지 그라디언트를 전파하게됩니다.

799
01:03:04,047 --> 01:03:06,583
떨어 뜨리지 않은 노드를 통해

800
01:03:06,583 --> 01:03:09,009
이것은

801
01:03:09,009 --> 01:03:10,366
당신이 중퇴로 훈련 할 때,

802
01:03:10,366 --> 01:03:12,226
일반적으로 교육은

803
01:03:12,226 --> 01:03:13,549
각 단계에서 업데이트 만하고 있습니다.

804
01:03:13,549 --> 01:03:15,356
네트워크의 일부 하위 항목.

805
01:03:15,356 --> 01:03:16,562
드롭 아웃을 사용하는 경우 일반적으로

806
01:03:16,562 --> 01:03:18,537
훈련하는 데 시간이 오래 걸리지 만

807
01:03:18,537 --> 01:03:22,287
그것이 수렴 된 후에 더 나은 일반화.

808
01:03:24,409 --> 01:03:27,093
탈락, 우리는 이런 종류의 것이 었습니다.

809
01:03:27,093 --> 01:03:28,374
구체적인 인스턴스화.

810
01:03:28,374 --> 01:03:29,909
좀 더 일반적인 전략이 있습니다.

811
01:03:29,909 --> 01:03:32,810
훈련 도중 정규화를 위해

812
01:03:32,810 --> 01:03:35,085
우리는 네트워크에 어떤 종류의 임의성을 추가합니다.

813
01:03:35,085 --> 01:03:37,482
트레이닝 데이터를 잘 맞추지 못하게합니다.

814
01:03:37,482 --> 01:03:39,134
그걸 엉망으로 만들지 마라.

815
01:03:39,134 --> 01:03:41,037
훈련 데이터를 완벽하게 피팅하는 것.

816
01:03:41,037 --> 01:03:42,525
이제 테스트 시간에 평균을 내고 싶습니다.

817
01:03:42,525 --> 01:03:44,591
희망을 갖고 개선 할 모든 무작위성

818
01:03:44,591 --> 01:03:46,160
우리 일반화.

819
01:03:46,160 --> 01:03:48,021
드롭 아웃이 가장 일반적인 예일 것입니다

820
01:03:48,021 --> 01:03:50,249
이 전략 유형의

821
01:03:50,249 --> 01:03:53,927
일괄 정규화 일종의 아이디어도이 아이디어에 부합한다.

822
01:03:53,927 --> 01:03:56,733
배치 정상화, 훈련 중 기억,

823
01:03:56,733 --> 01:03:58,996
하나의 데이터 요소가 다른 미니
배치에 나타날 수 있습니다.

824
01:03:58,996 --> 01:04:00,755
다른 다른 데이터 포인트로

825
01:04:00,755 --> 01:04:02,313
존경심과 관련하여 약간의 확률이 있습니다.

826
01:04:02,313 --> 01:04:04,427
얼마나 정확히 하나의 데이터 포인트로

827
01:04:04,427 --> 01:04:07,200
그 시점은 훈련 중에 정상화됩니다.

828
01:04:07,200 --> 01:04:09,307
하지만 이제 테스트 시간에, 우리는 평균 아웃

829
01:04:09,307 --> 01:04:11,046
이 확률론은

830
01:04:11,046 --> 01:04:12,787
글로벌 추정치가

831
01:04:12,787 --> 01:04:14,735
미니 배치 당 추정치.

832
01:04:14,735 --> 01:04:16,536
사실 일괄 정규화는

833
01:04:16,536 --> 01:04:18,425
비슷한 규칙 효과의 종류

834
01:04:18,425 --> 01:04:20,223
그들이 모두 소개하기 때문에 중퇴로

835
01:04:20,223 --> 01:04:22,222
어떤 종류의 확률이나 소음

836
01:04:22,222 --> 01:04:24,251
훈련 시간에, 그러나 그 후에 그것을 밖으로 평균

837
01:04:24,251 --> 01:04:25,478
시험 시간에.

838
01:04:25,478 --> 01:04:27,988
사실, 당신이 네트워크를 훈련 할 때

839
01:04:27,988 --> 01:04:30,000
일괄 정규화, 때때로 당신은 사용하지 않는다.

840
01:04:30,000 --> 01:04:31,982
드롭 아웃 및 배치 일괄 정규화

841
01:04:31,982 --> 01:04:33,759
충분히 정규화 효과를 더한다.

842
01:04:33,759 --> 01:04:35,744
귀하의 네트워크에.

843
01:04:35,744 --> 01:04:37,212
당신이 할 수 있기 때문에 dropout은 다소 멋지다.

844
01:04:37,212 --> 01:04:38,856
실제로 정규화 강도를 조정한다.

845
01:04:38,856 --> 01:04:40,996
그 매개 변수 p를 변화시킴으로써

846
01:04:40,996 --> 01:04:43,833
배치 정규화에서 제어.

847
01:04:43,833 --> 01:04:45,872
다른 종류의 전략

848
01:04:45,872 --> 01:04:48,928
이 패러다임은 데이터 증가에 대한 아이디어입니다.

849
01:04:48,928 --> 01:04:51,009
훈련 중, 바닐라 버전

850
01:04:51,009 --> 01:04:53,331
교육을 위해 데이터가 있으며 레이블이 있습니다.

851
01:04:53,331 --> 01:04:57,078
우리는 매 시간 단계마다 CNN을
업데이트하기 위해이 정보를 사용합니다.

852
01:04:57,078 --> 01:04:58,681
하지만 대신 우리가 할 수있는 일은 무작위 적입니다.

853
01:04:58,681 --> 01:05:01,748
훈련 도중 어떤 식 으로든 이미지를 변형시킨다.

854
01:05:01,748 --> 01:05:03,555
라벨이 보존되도록한다.

855
01:05:03,555 --> 01:05:06,087
이제 이러한 무작위 변환에 대해 교육합니다.

856
01:05:06,087 --> 01:05:09,418
원본 이미지가 아닌 이미지의

857
01:05:09,418 --> 01:05:11,979
때로는 임의의 수평 반전이 나타날 수 있습니다.

858
01:05:11,979 --> 01:05:13,570
고양이를 가져 와서 뒤집으면

859
01:05:13,570 --> 01:05:16,153
가로로, 그것은 여전히 고양이입니다.

860
01:05:17,690 --> 01:05:19,824
크기가 다른 작물을 무작위로 샘플링합니다.

861
01:05:19,824 --> 01:05:21,596
무작위로 자르기 때문에 이미지에서

862
01:05:21,596 --> 01:05:23,763
고양이의 고양이는 여전히 고양이입니다.

863
01:05:25,188 --> 01:05:27,192
그런 다음 테스트하는 동안 평범한

864
01:05:27,192 --> 01:05:30,317
이 확률은

865
01:05:30,317 --> 01:05:32,508
고정 된 작물 세트, 종종 네 모퉁이

866
01:05:32,508 --> 01:05:34,309
그리고 중간과 그들의 플립.

867
01:05:34,309 --> 01:05:35,958
아주 흔한 것은 당신이 읽을 때,

868
01:05:35,958 --> 01:05:38,041
예를 들어, ImageNet에 관한 논문은

869
01:05:38,041 --> 01:05:39,967
해당 모델의 단일 작물 성능,

870
01:05:39,967 --> 01:05:41,434
이것은 전체 이미지와 같습니다.

871
01:05:41,434 --> 01:05:43,075
그리고 그들의 모델에 대한 10 가지 작물 성능,

872
01:05:43,075 --> 01:05:45,891
이 5 가지 표준 작물입니다.

873
01:05:45,891 --> 01:05:47,308
플립 플러스.

874
01:05:48,238 --> 01:05:50,406
또한 데이터 증가와 함께, 당신은 때때로

875
01:05:50,406 --> 01:05:52,619
색상 지터 사용, 임의로

876
01:05:52,619 --> 01:05:54,876
이미지의 대비 또는 밝기를 변경하십시오.

877
01:05:54,876 --> 01:05:56,345
훈련 도중.

878
01:05:56,345 --> 01:05:57,683
좀 더 복잡해질 수 있어요.

879
01:05:57,683 --> 01:05:59,513
색상 지터를 사용하여

880
01:05:59,513 --> 01:06:01,425
어쩌면 색상 불안감을 줄 수 있습니다.

881
01:06:01,425 --> 01:06:04,642
데이터 공간의 PCA 방향 또는 기타

882
01:06:04,642 --> 01:06:06,683
색상 지터를하는 곳

883
01:06:06,683 --> 01:06:09,539
일부 데이터에 의존하는 방식으로

884
01:06:09,539 --> 01:06:11,456
조금 덜 일반적.

885
01:06:12,492 --> 01:06:14,650
일반적으로 데이터 증가는 실제로

886
01:06:14,650 --> 01:06:15,917
신청할 수있는 일반적인 사항

887
01:06:15,917 --> 01:06:18,037
단지 어떤 문제라도.

888
01:06:18,037 --> 01:06:19,721
당신이 해결하려고하는 모든 문제,

889
01:06:19,721 --> 01:06:21,569
너는 어떤 방법에 대해 생각하는거야?

890
01:06:21,569 --> 01:06:23,661
내 데이터를 변환 할 수있는

891
01:06:23,661 --> 01:06:24,940
라벨을 바꿨나요?

892
01:06:24,940 --> 01:06:26,236
이제 교육 기간 중에는

893
01:06:26,236 --> 01:06:29,145
이러한 무작위 변환은 입력 데이터에 적용됩니다.

894
01:06:29,145 --> 01:06:31,218
이런 종류의 정규화 효과가 있습니다.

895
01:06:31,218 --> 01:06:33,198
다시 말하면 네트워크에

896
01:06:33,198 --> 01:06:35,371
훈련 중 어떤 종류의 확률론,

897
01:06:35,371 --> 01:06:38,954
시험 시간에 그것을 밖으로 marginalize.

898
01:06:40,055 --> 01:06:42,646
이제 우리는이 패턴의 세 가지 예를 보았습니다.

899
01:06:42,646 --> 01:06:45,232
드롭 아웃, 일괄 정규화, 데이터 증가,

900
01:06:45,232 --> 01:06:47,154
그러나 다른 많은 예들도 있습니다.

901
01:06:47,154 --> 01:06:48,971
일단 당신이 당신의 마음에이 패턴을 가지고 있다면,

902
01:06:48,971 --> 01:06:50,582
너는 이걸 알아볼거야.

903
01:06:50,582 --> 01:06:53,049
때때로 다른 신문을 읽으면서.

904
01:06:53,049 --> 01:06:55,035
탈락과 관련된 또 다른 종류의 아이디어가 있습니다.

905
01:06:55,035 --> 01:06:56,722
DropConnect라고합니다.

906
01:06:56,722 --> 01:06:58,854
DropConnect를 사용하면 같은 생각입니다.

907
01:06:58,854 --> 01:07:01,493
그러나 활성화를 제로로하기보다는

908
01:07:01,493 --> 01:07:03,467
모든 전진 패스에서 대신 무작위로

909
01:07:03,467 --> 01:07:06,265
대신에 가중치 행렬의 일부 값을 제로화하십시오.

910
01:07:06,265 --> 01:07:09,652
다시 말하지만, 비슷한 종류의 풍미가 있습니다.

911
01:07:09,652 --> 01:07:12,882
내가 좋아하는 멋진 아이디어의 또 다른 종류,

912
01:07:12,882 --> 01:07:14,428
이건 너무 일반적으로 사용되지는 않지만 나는

913
01:07:14,428 --> 01:07:16,281
그냥 정말 멋진 생각이라고 생각해.

914
01:07:16,281 --> 01:07:19,400
분수 맥스 풀링에 대한 아이디어입니다.

915
01:07:19,400 --> 01:07:21,572
일반적으로 2 x 2 최대 풀링을 할 때,

916
01:07:21,572 --> 01:07:23,809
이 고정 된 2 x 2 영역이 있습니다.

917
01:07:23,809 --> 01:07:26,157
당신이 앞으로 나아가는 동안,

918
01:07:26,157 --> 01:07:29,067
그러나 이제 분수 최대 풀링과 함께,

919
01:07:29,067 --> 01:07:31,700
우리가 풀링 레이어를 가질 때마다,

920
01:07:31,700 --> 01:07:33,387
우리는 수영장을 정확히 무작위로 선정하려고합니다.

921
01:07:33,387 --> 01:07:35,851
우리가 넘는 지역.

922
01:07:35,851 --> 01:07:37,712
여기 오른쪽의 예에서,

923
01:07:37,712 --> 01:07:39,486
나는 세 가지 세트를 보여 줬어.

924
01:07:39,486 --> 01:07:41,469
보이는 임의의 풀링 영역 중

925
01:07:41,469 --> 01:07:43,070
훈련 도중.

926
01:07:43,070 --> 01:07:46,235
이제 테스트 시간 동안, 당신은 평범한 분이 십니다.

927
01:07:46,235 --> 01:07:48,857
많은 다른 것을 시도함으로써 확률론,

928
01:07:48,857 --> 01:07:52,397
풀링 영역의 일부 고정 세트에 고정하거나

929
01:07:52,397 --> 01:07:54,704
많은 샘플을 그려 평균화합니다.

930
01:07:54,704 --> 01:07:56,034
그것은 멋진 생각입니다.

931
01:07:56,034 --> 01:07:59,027
그것은 그렇게 일반적으로 사용되지 않습니다.

932
01:07:59,027 --> 01:08:01,308
정말 놀라운 또 다른 종이

933
01:08:01,308 --> 01:08:04,393
실제로 나온이 패러다임에서

934
01:08:04,393 --> 01:08:05,890
지난 1 년 동안

935
01:08:05,890 --> 01:08:08,127
우리가 수업을 마지막으로 가르쳤을 때,이 생각입니다.

936
01:08:08,127 --> 01:08:09,911
확률 적 깊이의

937
01:08:09,911 --> 01:08:12,602
여기에 우리는 왼쪽에 네트워크가 있습니다.

938
01:08:12,602 --> 01:08:15,490
아이디어는 우리가 매우 깊은
네트워크를 가지고 있다는 것입니다.

939
01:08:15,490 --> 01:08:17,275
네트워크에서 무작위로 레이어를 삭제하려고합니다.

940
01:08:17,275 --> 01:08:18,530
훈련 도중.

941
01:08:18,530 --> 01:08:20,974
훈련 도중, 우리는

942
01:08:20,974 --> 01:08:22,561
일부 레이어 및 일부 하위 집합 만 사용

943
01:08:22,561 --> 01:08:24,113
훈련 중 층의

944
01:08:24,114 --> 01:08:26,854
이제 테스트 시간 동안 우리는
전체 네트워크를 사용할 것입니다.

945
01:08:26,854 --> 01:08:28,262
이것은 일종의 미친 짓이다.

946
01:08:28,262 --> 01:08:30,251
이 작품이 놀랍습니다.

947
01:08:30,251 --> 01:08:31,852
그러나 이것은 비슷한 종류의 경향이있다.

948
01:08:31,852 --> 01:08:33,470
드롭 아웃으로 정규화 효과

949
01:08:33,470 --> 01:08:35,310
그리고이 다른 전략들.

950
01:08:35,310 --> 01:08:37,985
그러나 다시 말하지만, 이것은
슈퍼하고 최첨단의 연구입니다.

951
01:08:37,985 --> 01:08:40,208
이것은 실제적으로 일반적으로 많이 사용되는 것이 아니며,

952
01:08:40,209 --> 01:08:42,042
하지만 멋진 아이디어입니다.

953
01:08:44,694 --> 01:08:48,611
정규화에 관한 막판 질문이 있습니까?

954
01:08:49,944 --> 01:08:51,716
아니? 그걸 써. 좋은 생각입니다.

955
01:08:51,716 --> 01:08:52,673
네?

956
01:08:52,673 --> 01:08:57,046
- [학생] [듣기에는 너무 낮게 말하기]

957
01:08:57,046 --> 01:08:58,184
- 문제는 보통 당신이 사용하는 것입니까?

958
01:08:58,184 --> 01:09:01,184
하나 이상의 정규화 방법?

959
01:09:04,325 --> 01:09:06,580
일반적으로 배치 정규화를 사용해야합니다.

960
01:09:06,581 --> 01:09:08,030
가지고있는 좋은 일종의

961
01:09:08,030 --> 01:09:09,752
요즘 대부분의 네트워크에서

962
01:09:09,752 --> 01:09:12,650
특히 매우 깊은 것들에 대해 수렴하는 데 도움이됩니다.

963
01:09:12,650 --> 01:09:14,914
많은 경우, 일괄 정규화 만

964
01:09:14,914 --> 01:09:17,743
경향이 있지만, 때로는 때로는

965
01:09:17,743 --> 01:09:20,123
일괄 정규화만으로 충분하지 않다면,

966
01:09:20,123 --> 01:09:21,742
드롭 아웃 추가를 고려해 볼 수 있습니다.

967
01:09:21,742 --> 01:09:25,204
또는 다른 것들을 일단 네트워크
overfitting을 참조하십시오.

968
01:09:25,204 --> 01:09:27,397
일반적으로 시각 장애인 교차 검증을 수행하지 않습니다.

969
01:09:27,398 --> 01:09:28,526
이 일들에.

970
01:09:28,526 --> 01:09:30,524
대신, 당신은 그들을 타겟 방식으로 추가합니다.

971
01:09:30,525 --> 01:09:33,942
네트워크가 지나치게 적합하다는 것을 알게되면

972
01:09:36,400 --> 01:09:38,981
하나의 빠른 것은, 그것은 전송 학습의 아이디어입니다.

973
01:09:38,981 --> 01:09:40,740
우리는 정규화와 함께 보았습니다.

974
01:09:40,740 --> 01:09:42,519
우리는 사이의 간격을 줄이는 데 도움이 될 수 있습니다.

975
01:09:42,519 --> 01:09:44,851
열차와 시험 오류가 다른

976
01:09:44,851 --> 01:09:47,018
정규화 전략.

977
01:09:48,903 --> 01:09:51,091
overfitting의 한 가지 문제점은 때때로 있습니다.

978
01:09:51,091 --> 01:09:53,012
당신은 충분한 데이터가 없다는 이유로 과장됩니다.

979
01:09:53,012 --> 01:09:54,632
크고 강력한 모델을 사용하고 싶습니다.

980
01:09:54,632 --> 01:09:56,915
그 크고 강력한 모델은

981
01:09:56,915 --> 01:10:00,444
소규모 데이터 세트에 너무 많이 적용됩니다.

982
01:10:00,444 --> 01:10:02,831
정규화는 그것을 해결하는 한 가지 방법이며,

983
01:10:02,831 --> 01:10:05,909
다른 방법은 이전 학습을 사용하는 것입니다.

984
01:10:05,909 --> 01:10:08,182
이 신화를 흉상의 종류를 옮기십시오 옮기십시오

985
01:10:08,182 --> 01:10:10,695
방대한 양의 데이터가 필요 없다는 점

986
01:10:10,695 --> 01:10:12,730
CNN을 훈련시키기 위해서.

987
01:10:12,730 --> 01:10:15,300
아이디어는 정말 간단합니다.

988
01:10:15,300 --> 01:10:17,811
당신은 아마도 먼저 CNN을 가져갈 것입니다.

989
01:10:17,811 --> 01:10:20,798
다음은 VGG 스타일 아키텍처의 일종입니다.

990
01:10:20,798 --> 01:10:22,593
너 CNN 가져가, 너 훈련 할거야.

991
01:10:22,593 --> 01:10:25,031
ImageNet과 같은 매우 큰 데이터 세트에서,

992
01:10:25,031 --> 01:10:26,224
실제로 충분한 데이터가있는 곳

993
01:10:26,224 --> 01:10:28,039
전체 네트워크를 훈련시키는 것.

994
01:10:28,039 --> 01:10:30,003
이제 아이디어는 당신이

995
01:10:30,003 --> 01:10:32,230
이 데이터 세트에서 일부 기능에 이르기까지

996
01:10:32,230 --> 01:10:34,596
작은 데이터 세트.

997
01:10:34,596 --> 01:10:36,593
아마도 1,000 명을 분류하는 대신

998
01:10:36,593 --> 01:10:39,142
ImageNet 카테고리, 이제 분류하고 싶습니다.

999
01:10:39,142 --> 01:10:41,290
10 마리의 개 유형 또는 그런 것.

1000
01:10:41,290 --> 01:10:42,864
작은 데이터 세트 만 있습니다.

1001
01:10:42,864 --> 01:10:45,917
여기에 우리의 작은 데이터
세트에는 C 클래스 만 있습니다.

1002
01:10:45,917 --> 01:10:48,415
그러면 당신이 일반적으로 할 일은이 마지막 일입니다.

1003
01:10:48,415 --> 01:10:50,936
모든 연결 계층

1004
01:10:50,936 --> 01:10:53,968
마지막 층은 최종 학급 점수를 특징으로하며,

1005
01:10:53,968 --> 01:10:58,135
이제이 행렬을 무작위로 다시 초기화해야합니다.

1006
01:10:59,651 --> 01:11:01,706
ImageNet의 경우 4,096 x 1,000

1007
01:11:01,706 --> 01:11:02,952
3 차원 행렬.

1008
01:11:02,952 --> 01:11:06,057
이제 새로운 수업을 위해

1009
01:11:06,057 --> 01:11:09,182
4,096-by-C 또는 10 또는 어떤 것이 든 될 수 있습니다.

1010
01:11:09,182 --> 01:11:11,669
이 마지막 행렬을 무작위로 다시 초기화하면,

1011
01:11:11,669 --> 01:11:13,985
이전의 모든 레이어의 가중치 고정

1012
01:11:13,985 --> 01:11:16,921
이제는 기본적으로 선형 분류자를 학습합니다.

1013
01:11:16,921 --> 01:11:19,197
이 마지막 레이어의 매개 변수 만 트레이닝합니다.

1014
01:11:19,197 --> 01:11:21,947
귀하의 데이터에 수렴되도록하십시오.

1015
01:11:23,788 --> 01:11:25,265
이것은 꽤 잘 작동하는 경향이 있습니다.

1016
01:11:25,265 --> 01:11:28,756
함께 사용할 매우 작은 데이터 세트가 있어야합니다.

1017
01:11:28,756 --> 01:11:30,888
이제 조금 더 많은 데이터가 있다면,

1018
01:11:30,888 --> 01:11:32,389
시도 할 수있는 또 다른 사항은 실제로

1019
01:11:32,389 --> 01:11:35,166
전체 네트워크 미세 조정.

1020
01:11:35,166 --> 01:11:37,425
상위 레이어가 수렴 한 후

1021
01:11:37,425 --> 01:11:39,566
당신은 당신의 데이터에 대한 마지막 레이어를 배우고,

1022
01:11:39,566 --> 01:11:42,198
실제로 업데이트를 시도 할 수 있습니다.

1023
01:11:42,198 --> 01:11:44,935
뿐만 아니라 전체 네트워크.

1024
01:11:44,935 --> 01:11:47,047
더 많은 데이터가 있다면

1025
01:11:47,047 --> 01:11:49,434
네트워크의 더 큰 부분을 업데이트합니다.

1026
01:11:49,434 --> 01:11:51,819
일반적인 전략은 다음과 같습니다.

1027
01:11:51,819 --> 01:11:53,976
당신은 네트워크를 업데이트하고 있습니다.

1028
01:11:53,976 --> 01:11:56,143
초기 학습 률로부터 학습 률

1029
01:11:56,143 --> 01:11:59,487
아마 원래의 매개 변수들

1030
01:11:59,487 --> 01:12:01,644
ImageNet에 수렴 된이 네트워크에서

1031
01:12:01,644 --> 01:12:02,973
아마 꽤 잘 일반적으로 일했습니다,

1032
01:12:02,973 --> 01:12:04,737
그리고 당신은 그것들을 아주 작은
양으로 바꾸고 싶을뿐입니다.

1033
01:12:04,737 --> 01:12:08,605
데이터 세트의 성능을 조정할 수 있습니다.

1034
01:12:08,605 --> 01:12:10,223
그런 다음 이전 학습을 할 때,

1035
01:12:10,223 --> 01:12:12,243
이 2x2 격자를 상상해보세요.

1036
01:12:12,243 --> 01:12:15,490
시나리오의 한쪽에서

1037
01:12:15,490 --> 01:12:17,120
어쩌면 아주 작은 양의 데이터를

1038
01:12:17,120 --> 01:12:18,696
데이터 세트 또는 매우 많은 양의 데이터

1039
01:12:18,696 --> 01:12:20,113
귀하의 데이터 세트.

1040
01:12:21,188 --> 01:12:24,301
그러면 데이터가 이미지와 매우 유사 할 수도 있습니다.

1041
01:12:24,301 --> 01:12:26,538
마찬가지로 ImageNet에는
동물의 그림이 많이 있습니다.

1042
01:12:26,538 --> 01:12:28,780
식물과 그런 것들.

1043
01:12:28,780 --> 01:12:30,558
다른 유형을 분류하려는 경우

1044
01:12:30,558 --> 01:12:32,561
동식물 및 다른 유형의 이미지

1045
01:12:32,561 --> 01:12:35,335
그런 식으로, 당신은 꽤 좋은 모양입니다.

1046
01:12:35,335 --> 01:12:37,540
그런 다음 일반적으로 데이터가

1047
01:12:37,540 --> 01:12:41,748
ImageNet과 매우 비슷합니다.

1048
01:12:41,748 --> 01:12:43,335
데이터 양이 매우 적은 경우,

1049
01:12:43,335 --> 01:12:45,541
당신은 기본적으로 선형 분류자를 훈련시킬 수 있습니다.

1050
01:12:45,541 --> 01:12:47,200
기능 맨 위에있는

1051
01:12:47,200 --> 01:12:48,861
ImageNet 모델.

1052
01:12:48,861 --> 01:12:51,636
함께 작업 할 데이터가 조금 있으면,

1053
01:12:51,636 --> 01:12:54,786
그러면 데이터를 미세 조정할 수 있습니다.

1054
01:12:54,786 --> 01:12:56,245
그러나 때때로 문제가 생깁니다.

1055
01:12:56,245 --> 01:12:58,755
귀하의 데이터가 ImageNet과
매우 다른 것처럼 보이는 경우.

1056
01:12:58,755 --> 01:13:00,623
어쩌면 당신이 어쩌면

1057
01:13:00,623 --> 01:13:03,150
엑스레이 또는 CAT 스캔 인 의료 영상

1058
01:13:03,150 --> 01:13:04,773
매우 다른 것처럼 보이는

1059
01:13:04,773 --> 01:13:06,781
ImageNet의 이미지에서, 그 경우,

1060
01:13:06,781 --> 01:13:09,072
당신은 조금 더 창조적 일 필요가있을 것입니다.

1061
01:13:09,072 --> 01:13:10,847
때때로 그것은 아직도 여기에서 잘 작동합니다,

1062
01:13:10,847 --> 01:13:13,149
그 마지막 레이어 피처는

1063
01:13:13,149 --> 01:13:14,408
너무 유익하다.

1064
01:13:14,408 --> 01:13:16,689
더 큰 부품을 다시 초기화하는 것을 고려해보십시오.

1065
01:13:16,689 --> 01:13:17,820
네트워크 및 약간의 정보 얻기

1066
01:13:17,820 --> 01:13:21,507
더 창의적이고 더 많은 실험을 시도해보십시오.

1067
01:13:21,507 --> 01:13:22,932
당신이 가진다면 다소 완화됩니다.

1068
01:13:22,932 --> 01:13:25,274
매우 다른 데이터 세트의 많은 양의 데이터

1069
01:13:25,274 --> 01:13:26,481
그런 다음 실제로 조정할 수 있습니다.

1070
01:13:26,481 --> 01:13:29,015
네트워크의 더 큰 부분.

1071
01:13:29,015 --> 01:13:30,708
이 아이디어는 제가 만들고 싶습니다.

1072
01:13:30,708 --> 01:13:32,587
전달 학습은 매우 보편적이다.

1073
01:13:32,587 --> 01:13:35,660
실제로는 예외가 아니라 표준입니다.

1074
01:13:35,660 --> 01:13:37,192
컴퓨터 비전 보고서를 읽으면서,

1075
01:13:37,192 --> 01:13:39,038
당신은 종종 이런 시스템 다이어그램을 볼 것입니다.

1076
01:13:39,038 --> 01:13:40,562
다른 작업.

1077
01:13:40,562 --> 01:13:42,102
왼쪽에서 우리는 물체 감지 작업을하고 있습니다.

1078
01:13:42,102 --> 01:13:44,706
오른쪽에서 이미지 캡션 작업을하고 있습니다.

1079
01:13:44,706 --> 01:13:46,279
이 두 모델 모두 CNN

1080
01:13:46,279 --> 01:13:48,387
그것은 이미지 처리의 일종입니다.

1081
01:13:48,387 --> 01:13:50,633
컴퓨터 비전의 거의 모든 응용 프로그램에서

1082
01:13:50,633 --> 01:13:52,646
요즘 대부분의 사람들은 훈련을받지 않습니다.

1083
01:13:52,646 --> 01:13:53,913
이러한 것들을 처음부터.

1084
01:13:53,913 --> 01:13:55,859
거의 항상 CNN이 사전 교육을 받게 될 것입니다.

1085
01:13:55,859 --> 01:13:57,788
ImageNet에서 확인한 다음 미세 조정할 수 있습니다.

1086
01:13:57,788 --> 01:13:59,973
가까이에있는 작업을 위해.

1087
01:13:59,973 --> 01:14:02,613
또한 자막 의미에서 가끔은

1088
01:14:02,613 --> 01:14:05,387
실제로 일부 단어 벡터 사전 연습

1089
01:14:05,387 --> 01:14:07,089
또한 언어와 관련되어 있습니다.

1090
01:14:07,089 --> 01:14:09,362
CNN을 ImageNet에서 미리 연습 할 수도 있지만,

1091
01:14:09,362 --> 01:14:10,729
큰 단어에 몇 개의 단어 벡터를 프리 트레인하다.

1092
01:14:10,729 --> 01:14:12,469
텍스트 코퍼스를 선택하고 전체를 미세하게 조정하십시오.

1093
01:14:12,469 --> 01:14:14,143
귀하의 데이터 세트.

1094
01:14:14,143 --> 01:14:15,703
캡션의 경우에는

1095
01:14:15,703 --> 01:14:17,442
단어 벡터를 이용한이 사전 학습은

1096
01:14:17,442 --> 01:14:18,907
조금 덜 공통적이다.

1097
01:14:18,907 --> 01:14:22,278
조금 덜 중요합니다.

1098
01:14:22,278 --> 01:14:23,899
귀하의 프로젝트를위한 테이크 아웃,

1099
01:14:23,899 --> 01:14:26,069
더 일반적으로 다른 모델에서 작업 할 때,

1100
01:14:26,069 --> 01:14:29,503
큰 데이터 세트가있을 때마다,

1101
01:14:29,503 --> 01:14:30,620
언제든지 문제가 생길 때마다

1102
01:14:30,620 --> 01:14:33,225
태클하고 싶지만 대용량 데이터 세트가 없습니다.

1103
01:14:33,225 --> 01:14:37,116
그럼 당신이 일반적으로해야 할 일은 다운로드

1104
01:14:37,116 --> 01:14:39,194
상대적으로 가까운 일부 사전 훈련 된 모델

1105
01:14:39,194 --> 01:14:41,673
관심있는 작업에 연결 한 다음

1106
01:14:41,673 --> 01:14:43,374
그 모델의 일부분을 다시 초기화하거나 미세 조정

1107
01:14:43,374 --> 01:14:44,859
데이터 모델.

1108
01:14:44,859 --> 01:14:47,919
네가 가지고 있더라도 꽤 잘하는 경향이있어.

1109
01:14:47,919 --> 01:14:49,208
적당한 양의 훈련 데이터

1110
01:14:49,208 --> 01:14:50,676
함께 일해.

1111
01:14:50,676 --> 01:14:52,513
이것이 일반적인 전략이기 때문에,

1112
01:14:52,513 --> 01:14:53,682
모든 다른 깊은 학습

1113
01:14:53,682 --> 01:14:55,829
거기 밖으로 소프트웨어 패키지 제공

1114
01:14:55,829 --> 01:14:57,738
방금 다운로드 할 수있는 모델 동물원

1115
01:14:57,738 --> 01:15:01,099
다양한 모델의 사전 훈련 된 버전.

1116
01:15:01,099 --> 01:15:03,524
오늘 요약하면, 우리는 최적화,

1117
01:15:03,524 --> 01:15:06,043
이는 교육 손실을 개선하는 방법에 관한 것입니다.

1118
01:15:06,043 --> 01:15:08,083
우리는 정규화에 대해 이야기했습니다.

1119
01:15:08,083 --> 01:15:10,884
테스트 데이터에 대한 귀하의 실적.

1120
01:15:10,884 --> 01:15:12,838
거기에 맞는 앙상블을 모델로합니다.

1121
01:15:12,838 --> 01:15:14,239
또한 우리는 이전 학습에 대해서도 이야기했습니다.

1122
01:15:14,239 --> 01:15:15,738
당신이 실제로 더 잘할 수있는 방법입니다.

1123
01:15:15,738 --> 01:15:17,440
데이터가 적습니다.

1124
01:15:17,440 --> 01:15:19,066
이것들은 모두 매우 유용한 전략입니다.

1125
01:15:19,066 --> 01:15:21,940
프로젝트에서나 그 이상에서 사용해야합니다.

1126
01:15:21,940 --> 01:15:24,232
다음 번에는 더 구체적으로 이야기하겠습니다.

1127
01:15:24,232 --> 01:15:25,238
다른 깊은 학습의 일부

1128
01:15:25,238 --> -00:00:00,485
거기 밖으로 소프트웨어 패키지.

