1
00:00:08,435 --> 00:00:10,602
자 시작합시다.

2
00:00:13,372 --> 00:00:21,193
이제 5강입니다. 5강에서는 CNN을 배우겠습니다.

3
00:00:22,493 --> 00:00:25,933
시작하기 전에 몇가지 공지사항이 있습니다.

4
00:00:25,933 --> 00:00:30,563
첫 번째 과제가 4월 20일 목요일 21:59분에 마감됩니다.

5
00:00:31,440 --> 00:00:35,607
그리고 그때 두 번째 과제가 나갈 예정입니다.

6
00:00:38,320 --> 00:00:40,434
우선 지난 강의를 복습해 봅시다.

7
00:00:40,434 --> 00:00:48,337
지난 몇차례 동안 Neural Networks와 선형 함수들을 살펴보았습니다.

8
00:00:48,337 --> 00:00:56,969
선형 레이어를 쌓고 그 사이에 비선형 레이어를 추가하여
Neural Network를 만들었습니다.

9
00:00:56,969 --> 00:01:01,500
또한 NN은  "Mode 문제" 를 해결할 수 있습니다.

10
00:01:01,500 --> 00:01:06,618
가령 다양한 종류의 자동차를 올바르게 분류하기 위해
"중간 단계의 템플릿"을 학습시키는 것이죠.

11
00:01:06,618 --> 00:01:09,006
빨간색 차 노란색 차 등을 말이죠.

12
00:01:09,006 --> 00:01:14,790
그리고 이 템플릿들을 결합해서 
최종 클래스 스코어를 계산합니다.

13
00:01:14,790 --> 00:01:18,438
오늘은 CNN에 대해 말해보고자 합니다.

14
00:01:18,438 --> 00:01:20,825
기존의 NN과 같은 부류이긴 하지만

15
00:01:20,825 --> 00:01:23,300
이번에는 Convolutional Layer에 대해 배울 것입니다.

16
00:01:23,300 --> 00:01:29,217
이 레이어는 기본적으로 "공간적 구조"를 유지합니다.

17
00:01:31,817 --> 00:01:36,070
우선, Neural Networks에 대해 조금 얘기해 보려 합니다.

18
00:01:36,070 --> 00:01:39,067
그리고 CNN의 역사도 알아볼 것입니다.

19
00:01:39,067 --> 00:01:46,308
우선 1957년으로 돌아가보면, Frank Rosenblatt가 
 Mark I Perceptron machine 을 개발했습니다.

20
00:01:46,308 --> 00:01:51,785
이 기계는 "perceptron"을 구현한 최초의 기계입니다.

21
00:01:51,785 --> 00:01:58,437
"Perceptron"은 우리가 배운 Wx + b 와 
유사한 함수를 사용합니다.

22
00:01:58,437 --> 00:02:02,000
하지만 여기에서는 출력 값이 1 또는 0입니다.

23
00:02:02,000 --> 00:02:06,551
여기에서도 가중치 W를 Update 하는 
Update Rule이 존재합니다.

24
00:02:06,551 --> 00:02:12,304
이 Update Rule은 Backprop과 유사합니다.

25
00:02:12,304 --> 00:02:22,349
하지만 당시에는 backprob이라는 개념이 없어서, 단지 W를
이리저리 조절하면서 맞추는 식이었죠

26
00:02:23,771 --> 00:02:29,673
그리고 1960년에는 Widrow와 Hoff가 \
Adaline and Madaline을 개발했습니다.

27
00:02:29,673 --> 00:02:37,457
이는 최초의 Multilayer Perceptron Network 이었습니다.

28
00:02:38,986 --> 00:02:46,658
이 시점에서야 비로소 Neural network와 비슷한
모양을 하기 시작하긴 했지만

29
00:02:46,658 --> 00:02:50,992
아직 Backprob같은 학습 알고리즘은 없었습니다.

30
00:02:50,992 --> 00:02:56,015
최초의 Backporp은 1986에 Rumelhart가 제안하였습니다.

31
00:02:56,015 --> 00:03:03,906
보시다시피 우리에게 익숙한 
Chain rule과 Update rule을 볼 수 있습니다.

32
00:03:03,906 --> 00:03:09,874
이때 최초로 network를 학습시키는 것에 관한
개념이 정립되기 시작했습니다.

33
00:03:11,623 --> 00:03:18,076
하지만 그 이후로 NN을 더 크게 만들지는 못했습니다.

34
00:03:18,076 --> 00:03:26,237
그리고 한동안은 새로운 이론이 나오지 못했고
널리 쓰이지도 못 했습니다.

35
00:03:26,237 --> 00:03:32,790
그래서 2000년대가 되서야 다시 활기를 찾기 시작했습니다.

36
00:03:33,641 --> 00:03:40,719
Geoff Hinton 과 Ruslan Salakhutdinov의 2006년
논문에서 DNN의 학습가능성을 선보였고

37
00:03:40,719 --> 00:03:43,212
그것이 실제로 아주 효과적이라는 것을 보여주었습니다.

38
00:03:43,212 --> 00:03:47,428
하지막 그 때 까지도 아직 모던한 NN는 아니었습니다.

39
00:03:47,428 --> 00:03:52,439
backprop이 가능하려면 아주 세심하게
초기화를 해야 했습니다.

40
00:03:52,439 --> 00:03:57,601
그래서 여기에서는 전처리 과정이 필요했고

41
00:03:57,601 --> 00:04:07,331
초기화를 위해 RBM을 이용해서 각 히든레이어 가중치를 
학습시켜야 했습니다.

42
00:04:07,331 --> 00:04:20,224
이렇게 초기화된 히든 레이어를 이용해서 전체 신경망을
backprop하거나 fine tune하는 것이었습니다.

43
00:04:23,057 --> 00:04:39,233
실제로 NN의 광풍이 불기 시작한 때는 2012년 이었습니다.

44
00:04:40,268 --> 00:04:44,980
NN이 음성 인식에서 아주 좋은 성능을 보였습니다.

45
00:04:44,980 --> 00:04:50,606
이는 Hintin lab에서 나온 것인데 acoustic modeling과 
speech recognition에 관한 것이었습니다.

46
00:04:50,606 --> 00:04:58,604
또한 2012년에는 Hinton lab의 Alex Krizhevsky에서 
영상 인식에 관한 landmark paper가 하나 나옵니다.

47
00:04:59,638 --> 00:05:06,813
이 논문에서는 ImageNet Classification에서 최초로 NN을
사용했고, 결과는 정말 놀라웠습니다.

48
00:05:06,813 --> 00:05:15,519
AlexNet은 ImageNet benchmark의 Error를 
극적으로 감소시켰습니다.

49
00:05:16,793 --> 00:05:24,236
그 이후로  ConNets은 아주 널리 쓰이고 있습니다.

50
00:05:24,236 --> 00:05:31,714
다시 돌아가서 구체적으로 "CNN이 어떻게 유명해졌는지"
에 대해 한번 알아보도록 하겠습니다.

51
00:05:31,714 --> 00:05:42,538
다시 1950년대로 돌아가보면 Hubel과 Wiesel이 일차시각피질의
뉴런에 관한 연구를 수행했습니다.

52
00:05:42,538 --> 00:05:45,579
고양이에게 실험을 했습니다.

53
00:05:45,579 --> 00:05:53,526
첫 수업에서도 이야기하긴 했지만, 고양이의 뇌에 전극을
꽂았습니다.

54
00:05:53,526 --> 00:05:56,066
그리고 고양이에게 다양한 자극을 주며 실험을 했습니다.

55
00:05:56,066 --> 00:06:06,937
이 실험에서 뉴런이 oriented edges와 shapes같은 것에 
반응한다는 것을 알아냈습니다.

56
00:06:09,029 --> 00:06:14,993
그리고 이 실험에서 내린 몇 가지 결론은 아주 중요했습니다.

57
00:06:14,993 --> 00:06:19,534
그중 하나는 바로 피질 내부에 
지형적인 매핑(topographical mapping)이 있다는 것입니다.

58
00:06:19,534 --> 00:06:24,932
피질 내 서로 인접해 있는 세포들은 visual field내에
어떤 지역성을 띄고 있습니다.

59
00:06:24,932 --> 00:06:34,475
오른쪽 그림은 보면 해당하는 spatial mapping을 볼 수 있습니다.

60
00:06:34,475 --> 00:06:41,722
그리고 중심에서 더 벗어난 파란색 지역도 볼 수 있습니다.

61
00:06:41,722 --> 00:06:46,789
또한 이 실험에서 뉴런들이 계층구조를 지닌다는 것도 반견했습니다.

62
00:06:47,634 --> 00:06:57,837
다양한 종류의 시각자극을 관찰하면서 시각 신호가 가장 먼저
도달하는 곳이 바로 Retinal ganglion 이라는 것을 발견합니다.

63
00:06:57,837 --> 00:07:01,601
Retinal ganglion cell은 원형으로 생긴 지역입니다.

64
00:07:01,601 --> 00:07:11,146
가장 상위에는 Simple cells이 있는데, 이 세포들은 다양한 
edges의 방향과 빛의 방향에 반응했습니다.

65
00:07:11,146 --> 00:07:15,448
그리고 더 나아가, 그런 Simple Cells 이 
Complex cells과 연결되어 있다는 것을 발견했습니다.

66
00:07:15,448 --> 00:07:19,923
Complex cells는 빛의 방향 뿐만 아니라 움직임에서 반응했습니다.

67
00:07:19,923 --> 00:07:28,984
복잡도가 증가함게 따라, 가령  hypercomplex cells은 
끝 점(end point) 과 같은것에 반응하게 되는 것입니다.

68
00:07:28,984 --> 00:07:34,175
이런 결과로부터  "corner" 나 "blob"에 대한
아이디어를 얻기 시작한 것입니다.

69
00:07:38,143 --> 00:07:52,454
1980의 neocognitron은 Hubel과 Wiesel이 발견한
simple/complex cells의 아이디어를 사용한 최초의 NN입니다.

70
00:07:52,454 --> 00:07:59,038
Fukishima는 simple/complex cells을 교차시켰습니다. 
(SCSCSC..)

71
00:07:59,038 --> 00:08:03,129
Simple cells은 학습가능한 parameters를 가지고 있고

72
00:08:03,129 --> 00:08:12,958
Complex cells은 pooling과 같은 것으로 구현했는데
작은 변화에 Simple cells보다 좀 더 강인합니다.

73
00:08:14,786 --> 00:08:17,159
지금까지는 1980년대 까지의 업적이었습니다.

74
00:08:17,159 --> 00:08:27,743
1998년 Yann LeCun이 최초로 NN을 학습시키기 위해 
Backprob과 gradient-based learning을 적용했고

75
00:08:27,743 --> 00:08:32,063
실제로 그 방법은 문서인식에 아주 잘 동작했습니다.

76
00:08:32,063 --> 00:08:35,339
그리고 구체적으로 그들은 좋은 일을 할 수있었습니다.

77
00:08:35,340 --> 00:08:37,610
우편 번호의 자릿수 인식.

78
00:08:37,610 --> 00:08:41,028
그래서 이것들은 꽤 널리 사용되었습니다.

79
00:08:41,028 --> 00:08:45,082
우편 서비스에서 우편 번호 인식을 위해.

80
00:08:45,082 --> 00:08:48,320
그러나 그 이상으로는 아직 확장 할 수 없었습니다.

81
00:08:48,320 --> 00:08:51,579
더 까다 롭고 복잡한 데이터,

82
00:08:51,579 --> 00:08:53,837
숫자는 여전히 상당히 간단하다.

83
00:08:53,837 --> 00:08:56,350
인식 할 수있는 한정된 세트.

84
00:08:56,350 --> 00:09:00,901
그래서 2012 년 알렉스 크리 제프
스키 (Alex Krizhevsky)

85
00:09:00,901 --> 00:09:04,893
길쌈 신경 네트워크의 현대 화신을 주었다.

86
00:09:04,893 --> 00:09:08,900
그리고 그의 네트워크에서 우리는
구어체로 AlexNet을 호출합니다.

87
00:09:08,900 --> 00:09:11,543
그러나이 네트워크는 실제로 많이 달라 보이지 않았습니다.

88
00:09:11,543 --> 00:09:14,205
컨벌루션 신경망보다

89
00:09:14,205 --> 00:09:16,472
Yann LeCun이 다루고 있었다.

90
00:09:16,472 --> 00:09:18,363
이제 그들은 규모가 조정되었습니다.

91
00:09:18,363 --> 00:09:21,751
더 크고 깊어지며,

92
00:09:21,751 --> 00:09:23,753
가장 중요한 부분은 그들이 지금
할 수 있었다는 것이 었습니다.

93
00:09:23,753 --> 00:09:26,544
많은 양의 데이터를 활용한다.

94
00:09:26,544 --> 00:09:30,711
이제 웹 이미지, ImageNet
데이터 세트에서 사용할 수 있습니다.

95
00:09:32,078 --> 00:09:33,757
뿐만 아니라 활용

96
00:09:33,757 --> 00:09:37,724
GPU의 병렬 컴퓨팅 성능

97
00:09:37,724 --> 00:09:41,033
그리고 나중에 더 자세히 이야기 할 것입니다.

98
00:09:41,033 --> 00:09:43,127
그러나 오늘 빨리 전달하기 때문에, 이제 알다시피,

99
00:09:43,127 --> 00:09:45,434
ConvNets는 모든 곳에서 사용됩니다.

100
00:09:45,434 --> 00:09:49,999
그래서 우리는 초기 분류 결과를 가지고 있습니다.

101
00:09:49,999 --> 00:09:52,294
Alex Krizhevsky의 ImageNet에 있습니다.

102
00:09:52,294 --> 00:09:55,188
이것은 이미지 검색에 대한 훌륭한
작업을 수행 할 수 있습니다.

103
00:09:55,188 --> 00:09:57,274
우리가 꽃을 되 찾으려고 할 때 볼 수 있습니다.

104
00:09:57,274 --> 00:09:59,488
예를 들어, 학습 된 기능

105
00:09:59,488 --> 00:10:04,134
유사성 검색을 수행하는 데 정말 강력합니다.

106
00:10:04,134 --> 00:10:07,049
탐지에 사용되는 ConvNets도 있습니다.

107
00:10:07,049 --> 00:10:10,557
따라서 우리는 현지화 작업을 정말
훌륭하게 수행 할 수 있습니다.

108
00:10:10,557 --> 00:10:14,285
이미지에서, 예를 들어, 버스 또는 보트 인 경우,

109
00:10:14,285 --> 00:10:17,705
등등과 주위에 정확한 경계 상자를 그립니다.

110
00:10:17,705 --> 00:10:21,353
우리는 세분화를 위해 그 이상으로 나아갈 수 있습니다.

111
00:10:21,353 --> 00:10:23,145
맞아, 이제는 더 부유 한 일 이네.

112
00:10:23,145 --> 00:10:26,112
우리는 바운딩 박스를 찾고 있지 않습니다.

113
00:10:26,112 --> 00:10:27,958
하지만 실제로 모든 픽셀에 라벨을 지정하려고합니다.

114
00:10:27,958 --> 00:10:32,125
나무, 사람 등의 윤곽을 그리십시오.

115
00:10:34,126 --> 00:10:36,868
그리고 이런 종류의 알고리즘은,

116
00:10:36,868 --> 00:10:38,864
예를 들어,자가 운전 자동차,

117
00:10:38,864 --> 00:10:42,066
이전에 언급했듯이 많은 기능이 GPU에서 제공됩니다.

118
00:10:42,066 --> 00:10:45,114
병렬 처리를 할 수있는

119
00:10:45,114 --> 00:10:48,812
이러한 ConvNets을 효율적으로
교육하고 실행할 수 있습니다.

120
00:10:48,812 --> 00:10:52,406
그래서 현대적인 강력한 GPU와

121
00:10:52,406 --> 00:10:55,634
임베디드 시스템에서 작동하는, 예를 들어,

122
00:10:55,634 --> 00:10:59,207
당신이 자기 운전 차에서 사용할 것 인.

123
00:10:59,207 --> 00:11:01,695
그래서 우리는 다른 응용 프로그램
중 일부를 볼 수도 있습니다.

124
00:11:01,695 --> 00:11:03,399
ConvNets가 사용됩니다.

125
00:11:03,399 --> 00:11:06,227
그래서, 얼굴 인식, 맞아요, 우리는
입력 이미지를 넣을 수 있습니다.

126
00:11:06,227 --> 00:11:10,394
이 사람이 누구인지 확률을내어

127
00:11:12,626 --> 00:11:15,622
ConvNets는 비디오에 적용되므로이 예제가 있습니다.

128
00:11:15,622 --> 00:11:19,551
두 이미지를 보는 비디오 네트워크

129
00:11:19,551 --> 00:11:21,902
뿐만 아니라 시간 정보,

130
00:11:21,902 --> 00:11:25,951
거기에서 동영상을 분류 할 수 있습니다.

131
00:11:25,951 --> 00:11:28,569
우리는 또한 포즈 인식을 할 수 있습니다.

132
00:11:28,569 --> 00:11:30,215
인식 할 수있는 것, 당신도 알다시피,

133
00:11:30,215 --> 00:11:32,770
어깨, 팔꿈치 및 다른 관절.

134
00:11:32,770 --> 00:11:37,577
그리고 여기에 우리의 멋진 TA, Lane,

135
00:11:37,577 --> 00:11:42,234
다양한 종류의 예쁜 비표준 인간의 포즈.

136
00:11:42,234 --> 00:11:45,791
하지만 ConvNets는 꽤 좋은 일을 할 수 있습니다.

137
00:11:45,791 --> 00:11:48,465
요즘 포즈 인식의.

138
00:11:48,465 --> 00:11:51,741
게임 재생에도 사용됩니다.

139
00:11:51,741 --> 00:11:54,296
따라서 보강 학습의 일부 작업,

140
00:11:54,296 --> 00:11:56,509
당신이 보았을지도 모른다는 더 깊은 집행 학습,

141
00:11:56,509 --> 00:11:58,595
Atari 게임하기, Go하기 등등.

142
00:11:58,595 --> 00:12:02,981
ConvNets는이 모든 것의 중요한 부분입니다.

143
00:12:02,981 --> 00:12:06,656
일부 다른 응용 프로그램, 그래서 그들이 사용되는

144
00:12:06,656 --> 00:12:10,150
의료 영상의 해석 및 진단,

145
00:12:10,150 --> 00:12:14,317
은하 분류를 위해 거리 표지판 인식을 위해

146
00:12:18,059 --> 00:12:19,519
고래 인식,

147
00:12:19,519 --> 00:12:22,342
이것은 최근의 Kaggle Challenge에서 온 것입니다.

148
00:12:22,342 --> 00:12:26,067
항공지도를 보는 사례도 있습니다.

149
00:12:26,067 --> 00:12:28,485
거리가 어디에 있는지 알 수 있습니다.

150
00:12:28,485 --> 00:12:29,999
이지도에는 건물이 어디에 있는지,

151
00:12:29,999 --> 00:12:33,249
이들 모두를 분할 할 수 있습니다.

152
00:12:35,089 --> 00:12:39,170
그리고 분류 탐지의 인식 넘어,

153
00:12:39,170 --> 00:12:41,587
이러한 유형의 작업에는 작업이 있습니다.

154
00:12:41,587 --> 00:12:44,472
이미지 캡션처럼, 주어진 이미지,

155
00:12:44,472 --> 00:12:46,363
우리는 문장 설명을 쓰고 싶다.

156
00:12:46,363 --> 00:12:48,644
이미지에 무엇이 있는지.

157
00:12:48,644 --> 00:12:49,970
그래서 이것은 우리가 들어가는 것입니다.

158
00:12:49,970 --> 00:12:52,819
나중에 수업에서 조금.

159
00:12:52,819 --> 00:12:57,169
그리고 우리는 또한, 정말로, 정말 멋지고 시원합니다.

160
00:12:57,169 --> 00:13:01,251
우리가 신경 네트워크를 사용하여 할 수있는 작품의 종류.

161
00:13:01,251 --> 00:13:03,855
그리고 왼쪽에는 깊은 꿈의 예가 있습니다.

162
00:13:03,855 --> 00:13:08,022
우리가 이미지와 환각을 가질 수있는 곳

163
00:13:09,173 --> 00:13:12,412
다른 종류의 개체 및 이미지 개념.

164
00:13:12,412 --> 00:13:16,274
우리가 이미지를 찍는 신경 스타일 유형의 작업도 있습니다.

165
00:13:16,274 --> 00:13:19,817
이 이미지를 다시 렌더링 할 수 있습니다.

166
00:13:19,817 --> 00:13:23,808
특정 아티스트와 아트 워크 스타일을 사용합니다.

167
00:13:23,808 --> 00:13:27,899
그래서 여기서 우리는 예를 들어 반 고흐 (Van
Gogh)를 오른쪽으로 가져갈 수 있습니다.

168
00:13:27,899 --> 00:13:30,909
별의 밤, 그리고 그것을 사용하여 다시 그리기

169
00:13:30,909 --> 00:13:33,370
그 스타일을 사용하는 우리의 원래 이미지.

170
00:13:33,370 --> 00:13:36,473
그리고 저스틴은 이것에 대해 많은 연구를 해왔습니다.

171
00:13:36,473 --> 00:13:38,239
그래서 너희들이 관심이 있다면,

172
00:13:38,239 --> 00:13:42,163
이것들은 그의 코드에 의해 생성 된 이미지들입니다.

173
00:13:42,163 --> 00:13:46,244
그리고 너희들은 그것에 대해 더 많이 이야기해야한다.

174
00:13:46,244 --> 00:13:50,069
좋아, 기본적으로, 알다시피, 이것은
단지 작은 샘플 일 뿐이다.

175
00:13:50,069 --> 00:13:52,727
오늘 ConvNets가 사용되고있는 곳.

176
00:13:52,727 --> 00:13:55,289
그러나 실제로 할 수있는 엄청난 양이 있습니다.

177
00:13:55,289 --> 00:13:58,378
맞아요. 그리고 당신도 알다시피,
너희들 프로젝트를 위해서,

178
00:13:58,378 --> 00:14:00,624
일종의, 당신도 알다시피, 당신의
상상력을 야생하게하십시오.

179
00:14:00,624 --> 00:14:04,605
우리는 어떤 종류의 응용 프로그램이

180
00:14:04,605 --> 00:14:06,465
너와 함께 갈 수있어.

181
00:14:06,465 --> 00:14:08,031
그래서 오늘 우리는

182
00:14:08,031 --> 00:14:10,307
길쌈 신경 네트워크가 작동하는 방식.

183
00:14:10,307 --> 00:14:13,233
그리고 신경망과 마찬가지로 우리는 첫 번째로

184
00:14:13,233 --> 00:14:16,904
기능적 관점에서 어떻게 작동하는지 이야기하십시오.

185
00:14:16,904 --> 00:14:18,668
뇌 유추없이

186
00:14:18,668 --> 00:14:22,835
그리고 우리는이 연결들에 대해 간단히 이야기 할 것입니다.

187
00:14:25,453 --> 00:14:28,361
그래, 마지막 강의, 우리가 얘기 했어.

188
00:14:28,361 --> 00:14:31,444
완벽하게 연결된 레이어의 아이디어.

189
00:14:32,878 --> 00:14:36,257
그리고 완전히 연결된 레이어의 경우

190
00:14:36,257 --> 00:14:39,373
우리가하고있는 일은이 벡터들 위에 작동하는 것입니다.

191
00:14:39,373 --> 00:14:43,218
맞습니다. 그래서 우리가 알다시피,
이미지가 있다고 가정 해 봅시다.

192
00:14:43,218 --> 00:14:45,726
3D 이미지, 32 by 32 by three,

193
00:14:45,726 --> 00:14:48,443
그래서 우리가 이전에 보았던 일부 이미지들.

194
00:14:48,443 --> 00:14:51,548
우리는 그것을 취할 것이고, 우리는
모든 픽셀을 오른쪽으로 늘릴 것이고,

195
00:14:51,548 --> 00:14:55,196
그리고 나서 우리는이 3072
차원의 벡터를 가지고 있습니다,

196
00:14:55,196 --> 00:14:56,787
예를 들어이 경우.

197
00:14:56,787 --> 00:14:58,944
그리고 나서 우리는이 가중치를 가지고 있습니다. 맞습니다.

198
00:14:58,944 --> 00:15:01,741
우리는 이것을 가중치 행렬로 곱하려고합니다.

199
00:15:01,741 --> 00:15:05,908
여기 예를 들어 W가 3072
년에 10이라고 말하려고합니다.

200
00:15:07,264 --> 00:15:10,755
그리고 나서 우리는 활성화를 얻을 것입니다.

201
00:15:10,755 --> 00:15:13,943
이 레이어의 출력은 오른쪽입니다.이 경우,

202
00:15:13,943 --> 00:15:18,056
우리는 각각 10 개의 행을 취하고이 점을 계산합니다

203
00:15:18,056 --> 00:15:20,389
3072 차원 입력.

204
00:15:22,207 --> 00:15:24,835
그리고 거기에서 우리는 하나의 번호를 얻습니다.

205
00:15:24,835 --> 00:15:27,892
그것은 그 뉴런의 가치입니다.

206
00:15:27,892 --> 00:15:30,020
그래서이 경우에 우리는

207
00:15:30,020 --> 00:15:32,270
10 개의 뉴런 출력.

208
00:15:35,417 --> 00:15:38,355
그래서 컨볼 루션 레이어이므로 주된 차이점이 있습니다.

209
00:15:38,355 --> 00:15:39,988
이 부분과 완전히 연결된 층 사이

210
00:15:39,988 --> 00:15:41,203
우리가 얘기하고 있었던

211
00:15:41,203 --> 00:15:44,165
여기서 우리는 공간 구조를 보존하고자합니다.

212
00:15:44,165 --> 00:15:47,090
32 x 32 픽셀을 3 개의 이미지로 가져옵니다.

213
00:15:47,090 --> 00:15:49,838
우리가 이전에 가지고 있던 것,이 모든 것을 펴는 대신에

214
00:15:49,838 --> 00:15:53,186
하나의 긴 벡터로, 우리는 이제 구조를 유지하려고합니다.

215
00:15:53,186 --> 00:15:57,750
이 이미지의 오른쪽 3 차원 입력

216
00:15:57,750 --> 00:15:59,526
그러면 우리가 할 일은

217
00:15:59,526 --> 00:16:01,910
우리의 무게는이 작은 필터 일 것입니다.

218
00:16:01,910 --> 00:16:05,746
그래서이 경우 예를 들어, 5 x 5 x 3 필터,

219
00:16:05,746 --> 00:16:07,212
우리는이 필터를 사용하려고합니다.

220
00:16:07,212 --> 00:16:09,679
우리는 그것을 이미지 위에
공간적으로 슬라이드시킬 것입니다.

221
00:16:09,679 --> 00:16:13,153
모든 공간 위치에서 내적을 계산할 수 있습니다.

222
00:16:13,153 --> 00:16:17,320
그래서 우리는 이것이 어떻게 작동하는지에
대해 자세하게 설명 할 것입니다.

223
00:16:18,668 --> 00:16:20,523
그래서 우리 필터는 우선,

224
00:16:20,523 --> 00:16:23,957
항상 입력 음량의 전체 깊이를 확장하십시오.

225
00:16:23,957 --> 00:16:28,759
그래서 그들은 더 작은 공간 영역이 될 것입니다.

226
00:16:28,759 --> 00:16:30,357
그래서이 경우에는 다섯 씩, 오른쪽으로,

227
00:16:30,357 --> 00:16:33,425
우리의 전체 32 x 32 공간 입력 대신,

228
00:16:33,425 --> 00:16:37,536
그러나 그들은 항상 깊이있게, 오른쪽으로,

229
00:16:37,536 --> 00:16:42,499
그래서 여기서 우리는 5를 5만큼 취하려고합니다.

230
00:16:42,499 --> 00:16:44,619
그리고이 필터를 사용하려고합니다.

231
00:16:44,619 --> 00:16:46,996
주어진 공간 위치에서

232
00:16:46,996 --> 00:16:49,046
우리는 내적 제품을 할 것입니다.

233
00:16:49,046 --> 00:16:52,901
이 필터와 이미지의 덩어리 사이.

234
00:16:52,901 --> 00:16:54,492
따라서이 필터를 오버레이 할 것입니다.

235
00:16:54,492 --> 00:16:56,998
이미지 내의 공간 위치의 상부에,

236
00:16:56,998 --> 00:16:58,636
맞아, 그리고 내적 제품을해라.

237
00:16:58,636 --> 00:17:02,665
그 필터의 각 요소의 곱셈

238
00:17:02,665 --> 00:17:05,203
해당 공간 위치의 각 해당 요소

239
00:17:05,203 --> 00:17:07,099
우리는 방금 그것을 위에 pl다.

240
00:17:07,099 --> 00:17:09,732
그리고 나서 이것이 우리에게 내적을주는 것입니다.

241
00:17:09,733 --> 00:17:14,345
그래서이 경우 우리는 다섯 번 다섯 번,

242
00:17:14,345 --> 00:17:16,257
이것은 곱셈의 수입니다.

243
00:17:16,257 --> 00:17:18,755
우리가 할 일인가, 바이어스 용어를 더한 것.

244
00:17:18,755 --> 00:17:22,324
그리고 이것은 기본적으로 필터 W를 사용합니다.

245
00:17:22,324 --> 00:17:26,491
기본적으로 W는 X와 플러스 바이어스를 바꿉니다.

246
00:17:27,722 --> 00:17:30,299
어떻게 작동하는지 명확하게 알 수 있습니까?

247
00:17:30,299 --> 00:17:31,771
그래, 질문.

248
00:17:31,771 --> 00:17:34,521
[희미한 말]

249
00:17:35,656 --> 00:17:37,837
그래, 문제는 우리가 내적 제품을 할 때이다.

250
00:17:37,837 --> 00:17:40,722
우리는 5를 3으로 하나씩 벡터로 바꿉니 까?

251
00:17:40,722 --> 00:17:42,907
네, 본질적으로 그것이 당신이하는 일입니다.

252
00:17:42,907 --> 00:17:44,950
너는 그걸 그냥 생각할 수 있잖아.

253
00:17:44,950 --> 00:17:47,996
그것을 뿌리고 원소와 같은 곱셈을하는 것

254
00:17:47,996 --> 00:17:50,523
각 위치에서,하지만 이것은
당신에게 같은 결과를 줄 것입니다

255
00:17:50,523 --> 00:17:53,691
마치 그 지점에서 필터를 펼쳤던 것처럼,

256
00:17:53,691 --> 00:17:56,211
놓여진 입력 음량을 늘려서

257
00:17:56,211 --> 00:17:57,891
그리고 나서 내적을 가져 갔고,

258
00:17:57,891 --> 00:18:01,111
그리고 그것이 여기에 쓰여지는 것입니다, 예, 질문입니다.

259
00:18:01,111 --> 00:18:03,867
[희미한 말]

260
00:18:03,867 --> 00:18:05,305
오,이게 문제 야.

261
00:18:05,305 --> 00:18:07,997
이것이 왜 W 전조인가에 대한 어떤 직감인가?

262
00:18:07,997 --> 00:18:10,476
그리고 이것은 단지 사실이 아니 었습니다.

263
00:18:10,476 --> 00:18:12,329
이것은 우리가 여기있는 표기법입니다.

264
00:18:12,329 --> 00:18:15,978
수학을 일점으로 삼아야합니다.

265
00:18:15,978 --> 00:18:19,045
그래서 그것은 단지 당신이 W를 어떻게 대표하고 있는지,

266
00:18:19,045 --> 00:18:23,974
이 경우 우리가 W 매트릭스를 본다면

267
00:18:23,974 --> 00:18:26,781
이것은 각 열이되고 그래서 우리는 단지

268
00:18:26,781 --> 00:18:29,593
행열을 바꾸기 위해 전치.

269
00:18:29,593 --> 00:18:31,989
그러나 여기에는 직관이 없습니다.

270
00:18:31,989 --> 00:18:34,098
우리는 단지 W의 필터를 사용하고 있습니다.

271
00:18:34,098 --> 00:18:37,679
우리는 이것을 하나의 D 벡터로 뻗어서,

272
00:18:37,679 --> 00:18:39,067
그것이 내적 (dot product)이되기 위해서

273
00:18:39,067 --> 00:18:42,862
그것은 하나의 N 벡터에 의한 것이어야합니다.

274
00:18:42,862 --> 00:18:45,612
[희미한 말]

275
00:18:48,263 --> 00:18:49,829
그래, 문제는

276
00:18:49,829 --> 00:18:53,996
여기서 W는 5로 5가 아니라 3으로, 75로 하나입니다.

277
00:18:55,180 --> 00:18:57,307
그렇다면, 우리가 가면

278
00:18:57,307 --> 00:18:59,882
W의 전치 곱하기 X를 곱하면,

279
00:18:59,882 --> 00:19:01,120
우리는 먼저 스트레칭해야합니다.

280
00:19:01,120 --> 00:19:02,550
우리가 점 제품을하기 전에.

281
00:19:02,550 --> 00:19:05,312
그래서 우리는 다섯을 다섯 씩 취합니다.

282
00:19:05,312 --> 00:19:06,462
우리는이 모든 값들을 취합니다.

283
00:19:06,462 --> 00:19:09,629
그것을 긴 벡터로 뻗어 라.

284
00:19:10,913 --> 00:19:14,992
그리고 다시, 다른 질문과 유사하게,

285
00:19:14,992 --> 00:19:16,706
우리가 여기서하고있는 실제 작업

286
00:19:16,706 --> 00:19:18,691
우리의 필터가

287
00:19:18,691 --> 00:19:20,568
이미지의 공간적인 위치

288
00:19:20,568 --> 00:19:23,375
및 상기 대응하는 값들 모두를 함께 곱하는 단계로서,

289
00:19:23,375 --> 00:19:25,906
그러나 그것은 단지 일종의 쉬운
표현으로 만들기 위해서입니다.

290
00:19:25,906 --> 00:19:27,527
전에 보았던 것과 비슷하다.

291
00:19:27,527 --> 00:19:29,702
우리는 이것들을 각각 스트레칭 할 수 있습니다.

292
00:19:29,702 --> 00:19:32,707
치수가 올바르게 배치되었는지 확인하십시오.

293
00:19:32,707 --> 00:19:35,061
그래서 그것은 내적으로 작동합니다.

294
00:19:35,061 --> 00:19:36,311
그래, 질문.

295
00:19:37,232 --> 00:19:40,740
[희미한 말]

296
00:19:40,740 --> 00:19:41,698
좋아, 문제는,

297
00:19:41,698 --> 00:19:43,797
이미지 위에 필터를 어떻게 밀어 넣을 까?

298
00:19:43,797 --> 00:19:46,760
우리는 그 다음으로 들어갈 것입니다, 예.

299
00:19:46,760 --> 00:19:49,510
[희미한 말]

300
00:19:52,071 --> 00:19:55,068
자, 문제는 커널을 회전시켜야한다는 것입니다.

301
00:19:55,068 --> 00:19:58,111
회선에 더 잘 부합하기 위해 180도 각도로 회전합니다.

302
00:19:58,111 --> 00:20:00,178
회선의 정의.

303
00:20:00,178 --> 00:20:03,172
그래서 답은 우리가 방정식을 보여줄 것이라는 것입니다.

304
00:20:03,172 --> 00:20:05,870
나중에 이것을 위해, 그러나 우리는 회선을 사용하고있다.

305
00:20:05,870 --> 00:20:09,451
무슨 일이 일어나고 있는지에 대한 느슨한 정의의 일종.

306
00:20:09,451 --> 00:20:11,171
그래서 신호 처리에서 사람들을 위해,

307
00:20:11,171 --> 00:20:13,101
우리가 실제로 기술적으로하고있는 것,

308
00:20:13,101 --> 00:20:14,925
이것을 회선이라고 부르고 싶다면,

309
00:20:14,925 --> 00:20:18,738
우리는 뒤집힌 버전의 필터로
convolving하고 있습니다.

310
00:20:18,738 --> 00:20:21,947
하지만 대부분의 경우이 문제에 대해 걱정하지 않아도됩니다.

311
00:20:21,947 --> 00:20:24,689
우린 그냥,이 수술을 해.

312
00:20:24,689 --> 00:20:27,983
그것은 영혼 속의 회상과 같습니다.

313
00:20:27,983 --> 00:20:28,900
좋아, 그럼...

314
00:20:31,890 --> 00:20:35,077
알았어, 우리가 더 일찍 질문을
했어, 어떻게 우리가 알지?

315
00:20:35,077 --> 00:20:37,246
이 공간을 모든 공간 위치로 밀어 넣으십시오.

316
00:20:37,246 --> 00:20:38,526
맞아, 우리가 할 일은

317
00:20:38,526 --> 00:20:41,826
우리는이 필터를 사용하려고합니다. 우리는 시작할 것입니다.

318
00:20:41,826 --> 00:20:45,237
왼쪽 상단 모서리와 기본적으로 중심에

319
00:20:45,237 --> 00:20:49,975
이 입력 볼륨의 모든 픽셀 위에 필터가 있습니다.

320
00:20:49,975 --> 00:20:53,654
그리고 모든 직책에서 우리는이 내적 제품을 할 것입니다.

321
00:20:53,654 --> 00:20:55,949
그러면 하나의 값이 생성됩니다.

322
00:20:55,949 --> 00:20:57,511
우리의 출력 활성화 맵에서.

323
00:20:57,511 --> 00:21:00,927
그리고 나서 우리는 이것을
주변으로 미끄러지게 할 것입니다.

324
00:21:00,927 --> 00:21:02,844
가장 간단한 버전은 모든 픽셀에 있습니다.

325
00:21:02,844 --> 00:21:05,359
우리는이 작업을 수행하고 채울 것입니다.

326
00:21:05,359 --> 00:21:09,442
출력 활성화의 해당 지점.

327
00:21:10,352 --> 00:21:14,166
치수가 정확하지 않다는 것을 알 수 있습니다.

328
00:21:14,166 --> 00:21:15,532
네가이 일을한다면, 어떻게 될지, 맞아.

329
00:21:15,532 --> 00:21:17,748
나는 32에서 32의 입력을 받았다.

330
00:21:17,748 --> 00:21:20,126
그리고 나는 28의 출력을 가지고있다.

331
00:21:20,126 --> 00:21:22,920
그래서 우리는 나중에 수학에 대해 설명 할 것입니다.

332
00:21:22,920 --> 00:21:26,364
이 방법이 차원 적으로 어떻게
작동하는지 정확히 알 수 있습니다.

333
00:21:26,364 --> 00:21:29,767
하지만 기본적으로 선택의 여지가 있습니다.

334
00:21:29,767 --> 00:21:31,393
어떻게 이걸 미끄러 뜨릴 지,

335
00:21:31,393 --> 00:21:35,129
당신이 모든 픽셀 또는 슬라이드 여부에 상관없이,

336
00:21:35,129 --> 00:21:39,437
한 번에 두 개의 입력 값이 넘어서고,

337
00:21:39,437 --> 00:21:41,326
한 번에 2 픽셀 씩,

338
00:21:41,326 --> 00:21:42,958
그래서 다른 크기의 출력을 얻을 수 있습니다.

339
00:21:42,958 --> 00:21:44,823
어떻게 슬라이드를 선택 하느냐에 따라 다릅니다.

340
00:21:44,823 --> 00:21:48,990
그러나 기본적으로 그리드 방식으로이
작업을 수행하고 있습니다.

341
00:21:50,180 --> 00:21:52,623
그래, 우리가 이전에 보았던 것,

342
00:21:52,623 --> 00:21:55,792
하나의 필터를 가져 와서

343
00:21:55,792 --> 00:21:58,141
이미지의 모든 공간 위치

344
00:21:58,141 --> 00:22:00,620
그리고 나서 우리는이 활성화
맵을 꺼낼 것입니다, 맞습니다,

345
00:22:00,620 --> 00:22:04,731
이는 모든 공간 위치에서 해당 필터의 값입니다.

346
00:22:04,731 --> 00:22:07,669
그래서 컨볼 루션 레이어를 처리 할 때,

347
00:22:07,669 --> 00:22:09,778
우리는 여러 개의 필터로 작업하고 싶습니다.

348
00:22:09,778 --> 00:22:12,858
각 필터는 특정 유형을 찾고 있기 때문에

349
00:22:12,858 --> 00:22:16,250
템플릿 또는 입력 개념의 개념 유형.

350
00:22:16,250 --> 00:22:20,479
그래서 우리는 여러 개의 필터 세트를 갖게 될 것입니다.

351
00:22:20,479 --> 00:22:22,623
여기에서는 두 번째 필터를 사용하려고합니다.

352
00:22:22,623 --> 00:22:26,359
이 녹색 필터는 다시 5 x 5이며,

353
00:22:26,359 --> 00:22:30,059
모든 공간적 위치에 대해 이것을 슬라이드 할 것입니다.

354
00:22:30,059 --> 00:22:33,258
내 입력 음량으로 누른 다음 나갈거야.

355
00:22:33,258 --> 00:22:37,425
동일한 크기의이 두 번째 녹색 활성화 맵.

356
00:22:40,081 --> 00:22:41,628
그리고 우리는 많은 필터를 위해 이것을 할 수 있습니다.

357
00:22:41,628 --> 00:22:43,553
우리가이 계층에 갖고 싶어하는 것처럼.

358
00:22:43,553 --> 00:22:45,817
예를 들어 여섯 개의 필터가 있다면,

359
00:22:45,817 --> 00:22:47,871
이 5 개 중 5 개 필터 중 6 개,

360
00:22:47,871 --> 00:22:51,698
그러면 총 6 개의 활성화 맵을 얻을 것입니다.

361
00:22:51,698 --> 00:22:54,618
모두의, 그래서 우리는이 산출 양을 얻을 것이다

362
00:22:54,618 --> 00:22:58,368
기본적으로 28에 의해 28로 6이 될 것입니다.

363
00:23:01,607 --> 00:23:03,609
우리가 어떻게 사용할지 미리보기

364
00:23:03,609 --> 00:23:06,689
우리의 컨볼 루션 네트워크에서 이러한 길쌈 레이어

365
00:23:06,689 --> 00:23:08,644
ConvNet이 기본적으로 될 것입니다.

366
00:23:08,644 --> 00:23:11,152
이 길쌈 층의 순서

367
00:23:11,152 --> 00:23:13,769
우리가 가진 것과 같은 방식으로 서로 겹쳐서

368
00:23:13,769 --> 00:23:16,676
신경 네트워크에서 단순한 선형 레이어를 사용합니다.

369
00:23:16,676 --> 00:23:18,403
그리고 나서 우리는 이들을 산재 시키려고합니다.

370
00:23:18,403 --> 00:23:19,474
활성화 기능,

371
00:23:19,474 --> 00:23:23,057
예를 들어, ReLU 활성화 기능.

372
00:23:24,503 --> 00:23:28,670
맞아, 당신은 Conv, ReLU,

373
00:23:29,535 --> 00:23:31,257
그리고 보통 또한 어떤 풀링 레이어,

374
00:23:31,257 --> 00:23:33,975
그런 다음이 시퀀스를 가져올 것입니다.

375
00:23:33,975 --> 00:23:36,965
각각은 지금 될 출력을 생성합니다.

376
00:23:36,965 --> 00:23:40,465
다음 컨볼 루션 계층으로의 입력

377
00:23:43,638 --> 00:23:46,552
좋아요, 그래서 이전에 말했듯이이 레이어들 각각은,

378
00:23:46,552 --> 00:23:49,305
여러 개의 필터, 오른쪽, 여러 개의 필터가 있습니다.

379
00:23:49,305 --> 00:23:52,957
그리고 각 필터는 활성화 맵을 생성합니다.

380
00:23:52,957 --> 00:23:55,633
그래서 여러 레이어를 볼 때

381
00:23:55,633 --> 00:23:58,141
ConvNet에서 함께 쌓아 올리면
무슨 일이 일어나게 될까요?

382
00:23:58,141 --> 00:24:01,175
당신은이 필터의 계층화를 배우게되고,

383
00:24:01,175 --> 00:24:04,421
이전 계층의 필터는 대개

384
00:24:04,421 --> 00:24:06,318
당신이 찾고있는 저수준 기능.

385
00:24:06,318 --> 00:24:09,257
그래서 사물은 가장자리처럼 보입니다. 맞습니다.

386
00:24:09,257 --> 00:24:10,272
그리고 중간 수준에서,

387
00:24:10,272 --> 00:24:14,128
당신은 더 복잡한 종류의 특징들을 얻을 것입니다,

388
00:24:14,128 --> 00:24:16,478
그래서 어쩌면 그것은 물건을 더 찾고 있습니다.

389
00:24:16,478 --> 00:24:19,113
모서리와 얼룩 등등.

390
00:24:19,113 --> 00:24:20,602
그리고 더 높은 수준의 기능에서,

391
00:24:20,602 --> 00:24:22,823
당신은 시작하고있는 것들을 얻을 것입니다.

392
00:24:22,823 --> 00:24:25,852
얼룩보다 개념에 더 닮았습니다.

393
00:24:25,852 --> 00:24:27,905
그리고 우리는 나중에 수업에서 자세히 설명 할 것입니다.

394
00:24:27,905 --> 00:24:30,522
이러한 모든 기능을 실제로 시각화 할 수있는 방법

395
00:24:30,522 --> 00:24:33,165
그리고 당신의 네트워크,

396
00:24:33,165 --> 00:24:35,561
네트워크에서 어떤 종류의 기능을 배우고 있는지.

397
00:24:35,561 --> 00:24:38,974
그러나 지금 중요한 것은 이해하는 것입니다.

398
00:24:38,974 --> 00:24:40,378
이 특징들이 결국

399
00:24:40,378 --> 00:24:42,800
당신이이 모든 것을 가지고있을 때,

400
00:24:42,800 --> 00:24:46,967
이러한 유형의 단순함에서 더 복잡한 기능입니다.

401
00:24:48,305 --> 00:24:49,138
[희미한 말]

402
00:24:49,138 --> 00:24:49,971
네.

403
00:24:50,984 --> 00:24:51,817
오, 알았어.

404
00:24:59,067 --> 00:25:01,124
오, 알았어, 문제는 직감이란 무엇인가.

405
00:25:01,124 --> 00:25:03,113
매번 깊이를 늘리십시오.

406
00:25:03,113 --> 00:25:06,384
그래서 여기에 원래 레이어에 3 개의 필터가 있습니다.

407
00:25:06,384 --> 00:25:08,814
다음 레이어에서 여섯 개의 필터를 사용합니다.

408
00:25:08,814 --> 00:25:12,651
맞아요, 그래서 이것은 주로 디자인 선택입니다.

409
00:25:12,651 --> 00:25:14,274
아시다시피, 실제로 사람들은

410
00:25:14,274 --> 00:25:17,255
특정 유형의 구성이 더 잘 작동합니다.

411
00:25:17,255 --> 00:25:19,894
그리고 나중에 우리는 다른 유형의
사례 연구를 진행할 것입니다.

412
00:25:19,894 --> 00:25:23,185
컨벌루션 신경망 아키텍처의 종류

413
00:25:23,185 --> 00:25:25,658
디자인 선택

414
00:25:25,658 --> 00:25:28,344
그리고 왜 어떤 것들은 다른 것들보다 잘 작동하는지.

415
00:25:28,344 --> 00:25:30,516
하지만 그래, 기본적으로 선택,

416
00:25:30,516 --> 00:25:31,876
당신은 많은 디자인 선택을 할 것입니다.

417
00:25:31,876 --> 00:25:33,238
컨벌루션 뉴럴 네트워크에서,

418
00:25:33,238 --> 00:25:34,948
필터의 크기, 보폭,

419
00:25:34,948 --> 00:25:36,369
얼마나 많은 필터가 있는지,

420
00:25:36,369 --> 00:25:39,611
그래서 나중에 더 자세히 얘기하겠습니다.

421
00:25:39,611 --> 00:25:41,246
문제.

422
00:25:41,246 --> 00:25:43,996
[희미한 말]

423
00:25:50,300 --> 00:25:53,691
그래, 문제는 우리가이 필터를 슬라이딩 할 때

424
00:25:53,691 --> 00:25:56,364
이미지를 공간적으로 샘플링하는 것처럼 보입니다.

425
00:25:56,364 --> 00:26:00,177
가장자리와 모서리는 다른 위치보다 작습니다.

426
00:26:00,177 --> 00:26:01,676
그래, 그게 정말 좋은 지적이야.

427
00:26:01,676 --> 00:26:04,483
우리는 몇 가지 슬라이드에서 생각할 것입니다.

428
00:26:04,483 --> 00:26:07,900
우리가 그것을 어떻게 시도하고 보상하는지에 관해서.

429
00:26:12,009 --> 00:26:15,592
좋아, 그래서 각각의 길쌈 레이어

430
00:26:16,870 --> 00:26:20,797
우리가 함께 겹쳐서, 우리는 우리가
어떻게 시작하는지 보았습니다.

431
00:26:20,797 --> 00:26:23,877
보다 단순한 기능으로 이들을 통합

432
00:26:23,877 --> 00:26:26,228
나중에 더 복잡한 기능으로

433
00:26:26,228 --> 00:26:28,343
그래서 실제로 이것은 호환됩니다.

434
00:26:28,343 --> 00:26:32,549
Hubel과 Wiesel이 그들의 실험에서 발견 한 것과,

435
00:26:32,549 --> 00:26:35,895
맞아, 우리가이 단순한 세포들을 가졌어.

436
00:26:35,895 --> 00:26:37,406
처리의 초기 단계에서,

437
00:26:37,406 --> 00:26:39,532
나중에 더 복잡한 세포가 뒤 따른다.

438
00:26:39,532 --> 00:26:42,865
우리가 명시 적으로하지는 않았지만

439
00:26:44,067 --> 00:26:46,455
이러한 종류의 기능을 배우기 위해
ConvNet에 강요하십시오.

440
00:26:46,455 --> 00:26:48,295
실제로 이런 종류의

441
00:26:48,295 --> 00:26:51,623
계층 적 구조를 만들고 역 전파를
사용하여 그것을 훈련 시키며,

442
00:26:51,623 --> 00:26:55,041
이것들은 결국 배우게되는 종류의 필터입니다.

443
00:26:55,041 --> 00:26:57,791
[희미한 말]

444
00:27:05,555 --> 00:27:07,116
그래, 그래, 문제는,

445
00:27:07,116 --> 00:27:10,979
우리는 이러한 시각화에서 무엇을 보게됩니다.

446
00:27:10,979 --> 00:27:13,321
그리고 이렇게,이 시각화에서, 좋아,

447
00:27:13,321 --> 00:27:17,134
첫 Convolutional 레이어
인 Conv1을 살펴보면,

448
00:27:17,134 --> 00:27:20,975
각각의 그리드는이 그리드의 각 부분이 하나의 뉴런입니다.

449
00:27:20,975 --> 00:27:23,118
여기에 우리가 시각화 한 것

450
00:27:23,118 --> 00:27:26,701
입력이 최대화되는 모습입니다.

451
00:27:27,893 --> 00:27:29,956
그 특정한 뉴런의 활성화.

452
00:27:29,956 --> 00:27:31,826
어떤 종류의 이미지를 얻을 수 있습니까?

453
00:27:31,826 --> 00:27:34,070
그건 당신에게 가장 큰 가치를 줄 것입니다.

454
00:27:34,070 --> 00:27:36,594
그 뉴런을 발사하고 가장 큰 가치를 지니게하십시오.

455
00:27:36,594 --> 00:27:38,811
그래서 우리가하는 일은 기본적으로

456
00:27:38,811 --> 00:27:42,978
특정 뉴런 활성화로부터 역 전파를함으로써

457
00:27:44,415 --> 00:27:46,570
입력에서 무엇이 트리거되는지를보고,

458
00:27:46,570 --> 00:27:48,848
당신에게이 뉴런의 가장 높은 가치를 줄 것입니다.

459
00:27:48,848 --> 00:27:50,730
그리고 이것은 우리가 이야기 할 내용입니다.

460
00:27:50,730 --> 00:27:53,276
추후 강연에서 훨씬 더 깊이있게

461
00:27:53,276 --> 00:27:56,280
우리가 이러한 모든 시각화를 만드는 방법에 대해

462
00:27:56,280 --> 00:27:59,124
그러나 기본적으로이 그리드의 각 요소

463
00:27:59,124 --> 00:28:03,342
입력 내용이 어떻게 보이는지 보여줍니다.

464
00:28:03,342 --> 00:28:06,775
기본적으로 뉴런의 활성화를 극대화합니다.

465
00:28:06,775 --> 00:28:10,608
그래서 의미에서, 뉴런은 무엇을 찾고 있습니까?

466
00:28:13,537 --> 00:28:18,490
좋아, 여기 활성화 맵의 예제가있다.

467
00:28:18,490 --> 00:28:19,835
각 필터에 의해 생성 된 것입니다.

468
00:28:19,835 --> 00:28:22,200
그래서 우리는 위에서 위로 볼 수 있습니다.

469
00:28:22,200 --> 00:28:26,025
우리는 예제 5의 전체 행을 5
개의 필터로 가지고 있습니다.

470
00:28:26,025 --> 00:28:30,407
따라서 이것은 기본적으로 훈련 된
ConvNet의 실제 사례입니다

471
00:28:30,407 --> 00:28:34,490
이들 각각은 5 × 5 필터

472
00:28:35,593 --> 00:28:38,511
보이는 것처럼, 그리고 우리가 이미지
위에 이것을 convolve 할 때,

473
00:28:38,511 --> 00:28:41,197
그래서이 경우 나는 이것이 자동차
구석과 같다고 생각합니다.

474
00:28:41,197 --> 00:28:44,346
차 빛, 활성화가 어떻게 생겼는지.

475
00:28:44,346 --> 00:28:46,799
맞아, 여기 예를 들면,

476
00:28:46,799 --> 00:28:49,449
이 첫 번째 필터를 보면이 빨간색 필터,

477
00:28:49,449 --> 00:28:51,330
필터 주위에 빨간색 상자와 같은 필터,

478
00:28:51,330 --> 00:28:53,412
우리는 그것이 찾고있는 것을 볼 것입니다,

479
00:28:53,412 --> 00:28:56,432
템플릿은 가장자리, 오른쪽, 지향 가장자리처럼 보입니다.

480
00:28:56,432 --> 00:28:58,050
그리고 이미지 위로 슬라이드하면,

481
00:28:58,050 --> 00:29:01,812
그것은 높은 가치, 더 백색 가치를 가질 것입니다.

482
00:29:01,812 --> 00:29:06,601
이 방향의 유형에는 모서리가 있습니다.

483
00:29:06,601 --> 00:29:10,563
따라서 각 활성화 맵은 출력의 일종입니다.

484
00:29:10,563 --> 00:29:12,358
이 필터들 중 하나를 슬라이딩시키는 것

485
00:29:12,358 --> 00:29:16,444
그리고 이러한 필터가 원인 인 곳에서,

486
00:29:16,444 --> 00:29:20,747
이런 종류의 템플리트가 이미지에 더 많이 존재합니다.

487
00:29:20,747 --> 00:29:24,869
그래서 우리가 이러한 콘볼 루션이라고 부르는 이유는

488
00:29:24,869 --> 00:29:27,221
이는 두 신호의 컨볼 루션
(convolution)과 관련이 있으며,

489
00:29:27,221 --> 00:29:29,153
그래서 누군가가 이전에 지적했다.

490
00:29:29,153 --> 00:29:32,982
이것은 기본적으로 여기에있는이 회선 방정식입니다.

491
00:29:32,982 --> 00:29:35,333
전에 회선을 본 사람들을 위해

492
00:29:35,333 --> 00:29:37,340
신호 처리 및 실제로

493
00:29:37,340 --> 00:29:38,927
실제로는 상관 관계와 비슷합니다.

494
00:29:38,927 --> 00:29:41,583
우리는 뒤집힌 버전과

495
00:29:41,583 --> 00:29:46,154
필터의,하지만 이것은 일종의 미묘합니다,

496
00:29:46,154 --> 00:29:50,149
이 수업의 목적을 위해 정말로 중요하지 않습니다.

497
00:29:50,149 --> 00:29:52,292
그러나 근본적으로 당신이하고있는 것을 쓰고 있다면,

498
00:29:52,292 --> 00:29:55,450
그것은 다음과 같은 표현을 가지고 있습니다.

499
00:29:55,450 --> 00:29:58,385
이는 컨볼 루션 (convolution)의
표준 정의입니다.

500
00:29:58,385 --> 00:30:00,402
하지만 이것은 기본적으로 필터를 사용하는 것입니다.

501
00:30:00,402 --> 00:30:02,432
이미지 위로 공간적으로 밀어 넣는다.

502
00:30:02,432 --> 00:30:06,432
모든 위치에서 내적을 계산하는 단계를 포함한다.

503
00:30:09,088 --> 00:30:11,977
좋아, 너도 알다시피, 나는 이전에 언급했듯이,

504
00:30:11,977 --> 00:30:14,208
우리의 전체 컨볼 루션 신경망

505
00:30:14,208 --> 00:30:17,278
우리가 입력 이미지를 갖게 될 것처럼 보이게 될 것입니다.

506
00:30:17,278 --> 00:30:19,693
그리고 나서 우리는 그것을 통과시킬 것입니다.

507
00:30:19,693 --> 00:30:21,633
레이어의 순서, 오른쪽,

508
00:30:21,633 --> 00:30:23,915
먼저 우리는 길쌈 계층을 가질 것입니다.

509
00:30:23,915 --> 00:30:28,236
그 후에 우리는 보통 비선형 레이어를 가지고 있습니다.

510
00:30:28,236 --> 00:30:30,579
그래서 ReLU는 매우 일반적으로 사용되는 것입니다.

511
00:30:30,579 --> 00:30:33,608
우리는 나중에 더 자세히 이야기 할 것입니다.

512
00:30:33,608 --> 00:30:36,791
그리고 나서 우리는 이러한 Conv, ReLU,
Conv, ReLU 계층을 가지며,

513
00:30:36,791 --> 00:30:39,775
그런 다음 한 번씩 풀링 레이어를 사용합니다.

514
00:30:39,775 --> 00:30:41,244
우리는 나중에 얘기 할 것입니다.

515
00:30:41,244 --> 00:30:45,411
기본적으로 활성화 맵의 크기를 줄이는 방법입니다.

516
00:30:47,300 --> 00:30:50,785
그리고 마지막으로 이것의 마지막에
우리는 우리의 마지막 걸릴거야.

517
00:30:50,785 --> 00:30:54,403
컨벌루션 레이어 출력을 사용하고

518
00:30:54,403 --> 00:30:56,872
이전에 보았던 완전히 연결된 레이어

519
00:30:56,872 --> 00:31:00,316
이들 모든 컨벌루션 출력들에 연결되며,

520
00:31:00,316 --> 00:31:03,011
그것을 사용하여 최종 점수 함수를 얻습니다.

521
00:31:03,011 --> 00:31:07,178
기본적으로 우리가 이미 작업 한 것과 같습니다.

522
00:31:08,445 --> 00:31:10,931
자, 그럼 이제 몇 가지 예제를 만들어 보겠습니다.

523
00:31:10,931 --> 00:31:14,181
공간 차원이 어떻게 작동하는지에 대한

524
00:31:18,363 --> 00:31:23,087
32 x 32 x 3 이미지를 이전과 같이 보겠습니다.

525
00:31:23,087 --> 00:31:25,624
맞아요, 우리는 다섯개 씩 3
개의 필터를 가지고 있습니다.

526
00:31:25,624 --> 00:31:28,025
우리가이 이미지 위로 미끄러 져 갈 것입니다.

527
00:31:28,025 --> 00:31:29,816
그리고 우리는 어떻게 우리가 그것을
사용하려고하는지 볼 것입니다.

528
00:31:29,816 --> 00:31:34,337
28 정품 인증 맵에 의해 정확히 이것을 생성합니다.

529
00:31:34,337 --> 00:31:37,644
따라서 우리가 실제로 7x7 입력을
가졌다 고 가정 해 봅시다.

530
00:31:37,644 --> 00:31:39,104
그냥 간단하게 가정 해 봅시다.

531
00:31:39,104 --> 00:31:41,505
우리는 3 x 3 필터를 가지고 있습니다.

532
00:31:41,505 --> 00:31:42,522
그럼 우리가 할 일은

533
00:31:42,522 --> 00:31:44,969
우리는이 필터를 사용할 것입니다.

534
00:31:44,969 --> 00:31:47,418
그것을 우리 왼쪽 상단 구석에 엎어 버리고,

535
00:31:47,418 --> 00:31:50,253
맞아, 우리는 번식 할 것이고,

536
00:31:50,253 --> 00:31:53,169
이 모든 값들을 곱해서 우리의 첫 번째 가치를 얻고,

537
00:31:53,169 --> 00:31:54,918
그리고 이것은 왼쪽 상단 값으로 갈 것입니다.

538
00:31:54,918 --> 00:31:56,764
활성화 맵.

539
00:31:56,764 --> 00:31:58,217
맞아, 그럼 우리가 다음에 할 일은

540
00:31:58,217 --> 00:32:00,475
우리는이 필터를 사용하려고합니다.

541
00:32:00,475 --> 00:32:02,389
한 위치를 오른쪽으로 밀면,

542
00:32:02,389 --> 00:32:05,535
그런 다음 여기에서 다른 가치를 얻을 것입니다.

543
00:32:05,535 --> 00:32:09,895
그리고 우리는 이것을 계속해서 또
다른 가치를 가질 수 있습니다.

544
00:32:09,895 --> 00:32:12,797
또 다른, 그리고 결국 우리는 무엇을 얻을 것인가?

545
00:32:12,797 --> 00:32:14,528
5 x 5 출력, 오른쪽,

546
00:32:14,528 --> 00:32:17,776
기본적으로이 필터를 슬라이딩하는 것이 맞았 기 때문입니다.

547
00:32:17,776 --> 00:32:22,214
총 5 개의 공간적 위치가 수평 적으로

548
00:32:22,214 --> 00:32:25,381
수직으로 다섯 개의 공간적 위치가 있습니다.

549
00:32:27,834 --> 00:32:29,414
좋아, 전에 말했듯이

550
00:32:29,414 --> 00:32:31,906
우리가 할 수있는 디자인 선택의 종류가 있습니다.

551
00:32:31,906 --> 00:32:34,710
맞아, 이전에 나는 모든 사람에게 그것을 미끄러졌다.

552
00:32:34,710 --> 00:32:37,828
공간 위치 및 슬라이드 간격

553
00:32:37,828 --> 00:32:40,326
나는 큰 걸음을 부를 것이다.

554
00:32:40,326 --> 00:32:43,093
그리고 이전에 우리는 하나의 보폭을 사용했습니다.

555
00:32:43,093 --> 00:32:44,567
이제 무슨 일이 일어나는지 봅시다.

556
00:32:44,567 --> 00:32:46,700
우리가 2의 보폭을 가졌다면.

557
00:32:46,700 --> 00:32:48,625
맞아, 이제 우리는 첫 번째 위치를 택할 것입니다.

558
00:32:48,625 --> 00:32:51,898
이전과 같고 그 다음으로 건너 뛸 것입니다.

559
00:32:51,898 --> 00:32:55,527
이번에는 2 픽셀 넘어서 우리는

560
00:32:55,527 --> 00:32:58,944
우리의 다음 가치는이 지역에 집중되었습니다.

561
00:33:00,773 --> 00:33:02,938
맞습니다. 그래서 지금 우리가 2의 보폭을 사용한다면,

562
00:33:02,938 --> 00:33:07,340
우리는 이들 중 3 가지가 적합 할 수 있으며,

563
00:33:07,340 --> 00:33:11,257
그래서 우리는 3 x 3 출력을 얻을 것입니다.

564
00:33:13,035 --> 00:33:15,955
좋아, 그래서 우리가 3의 보폭을 가졌을 때 어떻게되는지,

565
00:33:15,955 --> 00:33:18,653
이것의 출력 크기는 얼마입니까?

566
00:33:18,653 --> 00:33:21,924
그리고이 경우에는, 맞습니다. 우리에게는 세 가지가 있습니다.

567
00:33:21,924 --> 00:33:25,014
우리는 그것을 다시 3 번 밀어 낸다.

568
00:33:25,014 --> 00:33:27,905
문제는 실제로 여기에 맞지 않는다는 것입니다.

569
00:33:27,905 --> 00:33:29,827
맞아, 그래서 우리는 3 개씩 밀어 넣는다.

570
00:33:29,827 --> 00:33:32,363
이제는 이미지 안에 잘 들어 맞지 않습니다.

571
00:33:32,363 --> 00:33:35,721
그리고 우리는 실제로 우리가
단지 그것을 작동하지 않습니다.

572
00:33:35,721 --> 00:33:37,736
우리는 이와 같은 회선을하지 않습니다.

573
00:33:37,736 --> 00:33:41,903
이는 비대칭 출력으로 이어질 것이기 때문입니다.

574
00:33:46,095 --> 00:33:49,561
맞아요, 그래서 그런 식으로보고있는 것 같아요.

575
00:33:49,561 --> 00:33:52,464
출력 크기가 얼마나 될지 계산했습니다.

576
00:33:52,464 --> 00:33:54,690
이것은 실제로 좋은 공식으로 작동 할 수 있습니다.

577
00:33:54,690 --> 00:33:57,687
여기서 우리는 입력 N의 차원을 취하고,

578
00:33:57,687 --> 00:34:01,430
우리는 필터 크기 F를 가지고 있습니다.
우리는 우리의 보폭을 가지고 있습니다.

579
00:34:01,430 --> 00:34:05,597
우리가 함께 미끄러지고있는, 그리고
우리의 최종 산출물 크기,

580
00:34:06,992 --> 00:34:09,000
각 출력 크기의 공간 차원

581
00:34:09,000 --> 00:34:12,850
보너스로 나누어 진 마이너스 F가 될 것입니다.

582
00:34:12,850 --> 00:34:16,547
맞아, 너는 이것을 너처럼 알 수있어.

583
00:34:16,547 --> 00:34:18,619
내 필터를 가져 가려고한다면, 내가
채우고 있다고 가정 해 봅시다.

584
00:34:18,620 --> 00:34:21,373
가능한 마지막 위치에

585
00:34:21,373 --> 00:34:23,159
그 전에 모든 픽셀을 가져가

586
00:34:23,159 --> 00:34:27,326
이 보폭으로 얼마나 많은 경우 움직일 수 있습니까?

587
00:34:29,257 --> 00:34:32,546
맞습니다. 그래서 이것이이 방정식이
어떻게 작용하는지 보여줍니다.

588
00:34:32,547 --> 00:34:35,422
그리고 우리가 전에 보았던 것처럼, 맞습니다.
N이 7 인 경우

589
00:34:35,422 --> 00:34:38,637
F가 3 인 경우, 우리는 1의 보폭을 원한다면

590
00:34:38,637 --> 00:34:40,795
우리는 이것을이 수식에 꽂습니다. 5로 5를 얻습니다.

591
00:34:40,795 --> 00:34:43,498
우리가 이전에했던 것처럼, 그리고 우리가 두
가지를 위해 가진 것과 똑같은 것이다.

592
00:34:43,498 --> 00:34:47,665
그리고 3의 보폭과 더불어,
이것은 정말로 잘되지 않습니다.

593
00:34:50,288 --> 00:34:52,870
그리고 실제로 그것은 실제로 일반적입니다.

594
00:34:52,870 --> 00:34:56,203
국경을 제로로 패를 만들기 위해

595
00:34:57,134 --> 00:34:59,552
그 크기는 우리가 원하는대로 작동합니다.

596
00:34:59,552 --> 00:35:01,504
그리고 이것은 일찌기 질문과 관련이 있습니다.

597
00:35:01,504 --> 00:35:04,140
그것은 바로 우리가하는 일입니다. 맞습니다.

598
00:35:04,140 --> 00:35:06,145
그래서 실제로 일어나는 것은

599
00:35:06,145 --> 00:35:09,222
우리는 실제로 입력 이미지를 0으로 채 웁니다.

600
00:35:09,222 --> 00:35:12,449
이제 필터를 배치 할 수있게 될 것입니다.

601
00:35:12,449 --> 00:35:16,303
오른쪽 위 픽셀 위치에 중심 맞춤

602
00:35:16,303 --> 00:35:19,134
실제 입력 이미지의

603
00:35:19,134 --> 00:35:22,784
좋아, 그럼 여기에 질문이있어,
누가 나에게 말할 수 있니?

604
00:35:22,784 --> 00:35:25,988
내가 똑같은 의견을 가지고 있다면, 7에 7,

605
00:35:25,988 --> 00:35:27,635
3 x 3 필터, 스트라이드 1,

606
00:35:27,635 --> 00:35:29,942
하지만 지금은 한 픽셀 경계로 채 웁니다.

607
00:35:29,942 --> 00:35:33,654
출력물의 크기는 얼마입니까?

608
00:35:33,654 --> 00:35:36,285
[희미한 말]

609
00:35:36,285 --> 00:35:39,535
그래서 여섯 명이 들었고,

610
00:35:41,211 --> 00:35:44,847
우리가 이전에 가지고 있던 공식을 기억하십시오.

611
00:35:44,847 --> 00:35:49,342
그래서 우리가 플러그를 꽂는다면
N은 7이고, F는 3입니다.

612
00:35:49,342 --> 00:35:52,594
그렇다면 우리의 보폭은 1과 같습니다.

613
00:35:52,594 --> 00:35:57,264
그래서 우리가 실제로 얻는 것, 실제로
이것은 우리에게주는 것입니다.

614
00:35:57,264 --> 00:36:01,522
7, 4, 7 마이너스 3은 4입니다.

615
00:36:01,522 --> 00:36:03,256
하나 더하기 하나는 5로 나눈 값입니다.

616
00:36:03,256 --> 00:36:04,998
그래서 이것은 우리가 이전에 가진 것입니다.

617
00:36:04,998 --> 00:36:06,707
그래서 우리는 실제로이 공식을 약간 조정해야합니다.

618
00:36:06,707 --> 00:36:09,139
맞아요, 그래서 이것은 실제로,이 공식은 사실입니다

619
00:36:09,139 --> 00:36:12,161
여기에는 0이 채워진 픽셀이 없습니다.

620
00:36:12,161 --> 00:36:16,328
그러나 만약 우리가 그것을 채우면, 당신이
지금 당신의 새로운 산출물을 가져 가면

621
00:36:17,347 --> 00:36:19,050
너는 그것을 따라 미끄러 져,

622
00:36:19,050 --> 00:36:22,128
실제로 필터 중 7 개가 적합하다는 것을 알 수 있습니다.

623
00:36:22,128 --> 00:36:24,173
그래서 당신은 7 x 7 출력을 얻습니다.

624
00:36:24,173 --> 00:36:26,467
그리고 우리의 원래 공식을 연결하면,

625
00:36:26,467 --> 00:36:30,178
그래서 우리 N은 지금 7이 아니며, 그것은 9입니다.

626
00:36:30,178 --> 00:36:33,385
그래서 우리가 여기로 되돌아
간다면 우리는 N이 9가됩니다.

627
00:36:33,385 --> 00:36:37,001
6을 제공하는 3의 필터 크기를 뺀 것입니다.

628
00:36:37,001 --> 00:36:39,298
우리의 보폭으로 나뉘어져 있습니다.

629
00:36:39,298 --> 00:36:42,253
그리고 여전히 여섯, 그리고
더하기 우리는 일곱을 얻습니다.

630
00:36:42,253 --> 00:36:43,807
맞아요, 일단 당신이 그것을 덧대면

631
00:36:43,807 --> 00:36:47,974
이 패딩을 수식에 통합하려고합니다.

632
00:36:49,739 --> 00:36:51,646
네, 질문.

633
00:36:51,646 --> 00:36:54,396
[희미한 말]

634
00:37:00,717 --> 00:37:03,589
일곱, 알겠습니다. 그래서 질문은,

635
00:37:03,589 --> 00:37:06,114
크기의 실제 출력은 얼마입니까?

636
00:37:06,114 --> 00:37:08,962
7시 7 분 7시 7 분 3 시까 지입니까?

637
00:37:08,962 --> 00:37:11,935
출력은 7 by 7이 될 것입니다.

638
00:37:11,935 --> 00:37:14,495
가지고있는 필터의 수만큼.

639
00:37:14,495 --> 00:37:18,162
각 필터가 내적 제품을 수행 할 것임을 기억하십시오.

640
00:37:18,162 --> 00:37:21,320
귀하의 입력 볼륨의 전체 깊이를 통해.

641
00:37:21,320 --> 00:37:23,801
그러나 그 다음에 하나의 숫자가 생길 것입니다, 맞습니다.

642
00:37:23,801 --> 00:37:27,968
그래서 각 필터는 우리가 여기로 돌아갈 수 있는지 보자.

643
00:37:29,540 --> 00:37:32,938
각 필터는 하나씩 7을 생성합니다.

644
00:37:32,938 --> 00:37:37,124
이 경우 활성화 맵 출력 및 깊이

645
00:37:37,124 --> 00:37:40,493
우리가 가지고있는 필터의 숫자가 될 것입니다.

646
00:37:40,493 --> 00:37:43,243
[희미한 말]

647
00:37:50,161 --> 00:37:53,411
미안 해요. 잠시만 요. 잠시만 요.

648
00:37:55,136 --> 00:37:57,350
알았어. 다시 질문을 할 수 있니?

649
00:37:57,350 --> 00:38:00,267
[말장난]

650
00:38:12,936 --> 00:38:16,011
좋아요. 문제는 이전에 어떻게 연결했는지입니다.

651
00:38:16,011 --> 00:38:19,735
우리가 32 x 32를 3 입력으로했을 때, 맞았습니다.

652
00:38:19,735 --> 00:38:21,830
따라서 우리의 의견은이 예에서 깊이가있었습니다.

653
00:38:21,830 --> 00:38:24,721
깊이가없는 2D 예제를 보여주고 있습니다.

654
00:38:24,721 --> 00:38:27,226
그리고 그래, 나는 이것을 단순함으로 보여주고있다.

655
00:38:27,226 --> 00:38:30,373
그러나 실제로는 당신은 당신을 데리고 갈 것입니다,

656
00:38:30,373 --> 00:38:32,334
너는 전체 깊이에 걸쳐 번식 할거야.

657
00:38:32,334 --> 00:38:34,188
우리가 이전에했던 것처럼, 그래서 당신은 가고 있습니다.

658
00:38:34,188 --> 00:38:36,765
당신의 필터는이 경우에 3이 될 것입니다.

659
00:38:36,765 --> 00:38:39,850
당신이 가지고있는 어떤 입력 깊이라도
공간 필터를 사용할 수 있습니다.

660
00:38:39,850 --> 00:38:43,183
그래서이 경우 3 x 3입니다.

661
00:38:44,059 --> 00:38:46,854
그래, 나머지는 그대로있어.

662
00:38:46,854 --> 00:38:48,390
네, 질문.

663
00:38:48,390 --> 00:38:51,307
[말장난]

664
00:38:53,529 --> 00:38:55,731
그래, 문제는 0 패딩

665
00:38:55,731 --> 00:38:58,664
모서리에 불필요한 기능을 추가 하시겠습니까?

666
00:38:58,664 --> 00:39:01,446
그리고 그래, 내 말은, 우리는
여전히 최선을 다하고 있습니다.

667
00:39:01,446 --> 00:39:03,779
가치를 얻고,

668
00:39:04,721 --> 00:39:06,289
이미지의 그 영역을 처리하고,

669
00:39:06,289 --> 00:39:10,343
그래서 제로 패딩은 이것을하기위한 일종의 방법입니다,

670
00:39:10,343 --> 00:39:12,999
우리가 할 수있는 곳에서

671
00:39:12,999 --> 00:39:16,097
이 영역에서이 템플릿의 일부.

672
00:39:16,097 --> 00:39:18,323
이 작업을 수행하는 다른 방법도 있습니다. 알다시피,

673
00:39:18,323 --> 00:39:20,729
당신은 시도하고 좋아할 수 있습니다.

674
00:39:20,729 --> 00:39:23,615
또는 확장 할 수 있으므로 0으로
채워야 할 필요가 없습니다.

675
00:39:23,615 --> 00:39:26,530
그러나 실제로 이것은 합리적으로 작동하는 한 가지입니다.

676
00:39:26,530 --> 00:39:29,930
그래서, 약간의 유물이 있습니다.

677
00:39:29,930 --> 00:39:31,503
가장자리에서 그리고 우리는 그냥 정렬,

678
00:39:31,503 --> 00:39:33,834
당신은 그것을 다루기 위해 최선을 다합니다.

679
00:39:33,834 --> 00:39:36,486
실제로 이것은 합리적으로 효과가 있습니다.

680
00:39:36,486 --> 00:39:39,503
나는 또 다른 질문이 있다고 생각한다.

681
00:39:39,503 --> 00:39:41,283
그래, 질문.

682
00:39:41,283 --> 00:39:44,033
[희미한 말]

683
00:39:48,015 --> 00:39:51,535
그래서 정사각형이 아닌 이미지가
있다면 우리는 보폭을 사용합니까?

684
00:39:51,535 --> 00:39:54,330
그것은 가로와 세로가 다릅니 까?

685
00:39:54,330 --> 00:39:57,039
그래서, 제 말은, 당신이 그 일을하는
것을 막을 수있는 아무것도 없습니다.

686
00:39:57,039 --> 00:39:59,816
너는 할 수 있겠지만, 실제로 우리는 보통

687
00:39:59,816 --> 00:40:02,841
똑같은 보폭을 취하고, 대개 사각형 영역을 조작합니다.

688
00:40:02,841 --> 00:40:04,909
그리고 우린 그냥, 보통 우린 그냥

689
00:40:04,909 --> 00:40:08,238
똑같은 보폭을 도처에 가져 가세요.

690
00:40:08,238 --> 00:40:10,218
어떤 의미에서 그것은 조금 비슷합니다.

691
00:40:10,218 --> 00:40:12,900
그것은 당신이있는 해상도와 조금 비슷합니다.

692
00:40:12,900 --> 00:40:14,699
이 이미지를 보면,

693
00:40:14,699 --> 00:40:18,100
그리고 보통 그렇게 종류가 있습니다.

694
00:40:18,100 --> 00:40:20,693
당신의 수평 및 수직 해상도의 종류.

695
00:40:20,693 --> 00:40:22,886
하지만, 네, 그래서 실제로

696
00:40:22,886 --> 00:40:25,553
하지만 정말로 사람들은 그렇게하지 않습니다.

697
00:40:26,555 --> 00:40:28,373
다른 질문입니다.

698
00:40:28,373 --> 00:40:31,453
[희미한 말]

699
00:40:31,453 --> 00:40:33,710
질문은 왜 우리가 제로 패딩을하는 것입니까?

700
00:40:33,710 --> 00:40:35,247
그래서 우리는 제로 패딩을합니다.

701
00:40:35,247 --> 00:40:39,376
이전과 같은 입력 크기를 유지하는 것입니다.

702
00:40:39,376 --> 00:40:41,297
맞아, 그래서 7시 7 분부터 시작 했어.

703
00:40:41,297 --> 00:40:44,182
필터를 시작하기 만하면

704
00:40:44,182 --> 00:40:46,756
왼쪽 상단 모서리에서 모든 것을 채우고,

705
00:40:46,756 --> 00:40:49,019
그렇다면 우리는 더 작은 크기의 출력을 얻습니다.

706
00:40:49,019 --> 00:40:53,186
그러나 우리는 전체 크기 출력을 유지하려고합니다.

707
00:40:56,276 --> 00:40:57,109
그래,

708
00:40:59,251 --> 00:41:02,664
예, 우리는 패딩이 기본적으로 당신을
도울 수있는 방법을 보았습니다.

709
00:41:02,664 --> 00:41:05,527
원하는 출력의 크기를 유지하고,

710
00:41:05,527 --> 00:41:08,237
뿐만 아니라 이러한 필터를 적용 할 수도 있습니다.

711
00:41:08,237 --> 00:41:10,753
모서리 영역 및 모서리 영역을 포함 할 수있다.

712
00:41:10,753 --> 00:41:13,142
그리고 선택의 측면에서 일반적으로,

713
00:41:13,142 --> 00:41:15,772
당신은 당신의 보폭, 필터, 필터 크기,

714
00:41:15,772 --> 00:41:18,998
귀하의 보폭, 제로 패딩, 볼 수있는 공통점

715
00:41:18,998 --> 00:41:22,405
3 x 5, 5 x 5 크기의 필터입니다.

716
00:41:22,405 --> 00:41:25,427
7 개 7 개는 꽤 일반적인 필터 크기입니다.

717
00:41:25,427 --> 00:41:27,908
그리고 이것들 각각, 3 ~ 3

718
00:41:27,908 --> 00:41:30,232
당신은 1로 패드를 제로로하고 싶을 것이다.

719
00:41:30,232 --> 00:41:33,567
동일한 공간 크기를 유지하기 위해

720
00:41:33,567 --> 00:41:35,618
5 명씩 5 명을 할 계획이라면,

721
00:41:35,618 --> 00:41:37,470
수학을 해결할 수는 있어도 나오지.

722
00:41:37,470 --> 00:41:39,422
당신이 2로 제로 패드 싶어요.

723
00:41:39,422 --> 00:41:43,505
그리고 나서 7에 대해 3으로 제로를 넣고 싶습니다.

724
00:41:44,722 --> 00:41:47,316
자, 다시 알다시피, 동기 부여

725
00:41:47,316 --> 00:41:50,167
이 유형의 제로 패딩을 수행하는 경우

726
00:41:50,167 --> 00:41:52,184
입력 크기를 유지하려고하면

727
00:41:52,184 --> 00:41:54,500
그래서 우리는 일종의 이것에 대해 전에 암시했다.

728
00:41:54,500 --> 00:41:58,667
하지만이 여러 레이어가 서로 쌓여 있다면...

729
00:42:03,354 --> 00:42:07,015
따라서 서로 겹쳐진 여러 레이어가있는 경우

730
00:42:07,015 --> 00:42:08,689
우리가 이런 종류의 일을하지
않으면, 당신도 알게 될 것입니다.

731
00:42:08,689 --> 00:42:10,566
제로 패딩 (zero padding), 또는 어떤 종류의 패딩 (padding)

732
00:42:10,566 --> 00:42:12,848
우리는 정말로 크기를 줄이려고 할 것입니다.

733
00:42:12,848 --> 00:42:14,602
우리가 가지고있는 산출물의

734
00:42:14,602 --> 00:42:16,616
맞습니다. 그래서 이것은 우리가
원하는 어떤 것이 아닙니다.

735
00:42:16,616 --> 00:42:19,302
마치 꽤 깊은 네트워크가 있다면 상상할 수 있습니다.

736
00:42:19,302 --> 00:42:23,293
그런 다음 매우 빠르게 활성화 맵의 크기를

737
00:42:23,293 --> 00:42:25,907
매우 작은 것으로 축소 될 것입니다.

738
00:42:25,907 --> 00:42:28,790
그리고 이것은 우리가 일종의 잃는
것이기 때문에 모두 나쁜 것입니다.

739
00:42:28,790 --> 00:42:29,990
이 정보 중 일부는

740
00:42:29,990 --> 00:42:34,272
이제 훨씬 적은 수의 값을 사용하고 있습니다.

741
00:42:34,272 --> 00:42:36,578
원본 이미지를 나타 내기 위해

742
00:42:36,578 --> 00:42:38,568
그래서 당신은 그것을 원치 않습니다.

743
00:42:38,568 --> 00:42:41,318
그리고 같은 시간에

744
00:42:42,983 --> 00:42:46,249
우리는 이것에 관해서 일찍 이야기했습니다.

745
00:42:46,249 --> 00:42:48,589
이런 엣지 정보 중 일부를 잃어 버리고,

746
00:42:48,589 --> 00:42:49,923
매번 코너 정보

747
00:42:49,923 --> 00:42:53,590
우리는 그로부터 길을 잃고 줄어들고 있습니다.

748
00:42:55,203 --> 00:42:57,310
좋아, 몇 가지 예를 더 살펴 보자.

749
00:42:57,310 --> 00:43:00,060
이러한 크기의 일부를 계산할 수 있습니다.

750
00:43:00,991 --> 00:43:03,018
그러면 입력 음량이 있다고 가정 해 봅시다.

751
00:43:03,018 --> 00:43:05,611
32 x 32입니다.

752
00:43:05,611 --> 00:43:09,244
여기에는 5 5 x 5 필터가 10 개 있습니다.

753
00:43:09,244 --> 00:43:12,388
스트라이드 1과 패드 2를 사용합시다.

754
00:43:12,388 --> 00:43:13,550
그리고 나에게 말할 수있는 사람

755
00:43:13,550 --> 00:43:16,717
이것의 출력 볼륨 크기는 얼마입니까?

756
00:43:18,188 --> 00:43:20,353
따라서 이전 공식에 대해 생각할 수 있습니다.

757
00:43:20,353 --> 00:43:21,728
미안, 뭐야?

758
00:43:21,728 --> 00:43:23,263
[희미한 말]

759
00:43:23,263 --> 00:43:26,180
32 by 32 by 10, 맞습니다.

760
00:43:27,572 --> 00:43:30,324
그래서 우리가 이것을 볼 수있는 방법입니다, 맞습니다.

761
00:43:30,324 --> 00:43:33,707
우리가 입력 크기를 가지므로, F는 32입니다.

762
00:43:33,707 --> 00:43:36,401
그런 다음이 경우에 우리는 그것을 증가시키고 자한다.

763
00:43:36,401 --> 00:43:38,396
우리가 덧붙인 덧대는 것.

764
00:43:38,396 --> 00:43:41,209
그래서 우리는 각 차원에서 두 개의 패딩을했습니다.

765
00:43:41,209 --> 00:43:44,122
그래서 우리는 실제로 얻을 것입니다, 총 너비와 총 높이

766
00:43:44,122 --> 00:43:47,181
각면에 32 + 4가 될 것입니다.

767
00:43:47,181 --> 00:43:49,992
그리고 필터 크기 5를 뺀 것입니다.

768
00:43:49,992 --> 00:43:51,716
1을 더한 다음 32를 얻습니다.

769
00:43:51,716 --> 00:43:55,883
따라서 각 필터의 출력은 32 x 32가 될 것입니다.

770
00:43:57,213 --> 00:44:00,302
그리고 나서 총 10 개의 필터가 있습니다.

771
00:44:00,302 --> 00:44:02,193
그래서 우리는 10 개의 활성화 맵을 가지고 있습니다.

772
00:44:02,193 --> 00:44:06,360
총 출력량은 32 x 32 x 10이 될 것입니다.

773
00:44:08,244 --> 00:44:10,040
좋아, 다음 질문,

774
00:44:10,040 --> 00:44:14,478
이 레이어의 매개 변수 수는 얼마입니까?

775
00:44:14,478 --> 00:44:18,145
5 × 5 필터가 10 개 있다는 것을 기억하십시오.

776
00:44:19,769 --> 00:44:22,698
[희미한 말]

777
00:44:22,698 --> 00:44:26,365
나는 종류의 것을 들었다. 그러나 그것은 조용했다.

778
00:44:29,407 --> 00:44:31,240
너희들이 말할 수 있니?

779
00:44:32,809 --> 00:44:36,226
250, 알았어. 그래서 나는 250 번을 들었다.

780
00:44:37,829 --> 00:44:40,018
그러나 우리는 또한 우리의 입력량이며,

781
00:44:40,018 --> 00:44:42,149
이 필터들 각각은 깊이있게지나갑니다.

782
00:44:42,149 --> 00:44:44,237
그래서 여기에 명확하게 쓰여지지 않았을 수도 있습니다.

783
00:44:44,237 --> 00:44:46,855
각각의 필터가 공간적으로 5 × 5이기 때문에,

784
00:44:46,855 --> 00:44:50,300
그러나 암묵적으로 우리는 깊이가 있습니다.

785
00:44:50,300 --> 00:44:52,835
그것은 전체 볼륨을 통과 할 것입니다.

786
00:44:52,835 --> 00:44:55,876
그래서 들었습니다, 예, 750 나는 들었습니다.

787
00:44:55,876 --> 00:44:57,430
거의 거기, 이것은 일종의 간계 질문입니다.

788
00:44:57,430 --> 00:44:59,445
항상 우리가 항상 기억하고 있기 때문에

789
00:44:59,445 --> 00:45:03,374
편견 용어, 맞아. 그래서 실제로는 각 필터

790
00:45:03,374 --> 00:45:08,084
3 5 x 5, 플러스 1 바이어스 기간,

791
00:45:08,084 --> 00:45:10,483
우리는 필터 당 76 개의 매개 변수를 가지고 있습니다.

792
00:45:10,483 --> 00:45:12,609
그리고 나서 우리는이 총 10 개를 가지고 있습니다.

793
00:45:12,609 --> 00:45:15,609
760 개의 전체 매개 변수가 있습니다.

794
00:45:18,412 --> 00:45:20,464
좋아요, 그래서 여기에 단지 요약되어 있습니다.

795
00:45:20,464 --> 00:45:24,105
당신이 읽을 수있는 길쌈 층의

796
00:45:24,105 --> 00:45:25,890
나중에 조금 더 신중하게.

797
00:45:25,890 --> 00:45:28,924
그러나 우리는 특정 차원의 입력 볼륨을 가지고 있습니다.

798
00:45:28,924 --> 00:45:31,137
우리는이 모든 선택권을 가지고 있습니다, 우리는
우리의 필터를 가지고 있습니다. 맞습니다.

799
00:45:31,137 --> 00:45:33,751
여기서 우리는 필터의 수, 필터 크기,

800
00:45:33,751 --> 00:45:36,170
크기의 보폭, 제로 패딩의 양,

801
00:45:36,170 --> 00:45:38,682
기본적으로이 모든 것을 사용할 수 있습니다.

802
00:45:38,682 --> 00:45:41,167
우리가 이전에 말한 계산을 거쳐야한다.

803
00:45:41,167 --> 00:45:43,866
출력 볼륨이 실제로 무엇인지 알아 내기 위해

804
00:45:43,866 --> 00:45:48,033
당신이 가지고있는 총 매개 변수가
얼마나 많은지 그리고 얼마나 많은지

805
00:45:49,282 --> 00:45:51,951
그래서 이것의 일반적인 설정입니다.

806
00:45:51,951 --> 00:45:55,526
이전에 공통 필터 크기에 대해 이야기했습니다.

807
00:45:55,526 --> 00:45:58,555
3 명, 5 명, 5 명

808
00:45:58,555 --> 00:46:01,739
보폭은 대개 1이고 2가 꽤 일반적입니다.

809
00:46:01,739 --> 00:46:04,505
그리고 당신의 패딩 P는 어떤 것이 든 될 것입니다.

810
00:46:04,505 --> 00:46:08,518
당신의 공간적 범위를 보존 할 것이 무엇이든간에

811
00:46:08,518 --> 00:46:10,401
공통점입니다.

812
00:46:10,401 --> 00:46:13,623
그리고 나서 총 필터 수 K,

813
00:46:13,623 --> 00:46:16,759
보통 우리는 단지 두 가지의
힘을 사용하여 멋지게 만듭니다.

814
00:46:16,759 --> 00:46:19,009
32, 64, 128 등, 512,

815
00:46:19,903 --> 00:46:24,505
이것들은 꽤 일반적인 숫자입니다.

816
00:46:24,505 --> 00:46:26,511
그리고 그냥 제쳐두고,

817
00:46:26,511 --> 00:46:29,488
우리는 하나의 회선을 하나씩 할 수 있습니다.

818
00:46:29,488 --> 00:46:31,557
이것은 여전히 완벽한 의미를 갖습니다.

819
00:46:31,557 --> 00:46:33,459
주어진 하나씩의 회선

820
00:46:33,459 --> 00:46:35,852
우리는 각 공간 범위에서 여전히 그것을 슬라이드합니다,

821
00:46:35,852 --> 00:46:37,700
하지만 이제는 알다시피, 공간 영역

822
00:46:37,700 --> 00:46:38,888
실제로 5에 5가 아니다.

823
00:46:38,888 --> 00:46:42,574
하나 하나의 사소한 사건 일뿐입니다.

824
00:46:42,574 --> 00:46:44,819
그러나 우리는 여전히이 필터를 가지고 있습니다.

825
00:46:44,819 --> 00:46:46,680
전체 깊이로 가라.

826
00:46:46,680 --> 00:46:48,273
맞아, 이건 내면의 제품이 될거야.

827
00:46:48,273 --> 00:46:52,053
귀하의 입력 볼륨의 전체 깊이를 통해.

828
00:46:52,053 --> 00:46:55,067
여기 출력은 입력 음량이있는 경우 출력됩니다.

829
00:46:55,067 --> 00:46:59,804
56 × 56 × 64 깊이로 하나씩 처리 할 것입니다.

830
00:46:59,804 --> 00:47:03,895
32 개의 필터가있는 컨볼 루션은 출력이 될 것입니다.

831
00:47:03,895 --> 00:47:07,062
우리의 필터 수에 의해 56에 의해 56 56.

832
00:47:10,076 --> 00:47:13,419
자, 여기에 길쌈 레이어의 예제가 있습니다.

833
00:47:13,419 --> 00:47:16,210
깊은 학습 틀인 TORCH에서

834
00:47:16,210 --> 00:47:18,747
그래서 마지막 강의를 보게 될 것입니다.

835
00:47:18,747 --> 00:47:20,799
우리가 어떻게이 문제에 개입 할 수 있는지 이야기했습니다.

836
00:47:20,799 --> 00:47:23,427
심층 학습 프레임 워크를 사용하면
이러한 정의를 볼 수 있습니다.

837
00:47:23,427 --> 00:47:25,017
각 레이어의 오른쪽, 그들이 어디에 있는지

838
00:47:25,017 --> 00:47:26,665
정방향 및 역방향 패스

839
00:47:26,665 --> 00:47:28,667
각 레이어에 대해 구현됩니다.

840
00:47:28,667 --> 00:47:30,638
그리고 당신은 회선을 볼 것입니다,

841
00:47:30,638 --> 00:47:33,562
공간 컨볼 루션은 이들 중 하나 일뿐입니다.

842
00:47:33,562 --> 00:47:35,360
그리고 나서 그것이 취할 논쟁

843
00:47:35,360 --> 00:47:39,890
이 모든 디자인 선택이 될 것입니다. 아시다시피,

844
00:47:39,890 --> 00:47:42,781
제 말은, 당신의 입력과 출력 크기를 추측합니다.

845
00:47:42,781 --> 00:47:45,759
커널 너비와 같은 선택도 가능합니다.

846
00:47:45,759 --> 00:47:50,161
커널 크기, 패딩 및 이러한 종류의 것들.

847
00:47:50,161 --> 00:47:53,226
맞습니다. 그래서 다른 프레임 워크를 보면, Caffe,

848
00:47:53,226 --> 00:47:54,737
당신은 매우 비슷한 것을 보게 될 것입니다.

849
00:47:54,737 --> 00:47:56,950
네트워크를 정의 할 때 다시 지금 어디에 있습니까?

850
00:47:56,950 --> 00:48:00,880
당신은 이런 종류의 것을 사용하여
Caffe에서 네트워크를 정의합니다.

851
00:48:00,880 --> 00:48:03,982
지정하는 원시 텍스트 파일

852
00:48:03,982 --> 00:48:07,160
레이어에 대한 각 디자인 선택

853
00:48:07,160 --> 00:48:09,279
컨벌루션 레이어를 볼 수 있습니다.

854
00:48:09,279 --> 00:48:11,806
출력의 수를 알 수 있습니다.

855
00:48:11,806 --> 00:48:14,077
우리가 가지고있는 필터의 개수가 될 것입니다.

856
00:48:14,077 --> 00:48:18,244
Caffe의 경우, 커널 크기와 보폭 등이 포함됩니다.

857
00:48:21,144 --> 00:48:24,701
좋아, 그래서 나는 내가 계속하기 전에,

858
00:48:24,701 --> 00:48:26,512
회선에 관한 질문,

859
00:48:26,512 --> 00:48:29,512
컨볼 루션 연산은 어떻게 작동합니까?

860
00:48:30,868 --> 00:48:32,161
네, 질문.

861
00:48:32,161 --> 00:48:34,911
[희미한 말]

862
00:48:51,604 --> 00:48:52,940
그래, 문제는,

863
00:48:52,940 --> 00:48:55,902
당신이 당신의 보폭을 선택하는
방법 뒤에있는 직관은 무엇입니까?

864
00:48:55,902 --> 00:49:00,037
그리고 한 가지 의미에서 그것은 일종의 해결책입니다.

865
00:49:00,037 --> 00:49:02,401
당신이 그것을 미끄러지게 할 때,
그리고 보통 이것 뒤에있는 이유

866
00:49:02,401 --> 00:49:04,870
우리가 더 큰 보폭을 가질 때

867
00:49:04,870 --> 00:49:06,908
우리가 결과물로 얻는 결과

868
00:49:06,908 --> 00:49:09,258
다운 샘플링 된 이미지, 오른쪽,

869
00:49:09,258 --> 00:49:13,425
그래서이 다운 샘플링 된 이미지로 우리가
가질 수있는 것은 둘 다입니다.

870
00:49:14,715 --> 00:49:17,202
그것은 한 가지 방법입니다. 어떤
의미에서 풀링과 같은 것입니다.

871
00:49:17,202 --> 00:49:19,352
그러나 그것은 단지 다른 것이고 때로는 더 잘 작동합니다.

872
00:49:19,352 --> 00:49:23,025
풀링하는 방법은이 배후의 직감 중 하나입니다.

873
00:49:23,025 --> 00:49:27,192
이미지를 다운 샘플링하는 것과 동일한 효과를 얻으므로,

874
00:49:28,183 --> 00:49:32,691
그리고 나서이 작업을 할 때 크기를 줄이고 있습니다.

875
00:49:32,691 --> 00:49:35,502
당신이 다루고있는 활성화 맵들 중

876
00:49:35,502 --> 00:49:38,892
각 레이어에서 오른쪽, 그리고
이것도 나중에 영향을 미칩니다

877
00:49:38,892 --> 00:49:40,825
가지고있는 매개 변수의 총 개수

878
00:49:40,825 --> 00:49:44,973
예를 들어 모든 전환 계층의 끝에서

879
00:49:44,973 --> 00:49:48,611
이제 당신은 완전히 연결된 레이어를
맨 위에 올릴 수도 있습니다.

880
00:49:48,611 --> 00:49:51,092
예를 들어, 이제 완전히 연결된 레이어의

881
00:49:51,092 --> 00:49:53,362
모든 값에 연결됨

882
00:49:53,362 --> 00:49:56,099
당신의 컨볼 루션 출력 중, 오른쪽,

883
00:49:56,099 --> 00:49:59,058
그래서 작은 것이 더 적은 수를 줄 것입니다.

884
00:49:59,058 --> 00:50:02,596
매개 변수를 사용하면 이제는

885
00:50:02,596 --> 00:50:04,960
기본적으로 트레이드 오프를 생각하면,

886
00:50:04,960 --> 00:50:08,025
보유하고있는 매개 변수의 수, 모델의 크기,

887
00:50:08,025 --> 00:50:10,076
overfitting, 그런 것들, 그래서 그래,

888
00:50:10,076 --> 00:50:11,371
이것들은 일종의 것들이다.

889
00:50:11,371 --> 00:50:15,538
당신은 당신의 보폭을 선택하는
것에 대해 생각하고 싶습니다.

890
00:50:18,496 --> 00:50:22,421
좋아요, 그래서 지금 우리가 종류의 조금을 보면,

891
00:50:22,421 --> 00:50:25,356
당신은 알고 있습니다, convolutional
layer의 뇌 뉴런 뷰,

892
00:50:25,356 --> 00:50:29,627
우리가 뉴런을 보았던 것과 비슷합니다.

893
00:50:29,627 --> 00:50:31,599
마지막 강의에서.

894
00:50:31,599 --> 00:50:35,610
그래서 우리는 모든 공간적 위치에서,

895
00:50:35,610 --> 00:50:37,488
우리는 필터 사이에 점을 찍는다.

896
00:50:37,488 --> 00:50:39,216
이미지의 특정 부분, 오른쪽,

897
00:50:39,216 --> 00:50:42,077
우리는 여기에서 하나의 번호를 얻습니다.

898
00:50:42,077 --> 00:50:43,506
그리고 이것은 같은 생각입니다.

899
00:50:43,506 --> 00:50:46,042
이런 종류의 내적 제품을 만드는 것,

900
00:50:46,042 --> 00:50:49,270
귀하의 의견을 가지고,이 Ws에 의해 가중치, 오른쪽,

901
00:50:49,270 --> 00:50:53,659
필터의 값, 시냅스 인이 가중치,

902
00:50:53,659 --> 00:50:55,227
가치를 얻는 것.

903
00:50:55,227 --> 00:50:57,559
하지만 여기서 가장 큰 차이점은 바로 지금입니다.

904
00:50:57,559 --> 00:50:59,517
뉴런에는 로컬 연결이 있습니다.

905
00:50:59,517 --> 00:51:02,191
따라서 전체 입력에 연결되는 대신,

906
00:51:02,191 --> 00:51:06,536
그것은 단지 당신의 이미지의 공간적인
지역을 바라 보는 것입니다.

907
00:51:06,536 --> 00:51:08,701
그래서 이것은 지역을 바라본 것입니다.

908
00:51:08,701 --> 00:51:11,859
그리고 지금 당신은 종류를 얻게
될 것입니다, 당신도 알다시피,

909
00:51:11,859 --> 00:51:15,111
이,이 뉴런이 얼마나 많이 발생했는지

910
00:51:15,111 --> 00:51:17,500
이미지의 모든 공간 위치에서

911
00:51:17,500 --> 00:51:19,631
맞습니다. 이제는 공간 구조를 보존합니다.

912
00:51:19,631 --> 00:51:22,485
당신은 추론 할 수 있다고 말할 수 있습니다.

913
00:51:22,485 --> 00:51:26,652
이러한 종류의 활성화 맵을 최신 레이어에 추가합니다.

914
00:51:30,048 --> 00:51:33,181
그리고 약간의 전문 용어,

915
00:51:33,181 --> 00:51:36,931
다시 알다시피, 우리는 5 x 5 필터를 가지고 있습니다.

916
00:51:36,931 --> 00:51:40,015
우리는 이것을 5 x 5 수용성 장

917
00:51:40,015 --> 00:51:41,726
뉴런의 경우 이것은,

918
00:51:41,726 --> 00:51:44,300
수용 분야는 기본적으로, 당신도 알다시피,

919
00:51:44,300 --> 00:51:46,535
이 시야의 입력 필드

920
00:51:46,535 --> 00:51:48,518
이 뉴런이 받고있는, 맞아,

921
00:51:48,518 --> 00:51:51,758
그래서 이것은 또 다른 공통된 용어 일뿐입니다.

922
00:51:51,758 --> 00:51:53,315
당신이 이것을 듣게 될 것입니다.

923
00:51:53,315 --> 00:51:55,743
그런 다음 5 개의 필터로 5 개를 다시 기억하십시오.

924
00:51:55,743 --> 00:51:58,442
우리는 그것들을 공간적 위치 위로 미끄러 져 움직이고있다.

925
00:51:58,442 --> 00:52:00,506
그러나 그들은 같은 무게의 세트입니다,

926
00:52:00,506 --> 00:52:03,089
그들은 동일한 매개 변수를 공유합니다.

927
00:52:05,440 --> 00:52:08,045
우리가 얘기 한 것처럼 알았어.

928
00:52:08,045 --> 00:52:09,485
우리가이 산출물에서 얻게 될 것

929
00:52:09,485 --> 00:52:11,200
이 권이 될거야. 맞아.

930
00:52:11,200 --> 00:52:13,874
우리가 공간적으로 가지고있는 곳, 28시 28 분

931
00:52:13,874 --> 00:52:16,373
필터의 수는 깊이입니다.

932
00:52:16,373 --> 00:52:18,357
예를 들어 5 개의 필터가있는 경우,

933
00:52:18,357 --> 00:52:20,663
우리가 나갈 것은이 3D 그리드입니다.

934
00:52:20,663 --> 00:52:23,381
그것은 28에 의해 28에 의해 5입니다.

935
00:52:23,381 --> 00:52:26,606
필터를 가로 질러 보면

936
00:52:26,606 --> 00:52:30,654
활성화 볼륨의 한 공간 위치에

937
00:52:30,654 --> 00:52:33,825
그리고이 5 개의 뉴런을 깊이 훑어 보면,

938
00:52:33,825 --> 00:52:36,003
이 모든 뉴런들,

939
00:52:36,003 --> 00:52:37,408
기본적으로 당신이 이것을 해석 할 수있는 방법

940
00:52:37,408 --> 00:52:39,471
그들은 모두 같은 지역을보고있는 것입니까?

941
00:52:39,471 --> 00:52:40,590
입력 볼륨에서,

942
00:52:40,590 --> 00:52:42,344
그러나 그들은 단지 다른 것들을 찾고 있습니다. 맞습니다.

943
00:52:42,344 --> 00:52:43,953
그래서 그들은 다른 필터입니다.

944
00:52:43,953 --> 00:52:48,120
이미지의 동일한 공간 위치에 적용됩니다.

945
00:52:49,152 --> 00:52:52,391
그리고 다시 한 번 비교해 보겠습니다.

946
00:52:52,391 --> 00:52:55,443
이전에 이야기했던 완전히 연결된 레이어

947
00:52:55,443 --> 00:52:57,805
이 경우, 우리가 각각의 뉴런을 본다면

948
00:52:57,805 --> 00:53:01,607
우리의 활성화 또는 산출물에서, 각각의 뉴런

949
00:53:01,607 --> 00:53:03,983
전체 뻗어 입력,

950
00:53:03,983 --> 00:53:06,637
그래서 전체 전체 입력 볼륨을 살펴 보았습니다.

951
00:53:06,637 --> 00:53:08,802
지금과 비교할 때마다

952
00:53:08,802 --> 00:53:12,805
이 로컬 공간 영역을 봅니다.

953
00:53:12,805 --> 00:53:14,255
문제.

954
00:53:14,255 --> 00:53:17,088
[말하지 않는 이야기]

955
00:53:22,648 --> 00:53:25,054
그래, 문제는 주어진 레이어 내에서,

956
00:53:25,054 --> 00:53:28,137
필터가 완전히 대칭입니까?

957
00:53:30,158 --> 00:53:34,325
그렇다면 대칭성이란 정확히 무엇을 의미합니까?

958
00:53:42,200 --> 00:53:46,389
그래, 좋아. 그래서 필터들, 필터들,

959
00:53:46,389 --> 00:53:50,556
그들은 같은 차원, 같은 계산을하고 있습니다, 그렇습니다.

960
00:53:52,784 --> 00:53:54,444
좋아요, 그래서 다른 게 있어요.

961
00:53:54,444 --> 00:53:58,122
그들은 같은 매개 변수 값을 가지고 있습니까?

962
00:53:58,122 --> 00:53:59,624
아니, 네 말이 맞아.

963
00:53:59,624 --> 00:54:02,690
우리는 주어진 집합으로 필터를 가져 가고 있습니다.

964
00:54:02,690 --> 00:54:04,973
3 개의 매개 변수 값에 의해 5 x 5,

965
00:54:04,973 --> 00:54:07,335
우리는 정확히 같은 방식으로이 슬라이드를합니다.

966
00:54:07,335 --> 00:54:11,502
활성화 맵을 얻으려면 전체 입력 볼륨에서

967
00:54:14,596 --> 00:54:17,668
좋아, 알다시피, 우리는 많은 세부 사항에 도달했다.

968
00:54:17,668 --> 00:54:20,592
이 콘볼 루션 레이어가 어떻게 생겼는지,

969
00:54:20,592 --> 00:54:22,372
그래서 지금 나는 간단히 간다.

970
00:54:22,372 --> 00:54:25,196
우리가 가지고있는 다른 층들을 통해

971
00:54:25,196 --> 00:54:28,802
이 전체 컨벌루션 네트워크를 형성합니다.

972
00:54:28,802 --> 00:54:31,071
그래, 다시 기억해, 우리는 길쌈 레이어

973
00:54:31,071 --> 00:54:33,365
때때로 풀링 레이어가 산재되어있다.

974
00:54:33,365 --> 00:54:36,653
이러한 비선형 성뿐만 아니라

975
00:54:36,653 --> 00:54:39,017
좋습니다. 그래서 풀링 레이어가하는 일은 무엇입니까?

976
00:54:39,017 --> 00:54:41,112
그들은 표현을한다는 것입니다.

977
00:54:41,112 --> 00:54:42,716
작고 관리하기 쉽고,

978
00:54:42,716 --> 00:54:45,107
그래서 우리는 이것에 대해 이전에

979
00:54:45,107 --> 00:54:48,683
누군가가 왜 우리가 원하는지에 대한 질문을했습니다.

980
00:54:48,683 --> 00:54:51,562
표현은 더 작다.

981
00:54:51,562 --> 00:54:54,919
그리고 이것은 다시 그것을 가지고 있습니다.

982
00:54:54,919 --> 00:54:58,343
그것은 당신이 마지막에 가지고있는
매개 변수의 수에 영향을 미칩니다.

983
00:54:58,343 --> 00:55:01,614
기본적으로 일부는 수행합니다.

984
00:55:01,614 --> 00:55:04,425
주어진 지역에 대한 불변성.

985
00:55:04,425 --> 00:55:05,830
풀링 레이어가하는 역할

986
00:55:05,830 --> 00:55:09,460
정확한 다운 샘플링을 수행하고 있는지,

987
00:55:09,460 --> 00:55:13,415
그리고 그것은 당신의 입력 볼륨을 필요로합니다.
예를 들어,

988
00:55:13,415 --> 00:55:17,762
224 by 224 by 64, 그리고이를
공간적으로 다운 샘플링합니다.

989
00:55:17,762 --> 00:55:20,861
결국 결국 112 점을 얻게 될 것입니다.

990
00:55:20,861 --> 00:55:23,429
그리고 이것이 아무 것도하지 않는다는
것을 알아 두는 것이 중요합니다.

991
00:55:23,429 --> 00:55:26,588
깊이, 오른쪽, 우리는 공간적으로 만 풀링하고 있습니다.

992
00:55:26,588 --> 00:55:30,168
따라서 입력 깊이의 수

993
00:55:30,168 --> 00:55:33,215
출력 깊이와 같을 것입니다.

994
00:55:33,215 --> 00:55:36,948
예를 들어,이를 수행하는 일반적인 방법은 최대 풀링입니다.

995
00:55:36,948 --> 00:55:41,317
따라서이 경우 풀링 레이어에는 필터 크기가 있습니다

996
00:55:41,317 --> 00:55:44,289
이 필터 크기는 지역이 될 것입니다.

997
00:55:44,289 --> 00:55:46,825
우리가 끝내고, 바로이 경우에

998
00:55:46,825 --> 00:55:50,562
우리가 두 개씩 두 개의 필터를 가지고
있다면 이것을 슬라이드 할 것입니다.

999
00:55:50,562 --> 00:55:53,572
그리고 여기, 우리는 또한이 경우에 두 가지를 보았습니다.

1000
00:55:53,572 --> 00:55:54,884
그래서 우리는이 필터를 사용하려고합니다.

1001
00:55:54,884 --> 00:55:58,999
우리는 입력 볼륨을 따라 슬라이드 할 것입니다.

1002
00:55:58,999 --> 00:56:01,672
우리가 컨볼 루션을했을 때와 똑같은 방식으로

1003
00:56:01,672 --> 00:56:03,619
그러나 여기에이 제품들을 사용하는 대신에,

1004
00:56:03,619 --> 00:56:06,205
우리는 단지 최대 값을 취한다.

1005
00:56:06,205 --> 00:56:08,338
해당 영역의 입력 볼륨

1006
00:56:08,338 --> 00:56:11,645
맞습니다. 여기 빨간색 값을 보면,

1007
00:56:11,645 --> 00:56:13,416
그 값은 6이 될 것입니다.

1008
00:56:13,416 --> 00:56:15,655
우리가 녹색을 보면 그것이 8을 줄 것입니다,

1009
00:56:15,655 --> 00:56:18,655
그리고 우리는 3과 4를가집니다.

1010
00:56:23,433 --> 00:56:24,931
네, 질문.

1011
00:56:24,931 --> 00:56:27,848
[말장난]

1012
00:56:29,010 --> 00:56:31,304
그래, 문제는 스트라이드를 설정하는
것이 일반적인 것인가하는 것입니다.

1013
00:56:31,304 --> 00:56:34,406
중복이 없도록?

1014
00:56:34,406 --> 00:56:36,850
그리고 네, 풀링 레이어는 그렇습니다.

1015
00:56:36,850 --> 00:56:38,196
나는 할 일이 더 많다고 생각한다.

1016
00:56:38,196 --> 00:56:41,256
그들에게 어떤 겹침도 갖지 말라는 것입니다.

1017
00:56:41,256 --> 00:56:44,688
너는 이것에 대해 생각할 수있는 방법을 생각해.

1018
00:56:44,688 --> 00:56:48,322
기본적으로 우리는 단지 다운 샘플을 원한다.

1019
00:56:48,322 --> 00:56:50,560
그래서이 지역을 좀 더 자세히 보아도 좋습니다.

1020
00:56:50,560 --> 00:56:52,977
이 영역을 나타 내기 위해 하나의 값을 얻습니다.

1021
00:56:52,977 --> 00:56:55,874
그리고 나서 다음 지역을 보게됩니다.

1022
00:56:55,874 --> 00:56:57,379
그래, 질문.

1023
00:56:57,379 --> 00:57:00,129
[희미한 말]

1024
00:57:02,415 --> 00:57:04,328
좋아, 그럼 질문은, 왜 최대 풀링인가

1025
00:57:04,328 --> 00:57:05,710
그냥 복용하는 것보다 낫다.

1026
00:57:05,710 --> 00:57:07,636
평균 풀링 같은 것을하고 있습니까?

1027
00:57:07,636 --> 00:57:10,058
그렇습니다. 평균 풀링과 같은 좋은 점입니다.

1028
00:57:10,058 --> 00:57:12,017
당신이 할 수있는 일이기도합니다.

1029
00:57:12,017 --> 00:57:15,417
최대 풀링이 일반적으로 사용되는 이유에 대한 직감

1030
00:57:15,417 --> 00:57:17,979
그것의 해석을 가질 수 있다는 것입니다,

1031
00:57:17,979 --> 00:57:21,471
알다시피, 이것이 사실이라면,
이것은 내 뉴런의 활성화입니다.

1032
00:57:21,471 --> 00:57:23,770
맞아. 그래서 각 가치는 종류가 비슷해.

1033
00:57:23,770 --> 00:57:26,972
이 뉴런이이 위치에서 얼마나 많이 발사 됐는지,

1034
00:57:26,972 --> 00:57:29,253
이 필터가이 위치에서 얼마나 많이 작동했는지.

1035
00:57:29,253 --> 00:57:31,927
그래서 당신은 맥스 풀링 (max pooling)을
말하는 것으로 생각할 수 있습니다.

1036
00:57:31,927 --> 00:57:36,094
이 필터가 얼마나 많이 작동했는지에
대한 신호를 제공합니다.

1037
00:57:37,000 --> 00:57:39,133
이 이미지의 어느 위치에서나.

1038
00:57:39,133 --> 00:57:41,264
맞습니다. 우리가 탐지를 생각하고 있다면,

1039
00:57:41,264 --> 00:57:44,022
알다시피, 인정하고,

1040
00:57:44,022 --> 00:57:46,535
이것은 당신이 말하는 곳에서
직관적으로 이해할 수 있습니다.

1041
00:57:46,535 --> 00:57:49,034
글쎄, 당신도 알다시피, 빛이든, 어떤면

1042
00:57:49,034 --> 00:57:52,206
당신이 찾고있는 이미지의

1043
00:57:52,206 --> 00:57:53,990
이 지역의 어느 곳에서나 일어날 지 여부

1044
00:57:53,990 --> 00:57:57,073
우리는 높은 가치로 발사하기를 원합니다.

1045
00:57:57,940 --> 00:57:59,129
문제.

1046
00:57:59,129 --> 00:58:02,046
[말장난]

1047
00:58:06,200 --> 00:58:08,746
응, 그래서 문제는 풀링과 보폭 때문이다.

1048
00:58:08,746 --> 00:58:10,959
둘 다 다운 샘플링의 동일한 효과를 가지며,

1049
00:58:10,959 --> 00:58:14,223
풀링 대신 스트라이드를 사용할 수 있습니까?

1050
00:58:14,223 --> 00:58:16,513
네, 그래서 실제로 저는 생각합니다.

1051
00:58:16,513 --> 00:58:19,771
보다 최근의 뉴럴 네트워크 아키텍처 살펴보기

1052
00:58:19,771 --> 00:58:23,103
사람들은 보폭을 더 많이 사용하기 시작했다.

1053
00:58:23,103 --> 00:58:27,704
풀링 대신에 다운 샘플링을하기 위해서입니다.

1054
00:58:27,704 --> 00:58:30,837
그리고 나는 이것이 마치 당신이 알고있는 것처럼,

1055
00:58:30,837 --> 00:58:32,801
당신이 할 수있는 부분적인 진보와 일도 좋아합니다.

1056
00:58:32,801 --> 00:58:36,968
하지만 실제로 이것은 약간의 의미가 있습니다.

1057
00:58:38,721 --> 00:58:41,892
그것을 사용하여 더 나은 결과를 얻는 더 좋은 방법입니다.

1058
00:58:41,892 --> 00:58:44,125
네, 보폭을 사용하는 것이 분명하다고 생각합니다.

1059
00:58:44,125 --> 00:58:47,292
당신은 그것을 할 수 있고 사람들은 그것을하고 있습니다.

1060
00:58:49,672 --> 00:58:52,505
좋아, 그럼 어디 보자.

1061
00:58:53,544 --> 00:58:56,553
그래, 그래, 그래서이 풀링 레이어와 함께,

1062
00:58:56,553 --> 00:59:00,358
다시 말하지만, 여러분이 디자인
선택을 할 수있는 권리가 있습니다.

1063
00:59:00,358 --> 00:59:04,057
이 입력 볼륨을 H만큼 D만큼 D만큼 취하면,

1064
00:59:04,057 --> 00:59:07,446
그런 다음 하이퍼 파라미터를 설정하려고합니다.

1065
00:59:07,446 --> 00:59:10,107
필터 크기의 설계 선택을위한

1066
00:59:10,107 --> 00:59:12,376
또는 풀링중인 공간 범위,

1067
00:59:12,376 --> 00:59:15,101
보폭뿐만 아니라 다시 계산할 수 있습니다.

1068
00:59:15,101 --> 00:59:18,676
사용한 방정식을 사용하여 출력량

1069
00:59:18,676 --> 00:59:21,325
이전에 회선을 위해, 그것은 여전히 여기, 오른쪽,

1070
00:59:21,325 --> 00:59:24,030
그래서 우리는 여전히 우리의 전체 범위를 가지고있다.

1071
00:59:24,030 --> 00:59:27,780
마이너스 필터 크기를 보폭 + 나눈 값으로 나눈 값입니다.

1072
00:59:30,880 --> 00:59:33,217
좋아요, 그래서 주목해야 할 또 하나의 것,

1073
00:59:33,217 --> 00:59:37,172
또한 일반적으로 사람들은 제로
패딩을 실제로 사용하지 않습니다.

1074
00:59:37,172 --> 00:59:39,647
당신이 그냥 시도하기 때문에 풀링 레이어

1075
00:59:39,647 --> 00:59:41,262
직접 다운 샘플링, 오른쪽,

1076
00:59:41,262 --> 00:59:43,003
그래서 이와 같은 문제는 없습니다.

1077
00:59:43,003 --> 00:59:44,423
코너에 필터 적용하기

1078
00:59:44,423 --> 00:59:47,045
필터의 일부가 입력 음량을 잃게됩니다.

1079
00:59:47,045 --> 00:59:49,526
그래서 우리는 보통 이것에 대해 걱정할 필요가 없습니다.

1080
00:59:49,526 --> 00:59:52,939
우리는 바로 직접 샘플링합니다.

1081
00:59:52,939 --> 00:59:56,304
풀링 레이어의 일반적인 설정

1082
00:59:56,304 --> 01:00:00,890
필터 크기는 2 또는 3 x 3 스트라이드입니다.

1083
01:00:00,890 --> 01:00:03,609
두명의 두명, 알다시피, 당신은 가질 수 있습니다.

1084
01:00:03,609 --> 01:00:06,269
또한 여전히 두 개씩 풀링 할 수 있습니다.

1085
01:00:06,269 --> 01:00:09,091
필터 크기가 3 x 3 인 경우에도,

1086
01:00:09,091 --> 01:00:10,789
나는 누군가가 일찌기 그것,

1087
01:00:10,789 --> 01:00:14,956
그러나 실제로는 2 개씩 2 개를 갖는 것이 일반적입니다.

1088
01:00:17,958 --> 01:00:21,527
자, 이제 우리는이 길쌈 층에 대해 이야기했습니다,

1089
01:00:21,527 --> 01:00:24,370
ReLU 층은 이전에 가지고 있던 층과 동일했습니다.

1090
01:00:24,370 --> 01:00:29,174
너는 알다시피, 기본 뉴럴 네트워크

1091
01:00:29,174 --> 01:00:31,492
우리가 마지막 강의에 관해서 이야기했던 것.

1092
01:00:31,492 --> 01:00:33,899
그래서 우리는 이것들을 섞어서 풀링 레이어를 만듭니다.

1093
01:00:33,899 --> 01:00:37,865
때때로 우리는 다운 샘플링과 같은 느낌을 갖습니다.

1094
01:00:37,865 --> 01:00:41,080
그리고 마지막으로 마지막은

1095
01:00:41,080 --> 01:00:43,766
우리는 완전히 연결된 계층을 원합니다.

1096
01:00:43,766 --> 01:00:46,210
그리고 이것은 정확히 똑같을 것입니다.

1097
01:00:46,210 --> 01:00:48,790
이전에 보았던 완전히 연결되어있는 레이어입니다.

1098
01:00:48,790 --> 01:00:50,506
그래서이 경우 지금 우리가하는 일

1099
01:00:50,506 --> 01:00:54,173
우리는 컨벌루션 네트워크 출력을 취하고 있으며,

1100
01:00:55,775 --> 01:00:57,503
마지막 층에는 약간의 음량이 있습니다.

1101
01:00:57,503 --> 01:01:00,421
그래서 우리는 어느 정도 깊이만큼 높이를 가질 것입니다.

1102
01:01:00,421 --> 01:01:01,626
우리는이 모든 것을 취합니다.

1103
01:01:01,626 --> 01:01:04,212
우리는 본질적으로 이것들을 스트레칭합니다.

1104
01:01:04,212 --> 01:01:06,322
그리고 이제 우리는 같은 종류의 것을 얻을 것입니다.

1105
01:01:06,322 --> 01:01:08,795
알다시피, 기본적으로 우리가 익숙한 1D 입력

1106
01:01:08,795 --> 01:01:12,962
바닐라 신경 회로망을 위해, 그리고 우리는

1107
01:01:14,153 --> 01:01:16,275
이 위에 완전히 연결된 레이어,

1108
01:01:16,275 --> 01:01:17,715
그래서 지금 우리는 연결을 할 것입니다.

1109
01:01:17,715 --> 01:01:21,715
이 컨벌루션 맵 출력들 모두에.

1110
01:01:22,676 --> 01:01:24,786
그래서 당신이 생각할 수있는 것은 기본적으로,

1111
01:01:24,786 --> 01:01:26,457
이제는 보존하는 대신에

1112
01:01:26,457 --> 01:01:28,616
우리가 공간 구조를 보존하기 전에,

1113
01:01:28,616 --> 01:01:30,897
맞아요.하지만 결국 마지막 층에서,

1114
01:01:30,897 --> 01:01:32,982
우리는이 모든 것을 하나로 모으고 싶다.

1115
01:01:32,982 --> 01:01:34,787
기본적으로 이성을 잃고 싶다.

1116
01:01:34,787 --> 01:01:37,081
이 모든 것은 우리가 전에했던 것처럼.

1117
01:01:37,081 --> 01:01:40,518
그래서 당신이 얻는 것은 우리의 것입니다.

1118
01:01:40,518 --> 01:01:43,185
우리가 이전에 가지고 있던 점수 산출물.

1119
01:01:45,744 --> 01:01:47,232
좋아, 그럼 ..

1120
01:01:47,232 --> 01:01:48,411
- [학생] 이것은 일종의 어리석은 질문입니다.

1121
01:01:48,411 --> 01:01:49,911
이 비주얼에 대해서.

1122
01:01:52,345 --> 01:01:56,123
맨 오른쪽에있는 16 픽셀과 마찬가지로,

1123
01:01:56,123 --> 01:02:00,357
그걸 해석해야하는 것처럼?

1124
01:02:00,357 --> 01:02:02,584
좋아요. 문제는 16 픽셀이란 무엇입니까?

1125
01:02:02,584 --> 01:02:04,238
맨 오른쪽에 있습니다.

1126
01:02:04,238 --> 01:02:05,888
- [학생] 그 칼럼처럼 -

1127
01:02:05,888 --> 01:02:07,566
- [강사] 오, 각 칸.

1128
01:02:07,566 --> 01:02:09,425
오른쪽 끝에있는 기둥, 네.

1129
01:02:09,425 --> 01:02:11,031
- [강사] 녹색 것입니까, 아니면 흑인입니까?

1130
01:02:11,031 --> 01:02:12,679
- [학생] 풀이라고 표시된 학생.

1131
01:02:12,679 --> 01:02:14,472
- 잠깐 만요, 수영장.

1132
01:02:14,472 --> 01:02:16,312
오, 알겠습니다. 그래, 질문은

1133
01:02:16,312 --> 01:02:20,566
예를 들어 수영장에서이 열을 어떻게 해석할까요?

1134
01:02:20,566 --> 01:02:24,645
여기에 우리가 보여주는 것은이 칼럼들 각각입니다.

1135
01:02:24,645 --> 01:02:28,376
출력 활성화 맵입니다, 오른쪽,

1136
01:02:28,376 --> 01:02:29,887
이 층들 중 하나의 출력.

1137
01:02:29,887 --> 01:02:34,028
그래서 처음부터 우리는 차를 가지고 있습니다.

1138
01:02:34,028 --> 01:02:35,465
컨벌루션 층 이후

1139
01:02:35,465 --> 01:02:37,795
이제 각 필터의 활성화 맵이 생겼습니다.

1140
01:02:37,795 --> 01:02:40,537
공간적으로 입력 이미지 위로 미끄러졌다.

1141
01:02:40,537 --> 01:02:42,484
그런 다음 ReLU를 통해 전달합니다.

1142
01:02:42,484 --> 01:02:45,306
그래서 거기에서 나오는 값을 볼 수 있습니다.

1143
01:02:45,306 --> 01:02:46,636
그리고 모든 것을 끝내고,

1144
01:02:46,636 --> 01:02:48,652
풀링 레이어로 얻는 것

1145
01:02:48,652 --> 01:02:51,850
정말로 복용하고 있다는 것입니다.

1146
01:02:51,850 --> 01:02:54,183
ReLU 층의 출력

1147
01:02:55,548 --> 01:02:58,270
그 직전에 온 것이고 그 다음에 풀링을합니다.

1148
01:02:58,270 --> 01:03:00,337
그래서 그것을 다운 샘플링하려고합니다.

1149
01:03:00,337 --> 01:03:01,711
맞아, 그 다음에 갈거야.

1150
01:03:01,711 --> 01:03:04,510
각 필터 위치의 최대치

1151
01:03:04,510 --> 01:03:06,548
그리고 이제이 풀 레이어 출력을 보면,

1152
01:03:06,548 --> 01:03:09,209
예를 들어, 마지막으로 언급 한 것과 같습니다.

1153
01:03:09,209 --> 01:03:11,704
이 ReLU 출력과 동일하게 보입니다.

1154
01:03:11,704 --> 01:03:15,871
단, 다운 샘플링이 가능하고, 이런 종류의

1155
01:03:17,311 --> 01:03:18,952
모든 공간 위치에서 최대 값

1156
01:03:18,952 --> 01:03:20,550
그래서 그것은 사소한 차이입니다

1157
01:03:20,550 --> 01:03:22,534
당신은 그 둘 사이에서 볼 것입니다.

1158
01:03:22,534 --> 01:03:25,451
[먼 거리]

1159
01:03:30,523 --> 01:03:32,559
그래서 질문은, 지금 이것은 다음과 같이 보입니다.

1160
01:03:32,559 --> 01:03:34,654
아주 적은 양의 정보, 바로,

1161
01:03:34,654 --> 01:03:36,991
그래서 그것을 여기에서 분류하는
것을 어떻게 알 수 있습니까?

1162
01:03:36,991 --> 01:03:39,553
그래서 당신이 이것에 대해 생각해야하는 방식입니다.

1163
01:03:39,553 --> 01:03:41,886
이 값들 각각

1164
01:03:43,365 --> 01:03:46,052
이 풀 출력 중 하나의 내부에는 실제로,

1165
01:03:46,052 --> 01:03:49,004
그것은 당신이 한 모든 프로세싱의 축적입니다.

1166
01:03:49,004 --> 01:03:50,696
이 전체 네트워크에서

1167
01:03:50,696 --> 01:03:53,890
따라서 계층 구조의 맨 위에 있습니다.

1168
01:03:53,890 --> 01:03:55,458
그래서 각각은 실제로 나타납니다.

1169
01:03:55,458 --> 01:03:57,602
종류의 높은 수준의 개념.

1170
01:03:57,602 --> 01:04:01,197
그래서 우리는 예전에 Hubel과
Wiesel을 보았습니다.

1171
01:04:01,197 --> 01:04:03,571
이러한 계층 적 필터를 구축하고,

1172
01:04:03,571 --> 01:04:07,466
하단에서 우리는 가장자리를 찾고 있습니다, 오른쪽,

1173
01:04:07,466 --> 01:04:10,257
또는 가장자리 같은 매우 단순한 구조와 같은 것들.

1174
01:04:10,257 --> 01:04:13,872
그래서 컨볼 루션 레이어 다음에

1175
01:04:13,872 --> 01:04:15,991
이 첫 번째 열에서 본 출력물

1176
01:04:15,991 --> 01:04:20,541
기본적으로 얼마나 구체적일까요? 예를 들어, 가장자리,

1177
01:04:20,541 --> 01:04:22,700
이미지의 다른 위치에서 발사하십시오.

1178
01:04:22,700 --> 01:04:25,268
그러나 당신이 지나갈 때 당신은 더 복잡해질 것입니다,

1179
01:04:25,268 --> 01:04:26,915
그것은 더 복잡한 것들을 찾고 있습니다. 맞습니다.

1180
01:04:26,915 --> 01:04:28,955
따라서 다음 컨볼 루션 계층

1181
01:04:28,955 --> 01:04:31,205
얼마나 많은 돈을 벌어들이 겠어?

1182
01:04:31,205 --> 01:04:34,674
어떤 종류의 구석이 이미지에 나타나면,

1183
01:04:34,674 --> 01:04:36,080
그것이 추론이기 때문에 그렇습니다.

1184
01:04:36,080 --> 01:04:37,957
그것의 입력은 원본 이미지가 아니며,

1185
01:04:37,957 --> 01:04:42,627
그것의 입력은 산출이다, 이미 가장자리지도, 맞다,

1186
01:04:42,627 --> 01:04:44,560
그래서 그것은 엣지지도 위에 추론하고 있습니다.

1187
01:04:44,560 --> 01:04:47,680
그래서 더 복잡해질 수 있습니다.

1188
01:04:47,680 --> 01:04:49,052
더 복잡한 것들을 탐지한다.

1189
01:04:49,052 --> 01:04:50,756
그래서 모든 걸 얻으실 때까지

1190
01:04:50,756 --> 01:04:53,212
이 마지막 풀링 레이어로, 각 값은

1191
01:04:53,212 --> 01:04:57,379
비교적 복잡한 종류의 템플릿이
얼마나 많이 발사되고 있는지.

1192
01:04:58,765 --> 01:05:01,613
맞아요, 그리고 그 때문에 지금 당신은 할 수 있습니다.

1193
01:05:01,613 --> 01:05:04,460
완전히 연결된 레이어는 집계 중입니다.

1194
01:05:04,460 --> 01:05:07,228
이 모든 정보를 종합 해 보면,

1195
01:05:07,228 --> 01:05:10,511
알다시피, 당신 수업의 점수.

1196
01:05:10,511 --> 01:05:13,134
그래서이 값들 각각은 얼마나 많은가?

1197
01:05:13,134 --> 01:05:17,051
꽤 복잡한 복잡한 개념이 발사되고 있습니다.

1198
01:05:19,043 --> 01:05:20,460
문제.

1199
01:05:20,460 --> 01:05:23,239
[희미한 말]

1200
01:05:23,239 --> 01:05:24,744
그래서 질문은, 언제 당신이 일을했는지 아십니까?

1201
01:05:24,744 --> 01:05:27,296
분류를하기에 충분한 풀링?

1202
01:05:27,296 --> 01:05:30,722
그리고 대답은 당신이 시도하고 보는 것입니다.

1203
01:05:30,722 --> 01:05:34,639
그래서 실제로, 이것들은 모두 디자인 선택입니다.

1204
01:05:34,639 --> 01:05:37,430
당신은 이것에 대해 조금 직관적으로 생각할 수 있습니다.

1205
01:05:37,430 --> 01:05:41,203
맞아, 네가 수영장에 가고
싶어하지만 너가 너무 많이 갇히면

1206
01:05:41,203 --> 01:05:43,585
당신은 가치가 거의 없을 것입니다.

1207
01:05:43,585 --> 01:05:45,960
귀하의 전체 이미지를 나타내는 등,

1208
01:05:45,960 --> 01:05:47,701
그래서 그것은 단순한 종류의 거래입니다.

1209
01:05:47,701 --> 01:05:50,581
합리적인 것 대 사람들이 시도한 것

1210
01:05:50,581 --> 01:05:52,290
다양한 구성

1211
01:05:52,290 --> 01:05:54,614
그래서 당신은 아마 유효성 검사를 할 것입니다. 맞습니다.

1212
01:05:54,614 --> 01:05:57,049
다른 풀링 크기를 시도해보십시오.

1213
01:05:57,049 --> 01:05:59,492
다른 필터 크기, 다른 레이어 수,

1214
01:05:59,492 --> 01:06:02,926
당신의 문제에 가장 적합한 것이 무엇인지보십시오. 예,

1215
01:06:02,926 --> 01:06:05,350
다른 데이터가있는 모든 문제가 그러 하듯이,

1216
01:06:05,350 --> 01:06:07,423
알다시피, 이런 종류의 다른 세트

1217
01:06:07,423 --> 01:06:10,340
하이퍼 매개 변수가 가장 잘 작동 할 수 있습니다.

1218
01:06:13,388 --> 01:06:16,836
좋아, 마지막으로, 너희들을 가리키고 싶었어.

1219
01:06:16,836 --> 01:06:19,753
ConvNet을 훈련하는 데모로,

1220
01:06:21,171 --> 01:06:24,143
앙드레 카르 파티 (Andre Karpathy)

1221
01:06:24,143 --> 01:06:26,424
이 클래스의 작성자.

1222
01:06:26,424 --> 01:06:28,755
그래서 그는이 데모를 썼습니다.

1223
01:06:28,755 --> 01:06:33,000
기본적으로 CIFAR-10에서
ConvNet을 교육 할 수 있습니다.

1224
01:06:33,000 --> 01:06:35,874
이전에 보았던 데이터 세트, 바로
10 개의 클래스로 구성됩니다.

1225
01:06:35,874 --> 01:06:39,341
이 데모에 대한 좋은 점은 할 수 있다는 것입니다.

1226
01:06:39,341 --> 01:06:42,014
그것은 기본적으로 각각의 필터

1227
01:06:42,014 --> 01:06:44,260
활성화 맵이 어떻게 생겼는지 보입니다.

1228
01:06:44,260 --> 01:06:46,137
그래서 이전에 보여 줬던 몇몇 이미지들

1229
01:06:46,137 --> 01:06:47,835
이 데모에서 가져 왔습니다.

1230
01:06:47,835 --> 01:06:50,048
그래서 당신은 그것을 밖으로 시도하고, 그것으로 놀고,

1231
01:06:50,048 --> 01:06:52,640
그리고 너도 알다시피, 그냥 가서 감각을 익히 라.

1232
01:06:52,640 --> 01:06:55,268
이 활성화지도가 어떻게 생겼는지.

1233
01:06:55,268 --> 01:06:57,134
그리고 주목할 사실은,

1234
01:06:57,134 --> 01:07:00,578
일반적으로 첫 번째 레이어 활성화 맵은

1235
01:07:00,578 --> 01:07:01,709
너는 그걸 해석 할 수있어. 맞아.

1236
01:07:01,709 --> 01:07:03,606
그들은 입력 이미지에서 직접 작동하기 때문에

1237
01:07:03,606 --> 01:07:05,532
그래서 당신은이 템플릿들이 의미하는 것을 볼 수 있습니다.

1238
01:07:05,532 --> 01:07:07,784
당신이 더 높은 수준의 레이어에 도착하면

1239
01:07:07,784 --> 01:07:08,975
정말 열심히 시작합니다.

1240
01:07:08,975 --> 01:07:11,163
어떻게 이런 의미를 실제로 해석합니까?

1241
01:07:11,163 --> 01:07:13,877
그래서 대부분 해석하기가 어렵습니다.

1242
01:07:13,877 --> 01:07:15,398
그래서 너는 걱정하지 말아야한다.

1243
01:07:15,398 --> 01:07:17,535
당신이 정말로 무슨 일이 일어나고
있는지 이해할 수 없다면.

1244
01:07:17,535 --> 01:07:19,604
그러나 전체 흐름을 보는 것만으로도 여전히 좋습니다.

1245
01:07:19,604 --> 01:07:22,271
그리고 어떤 산출물이 나오고 있는지.

1246
01:07:23,985 --> 01:07:27,313
좋습니다. 요약하면, 오늘 우리는

1247
01:07:27,313 --> 01:07:29,946
길쌈 신경 네트워크가 작동하는 방식,

1248
01:07:29,946 --> 01:07:31,257
그들이 기본적으로 스택 인 방법

1249
01:07:31,257 --> 01:07:34,204
이러한 길쌈 및 풀링 계층

1250
01:07:34,204 --> 01:07:38,291
끝에 완전히 연결된 층이 뒤 따른다.

1251
01:07:38,291 --> 01:07:40,940
작은 필터를 사용하는 경향이있었습니다.

1252
01:07:40,940 --> 01:07:44,069
더 심오한 아키텍처에 대해 더 자세히 이야기하겠습니다.

1253
01:07:44,069 --> 01:07:47,364
나중에 이들 중 일부에 대한
사례 연구에 대해 알아보십시오.

1254
01:07:47,364 --> 01:07:49,576
또한 이러한 문제를 없애는 경향이 있습니다.

1255
01:07:49,576 --> 01:07:52,215
풀링 및 완전히 연결된 레이어.

1256
01:07:52,215 --> 01:07:55,275
따라서 이것을 유지하는 것만으로도,

1257
01:07:55,275 --> 01:07:57,391
Conv 레이어의 매우 깊은 네트워크,

1258
01:07:57,391 --> 01:08:01,058
다시 한번 우리는이 모든 것을 나중에 논의 할 것입니다.

1259
01:08:01,898 --> 01:08:04,591
그리고 전형적인 아키텍처는 다시 이렇게 보입니다.

1260
01:08:04,591 --> 01:08:06,300
너도 알다시피, 우리가 이전에했던 것처럼.

1261
01:08:06,300 --> 01:08:08,964
Conv, ReLU 일부 N 단계

1262
01:08:08,964 --> 01:08:10,821
그 다음에 한 번씩 풀장이 뒤 따랐다.

1263
01:08:10,821 --> 01:08:13,197
이 모든 일이 몇 번 반복되었습니다.

1264
01:08:13,197 --> 01:08:16,314
그 다음에는 완전히 연결된 ReLU 층

1265
01:08:16,314 --> 01:08:18,987
우리가 이전에 보았던 것, 당신도 알다시피, 하나 또는 둘

1266
01:08:18,987 --> 01:08:20,287
또는 이들 중 단지 몇 가지 만,

1267
01:08:20,287 --> 01:08:24,060
그리고 나서 당신의 학급 점수를위한
softmax가 끝납니다.

1268
01:08:24,060 --> 01:08:26,100
그래서 몇 가지 일반적인 값이 있습니다.

1269
01:08:26,100 --> 01:08:29,183
당신은 N 개까지 5 개까지 가질 수 있습니다.

1270
01:08:30,408 --> 01:08:33,144
꽤 깊은 층이 생길거야.

1271
01:08:33,145 --> 01:08:36,759
Conv, ReLU, 풀 시퀀스 및 일반적으로

1272
01:08:36,759 --> 01:08:39,701
끝에 완전히 연결된 두 개의 레이어 만 있습니다.

1273
01:08:39,701 --> 01:08:42,221
그러나 우리는 또한 새로운 아키텍처로 갈 것입니다.

1274
01:08:42,221 --> 01:08:45,895
ResNet 및 GoogLeNet과 같이

1275
01:08:45,895 --> 01:08:49,755
꽤 다른 유형의 아키텍처를 제공 할 것입니다.

1276
01:08:49,756 --> 00:00:00,000
좋아요, 다음에 너희들을 만나서 고맙다.

