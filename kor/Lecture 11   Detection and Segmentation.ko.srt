1
00:00:08,691 --> 00:00:15,429
안녕하세요. 진행하도록 하겠습니다.
CS231N 11강입니다.

2
00:00:15,430 --> 00:00:23,259
오늘은 Detection과 Segmentation등 아주 재미있는
Compuver Vision task들을 소개해 드리겠습니다.

3
00:00:17,859 --> 00:00:20,038


4
00:00:20,038 --> 00:00:23,259


5
00:00:23,259 --> 00:00:25,590
수업에 앞서 공지사항을 전달해 드리겠습니다.

6
00:00:25,590 --> 00:00:31,358
지난 시간에는 수업 대신 중간고사가 있었습니다.

7
00:00:29,356 --> 00:00:31,358


8
00:00:31,358 --> 00:00:42,270
하지만 아직은 시험에 관한 이야기들은
삼가해 주시기 바랍니다.

9
00:00:34,379 --> 00:00:37,059


10
00:00:37,059 --> 00:00:39,369


11
00:00:39,369 --> 00:00:42,270


12
00:00:42,270 --> 00:00:48,518
아직 시험을 치루지 않은 학생들이 있습니다.

13
00:00:45,054 --> 00:00:47,059


14
00:00:47,059 --> 00:00:48,518


15
00:00:48,518 --> 00:00:53,668
중간고사 문제와 관련된 이야기들은
되도록이면 삼가해 주시기 바랍니다.

16
00:00:50,918 --> 00:00:53,668


17
00:00:56,329 --> 00:01:02,062
월요일까지만 참아주세요. [웃음]
좋습니다.

18
00:00:57,790 --> 00:01:00,040


19
00:01:00,899 --> 00:01:02,921


20
00:01:02,921 --> 00:01:07,761
현재 중간고사 채점을 진행 중입니다.
최대한 빨리 결과를 알려드리도록 하겠습니다.

21
00:01:04,971 --> 00:01:06,698


22
00:01:06,698 --> 00:01:07,761


23
00:01:07,761 --> 00:01:14,079
그리고 과제 2도 채점 중입니다. 이번 주에 거의 완료된
상태입니다. TA들이 아주 바쁩니다.

24
00:01:10,289 --> 00:01:11,980


25
00:01:11,980 --> 00:01:14,079


26
00:01:14,079 --> 00:01:18,479
그리고 모두들 프로젝트를 열심히 진행해 주시고 있습니다.

27
00:01:16,511 --> 00:01:18,479


28
00:01:18,479 --> 00:01:26,970
그리고 대부분 중간고사를 마쳤으므로 화요일까지는
프로젝트 마일스톤을 제출해 주시기 바랍니다.

29
00:01:21,679 --> 00:01:25,460


30
00:01:25,460 --> 00:01:26,970


31
00:01:26,970 --> 00:01:31,650
일부 인원들이 프로젝트 계획서 제출 이후에 프로젝트를
변경하거나, 팀원이 변경된 경우가 있었습니다.

32
00:01:28,650 --> 00:01:31,650


33
00:01:31,650 --> 00:01:39,677
이번에 제출하는 마일스톤에 남은 학기동안
진행할 사항들을 반드시 반영시켜 주시기 바랍니다.

34
00:01:34,499 --> 00:01:35,739


35
00:01:35,739 --> 00:01:37,220


36
00:01:37,220 --> 00:01:39,677


37
00:01:39,677 --> 00:01:43,900
그리고 Piazza에 많은 분들이 과제2에 대한 문의를 남겨주셨습니다.

38
00:01:41,519 --> 00:01:43,900


39
00:01:43,900 --> 00:01:50,189
과제3은 작년과 내용이 조금 변경되어
현재 열심히 작업 중에 있습니다.

40
00:01:46,900 --> 00:01:48,479


41
00:01:48,479 --> 00:01:50,189


42
00:01:50,189 --> 00:01:53,951
과제3은 아마도 오늘, 내일쯤 확인하실 수 있을 것입니다.

43
00:01:51,940 --> 00:01:53,951


44
00:01:53,951 --> 00:02:01,551
과제 제출기한은 출제일로부터 2주이므로
너무 걱적하지 않으셔도 됩니다.

45
00:01:56,351 --> 00:01:57,580


46
00:01:57,580 --> 00:02:01,551


47
00:02:01,551 --> 00:02:09,079
과제3에는 아주 흥미 진진한 내용들이 담겨 있으니
기대해 주시기 바랍니다.

48
00:02:03,170 --> 00:02:05,318


49
00:02:05,318 --> 00:02:09,079


50
00:02:09,079 --> 00:02:13,340
그리고 지난 강의에 "Train Game" 에 대해
말씀드린 적이 있습니다.

51
00:02:11,591 --> 00:02:13,340


52
00:02:13,340 --> 00:02:17,780
Train Game은 우리가 현재 Side Project로 진행중입니다.

53
00:02:15,380 --> 00:02:17,780


54
00:02:17,780 --> 00:02:27,340
Train Game은 실제 문제를 가지고 하이퍼파라미터를 조정하면서
학습시켜 볼 수 있는 반응형 도구입니다.

55
00:02:20,751 --> 00:02:24,391


56
00:02:24,391 --> 00:02:27,340


57
00:02:27,340 --> 00:02:37,963
Train Game을 하는 것이 필수는 아니지만 참여자에 한해서
소량의 extra credit을 지급할 예정입니다.

58
00:02:30,250 --> 00:02:33,119


59
00:02:33,119 --> 00:02:35,072


60
00:02:35,072 --> 00:02:37,963


61
00:02:37,963 --> 00:02:42,224
자세한 사항은 오후 중으로 Piazza에 다시 공지하겠습니다.

62
00:02:39,894 --> 00:02:42,224


63
00:02:42,224 --> 00:02:51,752
Train Game이 무엇인지 간략한 Demo를 보여드리겠습니다.
실제 이름은 HyperQuest 입니다.

64
00:02:45,123 --> 00:02:48,362


65
00:02:48,362 --> 00:02:51,752


66
00:02:51,752 --> 00:02:54,464
모델에 가장 적합한 하이퍼파라미터를
찾는 게임이라서 붙혀진 이름입니다.

67
00:02:54,464 --> 00:03:01,254
HyperQuest은 브라우저 환경에서 하이퍼파라미터를
조절해가며 모델을 학습시킬 수 있는 아주 멋있는 툴입니다.

68
00:02:56,523 --> 00:02:59,344


69
00:02:59,344 --> 00:03:01,254


70
00:03:01,254 --> 00:03:04,871
우선 Student ID와 Name으로 로그인합니다.

71
00:03:04,871 --> 00:03:08,830
그리고 여러분의 딥러닝 지식에 관한
몇 가지 설문조사를 수행합니다.

72
00:03:06,693 --> 00:03:08,830


73
00:03:08,830 --> 00:03:16,152
그리고 몇 가지 지시사항이 있습니다.
여러분은 매 시도마다 임의의 데이터를 부여받습니다.

74
00:03:11,747 --> 00:03:14,934


75
00:03:14,934 --> 00:03:16,152


76
00:03:16,152 --> 00:03:21,494
데이터셋은 이미지이거나 벡터일 수도 있습니다.
HyperQuest에서 여러분이 해야 할 일은

77
00:03:19,214 --> 00:03:21,494


78
00:03:21,494 --> 00:03:28,077
적절한 하이퍼파라미터를 선택해서 모델을 학습시키고
Tran/Validation Set의 성능을 높히는 것입니다.

79
00:03:23,792 --> 00:03:25,632


80
00:03:25,632 --> 00:03:28,077


81
00:03:28,077 --> 00:03:33,423
Leaderboard도 제공됩니다.

82
00:03:29,926 --> 00:03:31,382


83
00:03:31,382 --> 00:03:33,423


84
00:03:33,423 --> 00:03:38,723
게임을 시작하면 여러분께 데이터셋에 관련된
간단한 통계들을 알려드립니다.

85
00:03:36,054 --> 00:03:38,723


86
00:03:38,723 --> 00:03:42,397
가령 이 예제에서는 클래스 10개를 분류해야하는
Classification 문제입니다.

87
00:03:41,064 --> 00:03:42,397


88
00:03:43,424 --> 00:03:47,774
아래 임의의 데이터셋에 대한 통계가 있습니다.
클래스는 10가지 이고

89
00:03:45,323 --> 00:03:47,774


90
00:03:47,774 --> 00:03:52,987
그리고 입력 데이터는 3 x 32 x 32 이미지 데이터입니다.

91
00:03:50,094 --> 00:03:52,987


92
00:03:52,987 --> 00:03:58,832
그리고 Train Set 8500개
Validation Set 1500 개가 있습니다.

93
00:03:55,854 --> 00:03:58,832


94
00:03:58,832 --> 00:04:01,518
이들은 모두 임의로 결정되며
매번 조금씩 바뀝니다.

95
00:03:59,870 --> 00:04:01,518


96
00:04:01,518 --> 00:04:08,931
이 통계정보를 바탕으로 Init Learning rate, 네트워크 사이즈,
Init Dropout rate를 결정하시면 됩니다.

97
00:04:04,054 --> 00:04:06,912


98
00:04:06,912 --> 00:04:08,931


99
00:04:08,931 --> 00:04:13,811
설정이 끝나면 이 화면을 보실 수 있습니다.
모델은 선택된 하이퍼파라미터로 1 epoch 학습이 진행됩니다.

100
00:04:11,480 --> 00:04:13,811


101
00:04:13,811 --> 00:04:21,040
그리고 오른쪽 아래에 두 개의 그래프가 있습니다.
하나는 1 epoch 이후의 Train/Validation Loss 입니다.

102
00:04:17,822 --> 00:04:19,712


103
00:04:19,712 --> 00:04:21,040


104
00:04:21,040 --> 00:04:23,409
그리고 다른 하나는 Train/Validation Accuracy 입니다.

105
00:04:23,409 --> 00:04:26,280
그 첫 번째 신기원과 당신이 본 격차에 근거하여

106
00:04:26,280 --> 00:04:32,290
이 그래프를 보시고 다음 Epoch의 하이퍼 파라미터를
적절하게 조절하시면 되겠습니다.

107
00:04:28,651 --> 00:04:30,759


108
00:04:30,759 --> 00:04:32,290


109
00:04:32,290 --> 00:04:37,803
분석이 끝나면 기존의 또는 변경된 하이퍼파라미터로
학습을 계속 진행시키거나

110
00:04:35,142 --> 00:04:37,803


111
00:04:37,803 --> 00:04:43,872
학습을 끝내거나, 혹은 뭔가 잘못 되었을 시에
기존의 체크포인트로 돌아갈 수도 있습니다.

112
00:04:40,363 --> 00:04:41,523


113
00:04:41,523 --> 00:04:43,872


114
00:04:43,872 --> 00:04:54,971
학습을 계속한다면, 이런 식으로 다음 Epoch을 위해서
하이퍼파라미터를 새롭게 조정할 수 있습니다.

115
00:04:46,238 --> 00:04:48,691


116
00:04:48,691 --> 00:04:51,347


117
00:04:51,347 --> 00:04:54,971


118
00:04:54,971 --> 00:05:01,899
그리고 흥미롭게도 학습 도중에도
네트워크의 크기를 조절할 수 있습니다.

119
00:04:56,730 --> 00:04:59,808


120
00:04:59,808 --> 00:05:01,899


121
00:05:01,899 --> 00:05:07,562
여기에는 최근 발표된 논문들의 기법들이 적용되었습니다.

122
00:05:05,592 --> 00:05:07,562


123
00:05:07,562 --> 00:05:15,762
학습 도중에 네트워크의 기존 레이어들은 유지시키고
레이어를 넓히거나 새로운 레이어를 추가할 수 있습니다.

124
00:05:09,902 --> 00:05:12,083


125
00:05:12,083 --> 00:05:15,762


126
00:05:15,762 --> 00:05:20,131
이 방법을 이용해서 여러분도 학습 도중에
네트워크의 크기를 변경할 수 있습니다.

127
00:05:17,763 --> 00:05:20,131


128
00:05:20,131 --> 00:05:33,072
학습이 끝나면 최종 Validation accuracy가 leaderboard
에 기록되며, baseline models과 비교해 보실 수 있습니다.

129
00:05:22,371 --> 00:05:24,430


130
00:05:24,430 --> 00:05:26,811


131
00:05:26,811 --> 00:05:29,912


132
00:05:29,912 --> 00:05:33,072


133
00:05:33,072 --> 00:05:39,774
HyperQuest에 참여하여 Leaderboard에 좋은 성능을 기록하시면
소량의 Extra credit을 드릴 예정입니다.

134
00:05:35,380 --> 00:05:37,534


135
00:05:37,534 --> 00:05:39,774


136
00:05:39,774 --> 00:05:42,322
다시한번 말씀드리지만 선택사항입니다.

137
00:05:42,322 --> 00:05:49,243
하지만 여러분에게는 하이퍼파라미터가 학습과정에 어떤 영향을
미치는지 알아볼 수 있는 아주 좋은 경험이 될 것입니다.

138
00:05:44,632 --> 00:05:46,936


139
00:05:46,936 --> 00:05:49,243


140
00:05:49,243 --> 00:05:54,872
이는 저희에게도 도움이 됩니다.
여러분이 실험이 참여하므로써 얻는 것들이 있습니다.

141
00:05:50,742 --> 00:05:54,872


142
00:05:54,872 --> 00:06:04,422
저희는 사람들이 Neural Network를 학습시킬 때의 행동양상에
대한 관심이 많습니다. 여러분들의 결정이 도움이 될 것입니다.

143
00:05:57,662 --> 00:06:02,101


144
00:06:02,101 --> 00:06:04,422


145
00:06:04,422 --> 00:06:08,462
하지만 선택사항입니다.

146
00:06:08,462 --> 00:06:10,295
질문 있으십니까?
[학생이 질문]

147
00:06:15,080 --> 00:06:20,272
질문은 "이 실험이 논문으로 쓰여지는지" 입니다.

148
00:06:16,670 --> 00:06:18,680


149
00:06:18,680 --> 00:06:20,272


150
00:06:20,272 --> 00:06:29,510
희망사항입니다. 하지만 아직 실험의 초기단계라서
장담할 수는 없습니다. 그렇게 됐으면 좋겠군요

151
00:06:22,800 --> 00:06:26,760


152
00:06:26,760 --> 00:06:29,510


153
00:06:33,240 --> 00:06:35,000
[웃음]

154
00:06:35,000 --> 00:06:37,971
질문은 "학습 도중에 어떻게 레이어를 추가하는지"
입니다.

155
00:06:36,571 --> 00:06:37,971


156
00:06:37,971 --> 00:06:45,291
지금 깊이있는 설명은 해드릴 수 없지만, 해당 논문은
Ian Goodfello가 저자로 들어간 Net2Net 이라는 논문입니다.

157
00:06:39,680 --> 00:06:43,552


158
00:06:43,552 --> 00:06:45,291


159
00:06:45,291 --> 00:06:48,240
또 하나는 Microsoft의 Network Morphism
이라는 논문입니다.

160
00:06:48,240 --> 00:06:52,407
이 두 논문을 보시면 어떻게 동작하는지
살펴보실 수 있을 것입니다.

161
00:06:53,680 --> 00:06:59,792
좋습니다. 이제 수업을 진행하겠습니다. 지난 시간에
Recurrent Neural Networks를 소개했습니다.

162
00:06:56,232 --> 00:06:58,152


163
00:06:58,152 --> 00:06:59,792


164
00:06:59,792 --> 00:07:03,032
RNN을 이용해서 다양한 문제를 시도해 볼 수 있었습니다.

165
00:07:01,359 --> 00:07:03,032


166
00:07:03,032 --> 00:07:07,192
"one to one", "one to many", "many to one",
그리고 "many to many" 가 있었죠

167
00:07:05,340 --> 00:07:07,192


168
00:07:07,192 --> 00:07:10,679
RNN은 Language modeling에 적용해 보았습니다.

169
00:07:10,679 --> 00:07:15,460
RNN으로 다양한 Character level Language model을
구축해서 다양한 시도를 해보았습니다.

170
00:07:12,965 --> 00:07:15,460


171
00:07:15,460 --> 00:07:20,571
그리고 수학 수식, 셰익스피어, C 소스코드
등을 샘플링해 보았습니다.

172
00:07:18,912 --> 00:07:20,571


173
00:07:20,571 --> 00:07:31,011
그리고 CNN feature extracter와 RNN Language
model 을 결합한 Image caption 문제도 살펴보았습니다.

174
00:07:22,752 --> 00:07:26,560


175
00:07:26,560 --> 00:07:28,491


176
00:07:28,491 --> 00:07:31,011


177
00:07:31,011 --> 00:07:36,040
그리고 다양한 종류의 RNN을 살펴보았습니다.
Vanilla RNN이 있었죠

178
00:07:33,680 --> 00:07:36,040


179
00:07:36,040 --> 00:07:42,331
Vanilla RNN은 문헌에 따라 Simple RNN 혹은
Elman RNN이라고도 합니다.

180
00:07:37,872 --> 00:07:40,158


181
00:07:40,158 --> 00:07:42,331


182
00:07:42,331 --> 00:07:44,997
그리고 LSTM도 배웠습니다.
Long Short Term Memory 입니다.

183
00:07:44,997 --> 00:07:59,443
수식은 복잡했지만, 이로인해 backprop 시 그레디언트를 더 잘
전달할 수 있었고 더 긴 시퀀스도 잘 학습할 수 있었습니다.

184
00:07:46,872 --> 00:07:50,102


185
00:07:50,102 --> 00:07:53,021


186
00:07:53,021 --> 00:07:56,022


187
00:07:56,022 --> 00:07:59,443


188
00:07:59,443 --> 00:08:03,982
오늘은 주제를 조금 바꿔보고자 합니다.
다양한 재미있는 Tasks를 소개해 드리고자합니다.

189
00:08:01,403 --> 00:08:03,982


190
00:08:03,982 --> 00:08:08,992
지금까지는 image classification 문제를 위주로 다뤘습니다.

191
00:08:06,963 --> 00:08:08,992


192
00:08:08,992 --> 00:08:13,262
오늘은 다양한 Computer Vision Tasks를 다뤄볼 예정입니다.

193
00:08:10,883 --> 00:08:13,262


194
00:08:13,262 --> 00:08:21,942
Segmentation, Localization, Detection 등 다양한
Computer Vision Tasks와

195
00:08:17,162 --> 00:08:19,542


196
00:08:19,542 --> 00:08:21,942


197
00:08:21,942 --> 00:08:25,494
이 문제들을 CNN으로 어떻게 접근해 볼 수
있을지에 대해서 다룰 예정입니다.

198
00:08:22,912 --> 00:08:25,494


199
00:08:25,494 --> 00:08:32,163
잠시 주제를 환기시켜 보겠습니다. 이 수업에서는 주로
Image Classification 문제를 다룹니다.

200
00:08:28,140 --> 00:08:29,552


201
00:08:29,552 --> 00:08:32,163


202
00:08:32,163 --> 00:08:36,583
입력 이미지가 들어오면 Deep Conv Net을 통과합니다.

203
00:08:33,770 --> 00:08:34,842


204
00:08:34,842 --> 00:08:36,583


205
00:08:36,583 --> 00:08:42,991
네트워크를 통과하면 Feature Vector가 나옵니다.
가령 AlexNet이나 VGG의 경우 4096 차원이었죠

206
00:08:39,014 --> 00:08:42,991


207
00:08:42,991 --> 00:08:50,568
그리고 최종 Fully Connected Layer는
1000개의 클래스 스코어를 나타냅니다.

208
00:08:44,798 --> 00:08:46,222


209
00:08:46,222 --> 00:08:47,750


210
00:08:47,750 --> 00:08:50,568


211
00:08:50,568 --> 00:08:55,660
이 예제에서는 1000개의 클래스는
ImageNet의 클래스를 의미하죠

212
00:08:52,861 --> 00:08:55,660


213
00:08:55,660 --> 00:09:05,083
다시 말해, 입력 이미지가 들어오면
"전체 이미지"가 속하는 카테고리가 출력입니다.

214
00:08:57,143 --> 00:08:59,080


215
00:08:59,080 --> 00:09:01,437


216
00:09:01,437 --> 00:09:05,083


217
00:09:05,083 --> 00:09:14,314
Image Classification은 아주 기본적인 Task입니다.
Deep Learning으로 더 흥미로운 것들이 가능할지 모릅니다.

218
00:09:08,241 --> 00:09:09,879


219
00:09:09,879 --> 00:09:11,686


220
00:09:11,686 --> 00:09:14,314


221
00:09:14,314 --> 00:09:21,515
오늘은 그 흥미로운 것들 중 몇 가지를 다룰 것입니다.
이들이 Deep Learning으로 어떻게 동작하는지 살펴보겠습니다.

222
00:09:16,466 --> 00:09:18,609


223
00:09:18,609 --> 00:09:21,515


224
00:09:21,515 --> 00:09:28,852
더 자세히 들어가기 전에
요약 슬라이드를 먼저 살펴보겠습니다.

225
00:09:25,017 --> 00:09:26,944


226
00:09:26,944 --> 00:09:28,852


227
00:09:28,852 --> 00:09:31,480
우선 Semantic Segmentation에 대해 배울 것입니다.

228
00:09:31,480 --> 00:09:35,153
그리고 Classification + Localization 과
Object Detection에 대해서도 알아보겠습니다.

229
00:09:33,847 --> 00:09:35,153


230
00:09:35,153 --> 00:09:39,086
그리고 Instance Segmentation도 간단하게 알아보겠습니다.

231
00:09:36,753 --> 00:09:39,086


232
00:09:39,967 --> 00:09:44,035
우선 Sementic Segmentation 입니다.

233
00:09:44,035 --> 00:09:52,567
Sementic Segmentation 문제에서는 입력은 이미지이고
출력으로 이미지의 모든 픽셀에 카테고리를 정합니다.

234
00:09:46,348 --> 00:09:49,847


235
00:09:49,847 --> 00:09:52,567


236
00:09:52,567 --> 00:09:58,327
가령 이 예제의 경우는 입력이 고양이입니다.
아주 귀엽네요

237
00:09:55,514 --> 00:09:58,327


238
00:09:58,327 --> 00:10:07,701
출력은 모든 픽셀에 대해서 그 픽셀이
"고양이, 잔디, 하늘, 나무, 배경"인지를 결정하는 것이죠

239
00:10:00,333 --> 00:10:04,517


240
00:10:04,517 --> 00:10:07,701


241
00:10:07,701 --> 00:10:11,922
Sementic Segmentation 에서도 Classification에서
처럼 카테고리가 있습니다.

242
00:10:09,490 --> 00:10:11,922


243
00:10:11,922 --> 00:10:22,674
하지만 다른 점은 Classification 처럼 이미지 전체에
카테고리 하나가 아니라 모든 픽셀에 카테고리가 매겨집니다.

244
00:10:13,829 --> 00:10:15,820


245
00:10:15,820 --> 00:10:19,569


246
00:10:19,569 --> 00:10:22,674


247
00:10:22,674 --> 00:10:27,340
semantic segmentation은 개별 객체를
구분하지 않습니다.

248
00:10:25,086 --> 00:10:27,340


249
00:10:27,340 --> 00:10:31,523
가령 오른쪽 이미지를 보면 암소(Cow) 두 마리가 보입니다.

250
00:10:29,769 --> 00:10:31,523


251
00:10:31,523 --> 00:10:39,741
Semantic Segmentation의 경우에는
픽셀의 카테고리만 구분해 줍니다.

252
00:10:34,031 --> 00:10:36,859


253
00:10:36,859 --> 00:10:39,741


254
00:10:39,741 --> 00:10:48,309
그렇기 때문에 암소 두마리를 구별할 수 없습니다.

255
00:10:41,747 --> 00:10:44,510


256
00:10:44,510 --> 00:10:46,840


257
00:10:46,840 --> 00:10:48,309


258
00:10:48,309 --> 00:10:51,782
"암소" 라고 레이블링된 픽셀 덩어리만 얻을 수 있습니다.

259
00:10:50,098 --> 00:10:51,782


260
00:10:51,782 --> 00:10:58,910
이는 Sementatic Segmentation의 단점입니다. 나중에 배울
Instance Segmentation이 이를 해결할 수 있습니다.

261
00:10:54,868 --> 00:10:56,625


262
00:10:56,625 --> 00:10:58,910


263
00:10:58,910 --> 00:11:02,882
 우선 Semantoic Segmentation을 살펴보겠습니다.

264
00:11:00,549 --> 00:11:02,882


265
00:11:04,437 --> 00:11:12,544
Semantic Segmentation 문제에 접근해볼 수 있는 방법
중 하나는 Classification을 통한 접근 입니다.

266
00:11:07,595 --> 00:11:09,340


267
00:11:09,340 --> 00:11:12,544


268
00:11:12,544 --> 00:11:17,755
Semantic Segmentation을 위해서
Sliding Window를 적용해볼 수 있습니다.

269
00:11:14,553 --> 00:11:17,755


270
00:11:17,755 --> 00:11:27,763
입력 이미지를 아주 작은 단위로 쪼갭니다.

271
00:11:21,076 --> 00:11:24,315


272
00:11:24,315 --> 00:11:27,763


273
00:11:27,763 --> 00:11:31,310
이 예제에서는 암소의 머리 주변에서
영역 세 개를 추출했습니다.

274
00:11:31,310 --> 00:11:36,564
이제는 이 작은 영역만을 가지고 Classification 문제를
푼다고만 생각해 보겠습니다.

275
00:11:33,705 --> 00:11:36,564


276
00:11:36,564 --> 00:11:41,246
해당 영역이 어떤 카테고리에 속하는지를 정하는 것이죠

277
00:11:39,086 --> 00:11:41,246


278
00:11:41,246 --> 00:11:51,083
이미지 한장을 분류하기 위해서 만든 모델을 이용해서
이미지의 작은 영역을 분류하게 하게도 할 수 있을 것입니다.

279
00:11:43,828 --> 00:11:46,752


280
00:11:46,752 --> 00:11:48,760


281
00:11:48,760 --> 00:11:51,083


282
00:11:51,083 --> 00:11:56,601
이 방법이 어느정도는 동작할 지 모르겠습니다만.
그렇게 좋은 방법은 아닙니다.

283
00:11:54,412 --> 00:11:56,601


284
00:11:56,601 --> 00:12:02,498
왜냐하면 비용이 엄청나게 크기 떄문입니다.

285
00:11:58,422 --> 00:12:02,498


286
00:12:02,498 --> 00:12:10,910
모든 픽셀에 대해서 작은 영역으로 쪼개고, 이 모든 영역을
forward/backward pass 하는 일은 상당히 비효율적입니다.

287
00:12:04,701 --> 00:12:07,319


288
00:12:07,319 --> 00:12:09,407


289
00:12:09,407 --> 00:12:10,910


290
00:12:10,910 --> 00:12:25,509
그리고 서로 다른 영역이라도 인접해 있으면 어느정도는 겹쳐있기
때문에 특징들을 공유할 수도 있을 것입니다.

291
00:12:14,437 --> 00:12:17,085


292
00:12:17,085 --> 00:12:20,476


293
00:12:20,476 --> 00:12:22,950


294
00:12:22,950 --> 00:12:25,509


295
00:12:25,509 --> 00:12:37,194
이렇게 영역을 분할하는 경우에도 영역들 끼리
공유할만한 특징들이 아주 많을 것입니다.

296
00:12:28,242 --> 00:12:30,611


297
00:12:30,611 --> 00:12:32,644


298
00:12:32,644 --> 00:12:34,742


299
00:12:34,742 --> 00:12:37,194


300
00:12:37,194 --> 00:12:41,896
이렇게 개별적으로 적용하는 방법은 아주 나쁜 방법입니다.

301
00:12:39,801 --> 00:12:41,896


302
00:12:41,896 --> 00:12:48,683
하지만 Semantic Segmentation을 하고자 할 때 가장
먼저 생각해 볼 수 있는 방법이라고 할 수 있습니다.

303
00:12:44,913 --> 00:12:48,683


304
00:12:48,683 --> 00:12:53,372
이전 보다는 개선된 방법이 있습니다.
 Fully Convolutional Network 입니다.

305
00:12:50,598 --> 00:12:53,372


306
00:12:53,372 --> 00:12:58,305
이미지 영역을 나누고 독집적으로 분류하는 방법이 아닙니다.

307
00:12:56,080 --> 00:12:58,305


308
00:12:58,305 --> 00:13:06,501
FC-Layer가 없고 Convolution Layer 구성된
네트워크를 상상해 볼 수 있습니다.

309
00:13:00,959 --> 00:13:03,604


310
00:13:03,604 --> 00:13:06,501


311
00:13:06,501 --> 00:13:20,605
3 x 3 zero padding 을 수행하는 Conv Layer들을
쌓아올면 이미지의 공간정보를 손실하지 않을 것입니다.

312
00:13:10,631 --> 00:13:12,633


313
00:13:12,633 --> 00:13:15,422


314
00:13:15,422 --> 00:13:17,843


315
00:13:17,843 --> 00:13:20,605


316
00:13:20,605 --> 00:13:29,622
이 네트워크의 출력 Tensor는  C x H x W 입니다.
C는 카테고리의 수입니다.

317
00:13:23,090 --> 00:13:27,184


318
00:13:27,184 --> 00:13:29,622


319
00:13:29,622 --> 00:13:38,127
이 출력 Tensor는 입력 이미지의 모든 픽셀 값에 대해
Classification Scores를 매긴 값입니다.

320
00:13:32,491 --> 00:13:34,734


321
00:13:34,734 --> 00:13:38,127


322
00:13:38,127 --> 00:13:43,014
이는 Conv Layer만 쌓아올린 네트워크를
이용해서 계산할 수 있을 것입니다.

323
00:13:40,144 --> 00:13:43,014


324
00:13:43,014 --> 00:13:50,558
이 네트워크를 학습시키려면 우선 모든 픽셀의 Classification
loss를 계산하고 평균 값을 취합니다.

325
00:13:44,571 --> 00:13:47,216


326
00:13:47,216 --> 00:13:50,558


327
00:13:50,558 --> 00:13:55,137
그리고 기존처럼 back propagation을 수행하면 됩니다.

328
00:13:52,718 --> 00:13:55,137


329
00:13:55,137 --> 00:13:55,970
질문있나요?

330
00:13:58,430 --> 00:14:01,179
질문은 "Training data를 어떻게 만드는지" 입니다.

331
00:13:59,728 --> 00:14:01,179


332
00:14:01,179 --> 00:14:06,899
아주 비용이 큰 작업입니다. 입력 이미지에 모든 필셀에
대해서 레이블링을 해야 합니다.

333
00:14:02,687 --> 00:14:04,366


334
00:14:04,366 --> 00:14:06,899


335
00:14:06,899 --> 00:14:14,613
사람들이 툴을 만들기도 합니다.
객체의 외관선만 그려주면 안을 채워넣는 식이죠

336
00:14:09,654 --> 00:14:11,831


337
00:14:11,831 --> 00:14:14,613


338
00:14:14,613 --> 00:14:17,604
하지만 일반적으로는 Train data를 만들거나 수집하는것은
비용이 상당이 큽니다.

339
00:14:16,104 --> 00:14:17,604


340
00:14:29,243 --> 00:14:31,357
질문은 "손실 함수를 어떻게 디자인하는지" 입니다.

341
00:14:31,357 --> 00:14:39,025
이 문제에서는 모든 픽셀을 Classification하는 것입니다.
따라서 출력의 모든 픽셀에 Cross entropy를 적용합니다.

342
00:14:34,328 --> 00:14:37,009


343
00:14:37,009 --> 00:14:39,025


344
00:14:39,025 --> 00:14:42,212
출력의 모든 픽셀에는 Ground Truth가 존재합니다.

345
00:14:40,739 --> 00:14:42,212


346
00:14:42,212 --> 00:14:48,143
출력의 모든 픽셀과 Ground Truth 간의
Cross Entropy를 계산합니다.

347
00:14:44,363 --> 00:14:45,793


348
00:14:45,793 --> 00:14:48,143


349
00:14:48,143 --> 00:14:52,739
이 값들을 모두 더하거나 평균화시켜서 Loss를 계산합니다.
또는 Mini-batch 단위로 계산할 수도 있습니다.

350
00:14:50,437 --> 00:14:52,739


351
00:14:52,739 --> 00:14:53,572
질문있나요?

352
00:15:18,548 --> 00:15:22,669
질문은 "모든 픽셀의 카테고리를 알고 있다고
가정하는지" 입니다.

353
00:15:24,804 --> 00:15:26,505


354
00:15:26,505 --> 00:15:28,008


355
00:15:28,008 --> 00:15:31,258
맞습니다. 모든 픽셀의 카테고리를 알고
있다는 가정이 있어야합니다.

356
00:15:31,258 --> 00:15:33,716
Image Classification 의 경우와 마찬가지입니다.

357
00:15:33,716 --> 00:15:41,357
Image Classification의 경우에도 클래스의 수가
10, 20, 100, 1000이다 이렇게 딱 정해져 있었습니다.

358
00:15:36,785 --> 00:15:39,466


359
00:15:39,466 --> 00:15:41,357


360
00:15:41,357 --> 00:15:50,077
Semantic Segmentation에서도
클래스의 수가 고정되어 있습니다.

361
00:15:45,910 --> 00:15:50,077


362
00:15:51,012 --> 00:16:00,562
이 모델은 하이퍼파라미터만 잘 조절해서 학습시켜주면
비교적 잘 동작합니다. 하지만 문제가 하나 있습니다.

363
00:15:53,927 --> 00:15:56,206


364
00:15:56,206 --> 00:15:58,853


365
00:15:58,853 --> 00:16:00,562


366
00:16:00,562 --> 00:16:09,574
이 네트워크의 경우에는 입력 이미지의 Spatial Size를
계속 유지시켜야 합니다. 그래서 비용이 아주 큽니다.

367
00:16:02,346 --> 00:16:05,120


368
00:16:05,120 --> 00:16:07,479


369
00:16:07,479 --> 00:16:09,574


370
00:16:09,574 --> 00:16:18,982
가령 Convolution의 채널이 64/128/256 일 수 있습니다.
이정도는 보통 네트워크게 아주 흔합니다.

371
00:16:12,500 --> 00:16:16,435


372
00:16:16,435 --> 00:16:18,982


373
00:16:18,982 --> 00:16:27,361
이 네트워크에 고해상도의 이미지가 입력으로 들어오면
게산량과 메모리가 엄청 커서 감당할 수 없을 것입니다.

374
00:16:21,394 --> 00:16:24,111


375
00:16:24,111 --> 00:16:25,849


376
00:16:25,849 --> 00:16:27,361


377
00:16:27,361 --> 00:16:31,304
실제로는 이렇게 생긴 네트워크를 보기는 힘듭니다.

378
00:16:29,252 --> 00:16:31,304


379
00:16:31,304 --> 00:16:40,592
대신에 이렇게 생긴 네트워크가 대부분입니다.
특징맵을 Downsampling/Upsampling 합니다.

380
00:16:33,526 --> 00:16:37,512


381
00:16:37,512 --> 00:16:39,277


382
00:16:39,277 --> 00:16:40,592


383
00:16:40,592 --> 00:16:44,614
Spatial resolution 전체를 가지고
Convolution 수행하기 보다는

384
00:16:42,490 --> 00:16:44,614


385
00:16:44,614 --> 00:16:48,997
Original Resolution 에서는
Conv layer는 소량만 사용합니다.

386
00:16:46,304 --> 00:16:48,997


387
00:16:48,997 --> 00:16:55,719
그리고 Max pooling, Stride Convolution 등으로
특징맵을 Downsample 합니다.

388
00:16:50,858 --> 00:16:53,991


389
00:16:53,991 --> 00:16:55,719


390
00:16:55,719 --> 00:16:59,338
convolutions in downsampling
을 반복합니다.

391
00:16:57,656 --> 00:16:59,338


392
00:16:59,338 --> 00:17:04,640
Classification Network와 구조가 유사해 보입니다.
하지만 차이점은

393
00:17:02,199 --> 00:17:04,640


394
00:17:04,640 --> 00:17:15,213
Image Classification 에서는 FC-Layer가 있었죠. 하지만
여기에서는 Spatial Resolution을 다시 키웁니다.

395
00:17:06,800 --> 00:17:09,346


396
00:17:09,346 --> 00:17:12,071


397
00:17:12,071 --> 00:17:15,213


398
00:17:15,214 --> 00:17:22,136
결국 다시 입력 이미지의 해상도와 같아집니다.
이 방법을 사용하면 계산 효율이 더 좋아집니다.

399
00:17:17,598 --> 00:17:20,614


400
00:17:20,614 --> 00:17:22,136


401
00:17:22,136 --> 00:17:29,749
이 방법을 통해 네트워크가 lower resolution을 처리하도록
하여 네트워크를 더 깊게 만들 수 있습니다.

402
00:17:24,176 --> 00:17:26,417


403
00:17:26,417 --> 00:17:29,749


404
00:17:29,749 --> 00:17:36,418
Convolutional Networks에서의
Downsampling에 대해서는 이미 본 적이 있으실 것입니다.

405
00:17:33,205 --> 00:17:36,418


406
00:17:36,418 --> 00:17:44,050
이미지의 Spatial Size를 줄이기 위한 Stried Conv 라던가
다양한 Pooling 들은 다뤄본 적이 있었습니다.

407
00:17:38,343 --> 00:17:41,180


408
00:17:41,180 --> 00:17:44,050


409
00:17:44,050 --> 00:17:46,040
하지만 upsampling은 처음입니다.

410
00:17:46,040 --> 00:17:51,476
아마도 여러분은 upsampling이 네트워크 안에서
어떻게 동작하는지 궁금하실 수도 있습니다.

411
00:17:49,107 --> 00:17:51,476


412
00:17:51,476 --> 00:17:55,875
네트워크의 특징맵의 사이즈를 키울 수 있는
전략이 무엇일까요?

413
00:17:53,833 --> 00:17:55,875


414
00:17:55,875 --> 00:17:59,208
질문 있나요?

415
00:18:07,316 --> 00:18:09,061
질문은 "Upsampling을 어떻게 하는지" 입니다.

416
00:18:09,061 --> 00:18:11,758
이에 대한 답은
앞으로 진행할 내용입니다.

417
00:18:10,330 --> 00:18:11,758


418
00:18:11,758 --> 00:18:13,263
[웃음]

419
00:18:13,263 --> 00:18:21,075
Upsampling 전량 중 하나는 unpooling 입니다.

420
00:18:17,197 --> 00:18:21,075


421
00:18:21,075 --> 00:18:23,379
Downsample에서의 pooling에는 average/max
pooling 등이 있었습니다. average pooling은

422
00:18:23,379 --> 00:18:26,187
또는 우리가 평균 풀링에 대해 말했을 때 최대 풀링

423
00:18:26,187 --> 00:18:30,389
Pooling 하는 각 지역에 해당하는 Receptive Field 내에
spatial average를 계산했습니다.

424
00:18:27,754 --> 00:18:30,389


425
00:18:30,389 --> 00:18:34,853
upsampling 방법 중에는
nearest neighbor unpooling이 있습니다.

426
00:18:32,765 --> 00:18:34,853


427
00:18:34,853 --> 00:18:43,853
왼쪽에 nearest neighbor unpooling 예제가 있습니다.
입력은 2x2 그리드이고 출력은 4x4 그리드입니다.

428
00:18:36,761 --> 00:18:39,090


429
00:18:39,090 --> 00:18:41,379


430
00:18:41,379 --> 00:18:43,853


431
00:18:43,853 --> 00:18:56,149
2x2 stride nearest neighbor unpooling은
해당하는 receptive field로 값을 그냥 복사합니다.

432
00:18:47,698 --> 00:18:50,461


433
00:18:50,461 --> 00:18:53,177


434
00:18:53,177 --> 00:18:56,149


435
00:18:56,149 --> 00:19:03,472
bed of nails unpooling이란 방법도 있습니다.
bed of nails upsampling이라고도 합니다.

436
00:19:03,472 --> 00:19:09,116
이 방법은 unpooling region에만 값을 복사하고
다른 곳에는 모두 0을 채워넣습니다.

437
00:19:09,116 --> 00:19:23,462
이 경우 하나의 요소를 제외하고 모두 0으로 만듭니다. 이
예제에서는 왼쪽 위에만 값이 있고 나머지는 0입니다.

438
00:19:23,463 --> 00:19:24,867
"a  bed of nails" 이라고 불리는 이유는

439
00:19:24,867 --> 00:19:33,559
zero region은 평평하고 non-zero
region은 바늘처럼 뾰족하게 값이 튀기 때문입니다.

440
00:19:33,560 --> 00:19:39,591
Max unpooling이란 방법이 있습니다.

441
00:19:39,591 --> 00:19:52,046
대부분의 네트워크는 대칭적인 경향이 있습니다.
Downsampling/Upsampling 의 비율이 대칭적입니다.

442
00:19:52,047 --> 00:20:06,139
Max unpooling 이라는 방법이 있습니다. 각
unpooling과 pooling을 연관짓는 방법입니다.

443
00:20:06,140 --> 00:20:16,464
downsampling시에는 Max pooling에
사용했던 요소들을 잘 기억하고 있어야 합니다.

444
00:20:16,465 --> 00:20:26,390
Upsampling 시에 bed of nails
upsampling과 유사하되 같은 자리에 값을 넣는 것이 아니라

445
00:20:26,391 --> 00:20:33,697
이전 Maxpooling에서 선택된 위치에 맞게 넣어줍니다.

446
00:20:33,697 --> 00:20:38,321
제가 설명을 잘 했는지 모르겠네요.
백문이 불여일견이죠. 그림한번 보면 이해가 가실 것입니다.

447
00:20:39,248 --> 00:20:42,388
Maxunpooling이 끝나고 남은 자리를 0으로 채워줍니다.

448
00:20:42,388 --> 00:20:48,256
정리하자면,  Low Resolution 특징 맵을
 High Resolution 특징 맵으로 만들어 주는 것인데

449
00:20:48,256 --> 00:20:54,964
이 때 Low Resolution 의 값들을
Maxpooling에서 선택된 위치로 넣어주는 것입니다.

450
00:20:56,871 --> 00:21:00,723
아주 흥미로운 방법입니다.

451
00:21:00,723 --> 00:21:02,056
질문있나요?

452
00:21:08,696 --> 00:21:11,801
질문은 "왜 이 방법이 좋은 아이디어이고
어떤 점에서 중요한지" 입니다.

453
00:21:11,801 --> 00:21:16,806
Semantic segmentation 에서는 모든 픽셀들의
클래스를 모두 잘 분류해야 합니다.

454
00:21:16,806 --> 00:21:23,708
예측한 Segmentation 결과에서 객체들간의
디테일한 경계가 명확할수록 좋습니다.

455
00:21:23,708 --> 00:21:31,782
하지만 Maxpooling을 하게되면 특징맵의 비균진성이 발생합니다.
(2 x 2 pooling 에서 어디에서 왔는지 모름, 공간정보를 잃음)

456
00:21:31,782 --> 00:21:44,363
Maxpooling 후의 특징 맵만 봐서는 이 값들이
Receptive field 중 어디에서 왔는지 알 수 없습니다.

457
00:21:45,253 --> 00:21:53,759
Unpool 시에 기존 Maxpool 에서 뽑아온 자리로 값을 넣어주면
공간 정보를 조금은 더 디테일하게 다룰 수 있습니다.

458
00:21:53,759 --> 00:21:59,051
Max pooling 에서 읽어버린 공간정보를
조금은 더 잘 유지하도록 도와주는 것입니다.

459
00:21:59,051 --> 00:21:59,884
질문 있나요?

460
00:22:10,883 --> 00:22:13,809
질문은 "Maxunpooling이  Backprop을
더 수월하게 해주는지" 입니다.

461
00:22:13,809 --> 00:22:21,009
이 방법이 Backprob dynamics를 크게 바꾸지는 않습니다.
Maxpool indices를 저장하는 비용이 그렇게 크지 않습니다.

462
00:22:21,009 --> 00:22:24,851
다른 것들이 비하면 아주 작습니다.

463
00:22:24,851 --> 00:22:29,566
또 다른 방법이 있습니다.
Transpose Convolution 입니다.

464
00:22:29,566 --> 00:22:34,724
지금까지 살펴봤던 Unpooling 방법은 다시한번 말씀드리면

465
00:22:34,724 --> 00:22:38,945
Bed of nails, Nearest Neighbor,
Max unpooling 까지 다뤘습니다.

466
00:22:38,945 --> 00:22:44,964
이 방법은은 "고정된 함수" 이고
별도로 학습을 시키지는 않습니다.

467
00:22:44,964 --> 00:22:47,404
하지만 Strided convolution의 경우는 어떨까요?

468
00:22:47,404 --> 00:22:54,423
Strided convolution은 어떤 식으로 Downsampling을
해야할지를 네트워크가 학습할 수 있습니다.

469
00:22:54,423 --> 00:23:02,534
이와 유사하게 Upsampling에서도 학습가능한 방법이 있습니다.
바로 Transpose convolution 입니다.

470
00:23:02,534 --> 00:23:08,068
특징 맵을 Upsampling 할 때 어떤 방식으로
할 지를 학습할 수 있습니다.

471
00:23:08,068 --> 00:23:13,262
이 또한 특수한 방식의 Convolution의 일종입니다.

472
00:23:13,262 --> 00:23:16,663
일반적인 3 x 3 (stride = 1, padding =  1)
Convolution Filter가 동작하는 방식을 다시한번 살펴봅시다.

473
00:23:16,663 --> 00:23:20,488
이런 평범한 Convoltion은 우리 수업에서
수도없이 살펴봤습니다.

474
00:23:20,488 --> 00:23:24,316
입력은 4x4 입니다. 출력도 4x4죠.

475
00:23:24,316 --> 00:23:29,721
3x3 필터가 있고 이미지와 내적을 수행합니다.
우선 이미지의 좌 상단 구석부터 시작해 봅시다.

476
00:23:29,721 --> 00:23:35,409
내적의 결과는
출력(4x4)의 좌 상단 코너의 값이 됩니다.

477
00:23:35,409 --> 00:23:39,388
이 연산은 이미지 전체에 대해서 반복합니다.

478
00:23:39,388 --> 00:23:44,688
자 그럼 Strided convolution을 살펴보겠습니다.
Strided Convolution도 아주 유사합니다.

479
00:23:44,688 --> 00:23:49,648
다만 입력이 4x4 이고 출력은 2x2 입니다.

480
00:23:49,648 --> 00:24:00,808
계산 방식은 기존과 유사합니다. 3x3 필터가 있고
이미지의 좌상단 구석에서부터 내적을 계산합니다.

481
00:24:00,808 --> 00:24:08,879
하지만 Strided convolution은
한 픽셀씩 이동하면서 계산하지 않습니다.

482
00:24:08,879 --> 00:24:16,961
출력에서 한 픽셀 씩 움직이려면
입력에서는 두 픽셀 씩 움직여야 합니다.

483
00:24:16,961 --> 00:24:23,361
"Stride = 2" 는 입력/출력에서 움직이는 거리 사이의
비율이라고 해석할 수 있습니다.

484
00:24:23,361 --> 00:24:32,495
따라서 Stride = 2인 Strided convolution은
"학습 가능한 방법" 으로 2배 downsampling 하는 것을 의미합니다.

485
00:24:32,495 --> 00:24:42,638
Transpose convolution은 반대의 경우입니다.
입력이 2x2 이고 출력이 4x4 입니다.

486
00:24:42,638 --> 00:24:46,904
그리고 Transpose convolution을 위한 연산은
조금 다르게 생겼습니다.

487
00:24:46,904 --> 00:24:56,074
여기에서는 내적을 수행하지 않습니다.
우선 입력 특징맵에서 값을 하나 선택합니다.(빨간색)

488
00:24:56,074 --> 00:25:00,856
예제에서 처럼 좌상단에서 뽑은 이
하나의 값은 스칼라 값입니다.

489
00:25:00,856 --> 00:25:06,767
이 스칼라 값을 필터와 곱합니다. (3x3)
그리고 출력의 3x3 영역에 그 값을 넣습니다.

490
00:25:06,767 --> 00:25:14,428
Transpose convolution에서는 필터와 입력의
내적을 계산하는 것이 아니라

491
00:25:14,428 --> 00:25:24,911
입력 값이 필터에 곱해지는 가중치의 역할을 합니다.
출력 값은 필터 * 입력(가중치) 입니다.

492
00:25:24,911 --> 00:25:36,703
그리고 Upsampling 시에는 입력에서 한 칸씩 움직이는 동안
출력에서는 두 칸찍 움직입니다.

493
00:25:36,703 --> 00:25:43,713
왼쪽 입력의 파란색은 스칼라 값입니다.
이 값을 필터에 곱합니다.

494
00:25:43,713 --> 00:25:49,048
이렇게 가중치가 곱해진 필터 값을
출력 값에 넣어줍니다.

495
00:25:49,048 --> 00:25:54,765
출력에서는 Transpose convolution 간에
Receptive Field가 겹칠 수 있습니다.

496
00:25:54,765 --> 00:26:00,143
이렇게 겹치는 경우에는
간단하게 두 값을 더해줍니다.

497
00:26:00,143 --> 00:26:07,931
이 과정을 반복해서 끝마치면
학습 가능한 upsampling을 수행한 것입니다.

498
00:26:07,931 --> 00:26:14,466
Spatial size를 키워주기 위해서
학습된  필터 가중치를 이용한 것이죠

499
00:26:15,609 --> 00:26:19,975
그런데, 이 방법(transepose convolution) 은
문헌에 따라서 부르는 이름이 다양합니다.

500
00:26:19,975 --> 00:26:24,153
간혹 "deconvolution" 이라는 이름이 붙기도 합니다.

501
00:26:24,153 --> 00:26:27,024
하지만 그 이름은 그닥 좋지 않다고 생각합니다.

502
00:26:27,024 --> 00:26:34,066
신호처리의 관점에서 deconvolution은
Convolution 연산의 역 연산을 의미합니다.

503
00:26:34,066 --> 00:26:39,945
하지만 Transpose convolution은 그런 연산이 아닙니다.
그럼에도 불구하고 deconvolution라는 용어가 자주 보입니다.

504
00:26:39,945 --> 00:26:44,121
딥러닝 관련 논문에서 deconvolution이라는
용어는 주의해서 이해하셔야 합니다.

505
00:26:44,121 --> 00:26:48,280
그리고 간혹 upconvolution이라고 부르기도 합니다.
귀여운 이름이군요

506
00:26:48,280 --> 00:26:51,490
"fractionally strided convolution"
이라고도 합니다.

507
00:26:51,490 --> 00:27:01,437
stdie를 input/output간의 크기의 비율로 생각하면
이 예제는 stride 1/2 convolution 이라고 볼 수 있습니다.

508
00:27:01,437 --> 00:27:04,869
input : output = 1 : 2 이기 때문입니다.

509
00:27:04,869 --> 00:27:09,311
"backwards strided convolution" 라고도 부릅니다.

510
00:27:09,311 --> 00:27:15,287
왜냐하면 transpose conv의 forward pass를
수학적으로 계산해보면

511
00:27:15,287 --> 00:27:20,030
일반 Convolution의 backward pass와
수식이 동일하기 때문입니다.

512
00:27:20,030 --> 00:27:28,698
처음 그 용어를 접하면 생소할 수도 있지만,
그것이 사실이기 때문에 그 용어를 쓰기도 합니다.

513
00:27:28,698 --> 00:27:36,923
Transpose Convolution의 구체적인 예시를 살펴봅시다.
이해를 돕기위해서 1D Example을 가져왔습니다.

514
00:27:36,923 --> 00:27:41,272
이 예제에서는 1차원에서
3x3 Transpose Convolution을 수행합니다.

515
00:27:41,272 --> 00:27:46,091
죄송합니다. 3x3 이 아니라
3x1 transpose convolution 입니다.

516
00:27:46,091 --> 00:27:50,211
필터에는 세 개의 숫자가 있습니다. (x, y, z)
입력은 두 개의 숫자가 있습니다. (a, b)

517
00:27:50,211 --> 00:27:58,060
출력 값을 계산해봅시다. 입력이 가중치로 쓰이고 필터에
곱해집니다. 그리고 이 값을 출력에 넣습니다.

518
00:27:58,060 --> 00:28:03,597
그리고 Receptive field가 겹치는 부분은
그냥 더해줍니다.

519
00:28:03,597 --> 00:28:12,253
여러분은 왜 이 연산이 Transpose Conv라는 이름이 붙었는지
궁금하실 수 있습니다. 왜 이 이름이 적절한 이름일까요?

520
00:28:12,253 --> 00:28:15,530
이 이름의 유래는 Convolution 연산 자체를
해석해보면 알 수 있습니다.

521
00:28:15,530 --> 00:28:21,902
Convolution 연산은 언제든 행렬 곱 연산으로
나타낼 수 있습니다.

522
00:28:21,902 --> 00:28:25,737
이 예제에서도 쉬운 1차원 예제를 들어보겠습니다.

523
00:28:25,737 --> 00:28:33,470
Convolution 필터인 vector x는 3개의 원소를 가지고있습니다.

524
00:28:34,497 --> 00:28:38,706
그리고 입력 vector는 4개의 원소
A,B,C,D 를 가지고 있습니다.

525
00:28:38,706 --> 00:28:47,869
여기에서는 3x1 stride 1 convolution연산을 수행할
것입니다. 이 과정은 행렬곱으로 나타낼 수 있습니다.

526
00:28:47,869 --> 00:28:54,781
Convolution kernel x를
대문자 X 처럼 나타낼 수 있습니다.

527
00:28:54,781 --> 00:28:59,360
서로 다른 지역에 대해서 커널을 복사한 모양입니다.

528
00:28:59,360 --> 00:29:08,157
이렇게 만든 가중치 행렬 X를 가지고 Xa 연산을
수행하면 convolution과 결과가 동일합니다.

529
00:29:09,274 --> 00:29:17,770
Transpose Convolution은 같은 행렬을 사용해서 행렬곱
연산을 수행하되 이 행렬을 Transpose 시킵니다.

530
00:29:17,770 --> 00:29:26,491
왼쪽은 기존의 stride 1 convolution 이고
오른쪽은 stride 1 transpose convolution 입니다.

531
00:29:26,491 --> 00:29:31,018
조금 더 자세히 살펴보겠습니다.

532
00:29:31,018 --> 00:29:37,570
stride 1 transpose convolution의 모양을 보면
normal convolution과 유사하게 생겼습니다.

533
00:29:37,570 --> 00:29:42,334
border나 padding 같은 세부사항을 전부 고려하면
조금 다르겠지만 기본적으로 같은 연산입니다.

534
00:29:42,334 --> 00:29:45,879
하지만 stride = 2 인 경우에는 상황이 달라집니다.

535
00:29:45,879 --> 00:29:54,240
자 이제 왼쪽은 stride 2 convolution을
행렬곱 연산으로 표현한 것입니다.

536
00:29:54,240 --> 00:29:59,837
stride > 1 인 경우에는 transpose convolution
은 더이상 convolution이 아닙니다.

537
00:29:59,837 --> 00:30:04,985
왜 아닌지는 convolutions이 어떤 식으로
행렬 처럼 표현되는지 생각해보면 알 수 있습니다.

538
00:30:04,985 --> 00:30:13,913
이제 stride 2 transpose convolution은
normal convolution과 근본적으로 다른 연산이 됩니다.

539
00:30:13,913 --> 00:30:20,647
이것이 바로 Transpose Convolution 이라는 이름이
붙은 이유입니다. 상당히 좋은 이름이라고 생각합니다.

540
00:30:20,647 --> 00:30:22,980
질문있나요?

541
00:30:27,991 --> 00:30:29,646
[학생이 질문]

542
00:30:29,646 --> 00:30:36,523
슬라이드가 오타일 수도 있습니다.
Piazza에 올려주시면 수정하겠습니다.

543
00:30:36,523 --> 00:30:43,000
질문 있나요?

544
00:30:53,576 --> 00:30:56,360
질문은 "왜 평균을 내지 않고 합(sum)하는지" 입니다.

545
00:30:56,360 --> 00:31:03,404
그냥 더하기만 하는 이유는 앞서 살펴본
transpose convolution 수식 때문입니다.

546
00:31:03,404 --> 00:31:11,325
하지만 분명히 sum은 문제가 될 수 있습니다.
Receptive field의 크기에 따라서 magnitudes이 달라집니다.

547
00:31:11,325 --> 00:31:15,322
실제로 아주 최근들어 제기되고 있는 문제이기도 합니다.

548
00:31:15,322 --> 00:31:26,250
3x3 stride 2 transpose convolution을 사용하면
checkerboard artifacts가 발생하곤 합니다.

549
00:31:26,250 --> 00:31:37,127
최근 논문에서는 4x4 stride 2 혹은 2x2 stride 2
 사용하기도 합니다. 문제가 조금은 완화됩니다.

550
00:31:46,834 --> 00:31:52,515
질문은 "stride 1/2 convolution 이라는 용어는
어떻게 생겨났는지" 입니다.

551
00:31:52,515 --> 00:31:56,790
아마도 제 논문에서 나온 용어입니다.
사실은 그렇습니다 :D

552
00:31:56,790 --> 00:32:01,181
제가 논문을 쓸때 fractionally strided convolution
이라는 이름을 썼습니다.

553
00:32:01,181 --> 00:32:07,282
하지만 논문을 쓰고나서 다시한번 생각해보니
transpose convolution이 더 올바른 표현인 것 같습니다.

554
00:32:07,282 --> 00:32:13,746
이런 Semantic segmentation 구조는
상당히 일반적인 구조입니다.

555
00:32:13,746 --> 00:32:19,540
네트워크 내부에 downsampling/upsampling를 하는
거대한 convolution network가 있습니다.

556
00:32:19,540 --> 00:32:22,053
downsampling은
strided convolution이나 pooling을 사용하며

557
00:32:22,053 --> 00:32:28,035
upsampling은 transpose convolution 또는
다양한 종류의  unpooling or upsampling을 사용합니다.

558
00:32:28,035 --> 00:32:33,634
그리고 모든 픽셀에 대한 cross-entropy를 계산하면
네트워크 전체를 end-to-end로 학습시킬 수 있습니다.

559
00:32:33,634 --> 00:32:41,514
이 구조는 상당히 인상적입니다. Image classification에서
배웠던 많은 기법들이 녹아들어가 있습니다.

560
00:32:41,514 --> 00:32:45,414
이런 구조는 어떤 문제를 풀건
쉽게 확장시킬 수 있습니다. 대단한 구조입니다.

561
00:32:46,333 --> 00:32:52,024
다른 배울 Task는 "classification + localization"
입니다.

562
00:32:52,024 --> 00:32:54,953
image classification에 대해서는 수도없이 다뤘습니다.

563
00:32:54,953 --> 00:33:01,234
입력 이미지의 레이블을 매기는 문제였습니다. 하지만
때로는 더 많은 정보를 얻고 싶을 수 있습니다.

564
00:33:01,234 --> 00:33:09,077
이미지가 어떤 카테고리에 속하는지 뿐만 아니라
실제 객체가 어디에 있는지를 알 고 싶을 수 있습니다.

565
00:33:09,077 --> 00:33:17,874
이미지를 "Cat"에 분류하는 것 뿐만 아니라
이미지 내에 Cat이 어디에 있는지 네모박스를 그리는 것이죠

566
00:33:17,874 --> 00:33:22,713
classification plus localization 문제는
object detection 문제와는 구별됩니다.

567
00:33:22,713 --> 00:33:31,242
localization 문제에서는 이미지 내에서 내가 관심있는
객체가 오직 하나 뿐이라고 가정합니다.

568
00:33:31,242 --> 00:33:41,001
더 많은 객체를 찾고 싶을수도 있겠지만, 기본적으로 이미지 내에
객체 하나만 찾아서 레이블을 매기고 위치를 찾아냅니다.

569
00:33:41,001 --> 00:33:47,584
이 Task를 바로
classification plus localization라고 합니다.

570
00:33:47,584 --> 00:33:53,680
이 문제를 풀 때도 기존의 image classification에서
사용하던 기법들을 고스란히 녹일 수 있습니다.

571
00:33:53,680 --> 00:33:58,220
아키텍쳐의 기본 구조는 다음과 같습니다.

572
00:33:58,220 --> 00:34:09,301
력입력 이미지가 있고. 네트워크는 이 이미지를 입력으로합니다.
가령 이 예제에서는 AlexNet입니다. 그리고 출력이 있습니다.

573
00:34:09,301 --> 00:34:15,730
출력 레이어 직전의 FC-Layer는
"Class score"로 연결되서 카테고리를 결정합니다.

574
00:34:15,730 --> 00:34:21,109
하지만 여기에서는 FC-Layer가 하나 더 있습니다.
이는 4개의 원소를 가진 vector와 연결되어 있습니다.

575
00:34:21,109 --> 00:34:28,478
이 4개의 출력 값은 가령 width/height/x/y로
bounding box의 위치를 나타냅니다.

576
00:34:28,478 --> 00:34:34,228
이런 식으로 네트워크는 두 가지 출력값을 반환합니다.
하나는 Class Score입니다.

577
00:34:34,228 --> 00:34:39,094
다른 하나는 입력 영상 내의 객체 위치의
bounding box의 좌표입니다.

578
00:34:39,094 --> 00:34:44,489
이 네트워크를 학습시킬 때는
 loss가 두 개 존재합니다.

579
00:34:44,489 --> 00:34:47,210
그리고 이 문제는 fully supervised setting 을 가정합니다.

580
00:34:47,210 --> 00:34:55,330
따라서 학습 이미지에는 카테고리 레이블과
해당 객체의 bounding box GT를 동시에 가지고 있어야 합니다.

581
00:34:55,331 --> 00:34:57,118
자 이제 두 가지 Loss Function이 있습니다.

582
00:34:57,118 --> 00:35:03,360
우선 Class scores를 예측하기 위한
Softmax loss가 있습니다.

583
00:35:03,360 --> 00:35:13,669
그리고 Ground Truth Bbox와 예측한 Bbox 사이의
차이를 측정하는 Loss도 있습니다.

584
00:35:13,669 --> 00:35:20,509
L2 Loss로 BBox Loss를 가장 쉽게 디자인할 수 있습니다.

585
00:35:20,509 --> 00:35:27,728
또는 L1이나 smooth L1을 사용하셔도 됩니다.
조금은 달라지겠지만 기본적인 아이디어는 동일합니다.

586
00:35:27,728 --> 00:35:35,509
이 Loss들은 모두 예측한 Bbox와 GT Bbox 좌표
간의 차이에 대한 regression loss입니다.

587
00:35:35,509 --> 00:35:39,510
질문있나요?

588
00:35:49,410 --> 00:35:52,193
질문은 "왜 Classification과 Bbox Regression을
동시에 학습시키는 왜 좋으며,

589
00:35:52,193 --> 00:35:55,600
"가령 오분류에 대해서 Bbox가 있으면
어떻게 되는지" 입니다.

590
00:35:55,600 --> 00:35:59,901
일반적으로는 괜찮습니다.

591
00:35:59,901 --> 00:36:03,652
큰 문제가 되지 않습니다. 그리고 실제로 많은 사람들이
이 두 Loss를 동시에 학습시킵니다.

592
00:36:03,652 --> 00:36:09,592
두 Loss를 동시에 학습시켜도 됩니다. 하지만 가혹
오분류의 문제에서 까다로울 수도 있습니다.

593
00:36:09,592 --> 00:36:19,232
많이 쓰는 해결책 중 하나는  Bbox를  딱 하나만 예측하지 않고
Bbox를 카테고리마다 하나 씩 예측합니다.

594
00:36:19,232 --> 00:36:24,091
그리고 Ground Truth 카테고리에 속한
예측된 BBox에만 Loss와 연결시킵니다.

595
00:36:24,091 --> 00:36:28,318
사람들이 이런 방식을 많이 사용하며
실제로 아주 유용합니다.

596
00:36:28,318 --> 00:36:34,611
물론 이 설정은 아주 기본적인 설정이며 이것 만으로는
왁벽하지도 최적이지도 않을 수는 있습니다.

597
00:36:34,611 --> 00:36:37,361
질문 있나요?

598
00:36:41,226 --> 00:36:46,746
질문은 "두 개의 Loss의 단위가 달라서
Gradient 계산에 문제가 될 소지가 있지 않은지" 입니다.

599
00:36:46,746 --> 00:36:49,306
이 두 개의 Loss를 합친 Loss를
Multi-task Loss 라고 합니다.

600
00:36:49,306 --> 00:36:58,554
우선 그레디언트를 구하려면 네트워크 가중치들의
각각의 미분 값(scalar)을 계산해야 합니다.

601
00:36:58,554 --> 00:37:01,331
이제는 Loss가 두 개이니 미분 값(scalar)도 두 개 이고
이 두 개를 모두 최소화 시켜야 합니다.

602
00:37:01,331 --> 00:37:11,833
실제로는 두 Losses의 가중치를 조절하는 하이퍼파라이터가
있습니다. 두 Losses의 가중치 합이 최종 Loss입니다.

603
00:37:11,833 --> 00:37:15,642
그리고 이 두 Losses의 가중 합에 대한
그레디언트를 계산하는 것입니다.

604
00:37:15,642 --> 00:37:23,691
하지만 두 Losses 간의 가중치를 결정하는 것은 상당히 까다롭습니다.
이 가중치는 하이퍼파라미터로써 우리가 설정해 줘야만 합니다.

605
00:37:23,691 --> 00:37:27,851
이 하이퍼파라미터는 지금까지 우리가
살펴본 하이퍼파라미터들 보다는 상당히 다릅니다.

606
00:37:27,851 --> 00:37:32,390
이 하이퍼파라미터는 실제로
손실함수의 값 자체를 바꿔버리기 떄문입니다.

607
00:37:32,390 --> 00:37:43,091
지금까지는 하이퍼파라미터를 다양하게 조절하면서
 Loss가 어떻게 변하는 지를 관찰하였습니다.

608
00:37:43,091 --> 00:37:51,089
하지만 이 경우에는 하이퍼파라미터의 값이 변하면
Loss자체의 속성이 변하므로 비교하기가 상당히 까다롭습니다.

609
00:37:51,089 --> 00:37:54,473
따라서 이 하이퍼파라미터를 조절하는 것은
상당히 어렵습니다.

610
00:37:54,473 --> 00:38:00,393
따라서 이 하이퍼파라미터를 설정하는 적절한 방법은 여러분의
문제에 따라 케바케일 수 있습니다. 다만 일반적인 방법은

611
00:38:00,393 --> 00:38:08,163
Loss 값으로 비교할게 아니라
다른 성능 지표를 도입하는 것입니다.

612
00:38:08,163 --> 00:38:17,763
Cross Validation으로 하이퍼파라미터를 최적화 할 때
Loss가 아니라 실제 모델의 성능지표를 봐야 할 것입니다.

613
00:38:17,763 --> 00:38:18,596
질문있나요?

614
00:38:27,529 --> 00:38:32,682
질문은 "왜 굳이 학습을 한번에 시키는지
그냥 나눠서 하면 안되는지" 입니다.

615
00:38:38,131 --> 00:38:45,413
질문은 "앞 쪽의 큰 네트워크는 고정시키고
각 FC-layer만 학습시키는 방법은 어떤지" 입니다.

616
00:38:45,413 --> 00:38:52,702
사람들이 그렇게 하곤 합니다. 여러분이 이런 문제를
풀어야 할 때 시도해볼 만한 방법입니다.

617
00:38:52,702 --> 00:39:00,574
Transfer learning의 관점에서 보면 fine tune을
하면 항상 성능이 더 좋아집니다.

618
00:39:00,574 --> 00:39:09,280
ImageNet으로 학습시킨 모델을 가지고 여러분의 데이터셋에
작용한다면 적절한 Fine tune이 성능이 도움이 될 것입니다.

619
00:39:09,280 --> 00:39:16,870
실제로 사람들이 많이 하는 트릭 중 하나는 우선
네트워크를 Freeze하고 두  FC-layer를 학습시킵니다.

620
00:39:16,870 --> 00:39:20,398
그리고 두 FC-Layer가 수렴하면 다시
합쳐서 전체 시스템을 Fine-tune 하는 것입니다.

621
00:39:20,398 --> 00:39:24,558
실제로 사람들이 많이 하는 기법입니다.

622
00:39:24,558 --> 00:39:30,978
지난 시간에 ImageNet을 예로 들어서
Pre-Trained network를 소개해 드린 적 있었습니다.

623
00:39:31,979 --> 00:39:37,339
지 문제들은 잠시 재쳐두고. Bbox와 같이 이미지 내의
어떤 위치를 예측한다는 아이디어는

624
00:39:37,339 --> 00:39:41,881
classification plus localization 문제 이외에도
아주 다양한 문제에도 적용해 볼 수 있습니다.

625
00:39:41,881 --> 00:39:44,710
그 중 하나는 human pose estimation 입니다.

626
00:39:44,710 --> 00:39:49,440
human pose estimation 문제 에서는
사람 이미지가 입력으로 들어갑니다.

627
00:39:49,440 --> 00:39:56,462
출력은 이 사람의 각 관절의 위치입니다.
이 네트워크는 사람의 포즈를 예측합니다.

628
00:39:56,462 --> 00:39:59,030
이 사람의 팔다리가 어디에 있는지를 예측하는 것이죠

629
00:39:59,030 --> 00:40:04,060
일반적으로 대부분은 사람들의 관절의 수는 같습니다.
아주 심플한 가정입니다.

630
00:40:04,060 --> 00:40:06,862
모든 사람들이 그런 것은 아니겠지만
이 네트워크의 가정은 그렇습니다.

631
00:40:06,862 --> 00:40:15,430
이런 문제를 풀기 위해서는 가령 일부 Data sets은
14개의 관절의 위치로 사람의 포즈를 정의합니다.

632
00:40:10,251 --> 00:40:13,451


633
00:40:13,451 --> 00:40:15,430


634
00:40:15,430 --> 00:40:23,150
사람의 발, 무릎, 엉덩이와 같이 말이죠.
이 네트워크의 입력은 사람 이미지입니다.

635
00:40:16,932 --> 00:40:19,652


636
00:40:19,652 --> 00:40:23,150


637
00:40:23,150 --> 00:40:30,521
그리고 네트워크의 출력은 각 관절에 해당하는
14개의 좌표 값입니다.

638
00:40:27,132 --> 00:40:30,521


639
00:40:30,521 --> 00:40:40,619
예측된 14개의 점에 대해서 regression loss를
계산하고 backprop으로 학습시킵니다.

640
00:40:33,120 --> 00:40:35,961


641
00:40:35,961 --> 00:40:40,619


642
00:40:40,619 --> 00:40:46,571
가장 심플하게 L2 loss를 사용하기도 하고
또는 다양한 regression losses를 적용할 수 있습니다.

643
00:40:43,579 --> 00:40:46,571


644
00:40:46,571 --> 00:40:47,404
질문있나요?

645
00:40:50,934 --> 00:40:53,992
질문은 "Regression Loss" 가 무엇인지 입니다.

646
00:40:52,432 --> 00:40:53,992


647
00:40:53,992 --> 00:40:57,294
"Regression Loss" 는 cross entropy나
softmax가 아닌 Losses를 의미합니다.

648
00:40:56,099 --> 00:40:57,294


649
00:40:57,294 --> 00:41:04,494
Regression Loss에는 가령 L2, L1,
smooth L1 loss 등이 있습니다.

650
00:40:59,094 --> 00:41:02,382


651
00:41:02,382 --> 00:41:04,494


652
00:41:04,494 --> 00:41:10,502
Classification 과 Regression의 일반적인 차이점은
결과가 categorical 인지 continuous인지 입니다.

653
00:41:07,512 --> 00:41:10,502


654
00:41:10,502 --> 00:41:17,243
가령 고정된 갯수의 카테고리가 있고 모델을 이를
결정하기 위한 클래스 스코어를 출력하는 경우라면

655
00:41:12,643 --> 00:41:15,272


656
00:41:15,272 --> 00:41:17,243


657
00:41:17,243 --> 00:41:25,022
지난 강의에서 배웠던 Cross entropy / softmax/ SVM
margin Loss와 같은 것들을 사용할 수 있을 것입니다.

658
00:41:19,942 --> 00:41:23,094


659
00:41:23,094 --> 00:41:25,022


660
00:41:25,022 --> 00:41:30,222
반면 출력이 연속적인 값(continuous value) 인 경우라면,
가령 Pose estimation의 경우에는 관절의 위치입니다.

661
00:41:28,272 --> 00:41:30,222


662
00:41:30,222 --> 00:41:34,734
이런 경우에는 출력이 연속적인 값이기 때문에
다른 종류의 losses를 사용해야 합니다.

663
00:41:32,174 --> 00:41:34,734


664
00:41:34,734 --> 00:41:37,883
L2, L1 과 같은 Losses를 사용합니다.

665
00:41:37,883 --> 00:41:41,482
미리 확실한 설명을 해드리지 못한 점 죄송합니다.

666
00:41:41,482 --> 00:41:44,471
그러나 여기서 더 큰 요점은 언제든지

667
00:41:44,471 --> 00:41:46,832
고정 된 번호를 만들고 싶다는 것을 알고 있습니다.

668
00:41:46,832 --> 00:41:51,003
예를 들어 알고 있다면 네트워크 출력을

669
00:41:51,003 --> 00:41:58,763
가령 입력 이미지 안에 개와 고양이가 한마리씩
항상 존재한다는 사실을 이미 알고 있고,

670
00:41:54,344 --> 00:41:56,395


671
00:41:56,395 --> 00:41:58,763


672
00:41:58,763 --> 00:42:05,304
개, 고양이에 대해서 Bbox를 그리는 문제라면
출력해야 하는 Bbox의 갯수는 항상 고정되어 있습니다.

673
00:42:01,392 --> 00:42:03,062


674
00:42:03,062 --> 00:42:05,304


675
00:42:05,304 --> 00:42:10,743
이런 classification +   localization 문제를 풀기
위해서 Regression 개념을 도입하는 것입니다.

676
00:42:07,093 --> 00:42:09,264


677
00:42:09,264 --> 00:42:10,743


678
00:42:10,743 --> 00:42:17,039
Regression output이 고정된 숫자(ex 16관절) 의 경우라면
pose estimation과 같은 다양한 문제를 풀 수 있습니다.

679
00:42:13,094 --> 00:42:14,872


680
00:42:14,872 --> 00:42:17,039


681
00:42:19,062 --> 00:42:23,531
다음은 object detection 입니다.

682
00:42:23,531 --> 00:42:25,342
object detection은 논 할 거리가 정말 많은 (meaty)
주제입니다.

683
00:42:25,342 --> 00:42:27,422
 computer vision에서 가장 중요한 문제 중 하나입니다.

684
00:42:27,422 --> 00:42:33,902
Object Detection의 역사와 다양한 테크닉에 대해서만
하루종일 세미나를 진행할 수 있을 정도로 방대합니다.

685
00:42:29,910 --> 00:42:31,868


686
00:42:31,868 --> 00:42:33,902


687
00:42:33,902 --> 00:42:42,582
이번 시간에는 Object Detection  + Deep Learning 에
관련한 주요 아이디어만 간단하게 짚고 넘어가겠습니다.

688
00:42:35,931 --> 00:42:39,691


689
00:42:39,691 --> 00:42:42,582


690
00:42:42,582 --> 00:42:47,942
우선 Object Detection 문제에서도
고정된 카테고리가 있습니다.

691
00:42:44,731 --> 00:42:47,942


692
00:42:47,942 --> 00:42:55,321
가령 고양이, 개, 물고기 등 고정된 카테고리 갯수만 생각합니다.

693
00:42:52,182 --> 00:42:55,321


694
00:42:55,321 --> 00:42:59,030
Object Detection의 task는 입력 이미지가 주어지면

695
00:42:59,030 --> 00:43:05,641
이미지에 나타나는 객체들의 Bbox와 해당하는
카테고리를 예측합니다.

696
00:43:02,470 --> 00:43:05,641


697
00:43:05,641 --> 00:43:10,902
classification plus localization와는 조금 다릅니다.

698
00:43:08,710 --> 00:43:10,902


699
00:43:10,902 --> 00:43:15,302
왜냐하면 예측해야 하는 Bbox의 수가
입력 이미지에 따라 달라지기 떄문입니다.

700
00:43:13,620 --> 00:43:15,302


701
00:43:15,302 --> 00:43:22,870
각 이미지에 객체가 몇 개나 있을지가 미지수입니다.
꽤나 어려운 문제입니다.

702
00:43:17,910 --> 00:43:20,081


703
00:43:20,081 --> 00:43:22,870


704
00:43:22,870 --> 00:43:31,870
해마다 Image Classification의 성능이 얼마나
상승했는지를 보여주는 그래프를 살펴본 적이 있었죠

705
00:43:25,630 --> 00:43:28,988


706
00:43:28,988 --> 00:43:31,870


707
00:43:31,870 --> 00:43:34,761
매년 성능이 점점 향상되었습니다.

708
00:43:34,761 --> 00:43:37,342
object detection에서도 유사한 트랜드를 보입니다.

709
00:43:37,342 --> 00:43:44,110
Object Detection은 Computer Vision에서 중요한
문제이기 때문에 사람들이 오래 전부터 연구하던 분야입니다.

710
00:43:39,131 --> 00:43:41,291


711
00:43:41,291 --> 00:43:44,110


712
00:43:44,110 --> 00:43:48,742
이 그래프는 Object Detection 분야를 오랫동안 연구해온
 Ross Girshick의 슬라이드에서 가져왔습니다.

713
00:43:46,390 --> 00:43:48,742


714
00:43:48,742 --> 00:43:59,462
PASCAL VOC Dataset에서의 성능의 진보과정을 보여줍니다.
PASCAL VOC은 Detection 에서 아주 오래전부터 사용되었습니다.

715
00:43:51,070 --> 00:43:54,441


716
00:43:54,441 --> 00:43:57,230


717
00:43:57,230 --> 00:43:59,462


718
00:43:59,462 --> 00:44:08,161
2012년 까지 Object Detection의 성능은 점점 정체 되었습니다.

719
00:44:02,428 --> 00:44:04,761


720
00:44:04,761 --> 00:44:08,161


721
00:44:08,161 --> 00:44:16,171
하지만 2013년 Deep Learning 이 도입된 이후로는
매년 성능이 아주 빠르게 증가했습니다.

722
00:44:10,039 --> 00:44:12,141


723
00:44:12,141 --> 00:44:13,982


724
00:44:13,982 --> 00:44:16,171


725
00:44:16,171 --> 00:44:21,422
한 가지 주목할 만한 점은 2015년에 그래프가 끝납니다.
물론 그 이후로도 성능은 계속 올라갔을 것입니다.

726
00:44:21,422 --> 00:44:29,928
State of the art의 성능은 80%를 훌쩍 넘을 것입니다.
하지만 더 이상 논하지 않습니다. 이제는 너무 쉽기 때문이죠

727
00:44:29,929 --> 00:44:37,421
State of the art의 성능을 정확히 알진
못하지만 이 그래프의 최정상에 위치할 것입니다.

728
00:44:37,422 --> 00:44:40,924
질문있나요?
괜찮습니다.

729
00:44:42,051 --> 00:44:50,960
Object Detection이 Localization과는
다르게 객체의 수가 이미지마다 다릅니다.

730
00:44:50,961 --> 00:44:57,770
가령 좌상단의 고양이 이미지는 객체가 하나 뿐입니다.
따라서 네트워크는 4개의 숫자만 예측하면 됩니다.

731
00:44:57,771 --> 00:45:05,551
그 다음은 객체가 3개 입니다. 각 Bbox마다
좌표 4개 씩 도합 12개의 값을 예측해야 합니다.

732
00:45:05,552 --> 00:45:13,210
마지막 오리 사진의 경우에는 오리 한 마리 당 값이
4 개씩 아주 많은 값을 예측해야 할 것입니다.

733
00:45:13,211 --> 00:45:20,683
Object detection은 Localization과 꽤나 다릅니다.

734
00:45:20,683 --> 00:45:28,870
왜냐하면 Detection 문제에서는 이미지마다 객체의 
수가 달라지며, 이를 미리 알 수 없기 때문이죠

735
00:45:28,870 --> 00:45:34,568
따라서 Object Detection 문제로 Regression 문제를
풀기란 상당히 까다로운 문제가 아닐 수 없습니다.

736
00:45:34,568 --> 00:45:40,768
따라서 Object Detection 문제를 풀기 위해서는
색다른 패러다임이 필요합니다.

737
00:45:40,768 --> 00:45:49,958
Object Detection 문제를 풀 때 예전부터 사람들이
많이 시도했던 방법은 sliding window입니다.

738
00:45:49,958 --> 00:45:59,360
앞서 Semantic segmentation에서 작은 영역으로 쪼갰던
아이디어와 비슷한 방법을 사용합니다.

739
00:45:59,360 --> 00:46:05,118
Sliding window를 이용하려면 입력 이미지로부터 
다양한 영역을 나눠서 처리합니다.

740
00:46:05,118 --> 00:46:10,359
가령 이미지의 왼쪽 밑에서 작은 영역을 추출해서
그 작은 영역만 CNN의 입력으로 넣습니다.

741
00:46:10,359 --> 00:46:14,829
CNN은 이 작은 영역에 대해서 Classification을
수행할 것입니다.

742
00:46:14,829 --> 00:46:18,160
CNN은 여기에는 개도, 고양이도 없다고 말해줄 것입니다.

743
00:46:18,160 --> 00:46:23,899
단, 여기에서는 카테고리 하나를 추가해야 합니다.
배경(background) 카테고리를 추가합니다.

744
00:46:23,899 --> 00:46:32,288
네트워크가 배경이라고 예측했다면 
이 곳은 어떤 카테고리에도 속하지 않는다는 것을 의미합니다.

745
00:46:32,288 --> 00:46:39,008
이미지의 왼쪽 밑 영역에는 아무것도 없으므로 
네트워크는 배경이라고 예측할 것이고, 아무것도 없다는 뜻입니다.

746
00:46:39,008 --> 00:46:44,128
그렇다면 다른 영역을 추출해 봅시다. 여기에는 
개는 있고, 고양이는 없고, 배경은 아닙니다.

747
00:46:44,128 --> 00:46:47,680
또 다른 영역을 추출해 보겠습니다. 이 곳에는
개는 있고, 고양이는 없고, 배경도 없습니다.

748
00:46:47,680 --> 00:46:54,372
또 다른 영역은 개는 없고 고양이는 있고 배경은 없습니다.
그렇다면 이 방법의 문제는 무엇일까요?

749
00:47:00,324 --> 00:47:04,764
맞습니다. 어떻게 영역을 추출할지가 문제가 될 수 있습니다.
아주 큰 문제입니다.

750
00:47:04,764 --> 00:47:10,543
이미지에 Objects가 몇 개가 존재할지,
어디에 존재할지를 알 수가 없습니다.

751
00:47:10,543 --> 00:47:15,583
그리고 크기가 어떨지도 알 수 없습니다.
그리고 어떤 종횡비로 표현해야 할지도 모릅니다.

752
00:47:15,583 --> 00:47:29,523
따라서 이런 brute force 방식의 sliding window를 
하려면 너무나 많은 경우의 수가 존재합니다.

753
00:47:29,523 --> 00:47:37,532
작은 영역 하나 하나마다 거대한 CNN을 통과시키려면
이 때의 계산량은 도무지 다룰 수 조차 없습니다.

754
00:47:37,532 --> 00:47:45,920
Object Detection 문제를 풀려고 brute force
sliding window 를 하는 일은 절대로 없습니다.

755
00:47:47,044 --> 00:47:54,492
대신에  Region Proposals 이라는 방법이 있습니다. 
사실 이 방법은 Deep Learning 을 사용하지는 않습니다.

756
00:47:54,492 --> 00:47:56,332
약간은 전통적인 방식이라고 할 수 있습니다.

757
00:47:56,332 --> 00:48:05,401
Region Proposal Network은 전통적인 신호처리 
기법을 사용합니다.(faster-rcnn rpn 아님)

758
00:48:05,401 --> 00:48:14,341
Region Proposal Network는 Object가 있을법한
가령, 1000개의 Bbox를 제공해 줍니다.

759
00:48:14,341 --> 00:48:22,382
이미지 내에서 객체가 있을법한 후보 Region 
Proposas을 찾아내는 다양한 방법(edges, etc)이 있겠지만.

760
00:48:22,382 --> 00:48:30,132
Region Proposal Network는 
이미지 내에 뭉텅진(blobby) 곳들을 찾아냅니다.

761
00:48:30,132 --> 00:48:38,962
이 지역들은 객체가 있을지도 모르는 후보 영역들입니다.
이런 알고리즘은은 비교적 빠르게 동작합니다.

762
00:48:38,962 --> 00:48:44,703
Region Proposal을 만들어낼 수 있는 방법에는
Selective Search가 있습니다.

763
00:48:44,703 --> 00:48:49,284
 Selective Search은 슬라이드에 적힌 1000개가 아니라
2000개의 Region Proposal을 만들어 냅니다.

764
00:48:49,284 --> 00:48:59,404
CPU로 2초간 Selective Search를 돌리면 객체가 있을만한
2000개의 Region Proposal을 만들어냅니다.

765
00:48:59,404 --> 00:49:05,052
이 방법은 노이즈가 아주 심합니다. 대부분은
실제 객체가 아니겠지만 Recall 은 아주 높습니다.

766
00:49:05,052 --> 00:49:11,204
따라서 이미지에 객체가 존재한다면 Selective Search
의 Region Proposal 안에 속할 가능성이 높습니다.

767
00:49:11,204 --> 00:49:17,103
자 이제는 무식하게 이미지 내의 모든 위치와 
스케일을 전부 고려하는 것이 아니라

768
00:49:17,103 --> 00:49:25,164
우선 Region Proposal Networks를 적용하고
객체가 있을법한 Region Proposal 을 얻어냅니다.

769
00:49:25,164 --> 00:49:33,135
그리고 이 Region Proposals을 CNN의 입력으로 하는 것이죠.
이 방법을 사용하면 계산량을 다루기 훨씬 수월합니다.

770
00:49:33,135 --> 00:49:36,903
모든 위치와 스케일을 전부 고려하는 (brute force)
방법보다는 낫습니다.

771
00:49:36,903 --> 00:49:45,583
지금까지 말씀드린 아이디어가 모두 몇해 전 나온  
R-CNN이라는 논문에 등장합니다.

772
00:49:45,583 --> 00:49:53,263
이미지가 주어지면  Region Proposal을 얻기 위해
Region Proposal Network를 수행합니다.

773
00:49:53,263 --> 00:49:56,724
 Region Proposal은 Region of Interest (ROI) 
라고도 합니다. Selective Search를 통해

774
00:49:56,724 --> 00:49:59,692
2000개의 ROI를 얻어냅니다.

775
00:49:59,692 --> 00:50:07,043
하지만 여기에서는 각 ROI의 사이즈가 
각양각색이라는 점이 문제가 될 수 있습니다.

776
00:50:07,043 --> 00:50:08,204
그러나 우리가 그들을 전부 달릴 예정이라면

777
00:50:08,204 --> 00:50:11,063
길쌈 네트워크 (convolutional
network)를 통해 우리의 분류,

778
00:50:11,063 --> 00:50:13,143
분류를위한 컨볼 루션 네트워크

779
00:50:13,143 --> 00:50:15,922
모두 동일한 입력 크기의 이미지를 원합니다.

780
00:50:15,922 --> 00:50:18,149
완전히 연결된 네트 레이어와 이것 저것 때문에

781
00:50:18,149 --> 00:50:21,058
그래서 우리는이 지역 제안들 각각을 취할 필요가 있습니다.

782
00:50:21,058 --> 00:50:24,029
고정 된 정사각형 크기로 그들을 휘게한다.

783
00:50:24,029 --> 00:50:26,855
이는 다운 스트림 네트워크에 대한 입력으로 예상됩니다.

784
00:50:26,855 --> 00:50:29,170
그래서 우리는 지역 제안서를 작성합니다.

785
00:50:29,170 --> 00:50:32,018
지역 제안에 해당하는 지역,

786
00:50:32,018 --> 00:50:34,090
우리는 고정 된 크기로 그들을 휘게 할 것이고,

787
00:50:34,090 --> 00:50:35,549
그리고 나서 우리는 각각을 실행할 것입니다.

788
00:50:35,549 --> 00:50:37,418
컨벌루션 네트워크를 통해

789
00:50:37,418 --> 00:50:40,488
이 경우에는 SVM을 사용합니다

790
00:50:40,488 --> 00:50:44,237
그것들 각각에 대한 분류 결정을 내리기 위해서,

791
00:50:44,237 --> 00:50:48,479
각 작물에 대한 카테고리를 예측할 수 있습니다.

792
00:50:48,479 --> 00:50:52,506
그리고 나서 나는 슬라이드를 잃어 버렸다.

793
00:50:52,506 --> 00:50:55,957
그러나 또한 슬라이드에 표시되지 않습니다.

794
00:50:55,957 --> 00:50:59,757
그러나 R-CNN은 또한 회귀를 예측하며,

795
00:50:59,757 --> 00:51:02,946
테두리 상자를 수정하는 것과 같습니다.

796
00:51:02,946 --> 00:51:05,650
또한 각 입력 영역 제안서

797
00:51:05,650 --> 00:51:07,770
문제는 입력 영역 제안서

798
00:51:07,770 --> 00:51:10,498
일반적으로 물체의 올바른 위치에있다.

799
00:51:10,498 --> 00:51:13,549
그러나 그것들은 완벽하지 않을 수도 있습니다.
그래서 R-CNN은,

800
00:51:13,549 --> 00:51:17,038
각 제안서에 대한 카테고리 라벨 외에도,

801
00:51:17,038 --> 00:51:19,610
그것도 일종의 네 가지 숫자를 예측할 것입니다.

802
00:51:19,610 --> 00:51:22,469
오프셋 또는 예측 된 상자에 대한 수정

803
00:51:22,469 --> 00:51:24,658
지역 제안 단계에서

804
00:51:24,658 --> 00:51:26,418
다시 한번, 이것은 다중 작업 손실입니다

805
00:51:26,418 --> 00:51:27,919
너는이 모든 것을 훈련시킬거야.

806
00:51:27,919 --> 00:51:30,169
미안해, 질문 있니?

807
00:51:35,511 --> 00:51:36,692
문제는 그 변화가 얼마나

808
00:51:36,692 --> 00:51:39,359
종횡비에 영향을 미치는 정확도?

809
00:51:40,698 --> 00:51:41,772
말하기는 조금 어렵습니다.

810
00:51:41,772 --> 00:51:43,732
나는 통제 된 실험이 있다고 생각한다.

811
00:51:43,732 --> 00:51:46,551
이 신문 중 일부는 잘 모르겠다.

812
00:51:46,551 --> 00:51:48,738
나는 그에 대한 일반적인 답을 줄 수있다.

813
00:51:48,738 --> 00:51:49,571
문제?

814
00:51:53,602 --> 00:51:54,671
문제는 그것이 필요한 것입니다.

815
00:51:54,671 --> 00:51:56,772
관심 영역이 직사각형일까요?

816
00:51:56,772 --> 00:52:00,212
그래서 워프하기가 어렵 기 때문에 일반적으로

817
00:52:00,212 --> 00:52:03,731
이 비 지역 일이지만 한 번 이동하면됩니다.

818
00:52:03,731 --> 00:52:05,511
인스턴트 세그멘테이션과 같은 것으로

819
00:52:05,511 --> 00:52:08,911
그러면 사각형이 아닌 제안서를받는 경우가 있습니다.

820
00:52:08,911 --> 00:52:10,441
실제로 물건 예측에 관심이 있다면

821
00:52:10,441 --> 00:52:12,071
직사각형이 아닙니다.

822
00:52:12,071 --> 00:52:14,238
또 다른 질문이 있습니까?

823
00:52:18,704 --> 00:52:21,317
네, 그렇다면 문제는 배운 지역 제안입니다.

824
00:52:21,317 --> 00:52:24,375
그래서 R-CNN에서는 전통적인 것입니다.

825
00:52:24,375 --> 00:52:27,134
이것들은 배웠지 만, 이것은 일종의 고정 알고리즘입니다

826
00:52:27,134 --> 00:52:29,203
누군가가 적어 봤지만 몇 분 후에 보게 될거야.

827
00:52:29,203 --> 00:52:31,866
우리가 실제로 할 수있는 것, 우리는 조금 바꿨습니다.

828
00:52:31,866 --> 00:52:33,466
지난 몇 년 동안.

829
00:52:33,466 --> 00:52:35,633
또 다른 질문이 있습니까?

830
00:52:37,767 --> 00:52:39,486
문제는 항상 오프셋입니다.

831
00:52:39,486 --> 00:52:40,735
관심 지역?

832
00:52:40,735 --> 00:52:42,665
대답은 '아니오'일뿐입니다.

833
00:52:42,665 --> 00:52:45,346
당신은 그 관심 영역을 상상할 수 있습니다.

834
00:52:45,346 --> 00:52:48,687
사람의 주위에 상자를 놓고 머리를 놓쳤다.

835
00:52:48,687 --> 00:52:50,786
그러면 당신은 네트워크 추론을 상상할 수 있습니다.

836
00:52:50,786 --> 00:52:53,439
오,이 사람이 있지만 사람들은 대개 머리가 있습니다.

837
00:52:53,439 --> 00:52:55,906
그래서 네트워크는 상자가 조금 더
높아야한다고 보여주었습니다.

838
00:52:55,906 --> 00:52:57,515
때로는 최종 예측 상자

839
00:52:57,515 --> 00:52:59,666
관심 영역 밖일 것입니다.

840
00:52:59,666 --> 00:53:00,499
문제?

841
00:53:08,110 --> 00:53:08,943
네.

842
00:53:12,019 --> 00:53:13,619
네, 궁금한 점은 투자 수익 (ROI)이 많은 것입니다.

843
00:53:13,619 --> 00:53:15,877
실제 물건과 일치하지 않는 것들?

844
00:53:15,877 --> 00:53:18,179
그리고 우리가 말했듯이, 수업 외에도

845
00:53:18,179 --> 00:53:20,078
너는 너를 정말로 걱정한다.

846
00:53:20,078 --> 00:53:22,550
백그라운드 클래스를 사용하여 클래스
점수를 얻을 수도 있습니다.

847
00:53:22,550 --> 00:53:26,289
여기에 물건이 없다고 말하는 배경을 예언해라.

848
00:53:26,289 --> 00:53:27,122
문제?

849
00:53:37,716 --> 00:53:40,894
그래, 문제는 우리가 어떤 종류의
데이터를 필요로하는지이다.

850
00:53:40,894 --> 00:53:43,364
그리고 네, 이것은 완전히 감각적으로 감독됩니다.

851
00:53:43,364 --> 00:53:47,385
우리의 훈련 데이터는 각 이미지를
가지고 있으며 이미지로 구성됩니다.

852
00:53:47,385 --> 00:53:50,065
각 이미지에는 표시된 모든 객체 카테고리가 있습니다.

853
00:53:50,065 --> 00:53:53,383
해당 범주의 각 인스턴스에 대한 경계 상자가 있습니다.

854
00:53:53,383 --> 00:53:55,404
확실히 이것에 접근하려고하는 논문이 있습니다.

855
00:53:55,404 --> 00:53:56,904
당신이 데이터를 가지고 있지 않다면 어떻게 될까요?

856
00:53:56,904 --> 00:54:00,759
일부 이미지에 대해서만 데이터를 가지고 있다면 어떨까요?

857
00:54:00,759 --> 00:54:02,945
아니면 그 데이터가 시끄 럽긴하지만 적어도

858
00:54:02,945 --> 00:54:04,735
일반적인 경우 전체 감독을 맡는다.

859
00:54:04,735 --> 00:54:08,568
교육 시간에 이미지의 모든 개체를

860
00:54:09,835 --> 00:54:12,974
알았어. 그래서 우리가 이걸 암시한다고 생각해.

861
00:54:12,974 --> 00:54:14,825
하지만 문제가 많습니다.

862
00:54:14,825 --> 00:54:16,535
이 R-CNN 프레임 워크와

863
00:54:16,535 --> 00:54:18,044
그리고 실제로 여기 오른쪽 그림을 보면

864
00:54:18,044 --> 00:54:19,975
추가 경계 상자 머리를 볼 수 있습니다.

865
00:54:19,975 --> 00:54:21,644
그래서 나는 그것을 되돌릴 것이다.

866
00:54:21,644 --> 00:54:25,811
그러나 이것은 여전히 계산 상 꽤 비쌉니다.

867
00:54:27,436 --> 00:54:30,004
2000 개의 지역 제안서가 있다면,

868
00:54:30,004 --> 00:54:32,196
우리는 각 제안을 독립적으로 운영하고 있습니다.

869
00:54:32,196 --> 00:54:34,415
그것은 꽤 비쌀 수 있습니다.

870
00:54:34,415 --> 00:54:37,111
이 질문에 의거하여

871
00:54:37,111 --> 00:54:40,015
고정 영역 제안 네트워크,이 고정 영역 제안,

872
00:54:40,015 --> 00:54:42,895
우리는 그런 것들을 배우지 않고있는 것이 문제입니다.

873
00:54:42,895 --> 00:54:46,015
그리고 실제로는 아주 천천히 끝납니다.

874
00:54:46,015 --> 00:54:48,164
원래 구현에서 R-CNN

875
00:54:48,164 --> 00:54:50,563
실제로 모든 기능을 디스크에 덤프합니다.

876
00:54:50,563 --> 00:54:52,863
그래서 그것은 수백 기가 바이트의
디스크 공간을 필요로 할 것입니다.

877
00:54:52,863 --> 00:54:54,721
이러한 모든 기능을 저장합니다.

878
00:54:54,721 --> 00:54:56,862
그렇다면 훈련은 매우 느릴 것입니다.

879
00:54:56,862 --> 00:54:58,472
이 모든 전방 및 후방 통과

880
00:54:58,472 --> 00:55:01,569
이미지를 통해 84 시간이 걸렸습니다.

881
00:55:01,569 --> 00:55:04,356
그들이 훈련 시간 동안 기록한 하나의 번호입니다.

882
00:55:04,356 --> 00:55:06,134
그래서 이것은 슈퍼 슈퍼 슬로우입니다.

883
00:55:06,134 --> 00:55:07,984
그리고 이제 테스트 시간에 그것은 또한 매우 천천히,

884
00:55:07,984 --> 00:55:11,076
이미지 당 대략 30 초 정도의 시간

885
00:55:11,076 --> 00:55:13,134
당신은 수천 번 전진 패스를 실행해야하기 때문에

886
00:55:13,134 --> 00:55:14,454
컨벌루션 네트워크를 통해

887
00:55:14,454 --> 00:55:16,004
각 지역 제안서

888
00:55:16,004 --> 00:55:18,316
그래서 이것은 꽤 느리게 끝납니다.

889
00:55:18,316 --> 00:55:21,505
고맙게도 우리는 빠른 R-CNN을 많이 고정 시켰습니다.

890
00:55:21,505 --> 00:55:25,084
우리가 빨리 할 때 이렇게 문제의 R-CNN

891
00:55:25,084 --> 00:55:27,404
그러면 똑같은 모양이 될 것입니다.

892
00:55:27,404 --> 00:55:29,036
우리는 입력 이미지로 시작할 것입니다.

893
00:55:29,036 --> 00:55:31,465
이제 각 관심 영역을 처리하는 것이 아니라

894
00:55:31,465 --> 00:55:34,116
별도로 대신 전체 이미지를 실행합니다.

895
00:55:34,116 --> 00:55:36,923
한 번에 일부 길쌈 레이어를 통해

896
00:55:36,923 --> 00:55:39,494
이 고해상도 콘볼 루션 특징 맵 제공

897
00:55:39,494 --> 00:55:41,924
전체 이미지에 해당합니다.

898
00:55:41,924 --> 00:55:44,345
이제 우리는 여전히 일부 지역 제안을 사용하고 있습니다.

899
00:55:44,345 --> 00:55:46,652
선택적 검색과 같은 고정 된 것에서

900
00:55:46,652 --> 00:55:50,414
이미지의 픽셀을 잘라내 기보다는

901
00:55:50,414 --> 00:55:52,334
지역 제안서에 해당하는

902
00:55:52,334 --> 00:55:55,164
대신 우리는 그 지역 제안을 예상한다고 상상합니다.

903
00:55:55,164 --> 00:55:57,705
이 길쌈 특성지도에

904
00:55:57,705 --> 00:56:00,673
길쌈 특징지도에서 작물을 가져온다.

905
00:56:00,673 --> 00:56:02,633
오히려 각 제안에 해당

906
00:56:02,633 --> 00:56:04,745
이미지에서 작물을 직접 가져 오는 것보다.

907
00:56:04,745 --> 00:56:06,713
그리고 이것으로 우리는 많은
비용을 재사용 할 수 있습니다.

908
00:56:06,713 --> 00:56:09,532
전체 이미지에 대한 길쌈 연산

909
00:56:09,532 --> 00:56:13,425
이미지 당 여러 작물이있을 때.

910
00:56:13,425 --> 00:56:15,932
그러나 다시 말하지만, 우리가
완전히 연결된 레이어가 있다면

911
00:56:15,932 --> 00:56:18,052
하류에 완전히 연결된 레이어

912
00:56:18,052 --> 00:56:20,052
일부 고정 크기 입력을 예상하고 있습니다.

913
00:56:20,052 --> 00:56:23,844
그래서 지금 우리는 그 작물의
재편성을 할 필요가 있습니다.

914
00:56:23,844 --> 00:56:26,131
길쌈 특성 맵으로부터

915
00:56:26,131 --> 00:56:28,572
차별화 된 방식으로

916
00:56:28,572 --> 00:56:31,673
그들은 ROI 풀링 레이어라고 부르는 것을 사용합니다.

917
00:56:31,673 --> 00:56:35,362
이 뒤틀린 작물을 가지고 나면

918
00:56:35,362 --> 00:56:37,084
길쌈 특성 맵으로부터

919
00:56:37,084 --> 00:56:38,622
그런 다음 당신은 어떤 것을 통해 이러한 것들을 실행할 수 있습니다.

920
00:56:38,622 --> 00:56:41,191
완전히 연결된 레이어 및 분류 예측

921
00:56:41,191 --> 00:56:43,853
점수 및 선형 회귀 옵셋

922
00:56:43,853 --> 00:56:45,673
테두리 상자로.

923
00:56:45,673 --> 00:56:47,484
이제 우리가이 일을 훈련하면 다시

924
00:56:47,484 --> 00:56:49,062
서로 상쇄되는 다중 작업 손실이있다.

925
00:56:49,062 --> 00:56:51,654
이 두 가지 제약 조건 사이 및 역 전파 중에

926
00:56:51,654 --> 00:56:53,362
우리는이 모든 것을 통해 소품을지지 할 수 있습니다.

927
00:56:53,362 --> 00:56:56,124
공동으로 모두 배우십시오.

928
00:56:56,124 --> 00:56:59,833
이 ROI 풀링은 마치 최대 풀링과 비슷합니다.

929
00:56:59,833 --> 00:57:00,973
나는 정말로 들어가고 싶지 않다.

930
00:57:00,973 --> 00:57:03,575
지금 그 세부 사항.

931
00:57:03,575 --> 00:57:07,887
그리고 우리가 R-CNN 대 빠른
R-CNN을 본다면 속도면에서

932
00:57:07,887 --> 00:57:10,422
이 모델은 SPP net

933
00:57:10,422 --> 00:57:12,014
두 사람 사이에 일종의

934
00:57:12,014 --> 00:57:14,494
그 때 당신은 훈련 시간에 빨리 볼 수 있습니다 R-CNN

935
00:57:14,494 --> 00:57:16,924
기차가 10 배 더 빠르다.

936
00:57:16,924 --> 00:57:18,433
이 모든 계산을 공유하기 때문에

937
00:57:18,433 --> 00:57:20,134
서로 다른 기능 맵간에

938
00:57:20,134 --> 00:57:23,272
그리고 지금 시험 시간에 빠른 R-CNN은 초고속입니다.

939
00:57:23,272 --> 00:57:27,222
실제로 빠른 R-CNN은 테스트 시간에 너무 빠릅니다.

940
00:57:27,222 --> 00:57:31,352
계산 시간이 실제로 지배적이라는 것

941
00:57:31,352 --> 00:57:33,764
지역 제안을 계산하여

942
00:57:33,764 --> 00:57:36,433
그래서 우리는 2000 년 지역 제안서

943
00:57:36,433 --> 00:57:39,334
선택 검색을 사용하면 2 초 정도 걸립니다.

944
00:57:39,334 --> 00:57:41,553
이제는 이러한 지역 제안을 모두 얻은 후에

945
00:57:41,553 --> 00:57:44,534
그렇다면 우리는 모든 것을 공유하고 있기 때문에

946
00:57:44,534 --> 00:57:46,724
이러한 비싼 회선을 공유함으로써

947
00:57:46,724 --> 00:57:49,724
이미지 전체에 걸쳐 이러한 모든
것을 처리 할 수 있습니다.

948
00:57:49,724 --> 00:57:53,273
지역 제안을 1 초도 안 남았습니다.

949
00:57:53,273 --> 00:57:55,494
이렇게 빨리 R-CNN이 병목 현상을 일으키게됩니다.

950
00:57:55,494 --> 00:57:59,142
이 지역 제안을 계산하는 것만으로

951
00:57:59,142 --> 00:58:03,804
고맙게도 우리는 빠른 R-CNN으로이 문제를 해결했습니다.

952
00:58:03,804 --> 00:58:07,883
그래서 더 빠른 R-CNN에서의 아이디어는,

953
00:58:07,883 --> 00:58:11,324
그래서 문제는 지역 제안을 계산하는 것이 었습니다.

954
00:58:11,324 --> 00:58:13,734
이 고정 기능을 사용하는 것이 병목이었습니다.

955
00:58:13,734 --> 00:58:15,832
그래서 대신 우리는 네트워크 자체를 만들 것입니다.

956
00:58:15,832 --> 00:58:18,054
자체 지역 제안을 예측하십시오.

957
00:58:18,054 --> 00:58:20,822
그래서 이런 종류의 일이 다시 일어난다는 것입니다.

958
00:58:20,822 --> 00:58:23,993
입력 이미지를 가져 와서 전체 입력 이미지를 실행합니다.

959
00:58:23,993 --> 00:58:26,433
모두 일부 길쌈 레이어를 통해

960
00:58:26,433 --> 00:58:28,062
일부 길쌈 기능 맵 가져 오기

961
00:58:28,062 --> 00:58:30,572
고해상도 이미지 전체를 나타내는

962
00:58:30,572 --> 00:58:33,204
이제 별도의 지역 제안 네트워크가 있습니다.

963
00:58:33,204 --> 00:58:35,913
그 길쌈 특징의 위에 작동하는

964
00:58:35,913 --> 00:58:39,204
네트워크 내에서 자체 지역 제안을 예측합니다.

965
00:58:39,204 --> 00:58:41,964
예측 된 지역 제안이 있으면

966
00:58:41,964 --> 00:58:44,542
그러면 마치 빠른 R-CNN처럼 보입니다.

967
00:58:44,542 --> 00:58:46,913
이제 우리는 지역 제안서에서 작물을 가져옵니다.

968
00:58:46,913 --> 00:58:48,262
컨볼 루션 특징들로부터,

969
00:58:48,262 --> 00:58:50,662
네트워크의 나머지 부분까지 그들을 전달하십시오.

970
00:58:50,662 --> 00:58:53,108
이제는 다중 작업 손실에 대해 이야기했습니다.

971
00:58:53,108 --> 00:58:55,182
및 다중 작업 교육 네트워크

972
00:58:55,182 --> 00:58:57,094
한 번에 여러 가지 일을 할 수 있습니다.

973
00:58:57,094 --> 00:58:59,372
이제 우리는 네 가지 일을하도록
네트워크에 말하고 있습니다.

974
00:58:59,372 --> 00:59:02,978
한 번에 모든 것이 이루어 지므로이 4 가지 방법으로 균형을 이룬다.

975
00:59:02,978 --> 00:59:05,019
다중 작업 손실은 다소 까다 롭습니다.

976
00:59:05,019 --> 00:59:07,059
그러나 지역 제안 네트워크

977
00:59:07,059 --> 00:59:09,648
두 가지 일을해야합니다.

978
00:59:09,648 --> 00:59:11,979
각각의 잠재적 제안에 대해

979
00:59:11,979 --> 00:59:14,848
또는 객체가 아니라 실제로 회귀해야합니다.

980
00:59:14,848 --> 00:59:18,186
그 제안들 각각에 대한 바운딩 박스 좌표,

981
00:59:18,186 --> 00:59:20,035
그리고 지금 최종 네트워크

982
00:59:20,035 --> 00:59:21,787
이 두 가지 일을 다시해야합니다.

983
00:59:21,787 --> 00:59:23,576
최종 분류 결정하기

984
00:59:23,576 --> 00:59:26,288
이 제안들 각각에 대한 학급 점수는 얼마인가?

985
00:59:26,288 --> 00:59:29,565
경계 상자 회귀의 두 번째 라운드가 있습니다.

986
00:59:29,565 --> 00:59:31,059
가질 수있는 오류를 다시 수정하십시오.

987
00:59:31,059 --> 00:59:34,086
지역 제안 단계에서 왔습니다.

988
00:59:34,086 --> 00:59:34,919
문제?

989
00:59:45,231 --> 00:59:47,453
그래서 질문은 때로는 다중 작업 학습

990
00:59:47,453 --> 00:59:48,862
정규화로 볼 수있다.

991
00:59:48,862 --> 00:59:50,703
우리가 여기에 영향을 미치고 있습니까?

992
00:59:50,703 --> 00:59:52,602
수퍼 컨트롤 연구가 있는지 확실하지 않습니다.

993
00:59:52,602 --> 00:59:55,562
그것에 있지만 원래 버전에서 실제로

994
00:59:55,562 --> 00:59:58,903
더 빨랐던 R-CNN 논문의

995
00:59:58,903 --> 01:00:01,162
우리가 공유하면 어떻게 될까?

996
01:00:01,162 --> 01:00:03,951
지역 제안 네트워크, 우리가 공유하지 않으면 어떻게 될까?

997
01:00:03,951 --> 01:00:05,530
별도의 컨벌루션 네트워크를 학습하면 어떨까요?

998
01:00:05,530 --> 01:00:06,682
지역 제안 네트워크

999
01:00:06,682 --> 01:00:08,522
분류 네트워크 대?

1000
01:00:08,522 --> 01:00:10,111
그리고 사소한 차이가 있다고 생각합니다.

1001
01:00:10,111 --> 01:00:12,970
그러나 어느 쪽이든 극적인 차이는 아니었다.

1002
01:00:12,970 --> 01:00:15,141
따라서 실제적으로 하나만 배우는 것이 더 친절합니다.

1003
01:00:15,141 --> 01:00:18,380
계산적으로 저렴하기 때문입니다.

1004
01:00:18,380 --> 01:00:19,713
미안 해요, 질문?

1005
01:00:33,583 --> 01:00:35,292
그래, 문제는 어떻게 훈련시키는거야?

1006
01:00:35,292 --> 01:00:38,143
이 지역 제안 네트워크는 모르기 때문에,

1007
01:00:38,143 --> 01:00:40,351
당신은 근거 진실 지역 제안을 가지고 있지 않다.

1008
01:00:40,351 --> 01:00:41,903
지역 제안 네트워크.

1009
01:00:41,903 --> 01:00:43,282
그래서 약간 털이 있습니다.

1010
01:00:43,282 --> 01:00:45,172
나는 그 세부 사항에 너무 많이 들어가고 싶지 않다.

1011
01:00:45,172 --> 01:00:49,092
그러나 아이디어는 언제든지 지역
제안서를 가지고 있다는 것입니다.

1012
01:00:49,092 --> 01:00:51,583
오버랩의 임계 값 이상을 갖는

1013
01:00:51,583 --> 01:00:53,452
지상 진실 물체들

1014
01:00:53,452 --> 01:00:55,652
그런 다음 긍정적 인 지역 제안이라고 말하면됩니다.

1015
01:00:55,652 --> 01:00:57,771
지역 제안으로 예측해야합니다.

1016
01:00:57,771 --> 01:01:01,642
오버랩이 매우 낮은 잠재적 인 제안

1017
01:01:01,642 --> 01:01:02,942
어떤 진실의 물건으로

1018
01:01:02,942 --> 01:01:04,471
부정적인 것으로 예측되어야합니다.

1019
01:01:04,471 --> 01:01:06,652
하지만 어두운 마법의 하이퍼 파라미터가 많이 있습니다.

1020
01:01:06,652 --> 01:01:09,550
그 과정에서 그것은 약간 털이 있습니다.

1021
01:01:09,550 --> 01:01:10,383
문제?

1022
01:01:15,394 --> 01:01:17,554
그래, 문제는 분류 손실이란 무엇인가?

1023
01:01:17,554 --> 01:01:19,793
지역 제안 네트워크에서 대답은

1024
01:01:19,793 --> 01:01:22,164
바이너리를 만들고있어. 그래서 얻고 싶지 않았어.

1025
01:01:22,164 --> 01:01:23,938
그 건축물에 대한 너무 많은 세부 사항들

1026
01:01:23,938 --> 01:01:25,320
그것이 조금 털이 많기 때문에

1027
01:01:25,320 --> 01:01:26,648
그러나 그것은 2 진 결정을 내리고있다.

1028
01:01:26,648 --> 01:01:29,258
그래서 잠재적 인 영역 집합을 가지고 있습니다.

1029
01:01:29,258 --> 01:01:30,686
그것을 고려하고 그것을 만들고있다.

1030
01:01:30,686 --> 01:01:32,269
각각에 대한 이진 결정.

1031
01:01:32,269 --> 01:01:34,078
이 개체 또는 개체가 아닌가요?

1032
01:01:34,078 --> 01:01:37,578
따라서 이진 분류 손실과 같습니다.

1033
01:01:38,520 --> 01:01:40,858
그래서 일단이 일을 훈련하면 빠른 R-CNN

1034
01:01:40,858 --> 01:01:43,658
매우 빨리 끝내야한다.

1035
01:01:43,658 --> 01:01:46,248
이제 우리는이 오버 헤드를 제거했기 때문에

1036
01:01:46,248 --> 01:01:48,706
네트워크 외부의 컴퓨팅 지역 제안들로부터,

1037
01:01:48,706 --> 01:01:51,008
이제는 더 빨라진 R-CNN이 매우 빨리 끝납니다.

1038
01:01:51,008 --> 01:01:53,588
이러한 다른 대안들에 비해.

1039
01:01:53,588 --> 01:01:56,693
또한 흥미로운 점 중 하나는

1040
01:01:56,693 --> 01:01:59,388
여기에있는 지역 제안은 당신이 상상할 수도 있습니다.

1041
01:01:59,388 --> 01:02:00,848
아마도 약간의 불일치가 있다면 어떨까요?

1042
01:02:00,848 --> 01:02:05,086
이 고정 영역 제안 알고리즘과 내 데이터 사이에?

1043
01:02:05,086 --> 01:02:06,938
그래서 이번 경우에 한 번 배우고 있습니다.

1044
01:02:06,938 --> 01:02:09,240
자신의 지역 제안서를 작성하면 극복 할 수 있습니다.

1045
01:02:09,240 --> 01:02:12,018
지역 제안 사항이 불일치하는 경우

1046
01:02:12,018 --> 01:02:16,320
다른 데이터 세트와 다소 이상하거나 다르다.

1047
01:02:16,320 --> 01:02:19,926
그래서 R-CNN 방법의 모든 가족,

1048
01:02:19,926 --> 01:02:22,914
R은 region을 나타내므로 이것들은
모두 region 기반 메소드입니다

1049
01:02:22,914 --> 01:02:25,116
어떤 종류의 지역 제안이 있기 때문에

1050
01:02:25,116 --> 01:02:27,796
그리고 나서 우리는 약간의 처리를하고 있습니다.

1051
01:02:27,796 --> 01:02:29,178
각각을위한 독립적 인 처리

1052
01:02:29,178 --> 01:02:30,716
그 잠재적 인 지역의.

1053
01:02:30,716 --> 01:02:32,447
그래서이 모든 종류의 메소드가 호출됩니다.

1054
01:02:32,447 --> 01:02:36,708
객체 검출을위한 이러한 영역 기반 방법.

1055
01:02:36,708 --> 01:02:38,196
하지만 또 다른 방법이 있습니다.

1056
01:02:38,196 --> 01:02:40,676
당신이 가끔씩 물체 감지를 위해 보게되는

1057
01:02:40,676 --> 01:02:43,818
이것은 한 번에 모든 피드를 전달하는 것입니다.

1058
01:02:43,818 --> 01:02:48,076
그래서 이것들 중 하나는 You Only
Look Once를위한 YOLO입니다.

1059
01:02:48,076 --> 01:02:50,796
또 하나의 SSD는 단발 탐지

1060
01:02:50,796 --> 01:02:54,067
이 두 사람은 같은시기에 다소 나왔습니다.

1061
01:02:54,067 --> 01:02:55,959
그러나 아이디어는 독립적 인 행동보다는

1062
01:02:55,959 --> 01:02:58,496
이들 각각의 잠재 영역에 대한 프로세싱

1063
01:02:58,496 --> 01:03:00,138
대신 우리는 이것을 대우하려고합니다.

1064
01:03:00,138 --> 01:03:02,348
회귀 문제와 마찬가지로

1065
01:03:02,348 --> 01:03:03,916
이 모든 예측은 한 번에

1066
01:03:03,916 --> 01:03:06,156
큰 길쌈 네트워크.

1067
01:03:06,156 --> 01:03:08,367
여러분이 상상하는 입력 이미지가 주어졌습니다.

1068
01:03:08,367 --> 01:03:11,327
그 입력 이미지를 거친 격자로 분할하고,

1069
01:03:11,327 --> 01:03:13,468
이 경우에는 7x7 격자입니다.

1070
01:03:13,468 --> 01:03:15,698
이제 각 그리드 셀 내에서

1071
01:03:15,698 --> 01:03:18,556
당신은 기본 테두리 상자 몇 세트를 상상해보십시오.

1072
01:03:18,556 --> 01:03:20,995
여기에 3 개의 기본 경계 상자가 그려져 있습니다.

1073
01:03:20,995 --> 01:03:23,418
키가 큰 것, 너비가 넓은 것, 사각형이되는 것

1074
01:03:23,418 --> 01:03:25,748
실제로는 3 개 이상을 사용합니다.

1075
01:03:25,748 --> 01:03:28,098
이제 각 그리드 셀에 대해

1076
01:03:28,098 --> 01:03:30,314
이들 각각의 기본 경계 상자

1077
01:03:30,314 --> 01:03:32,858
몇 가지 것을 예측하고 싶습니다.

1078
01:03:32,858 --> 01:03:37,025
하나, 기본 테두리 상자에서 오프셋을 예측하고 싶습니다.

1079
01:03:38,177 --> 01:03:40,087
실제 위치가 무엇인지 예측하기

1080
01:03:40,087 --> 01:03:43,020
이 기본 경계 상자에서 개체의

1081
01:03:43,020 --> 01:03:46,340
또한 분류 점수를 예측하기를 원합니다.

1082
01:03:46,340 --> 01:03:49,820
그래서 아마도 각각에 대한 분류 점수

1083
01:03:49,820 --> 01:03:51,460
이러한 기본 테두리 상자 중.

1084
01:03:51,460 --> 01:03:53,619
이 카테고리의 대상이 얼마나 될 가능성이 높습니까?

1085
01:03:53,619 --> 01:03:55,503
이 경계 상자에 나타납니다.

1086
01:03:55,503 --> 01:03:58,250
결국 결국 우리는 예측을 끝내게됩니다.

1087
01:03:58,250 --> 01:03:59,762
우리의 입력 이미지에서 우리는 결국

1088
01:03:59,762 --> 01:04:03,929
이 거대한 텐서는 7 × 7 격자로
5B + C로 나타납니다.

1089
01:04:04,951 --> 01:04:08,130
그래서 우리는 B 기본 경계 박스를 가지고 있습니다.

1090
01:04:08,130 --> 01:04:10,231
우리는 각각 오프셋을주는 다섯 개의 숫자가 있습니다.

1091
01:04:10,231 --> 01:04:12,700
기본 테두리 상자에 대한 우리의 확신

1092
01:04:12,700 --> 01:04:16,340
우리 C 범주의 C 분류 점수.

1093
01:04:16,340 --> 01:04:19,549
그러면 우리는이 입력으로 물체 감지를 보게됩니다.

1094
01:04:19,549 --> 01:04:23,522
이 3 차원 텐서의 출력

1095
01:04:23,522 --> 01:04:25,642
너는이 모든 것을 훈련하는 것을 상상할 수있다.

1096
01:04:25,642 --> 01:04:27,722
거대한 길쌈 네트워크.

1097
01:04:27,722 --> 01:04:30,682
그리고 그것은 이러한 단일 샷 방법이하는 일종의 것입니다.

1098
01:04:30,682 --> 01:04:33,320
그들은 단지 진실로 진실과 일치하는 곳에서

1099
01:04:33,320 --> 01:04:37,050
이러한 잠재적 인 기본 상자에 개체

1100
01:04:37,050 --> 01:04:41,180
조금 털이되지만 이것이 그 방법들입니다.

1101
01:04:41,180 --> 01:04:43,060
그런데 지역 제안 네트워크

1102
01:04:43,060 --> 01:04:45,388
더 빨라진 R-CNN에 익숙해 져서

1103
01:04:45,388 --> 01:04:48,539
그들이 세트를 가지고있는 이들과 아주 비슷합니다.

1104
01:04:48,539 --> 01:04:51,210
일부 바둑판 모양의 이미지 위에 기본 경계 상자 포함

1105
01:04:51,210 --> 01:04:53,899
다른 지역 제안 네트워크는 약간의 회귀를합니다.

1106
01:04:53,899 --> 01:04:55,279
일부 분류 플러스.

1107
01:04:55,279 --> 01:04:59,196
여기에는 몇 가지 겹쳐진 아이디어가 있습니다.

1108
01:05:00,388 --> 01:05:04,555
그래서 빠른 R-CNN에서 우리는
그 대상을 다루는 종류입니다.

1109
01:05:05,390 --> 01:05:08,372
이 문제의 종류로 지역 제안 단계

1110
01:05:08,372 --> 01:05:11,199
엔드 - 투 - 엔드 회귀 문제를 해결하고 우리는 별도의

1111
01:05:11,199 --> 01:05:13,892
지역별 처리는하지만 이러한 단일 촬영 방법을 사용합니다.

1112
01:05:13,892 --> 01:05:16,350
우리는 첫 번째 단계 만 수행하고 모두 수행합니다.

1113
01:05:16,350 --> 01:05:19,761
하나의 순방향 패스로 우리의 물체 감지.

1114
01:05:19,761 --> 01:05:21,740
따라서 물체 감지에는 많은 변수가 있습니다.

1115
01:05:21,740 --> 01:05:23,950
VGG와 같은 다른 기본 네트워크가있을 수 있습니다.

1116
01:05:23,950 --> 01:05:26,459
ResNet, 우리는 다른 전이를 보았습니다.

1117
01:05:26,459 --> 01:05:29,601
이 빠른 R-CNN을 포함하는 물체 탐지

1118
01:05:29,601 --> 01:05:31,820
타입 영역 기반의 패밀리,

1119
01:05:31,820 --> 01:05:34,060
이 단일 샷 탐지 패밀리 메소드.

1120
01:05:34,060 --> 01:05:35,492
내가 말하지 않은 종류의 잡종이있다.

1121
01:05:35,492 --> 01:05:38,153
사이에 다소있는 R-FCN이라고 불리는 것입니다.

1122
01:05:38,153 --> 01:05:39,580
다양한 하이퍼 파라미터가 있습니다.

1123
01:05:39,580 --> 01:05:40,911
이미지 크기와 마찬가지로,

1124
01:05:40,911 --> 01:05:43,590
얼마나 많은 지역 제안을 사용하십니까?

1125
01:05:43,590 --> 01:05:44,938
실제로이 멋진 종이가 있습니다.

1126
01:05:44,938 --> 01:05:48,022
이번 여름에 CVPR에 나타납니다.

1127
01:05:48,022 --> 01:05:50,102
이 많은 것들을 중심으로 제어 된 실험

1128
01:05:50,102 --> 01:05:53,102
다른 변수들과 당신에게 말하려고한다.

1129
01:05:53,102 --> 01:05:54,732
이 방법들은 모두 어떻게 수행합니까?

1130
01:05:54,732 --> 01:05:56,353
이 다른 변수들 하에서

1131
01:05:56,353 --> 01:05:58,676
그래서 당신이 관심이 있다면 나는 그것을
밖으로 체크하는 것이 좋습니다 것이

1132
01:05:58,676 --> 01:06:01,171
그러나 중요한 테이크 아웃의 종류 중 하나는

1133
01:06:01,171 --> 01:06:04,012
지역 기반 방법의 빠른 R-CNN 스타일

1134
01:06:04,012 --> 01:06:06,702
더 높은 정확도를주는 경향이 있지만

1135
01:06:06,702 --> 01:06:08,972
단일 샷 방법보다 훨씬 느림

1136
01:06:08,972 --> 01:06:10,612
단일 샷 방법은

1137
01:06:10,612 --> 01:06:12,486
지역별 처리 당.

1138
01:06:12,486 --> 01:06:14,542
하지만이 신문을 읽어 보시기 바랍니다.

1139
01:06:14,542 --> 01:06:17,204
더 자세한 정보가 필요하면.

1140
01:06:17,204 --> 01:06:20,062
또한 조금 곁에, 나는이 재미있는 종이를 가지고 있었다.

1141
01:06:20,062 --> 01:06:21,852
Andre와 2 년 전에 그런 종류의

1142
01:06:21,852 --> 01:06:24,621
결합 된 물체 감지와 이미지 캡션

1143
01:06:24,621 --> 01:06:27,273
이 문제는 조밀 한 자막이라고 불렀습니다.

1144
01:06:27,273 --> 01:06:30,324
그래서 이제 아이디어는 예측보다는

1145
01:06:30,324 --> 01:06:32,472
각 지역에 대한 고정 카테고리 라벨,

1146
01:06:32,472 --> 01:06:35,084
대신 우리는 각 지역에 대한 캡션을 작성하려고합니다.

1147
01:06:35,084 --> 01:06:37,902
그리고 다시, 우리는 이런 종류의 데이터를
가지고있는 데이터 세트를 가지고있었습니다.

1148
01:06:37,902 --> 01:06:41,033
캡션과 함께 데이터 세트가있는 곳

1149
01:06:41,033 --> 01:06:43,302
그런 다음 우리는 훈련 된이 거대한 종단 모델

1150
01:06:43,302 --> 01:06:46,153
방금 이러한 자막을 모두 공동으로 예측했습니다.

1151
01:06:46,153 --> 01:06:48,993
그리고 이것은 마치 좀 더 빨라진 R-CNN처럼 보입니다.

1152
01:06:48,993 --> 01:06:50,962
지역 제안 단계

1153
01:06:50,962 --> 01:06:53,764
다음 경계 상자, 그리고 지역별 처리 당.

1154
01:06:53,764 --> 01:06:56,657
하지만 SVM이나 소프트 맥스 손실보다

1155
01:06:56,657 --> 01:06:59,382
대신에 지역별 처리 당

1156
01:06:59,382 --> 01:07:03,454
각 지역에 대한 캡션을 예측하는 RNN 언어 모델입니다.

1157
01:07:03,454 --> 01:07:06,814
그래서 결국 빠른 R-CNN처럼 보입니다.

1158
01:07:06,814 --> 01:07:07,953
여기 비디오가 있지만 생각합니다.

1159
01:07:07,953 --> 01:07:11,524
우리는 시간이 없어서 나는 그것을 건너 뛸 것이다.

1160
01:07:11,524 --> 01:07:15,108
그러나 여기 아이디어는 일단 당신이 이것을 가지면,

1161
01:07:15,108 --> 01:07:17,897
당신은 이런 종류의 아이디어를 많이 묶을 수 있습니다.

1162
01:07:17,897 --> 01:07:19,727
관심이있는 새로운 문제가 생기면

1163
01:07:19,727 --> 01:07:21,508
조밀 한 자막과 같은 태클에서,

1164
01:07:21,508 --> 01:07:23,156
당신은 많은 구성 요소를 재활용 할 수 있습니다.

1165
01:07:23,156 --> 01:07:24,607
당신이 다른 문제들로부터 배웠던

1166
01:07:24,607 --> 01:07:26,860
유사 물체 감지 및 이미지 캡션

1167
01:07:26,860 --> 01:07:28,786
하나의 엔드 - 투 - 엔드
(end-to-end) 네트워크

1168
01:07:28,786 --> 01:07:30,356
당신이 신경 쓰는 출력을 만들어내는

1169
01:07:30,356 --> 01:07:32,565
당신 문제.

1170
01:07:32,565 --> 01:07:34,386
그래서 내가 얘기하고 싶은 마지막 과제

1171
01:07:34,386 --> 01:07:36,567
인스턴스 분할에 대한 아이디어입니다.

1172
01:07:36,567 --> 01:07:38,165
여기서 인스턴스 분할은

1173
01:07:38,165 --> 01:07:40,636
어떤면에서는 완전한 문제처럼

1174
01:07:40,636 --> 01:07:45,007
우리는 입력 이미지가 주어지며 우리는
하나의 이미지를 예측하기를 원합니다.

1175
01:07:45,007 --> 01:07:48,028
그 이미지 내의 객체의 위치와 동일성

1176
01:07:48,028 --> 01:07:50,594
물체 감지와 유사하지만 단지

1177
01:07:50,594 --> 01:07:52,847
각각의 객체에 대한 바운딩 박스를 예측하고,

1178
01:07:52,847 --> 01:07:55,385
대신 전체 세그먼트 마스크를 예측하려고합니다.

1179
01:07:55,385 --> 01:07:57,943
그 각각의 물체에 대해 그리고 어떤 픽셀을 예측할지

1180
01:07:57,943 --> 01:08:02,785
입력 이미지에서 각 객체 인스턴스에 해당합니다.

1181
01:08:02,785 --> 01:08:04,575
그래서 이것은 잡종과 같은 종류입니다.

1182
01:08:04,575 --> 01:08:07,484
의미 론적 세분화와 객체 검출 간의 관계

1183
01:08:07,484 --> 01:08:09,815
우리는 객체 감지와 같이

1184
01:08:09,815 --> 01:08:12,271
여러 개체와 우리는 ID를 구별합니다.

1185
01:08:12,271 --> 01:08:15,196
이 예제에서 다른 인스턴스의

1186
01:08:15,196 --> 01:08:17,215
이미지에는 두 마리의 개가 있기 때문에

1187
01:08:17,215 --> 01:08:19,385
인스턴스 분할 방법

1188
01:08:19,385 --> 01:08:21,924
실제로 두 개 인스턴스를 구별합니다.

1189
01:08:21,924 --> 01:08:25,425
같은 의미 론적 세분화의 출력과 종류

1190
01:08:25,425 --> 01:08:27,948
우리는이 픽셀 현명한 정확성을 가지고있다.

1191
01:08:27,948 --> 01:08:30,268
우리가 말하고 싶은이 물건들 각각에 대해

1192
01:08:30,268 --> 01:08:32,765
어떤 픽셀이 그 객체에 속하는지.

1193
01:08:32,765 --> 01:08:34,709
그래서 많은 다른 방법들이있었습니다.

1194
01:08:34,709 --> 01:08:38,247
예를 들어 세분화와 같은 사람들이 다뤄야 만합니다.

1195
01:08:38,247 --> 01:08:40,567
그러나 현재의 최신 기술은이 새로운 논문이다.

1196
01:08:40,567 --> 01:08:44,636
실제로 방금 나온 Mask R-CNN

1197
01:08:44,636 --> 01:08:47,846
약 한 달 전에 보관 용으로 제공되므로
아직 게시되지 않았습니다.

1198
01:08:47,846 --> 01:08:49,868
이것은 슈퍼 신선한 물건 같다.

1199
01:08:49,868 --> 01:08:52,675
그리고 이것은 더 빠른 R-CNN처럼 많이 보입니다.

1200
01:08:52,676 --> 01:08:55,296
따라서이 다단계 처리 방식을 사용합니다.

1201
01:08:55,296 --> 01:08:57,508
여기서 우리는 전체 입력 이미지를 취하고,

1202
01:08:57,509 --> 01:09:00,117
그 전체 입력 이미지가 어떤 길쌈
(convolutional)으로 들어간다.

1203
01:09:00,117 --> 01:09:03,127
네트워크 및 학습 된 지역 제안 네트워크

1204
01:09:03,127 --> 01:09:05,622
그것은 빠른 R-CNN과 정확히 동일합니다.

1205
01:09:05,622 --> 01:09:08,206
이제 우리는 배운 지역 제안을 한 번 가지고 있습니다.

1206
01:09:08,207 --> 01:09:09,557
그런 다음 제안서를 제출합니다.

1207
01:09:09,557 --> 01:09:11,247
길쌈 기능 맵에

1208
01:09:11,247 --> 01:09:14,796
우리가 빠르고 더 빠른 R-CNN에서했던 것처럼.

1209
01:09:14,796 --> 01:09:17,196
하지만 지금은 분류를하기보다는

1210
01:09:17,197 --> 01:09:19,167
회귀 결정을위한 바운딩 박스

1211
01:09:19,167 --> 01:09:21,229
그 상자들 각각에 대해서 우리는

1212
01:09:21,229 --> 01:09:23,419
세그먼테이션 마스크를 예측하고 싶다.

1213
01:09:23,420 --> 01:09:25,729
그 각각의 바운딩 박스에 대해,

1214
01:09:25,729 --> 01:09:27,478
해당 지역 제안서 각각에 대해

1215
01:09:27,478 --> 01:09:30,478
그래서 지금은 마치 미니처럼 보입니다.

1216
01:09:30,478 --> 01:09:32,529
의미 론적 세분화 문제처럼

1217
01:09:32,529 --> 01:09:34,408
각 지역 제안서 내부

1218
01:09:34,408 --> 01:09:36,888
지역 제안 네트워크에서 얻은 정보

1219
01:09:36,889 --> 01:09:40,288
이제 ROI를 warp에 맞추면됩니다.

1220
01:09:40,288 --> 01:09:42,888
제안 영역에 해당하는 우리의 기능

1221
01:09:42,889 --> 01:09:45,948
오른쪽 모양으로, 그럼 우리는 두 가지 가지가 있습니다.

1222
01:09:45,948 --> 01:09:48,209
하나의 가지가 정확히 생겼고,

1223
01:09:48,209 --> 01:09:50,198
상단의이 첫 번째 분기는 마치

1224
01:09:50,198 --> 01:09:53,750
보다 빠른 R-CNN을 통해 분류
점수를 예측할 수 있습니다.

1225
01:09:53,750 --> 01:09:55,580
해당 범주가 무엇인지 알려줍니다.

1226
01:09:55,580 --> 01:09:57,838
해당 제안 지역으로

1227
01:09:57,838 --> 01:09:59,318
그것이 배경이든 아니든.

1228
01:09:59,318 --> 01:10:01,369
그리고 경계 상자 좌표도 예측할 것입니다.

1229
01:10:01,369 --> 01:10:04,596
지역 제안을 조정했다.

1230
01:10:04,596 --> 01:10:06,830
그리고 이제 우리는이 지류를 바닥에 둘 것입니다.

1231
01:10:06,830 --> 01:10:09,738
기본적으로 의미 론적 세분화와 비슷하게 보입니다.

1232
01:10:09,738 --> 01:10:13,550
각 픽셀별로 분류 할 미니 네트워크

1233
01:10:13,550 --> 01:10:17,780
해당 입력 영역 제안서에 객체인지 여부

1234
01:10:17,780 --> 01:10:22,180
그래서이 마스크 R - CNN 문제,이 마스크 R
- CNN 아키텍처

1235
01:10:22,180 --> 01:10:24,249
이러한 종류의 문제를 모두 하나로 통합 할 수 있습니다.

1236
01:10:24,249 --> 01:10:26,928
우리가 오늘 한 가지 멋진 이야기를하고 있습니다.

1237
01:10:26,928 --> 01:10:29,230
공동으로 엔드 - 투 - 엔드 훈련 가능한 모델.

1238
01:10:29,230 --> 01:10:31,238
그리고 그것은 정말로 시원하고 실제로 작동합니다.

1239
01:10:31,238 --> 01:10:34,958
정말 정말 잘 그래서 예제를 보면

1240
01:10:34,958 --> 01:10:36,710
종이에서 그들은 일종의 놀랍습니다.

1241
01:10:36,710 --> 01:10:39,078
그들은 진실의 진리와 구별되지 않습니다.

1242
01:10:39,078 --> 01:10:41,012
왼쪽의이 예에서 볼 수 있습니다.

1243
01:10:41,012 --> 01:10:42,623
이 두 사람이 서있는 것을

1244
01:10:42,623 --> 01:10:44,838
오토바이 앞에서 상자가 그려져 있습니다.

1245
01:10:44,838 --> 01:10:46,820
이 사람들의 주위에, 그것은 또한
안으로 들어가고 레테르를 붙인다

1246
01:10:46,820 --> 01:10:49,497
그 사람들의 모든 픽셀과 그것은 정말로 작습니다.

1247
01:10:49,497 --> 01:10:51,038
하지만 실제로 그 이미지의 배경에

1248
01:10:51,038 --> 01:10:52,868
왼쪽에는 수많은 사람들이 또한 있습니다.

1249
01:10:52,868 --> 01:10:54,961
백그라운드에서 아주 작은 서.

1250
01:10:54,961 --> 01:10:56,478
또한 각 상자 주위에 상자가 그려져 있습니다.

1251
01:10:56,478 --> 01:10:58,628
그 이미지들 각각의 픽셀들을 움켜 잡았다.

1252
01:10:58,628 --> 01:11:00,729
그리고 당신은 이것이 단지,

1253
01:11:00,729 --> 01:11:02,118
그것은 정말로 정말로 잘 작동하게된다.

1254
01:11:02,118 --> 01:11:04,215
비교적 간단한 추가 작업입니다.

1255
01:11:04,215 --> 01:11:08,028
기존의 더 빨랐던 R-CNN 프레임 워크의 위에.

1256
01:11:08,028 --> 01:11:11,146
그래서 나는 마스크 R-CNN이

1257
01:11:11,146 --> 01:11:13,318
우리는 오늘에 대해서 이야기했고 그것은 또한합니다.

1258
01:11:13,318 --> 01:11:15,108
그런데 포즈 추정.

1259
01:11:15,108 --> 01:11:18,417
우리가 말했던 것처럼 포즈 추정을 할 수 있습니다.

1260
01:11:18,417 --> 01:11:20,478
이들 조인트 좌표를 예측함으로써

1261
01:11:20,478 --> 01:11:22,257
그 사람의 관절 각각을 위해

1262
01:11:22,257 --> 01:11:26,214
그래서 당신은 조준 물체 탐지를하기 위해
마스크 R-CNN을 할 수 있습니다.

1263
01:11:26,214 --> 01:11:29,388
자세 추정 및 인스턴스 세분화.

1264
01:11:29,388 --> 01:11:31,246
우리가 만들 수있는 유일한 추가

1265
01:11:31,246 --> 01:11:33,337
이 지역 제안들 각각에 대한 것입니다.

1266
01:11:33,337 --> 01:11:35,246
작은 지점을 더 추가합니다.

1267
01:11:35,246 --> 01:11:39,086
관절의 좌표를 예측합니다.

1268
01:11:39,086 --> 01:11:42,628
현재 지역 제안의 경우

1269
01:11:42,628 --> 01:11:44,506
이제 이것은 또 다른 손실 일뿐입니다.

1270
01:11:44,506 --> 01:11:46,137
우리가 추가하는 또 다른 레이어처럼,

1271
01:11:46,137 --> 01:11:47,836
다른 머리가 네트워크에서 나온다.

1272
01:11:47,836 --> 01:11:51,715
멀티 태스크 손실에 대한 추가 용어입니다.

1273
01:11:51,715 --> 01:11:54,027
그러나 일단이 작은 지점 하나를 추가하면

1274
01:11:54,027 --> 01:11:56,684
그러면이 모든 다른 문제들을 공동으로 해결할 수 있습니다.

1275
01:11:56,684 --> 01:11:59,406
당신은 이런 결과를 얻습니다.

1276
01:11:59,406 --> 01:12:02,705
이제이 네트워크는 단일 피드 전달 네트워크

1277
01:12:02,705 --> 01:12:06,126
얼마나 많은 사람들이 이미지에 있는지 결정하고 있습니다.

1278
01:12:06,126 --> 01:12:07,876
그 사람들이 어디에 있는지 감지하고,

1279
01:12:07,876 --> 01:12:09,792
각각에 해당하는 픽셀을 계산

1280
01:12:09,792 --> 01:12:12,283
그 사람들의 추측과 골격 추정

1281
01:12:12,283 --> 01:12:14,593
그 사람들의 자세와 이것이 정말로 잘 돌아 간다.

1282
01:12:14,593 --> 01:12:16,993
이 교실과 같이 붐비는 장면에서도

1283
01:12:16,993 --> 01:12:18,102
엄청난 사람들이 앉아있는 곳

1284
01:12:18,102 --> 01:12:19,273
그들 모두는 서로 겹쳐있다.

1285
01:12:19,273 --> 01:12:22,742
엄청나게 잘 작동하는 것 같습니다.

1286
01:12:22,742 --> 01:12:25,392
더 빠른 R-CNN 프레임 워크를 기반으로하기 때문에

1287
01:12:25,392 --> 01:12:28,291
또한 실시간으로 비교적 가깝게 실행됩니다.

1288
01:12:28,291 --> 01:12:31,153
그래서 이것은 초당 5 프레임과 같은 것을 실행합니다.

1289
01:12:31,153 --> 01:12:33,582
이것이 일종의 일종이기 때문에 GPU에서

1290
01:12:33,582 --> 01:12:36,061
네트워크의 단일 순방향 패스에서.

1291
01:12:36,061 --> 01:12:37,603
그래서 이것은 다시 새로운 슈퍼 종이입니다.

1292
01:12:37,603 --> 01:12:39,622
그러나 나는 이것이 아마 얻을 것이라고 생각한다.

1293
01:12:39,622 --> 01:12:42,833
앞으로 몇 달 동안 많은 주목을 받게 될 것입니다.

1294
01:12:42,833 --> 01:12:45,430
요약하자면, 우리는 이야기했습니다.

1295
01:12:45,430 --> 01:12:46,680
미안 해요?

1296
01:12:53,800 --> 01:12:55,781
문제는 얼마나 많은 교육 자료가 필요한가입니다.

1297
01:12:55,781 --> 01:12:58,610
따라서 이러한 모든 즉각적인 세분화 결과

1298
01:12:58,610 --> 01:13:00,948
Microsoft Coco 데이터
세트에 대한 교육을 받았습니다.

1299
01:13:00,948 --> 01:13:05,349
그래서 Microsoft Coco는 대략
200,000 개의 교육 이미지입니다.

1300
01:13:05,349 --> 01:13:08,320
80 개의 카테고리가 있습니다.

1301
01:13:08,320 --> 01:13:11,101
200,000 개의 교육용 이미지 각각에서

1302
01:13:11,101 --> 01:13:14,010
80 개 카테고리의 모든 인스턴스가 라벨링되어 있습니다.

1303
01:13:14,010 --> 01:13:17,139
그래서 훈련을 위해 20 만 개의 이미지가 있습니다.

1304
01:13:17,139 --> 01:13:18,548
내가 평균이라고 생각하는 것과 같은 것이있다.

1305
01:13:18,548 --> 01:13:21,069
이미지 당 5 개 또는 6 개의 인스턴스로 구성됩니다.

1306
01:13:21,069 --> 01:13:23,285
따라서 실제로는 많은 양의 데이터입니다.

1307
01:13:23,285 --> 01:13:26,970
모든 사람들을위한 Microsoft Coco

1308
01:13:26,970 --> 01:13:28,909
Microsoft Coco에는 모든 관절이 있습니다.

1309
01:13:28,909 --> 01:13:32,000
주석을 달아서 실제로 이것은 많은 것을 가지고있다.

1310
01:13:32,000 --> 01:13:34,320
훈련 시간에 당신이 옳다는 감독의,

1311
01:13:34,320 --> 01:13:36,669
실제로 많은 양의 데이터로 교육을 받았습니다.

1312
01:13:36,669 --> 01:13:39,638
그래서 나는 공부할 흥미로운
주제가 하나 있다고 생각합니다.

1313
01:13:39,638 --> 01:13:42,050
앞으로 나아가는 것은 우리가 알고있는 것입니다.

1314
01:13:42,050 --> 01:13:44,620
당신이 어떤 문제를 해결하기 위해
많은 양의 데이터를 가지고 있다면,

1315
01:13:44,620 --> 01:13:46,349
이 시점에서 우리는 당신이

1316
01:13:46,349 --> 01:13:48,088
일부 길쌈 네트워크를 꿰매다

1317
01:13:48,088 --> 01:13:50,701
아마 그 문제에서 합당한 직업을 할 수 있습니다.

1318
01:13:50,701 --> 01:13:53,701
이런 성능을 얻을 수있는 방법을 찾아 냈어.

1319
01:13:53,701 --> 01:13:55,809
훈련 데이터가 적 으면 매우 흥미 롭습니다.

1320
01:13:55,809 --> 01:13:57,700
연구 활동 영역이 활발합니다.

1321
01:13:57,700 --> 01:13:59,069
사람들이 쓸 돈입니다.

1322
01:13:59,069 --> 01:14:03,301
앞으로 몇 년 동안 많은 노력을 기울일 것입니다.

1323
01:14:03,301 --> 01:14:05,749
그래서 요점을 되풀이하기 위해, 오늘 우리는
일종의 회오리 바람 여행을 가지고 있었다.

1324
01:14:05,749 --> 01:14:08,068
컴퓨터 비전 주제의 전체 모음

1325
01:14:08,068 --> 01:14:10,141
우리는 우리가 만든 기계가 얼마나 많은지 보았습니다.

1326
01:14:10,141 --> 01:14:13,061
이미지 분류로부터 비교적 쉽게 적용 할 수있다.

1327
01:14:13,061 --> 01:14:15,925
이러한 다양한 컴퓨터 비전 주제를 해결할 수 있습니다.

1328
01:14:15,925 --> 01:14:18,013
그리고 다음에 우리가 얘기 할게,

1329
01:14:18,013 --> 01:14:20,835
CNN 기능을 시각화하는 데 정말
재미있는 강의가 진행됩니다.

1330
01:14:20,835 --> 01:14:22,835
또한 DeepDream과 신경 스타일
전달에 대해서도 이야기합니다.

