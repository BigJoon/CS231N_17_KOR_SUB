1
00:00:09,784 --> 00:00:11,867
시작하겠습니다!

2
00:00:13,038 --> 00:00:15,888
CS231n 14강에 오신 것을 환영합니다.

3
00:00:15,888 --> 00:00:20,884
이번 시간에는 강화학습을 배워보겠습니다.

4
00:00:20,884 --> 00:00:23,222
우선 공지사항을 전달하겠습니다.

5
00:00:23,222 --> 00:00:30,346
우선 학점 관련해서, 어제 저녁 중간고사 점수가 공개되었습니다.
Piazza를 통해 자세한 정보를 살펴보시기 바랍니다.

6
00:00:30,346 --> 00:00:35,402
그리고 과제 2와 마일스톤에 대한 점수는 
이번 주말 중으로 계획하고 있습니다.

7
00:00:36,768 --> 00:00:40,682
그리고 프로젝트와 관련해서는 
모든 팀들은 반드시 프로젝트를 등록하셔야 합니다.

8
00:00:40,682 --> 00:00:47,580
Piazza에 양식이 있으니 확인 후 작성 바라며

9
00:00:47,580 --> 00:00:53,214
작성된 문서를 최종 학점과
포스터 세션에 활용할 예정입니다.

10
00:00:53,214 --> 00:01:01,779
 Tiny ImageNet challenge를 위한 
 Tiny ImageNet 평가 서버가 개설되었습니다.

11
00:01:01,779 --> 00:01:06,193
그리고 몇 일 전 CS231n 수업 평가 설문을
Piazza에 게시하였습니다.

12
00:01:06,193 --> 00:01:13,600
아직 설문을 진행하지 않으신 분들께서는 설문을 부탁드리며
CS231n를 더 성장시키기 위한 많은 피드백 바랍니다.

13
00:01:16,589 --> 00:01:19,650
자 오늘의 주제는 강화학습(reinforcement learning) 입니다.

14
00:01:19,650 --> 00:01:22,544
우리는 지금까지는 
supervised learning을 배웠습니다.

15
00:01:22,544 --> 00:01:30,498
데이터 x와 레이블 y가 있고, 목표는
x를 y에 매핑하는 함수를 학습하는 것이죠

16
00:01:30,498 --> 00:01:35,067
우리가 살펴보았던 classification 문제가
supervised learning의 예시였습니다.

17
00:01:35,067 --> 00:01:37,753
그리고 지난 강의에서는
unsupervised learning를 배웠습니다.

18
00:01:37,753 --> 00:01:45,362
데이터는 있지만 레이블이 없는 경우였죠, 목표는
데이터에 내재된 숨겨진 구조를 학습하는 것이었습니다.

19
00:01:45,362 --> 00:01:50,528
가령 지난 강의에서 배운 
생성모델(generative models)이 있습니다.

20
00:01:52,040 --> 00:01:57,370
이번 시간에는 배울 내용은 문제가 조금 다릅니다. 
강화학습 이라는 문제입니다.

21
00:01:57,370 --> 00:02:01,824
강화학습에는 에이전트(agent)가 있습니다. 에이전트(agent)는 
환경(environment) 에서 행동(action)을 취하는 주체입니다.

22
00:02:01,824 --> 00:02:04,352
에이전트는 에이전트가 취한 행동에 따라서
적절한 보상(rewards)을 받습니다.

23
00:02:04,352 --> 00:02:09,959
강화학습의 목적은 에이전트의 보상을 최대화할 수 있는
액션이 무엇인지를 학습하려는 것입니다.

24
00:02:09,959 --> 00:02:14,101
이번 시간에는 이에 대해서 자세히 다뤄볼 것입니다.

25
00:02:14,101 --> 00:02:16,020
이번 수업의 개요를 말씀드리자면

26
00:02:16,020 --> 00:02:18,116
먼저 강화학습 문제가 무엇인지를 다루고

27
00:02:18,116 --> 00:02:20,927
그리고 Markov Decision Processes(MDP)
에 대해서 다룰 것입니다.

28
00:02:20,927 --> 00:02:24,747
MDP는 강화학습 문제의 수식체계(formalism) 입니다.

29
00:02:24,747 --> 00:02:31,095
그리고 강화학습의 주 분류인 Q-Learning과 
Policy Gradients에 대해서 배워보겠습니다.

30
00:02:32,876 --> 00:02:38,936
강화학습의 문제에서는 (set up)
에이전트(agent)와 환경(environment)이 있습니다.

31
00:02:38,936 --> 00:02:43,268
어떤 환경 속에서 에이전트에는 상태(state)가 주어집니다.

32
00:02:43,268 --> 00:02:46,877
그리고 에이전트는 어떤 행동(action)을 취합니다.

33
00:02:46,877 --> 00:02:52,609
그러면 환경은 에이전트에게 보상(reward)과
다음 상태(Next state)를 줍니다.

34
00:02:52,609 --> 00:03:00,918
이 과정은 환경이 에이전트에게 종료상태(terminal state)를
줄때까지 지속적으로 반복됩니다. 한 에피소드가 끝나는 것이죠

35
00:03:00,918 --> 00:03:03,401
자 그럼 몇 가지 예시를 한번 살펴보겠습니다.

36
00:03:03,401 --> 00:03:05,536
우선 "cart-pole" 문제입니다.

37
00:03:05,536 --> 00:03:11,142
"cart-pole"문제는 아주 고전적인 문제입니다.
CS229 에서 이미 살펴보신 분들도 계실 것입니다.

38
00:03:11,142 --> 00:03:16,252
"cart-pole" 문제의 목표는 움직이는 카트(cart)와 카트 위에
매달려있는 막대기(pole)의 균형을 유지하는 것입니다.

39
00:03:16,252 --> 00:03:20,280
상태(state)에는 현재 시스템이 기술되어있습니다.

40
00:03:20,280 --> 00:03:28,206
가령, 막대기의 각,막대기의 각속도, 
카트의 위치,카트의 수평속도 가 있습니다.

41
00:03:28,206 --> 00:03:33,224
에이전트가 취할수 있는 행동은 카트를 미는 힘입니다.
(horizontal forces )

42
00:03:33,224 --> 00:03:38,387
우리가 해야하는 일은, 카트를 밀면서
막대기의 균형을 잘 유지하는 것입니다.

43
00:03:38,387 --> 00:03:43,990
환경으로 부터 받을 수 있는 보상은 
"막대기가 제대로 서 있다면 1점" 입니다.

44
00:03:43,990 --> 00:03:48,143
여러분은 막대기를 가능한 똑바로 서게 해야합니다.

45
00:03:49,286 --> 00:03:52,192
고전적인  RL문제에서 다뤘던 다른 예시를 살펴보겠습니다.

46
00:03:52,192 --> 00:03:53,998
"로봇 보햄" (robot locomotion) 과 관련된 문제입니다.

47
00:03:53,998 --> 00:03:59,670
여기 휴머노이드 로봇 모델과 
개미 로봇 모델이 있습니다.

48
00:03:59,670 --> 00:04:03,128
우리의 목표는 로봇을 앞으로 나아가도록 하는 것입니다.

49
00:04:03,128 --> 00:04:10,807
이 문제에서 상태(state)는 로봇의 
모든 관절들의 각과 위치입니다.

50
00:04:10,807 --> 00:04:15,887
에이전트가 취할 수 있는 행동(Action)은 
각 관절들에 가해지는 토크(torques) 입니다.

51
00:04:15,887 --> 00:04:21,228
이 문제에서 하고싶은 것은 로봇을 앞으로 나아가게 하는 것이고
앞으로 이동하면 보상을 받고

52
00:04:21,228 --> 00:04:31,701
휴머노이드 로봇의 경우에는 로봇에 똑바로 서 있는
경우에도 추가적인 보상을 받습니다.

53
00:04:33,521 --> 00:04:38,384
게임(games) 또한 RL로 풀 수 있는 문제 중 하나입니다

54
00:04:38,384 --> 00:04:40,700
가령 여기 아타리 게임(Atrari games)이 있습니다.

55
00:04:40,700 --> 00:04:44,280
깊은 강화학습(Deep reinforcement learning)으로 
아주 큰 성과를 기록했습니다.

56
00:04:44,280 --> 00:04:48,574
아타리 게임에서의 목적은 가능한 가장 높은 
점수로 게임을 끝마치는 것입니다.

57
00:04:48,574 --> 00:04:52,753
에이전트가 게임 플레이어가 되어
게임을 진행하게 됩니다.

58
00:04:52,753 --> 00:04:57,506
상태(state)로는 현재 게임 진행 상황이 
픽셀 그대로(raw pixels) 주어집니다.

59
00:04:57,506 --> 00:05:02,882
여러분이 게임을 할 때 보이는 그 픽셀
그대로가 state로 주어지는 것입니다.

60
00:05:02,882 --> 00:05:09,912
그리고 에이전트가 취할 수 있는 행동(action)은 우리가 게임할 때와
마찬가지로 가령 위, 아래, 좌, 우로 움직일 수 잇습니다.

61
00:05:09,912 --> 00:05:12,534
그런 다음 점수를 올리는 것입니다.

62
00:05:12,534 --> 00:05:15,667
또는 매 시간 단계마다 감소하고 목표는

63
00:05:15,667 --> 00:05:19,834
게임 도중 총점을 최대화 할 수 있습니다.

64
00:05:21,312 --> 00:05:24,179
그리고 마지막으로 여기에 게임의 또 다른 예가 있습니다.

65
00:05:24,179 --> 00:05:25,587
그

66
00:05:25,587 --> 00:05:26,587
가라.

67
00:05:27,573 --> 00:05:28,893
뭔가

68
00:05:28,893 --> 00:05:31,697
작년의 중후 심화 학습의 커다란 성취,

69
00:05:31,697 --> 00:05:34,721
Deep Minds AlphaGo가 Lee Sedol을 이길 때,

70
00:05:34,721 --> 00:05:36,867
그 중 하나 인

71
00:05:36,867 --> 00:05:38,589
가장 좋은 지난 몇 년 간 선수들,

72
00:05:38,589 --> 00:05:41,685
그리고 이것은 실제로 뉴스에 다시있다.

73
00:05:41,685 --> 00:05:45,667
너의 일부가 보았을지도 모르는 것처럼, 다른 Go

74
00:05:45,667 --> 00:05:47,529
경쟁은 지금 계속되고있다.

75
00:05:47,529 --> 00:05:50,919
AlphaGo 대 Go 상위 플레이어.

76
00:05:50,919 --> 00:05:53,495
그래서 여기에있는 목표는

77
00:05:53,495 --> 00:05:56,295
게임에서 이기고, 우리 국가는

78
00:05:56,295 --> 00:05:58,349
모든 조각 중, 행동은 다음을 놓을 곳입니다.

79
00:05:58,349 --> 00:06:02,062
한 장 떨어지면 마지막에 이기면 보상이 하나 있습니다.

80
00:06:02,062 --> 00:06:03,912
그렇지 않으면 0입니다.

81
00:06:03,912 --> 00:06:05,032
그리고 우리는 이것에 대해서도 이야기 할 것입니다.

82
00:06:05,032 --> 00:06:08,411
나중에 좀 더 자세하게 설명합니다.

83
00:06:08,411 --> 00:06:09,891
좋아요.

84
00:06:09,891 --> 00:06:12,046
우리는 어떻게 수학적으로 형식화 할 수 있는가?

85
00:06:12,046 --> 00:06:13,330
RL 문제, 맞죠?

86
00:06:13,330 --> 00:06:15,817
우리가 이전에 이야기했던이 루프는,

87
00:06:15,817 --> 00:06:18,051
에이전트 상태를 제공하는 환경,

88
00:06:18,051 --> 00:06:20,634
그리고 행동을 취하는 에이전트.

89
00:06:22,394 --> 00:06:24,884
따라서 Markov 결정 프로세스는 다음과 같습니다.

90
00:06:24,884 --> 00:06:28,512
RL 문제의 수학 공식,

91
00:06:28,512 --> 00:06:31,447
MDP는 Markov 속성을 만족하며,

92
00:06:31,447 --> 00:06:33,054
그것은 현재 상태가 완전히

93
00:06:33,054 --> 00:06:36,107
세계의 상태를 특성화합니다.

94
00:06:36,107 --> 00:06:40,164
그리고 여기에있는 MDP는 객체의 튜플
(tuple of objects)로 정의됩니다.

95
00:06:40,164 --> 00:06:43,170
가능한 상태의 집합 인 S로 구성됩니다.

96
00:06:43,170 --> 00:06:45,762
우리는 가능한 행동의 집합을 가지고 있습니다.

97
00:06:45,762 --> 00:06:50,018
우리는 또한 우리의 보상의 우리의 분배를 가지고 있습니다.

98
00:06:50,018 --> 00:06:51,694
주어진 상태, 행동 쌍,

99
00:06:51,694 --> 00:06:53,824
그래서 그것은 국가 행동에서 함수 매핑이다.

100
00:06:53,824 --> 00:06:55,323
보상에.

101
00:06:55,323 --> 00:06:57,430
당신은 또한 전이 확률 인 P를가집니다.

102
00:06:57,430 --> 00:07:00,079
다음 주에 분배해라.

103
00:07:00,079 --> 00:07:02,940
당신의 상태, 행동 쌍이 주어 지도록 전환 할 것입니다.

104
00:07:02,940 --> 00:07:05,718
그리고 마침내 우리는 감마, 할인 요인,

105
00:07:05,718 --> 00:07:09,720
근본적으로 얼마나 가치있는 일인지 말하고 있습니다.

106
00:07:09,720 --> 00:07:12,970
보상은 곧 나올 것인가?

107
00:07:14,203 --> 00:07:17,395
Markov Decision
Process가 작동하는 방식은

108
00:07:17,395 --> 00:07:20,053
우리의 초기 단계에서 t는 0이고,

109
00:07:20,053 --> 00:07:21,523
환경이 샘플을 일부 채 보려고합니다.

110
00:07:21,523 --> 00:07:24,615
초기 상태 분포에서 0으로 초기 상태,

111
00:07:24,615 --> 00:07:26,363
p는 0이다.

112
00:07:26,363 --> 00:07:29,271
그리고 나서, 일단 그것이
생기면, 시간 t에서 0이됩니다.

113
00:07:29,271 --> 00:07:32,253
완료 될 때까지이 루프를 반복 할 것입니다.

114
00:07:32,253 --> 00:07:35,797
여기서 에이전트는 하나의 작업을 선택합니다.

115
00:07:35,797 --> 00:07:38,885
환경은 여기에서 보상을 샘플링 할 것입니다.

116
00:07:38,885 --> 00:07:41,907
당신의 주와

117
00:07:41,907 --> 00:07:44,032
당신이 방금 한 행동.

118
00:07:44,032 --> 00:07:47,640
또한 다음 상태를 샘플링 할 예정이며,

119
00:07:47,640 --> 00:07:51,534
확률 분포가 주어진 시간 t + 1에

120
00:07:51,534 --> 00:07:54,467
그 다음에 에이전트가

121
00:07:54,467 --> 00:07:56,790
보상뿐만 아니라 다음 주까지, 그리고 우리는

122
00:07:56,790 --> 00:07:58,707
이 과정을 다시 거치면서,

123
00:07:58,707 --> 00:08:01,769
계속 반복한다. 에이전트는 다음 조치를 선택합니다.

124
00:08:01,769 --> 00:08:05,542
에피소드가 끝날 때까지 계속됩니다.

125
00:08:05,542 --> 00:08:06,989
좋아요.

126
00:08:06,989 --> 00:08:10,724
이제이를 바탕으로 우리는 정책
pi를 정의 할 수 있습니다.

127
00:08:10,724 --> 00:08:13,593
당신의 상태에서 당신의 행동에 이르는 기능입니다

128
00:08:13,593 --> 00:08:16,651
각 상태에서 수행 할 작업을 지정합니다.

129
00:08:16,651 --> 00:08:19,748
그리고 이것은 결정론 적이거나 확률 론적 일 수 있습니다.

130
00:08:19,748 --> 00:08:22,447
그리고 이제 우리의 목표는

131
00:08:22,447 --> 00:08:24,727
귀하의 최적 정책 인 파이 스타, 귀하의

132
00:08:24,727 --> 00:08:27,205
누적 할인 된 보상.

133
00:08:27,205 --> 00:08:29,059
그래서 우리는 여기서 우리가

134
00:08:29,059 --> 00:08:31,813
우리의 미래의 보상 중 일부는 또한있을 수 있습니다.

135
00:08:31,813 --> 00:08:35,509
귀하의 할인율로 할인됩니다.

136
00:08:35,509 --> 00:08:39,327
간단한 MDP의 예를 살펴 보겠습니다.

137
00:08:39,327 --> 00:08:42,034
여기 그리드 월드가 있습니다.

138
00:08:42,034 --> 00:08:44,533
우리가이 상태 그리드를 가지고있는 작업.

139
00:08:44,533 --> 00:08:46,950
그래서 당신은이 중 어느 곳에 나있을 수 있습니다.

140
00:08:48,112 --> 00:08:50,295
귀하의 상태 인 귀하의 그리드의 셀.

141
00:08:50,295 --> 00:08:52,613
그리고 당신은 당신의 주에서 행동을 취할 수 있습니다.

142
00:08:52,613 --> 00:08:54,713
그래서이 행동들은

143
00:08:54,713 --> 00:08:56,527
간단한 움직임, 오른쪽으로 이동,

144
00:08:56,527 --> 00:08:59,299
왼쪽, 위, 아래.

145
00:08:59,299 --> 00:09:02,683
그리고 당신은 각각에 대해 부정적인 보상을 얻을 것입니다.

146
00:09:02,683 --> 00:09:07,163
전환 또는 매 시간마다, 기본적으로 그렇게됩니다.

147
00:09:07,163 --> 00:09:08,859
당신이 취하는 각 운동,

148
00:09:08,859 --> 00:09:11,989
이것은 R이 음의 것과 같을 수 있습니다.

149
00:09:11,989 --> 00:09:13,871
그래서 당신의 목표는

150
00:09:13,871 --> 00:09:15,588
터미널 상태 중 하나에 도달하기 위해,

151
00:09:15,588 --> 00:09:17,793
여기에 표시된 회색 상태 인,

152
00:09:17,793 --> 00:09:20,055
가장 적은 수의 행동으로

153
00:09:20,055 --> 00:09:22,249
맞아, 네가 도달하는 데 더 오래 걸릴거야.

154
00:09:22,249 --> 00:09:23,522
당신의 터미널 상태, 당신은

155
00:09:23,522 --> 00:09:26,522
이러한 부정적인 보상을 누적합니다.

156
00:09:27,625 --> 00:09:30,540
그래, 여기 무작위 정책을 보면,

157
00:09:30,540 --> 00:09:33,141
무작위 정책은 기본적으로,

158
00:09:33,141 --> 00:09:35,305
현재있는 주나 세포에서

159
00:09:35,305 --> 00:09:37,770
어떤 방향으로 무작위로 샘플링

160
00:09:37,770 --> 00:09:39,090
당신은 다음에 이사 할 것입니다.

161
00:09:39,090 --> 00:09:41,843
맞습니다. 그래서이 모든 것들은 동등한 확률을 갖습니다.

162
00:09:41,843 --> 00:09:44,115
반면, 최적의 정책은

163
00:09:44,115 --> 00:09:46,518
우리는 갖고 싶다.

164
00:09:46,518 --> 00:09:48,672
기본적으로 행동을 취하고, 방향

165
00:09:48,672 --> 00:09:51,866
그것은 터미널 상태에 가장 가까운 우리를 움직일 것입니다.

166
00:09:51,866 --> 00:09:53,164
그래서 여기에서 볼 수 있습니다.

167
00:09:53,164 --> 00:09:54,808
우리가 바로 옆에 있다면

168
00:09:54,808 --> 00:09:56,156
터미널 상태

169
00:09:56,156 --> 00:09:57,506
항상 방향으로 움직인다.

170
00:09:57,506 --> 00:09:59,171
이 터미널 상태가됩니다.

171
00:09:59,171 --> 00:10:01,385
그리고 그렇지 않으면, 당신이이
다른 주들 중 하나에 있다면,

172
00:10:01,385 --> 00:10:03,822
너는 너를 데려 갈 방향을 원해.

173
00:10:03,822 --> 00:10:06,405
이 상태 중 하나에 가장 가깝습니다.

174
00:10:09,119 --> 00:10:11,644
좋아, 이제 이걸 받았다.

175
00:10:11,644 --> 00:10:13,745
MDP에 대한 설명, 우리가하고 싶은 일

176
00:10:13,745 --> 00:10:17,155
우리는 우리의 최적 정책 인 파이 스타를 찾고 싶습니까?

177
00:10:17,155 --> 00:10:20,755
맞습니다. 보상 정책의 총액을 최대화하는 정책입니다.

178
00:10:20,755 --> 00:10:22,955
그래서이 최적의 정책은 우리에게 말할 것입니다.

179
00:10:22,955 --> 00:10:25,655
우리가있는 어떤 국가든지 주어질
때, 그 행동은 무엇입니까?

180
00:10:25,655 --> 00:10:27,851
우리는 합계를 극대화하기 위해 취해야한다.

181
00:10:27,851 --> 00:10:29,731
우리가 얻을 수있는 보상들.

182
00:10:29,731 --> 00:10:32,011
그래서 한 가지 질문은 어떻게 우리가

183
00:10:32,011 --> 00:10:34,091
MDP의 무작위성을 다루지, 그렇지?

184
00:10:34,091 --> 00:10:36,459
우리는 무작위성을 가지고 있습니다.

185
00:10:36,459 --> 00:10:39,073
우리가 샘플링하는 우리 초기 상태의 조건,

186
00:10:39,073 --> 00:10:40,727
이 전환 확률의 열

187
00:10:40,727 --> 00:10:42,303
우리에게주는 분배

188
00:10:42,303 --> 00:10:46,341
우리의 다음주의 국가 분포 등등.

189
00:10:46,341 --> 00:10:49,292
또한 우리가 할 일은 최대화로 일할 것입니다.

190
00:10:49,292 --> 00:10:51,947
보상의 우리의 예상 합계.

191
00:10:51,947 --> 00:10:55,451
공식적으로, 우리는 우리의 최적 정책
인 파이 스타를 쓸 수 있습니다.

192
00:10:55,451 --> 00:10:59,129
기대되는 미래 보상의 총액을 최대화하는 것으로

193
00:10:59,129 --> 00:11:02,957
over 정책의 pi, 여기서 우리는 초기 상태

194
00:11:02,957 --> 00:11:05,103
우리의 주 분포에서 추출한 것입니다.

195
00:11:05,103 --> 00:11:07,388
우리에게는 행동이 있습니다.

196
00:11:07,388 --> 00:11:09,127
국가에서 주어진 우리의 정책에서 표본 추출.

197
00:11:09,127 --> 00:11:11,929
그런 다음 우리는 다음 번 샘플을 샘플링합니다.

198
00:11:11,929 --> 00:11:16,423
우리의 전환 확률 분포로부터.

199
00:11:16,423 --> 00:11:17,256
좋아요.

200
00:11:18,351 --> 00:11:19,668
우리가 이야기하기 전에

201
00:11:19,668 --> 00:11:22,143
정확히 우리가이 정책을 어떻게 찾을 지,

202
00:11:22,143 --> 00:11:23,909
먼저 몇 가지 정의에 대해 이야기 해 봅시다.

203
00:11:23,909 --> 00:11:26,787
그렇게하면 우리에게 도움이 될 것입니다.

204
00:11:26,787 --> 00:11:29,115
그래서, 구체적으로, value 함수

205
00:11:29,115 --> 00:11:31,405
및 Q 값 함수.

206
00:11:31,405 --> 00:11:33,647
그래서, 우리가 정책을 따를 때,

207
00:11:33,647 --> 00:11:35,489
우리는 궤적을 샘플링하려고합니다.

208
00:11:35,489 --> 00:11:37,426
또는 경로, 모든 에피소드마다 맞습니다.

209
00:11:37,426 --> 00:11:40,287
그리고 우리는 초기 상태를 0으로하려고합니다.

210
00:11:40,287 --> 00:11:43,611
a-zero, r-zero, s-one,
a-one, r-one 등이 있습니다.

211
00:11:43,611 --> 00:11:44,905
우리는이 궤적을 가질 것입니다.

212
00:11:44,905 --> 00:11:49,331
우리가 얻는 국가, 행동 및 보상에 대해

213
00:11:49,331 --> 00:11:52,613
우리가 현재있는 상태가 얼마나 좋은가?

214
00:11:52,613 --> 00:11:55,985
음, 모든 상태 s에서의 값 함수는,

215
00:11:55,985 --> 00:11:58,513
기대 누적 보상액이다.

216
00:11:58,513 --> 00:12:01,770
국가의 정책에 따라, 여기부터.

217
00:12:01,770 --> 00:12:05,258
맞습니다. 그래서 그것은 기대되는 가치가 될 것입니다.

218
00:12:05,258 --> 00:12:07,635
우리가 기대하는 누적 보상,

219
00:12:07,635 --> 00:12:10,800
현재 상태에서 시작합니다.

220
00:12:10,800 --> 00:12:13,286
그리고 상태, 행동 쌍이 얼마나 좋은가요?

221
00:12:13,286 --> 00:12:17,370
그렇다면 주에서 행동을 취하는 것이 얼마나 좋은가?

222
00:12:17,370 --> 00:12:20,468
그리고 이것을 Q- 값 함수를 사용하여 정의합니다.

223
00:12:20,468 --> 00:12:23,574
예상되는 누적 보상

224
00:12:23,574 --> 00:12:27,741
상태 s에서 조치 a를 수행 한 후 정책을 수행하십시오.

225
00:12:29,708 --> 00:12:32,708
그렇다면 최적의 Q 값 함수

226
00:12:32,708 --> 00:12:36,404
우리가 얻을 수있는 것은 Q 스타가 될 것입니다.

227
00:12:36,404 --> 00:12:39,216
우리가 얻을 수있는 예상 누적 보상

228
00:12:39,216 --> 00:12:43,383
여기에 정의 된 주어진 상태 액션 쌍으로부터.

229
00:12:45,099 --> 00:12:48,592
이제 우리는 중요한 한 가지를 보게 될 것입니다.

230
00:12:48,592 --> 00:12:50,018
강화 학습에서,

231
00:12:50,018 --> 00:12:52,018
벨맨 방정식이라고합니다.

232
00:12:52,018 --> 00:12:54,485
이제 이것을 Q 값 함수라고 생각해 봅시다.

233
00:12:54,485 --> 00:12:57,697
최적의 정책 Q 별에서,

234
00:12:57,697 --> 00:13:00,911
그러면이 벨맨 방정식을 만족시킬 것입니다.

235
00:13:00,911 --> 00:13:03,533
여기에 표시된이 정체성은

236
00:13:03,533 --> 00:13:05,194
이것이 의미하는 바는

237
00:13:05,194 --> 00:13:08,873
어떤 상태, 행동 쌍, s와 a,

238
00:13:08,873 --> 00:13:11,748
이 쌍의 가치는 보상이 될 것입니다.

239
00:13:11,748 --> 00:13:15,092
당신이 얻으려고하는 것, r, 무엇이든지의 가치를 더한 것

240
00:13:15,092 --> 00:13:16,517
네가 끝내는 상태 야.

241
00:13:16,517 --> 00:13:18,868
그래서, 프라임이라고합시다.

242
00:13:18,868 --> 00:13:22,319
그리고 우리는 우리에게 최적의 정책이
있다는 것을 알고 있기 때문에,

243
00:13:22,319 --> 00:13:24,150
우리는 또한 우리가

244
00:13:24,150 --> 00:13:26,202
우리가 할 수있는 최선의 행동을해라.

245
00:13:26,202 --> 00:13:28,746
맞아, 우리 주에서는 프라임이야.

246
00:13:28,746 --> 00:13:31,413
그리고 나서, 상태 s의 값은 소수입니다.

247
00:13:31,413 --> 00:13:34,432
우리 행동에 대해 최대가 될 것입니다.

248
00:13:34,432 --> 00:13:38,626
프라임 (Prime)의 프라임 (Q prime) 프라임 프라임.

249
00:13:38,626 --> 00:13:41,325
그리고 나서 우리는 이것을 얻습니다.

250
00:13:41,325 --> 00:13:44,119
최적의 Q 값을 위해 여기에 ID를 입력하십시오.

251
00:13:44,119 --> 00:13:46,753
맞습니다. 그리고 나서 언제나처럼, 우리는

252
00:13:46,753 --> 00:13:48,075
이 기대,

253
00:13:48,075 --> 00:13:49,880
우리는 어떤 상태에 대한 무작위성을 가지고 있기 때문에

254
00:13:49,880 --> 00:13:52,380
우리는 결국 끝날 것입니다.

255
00:13:54,252 --> 00:13:56,782
그리고 나서 우리는 여기에서 우리가

256
00:13:56,782 --> 00:13:58,928
최적의 정책, 권리, 구성 될 것입니다.

257
00:13:58,928 --> 00:14:00,860
어떤 주에서도 최선의 행동을 취하는 것,

258
00:14:00,860 --> 00:14:02,488
Q 별에 지정된대로.

259
00:14:02,488 --> 00:14:04,295
Q 스타가 우리에게 말할 것입니다.

260
00:14:04,295 --> 00:14:05,462
최대의

261
00:14:06,540 --> 00:14:08,437
우리의 행동에서 얻을 수있는 미래의 보상,

262
00:14:08,437 --> 00:14:09,456
그래서 우리는 단지해야만합니다.

263
00:14:09,456 --> 00:14:11,356
이 정책을 따르십시오.

264
00:14:11,356 --> 00:14:13,615
그 행동을 취하는 것만으로

265
00:14:13,615 --> 00:14:16,863
최고의 보상으로 이어질 것입니다.

266
00:14:16,863 --> 00:14:21,025
좋아요, 그렇다면이 최적의 정책을 어떻게 풀 수 있을까요?

267
00:14:21,025 --> 00:14:23,381
그래서 우리가 해결할 수있는 한 가지 방법은 다음과 같습니다.

268
00:14:23,381 --> 00:14:25,692
값 반복 알고리즘 (value
iteration algorithm)

269
00:14:25,692 --> 00:14:28,046
여기서 우리는이 Bellman 방정식을 사용할 것입니다.

270
00:14:28,046 --> 00:14:29,527
반복 업데이트로.

271
00:14:29,527 --> 00:14:33,830
그래서 각 단계에서 우리는 우리의
근사치를 다듬을 것입니다.

272
00:14:33,830 --> 00:14:37,997
Bellman 방정식을 시행하려고 함으로서 Q 별의

273
00:14:39,347 --> 00:14:42,602
그리고 몇몇 수학적 조건 하에서,

274
00:14:42,602 --> 00:14:45,602
우리는 또한이 시퀀스 Q, i

275
00:14:47,008 --> 00:14:49,569
우리의 Q-function은 우리의
최적 상태로 수렴 할 것입니다

276
00:14:49,569 --> 00:14:52,236
내가 무한대에 접근 할 때 Q 별.

277
00:14:54,257 --> 00:14:55,579
그리고 이렇게, 이것은 잘 작동합니다,

278
00:14:55,579 --> 00:14:58,329
하지만이 문제는 무엇입니까?

279
00:14:59,184 --> 00:15:01,887
글쎄, 중요한 문제는 이것이 확장 할 수 없다는 것입니다.

280
00:15:01,887 --> 00:15:02,720
권리?

281
00:15:02,720 --> 00:15:03,553
우리는

282
00:15:03,553 --> 00:15:05,793
Q의 s, 모든 상태, 행동 쌍에 대한 여기

283
00:15:05,793 --> 00:15:08,597
반복 업데이트를하기 위해

284
00:15:08,597 --> 00:15:10,382
맞아,하지만이게 문제 야.

285
00:15:10,382 --> 00:15:13,049
예를 들어, 우리가 이것들을 본다면

286
00:15:14,021 --> 00:15:15,865
예를 들어 Atari 게임의 상태

287
00:15:15,865 --> 00:15:17,519
우리가 이전에 가지고 있었던 것, 그것이 될 것입니다.

288
00:15:17,519 --> 00:15:18,933
너의 화소의 스크린.

289
00:15:18,933 --> 00:15:22,229
그리고 이것은 거대한 국가 공간이며 기본적으로

290
00:15:22,229 --> 00:15:23,865
계산 불가능한

291
00:15:23,865 --> 00:15:27,448
전체 상태 공간에 대해이를 계산합니다.

292
00:15:28,725 --> 00:15:31,200
좋아요, 그래서 이것에 대한 해결책은 무엇입니까?

293
00:15:31,200 --> 00:15:33,141
음, 함수 근사를 사용할 수 있습니다.

294
00:15:33,141 --> 00:15:35,908
s의 Q를 추정하기 위해, a

295
00:15:35,908 --> 00:15:37,620
그래서, 예를 들어, 신경망, 맞아.

296
00:15:37,620 --> 00:15:40,400
그래서 우리는 언제든지 그 전에 보았습니다.

297
00:15:40,400 --> 00:15:42,367
알지 못하는 정말로 복잡한 기능, 우리가 원하는 기능

298
00:15:42,367 --> 00:15:44,360
추정하기 위해 신경망은

299
00:15:44,360 --> 00:15:46,693
이것을 추정하는 좋은 방법입니다.

300
00:15:48,472 --> 00:15:51,458
좋아요. 그러면 우리를 우리에게 데려 갈 것입니다.

301
00:15:51,458 --> 00:15:54,242
우리가 보게 될 Q- 학습의 공식화.

302
00:15:54,242 --> 00:15:56,646
그래서 우리가 할 일은 우리가 가고있는 것입니다.

303
00:15:56,646 --> 00:15:58,906
함수 근사자를 사용하는 방법

304
00:15:58,906 --> 00:16:02,118
우리의 행동 가치 기능을 평가하기 위해

305
00:16:02,118 --> 00:16:02,951
권리?

306
00:16:02,951 --> 00:16:04,502
그리고 만약이 함수 접근법

307
00:16:04,502 --> 00:16:06,013
깊은 신경 네트워크입니다.

308
00:16:06,013 --> 00:16:08,142
최근에 사용 된 것은,

309
00:16:08,142 --> 00:16:10,782
그러면 이것은 깊은
Q-learning이라고 불릴 것입니다.

310
00:16:10,782 --> 00:16:12,322
그래서 이것은

311
00:16:12,322 --> 00:16:15,742
일반적인 접근 방식 중 하나로서 주변에서들을 것입니다.

312
00:16:15,742 --> 00:16:20,150
사용중인 심층적 인 학습에 이르기까지

313
00:16:20,150 --> 00:16:21,259
맞아, 그리고이 경우에,

314
00:16:21,259 --> 00:16:23,474
함수 매개 변수도 있습니다.

315
00:16:23,474 --> 00:16:26,134
여기에 우리의 Q 값 함수

316
00:16:26,134 --> 00:16:28,348
이 가중치에 의해 결정되며,

317
00:16:28,348 --> 00:16:30,765
우리의 신경 네트워크의 세타.

318
00:16:33,050 --> 00:16:35,425
좋아요,이 함수 근사값이 주어지면,

319
00:16:35,425 --> 00:16:37,970
최적의 정책을 어떻게 풀 수 있을까요?

320
00:16:37,970 --> 00:16:39,814
우리가 원하는 것을 기억하십시오.

321
00:16:39,814 --> 00:16:44,744
Bellman 방정식을 만족시키는 Q 함수.

322
00:16:44,744 --> 00:16:47,017
맞아, 그래서 우리는이 Bellman
방정식을 시행하기를 원합니다.

323
00:16:47,017 --> 00:16:50,452
일어날 일 이니, 우리가 이것을
할 때 우리가 할 수있는 일.

324
00:16:50,452 --> 00:16:54,713
우리의 Q 함수를 근사화하는 신경망은

325
00:16:54,713 --> 00:16:56,811
우리는 우리의 손실 함수가

326
00:16:56,811 --> 00:16:58,169
시도하고 최소화하려고합니다.

327
00:16:58,169 --> 00:17:00,240
우리의 벨만 방정식의 오류, 맞죠?

328
00:17:00,240 --> 00:17:03,689
또는 얼마나 멀리의 q, 그것의 표적에서이고,

329
00:17:03,689 --> 00:17:06,454
여기는 Y_i, 오른쪽은

330
00:17:06,454 --> 00:17:09,853
우리가 전에 보았던 벨만 방정식의

331
00:17:09,853 --> 00:17:12,103
그래서 우리는 기본적으로 이들을 취할 것입니다.

332
00:17:12,103 --> 00:17:13,994
우리의

333
00:17:13,994 --> 00:17:16,928
손실 함수,이 오류를 최소화하려고합니다.

334
00:17:16,929 --> 00:17:19,332
그리고 우리의 후방 통과, 그라디언트 업데이트,

335
00:17:19,332 --> 00:17:20,863
그냥있을거야.

336
00:17:20,863 --> 00:17:23,243
너는 이것의 그라디언트를 가져 가라.

337
00:17:23,243 --> 00:17:28,182
손실, 우리의 네트워크 매개 변수의 세타에 관해서.

338
00:17:28,183 --> 00:17:31,568
맞습니다. 그래서 우리의 목표는 다시

339
00:17:31,568 --> 00:17:33,752
그라데이션 단계를 취할 때이 효과가 있습니다.

340
00:17:33,752 --> 00:17:36,107
반복적으로 우리의 Q-function을 만들기 위해

341
00:17:36,107 --> 00:17:38,436
우리의 목표 값에 더 가깝습니다.

342
00:17:38,436 --> 00:17:40,853
그럼, 이것에 대해 질문이 있으십니까?

343
00:17:42,691 --> 00:17:43,524
괜찮아.

344
00:17:44,537 --> 00:17:48,719
이제 사례에 대한 사례 연구를 살펴 보겠습니다.

345
00:17:48,719 --> 00:17:50,824
심층 강화 학습의 고전적인 사례 중 하나

346
00:17:50,824 --> 00:17:53,370
이 접근법이 적용된 곳.

347
00:17:53,370 --> 00:17:56,174
그래서 우리는 이전에 보았던이 문제를 살펴볼 것입니다.

348
00:17:56,174 --> 00:17:59,744
우리 게임의 목표가

349
00:17:59,744 --> 00:18:01,746
최고 점수로 게임을 끝내라.

350
00:18:01,746 --> 00:18:04,150
우리의 상태가 원시 픽셀이 될 것임을 기억하십시오.

351
00:18:04,150 --> 00:18:05,460
게임 상태의 입력,

352
00:18:05,460 --> 00:18:07,064
우리는 이러한 행동을 취할 수있다.

353
00:18:07,064 --> 00:18:09,308
왼쪽, 오른쪽, 위, 아래,

354
00:18:09,308 --> 00:18:12,964
또는 특정 게임의 어떤 행동이든간에.

355
00:18:12,964 --> 00:18:15,210
그리고 매 단계마다 우리의 보상, 우리는

356
00:18:15,210 --> 00:18:18,509
우리 점수의 보상은 우리가

357
00:18:18,509 --> 00:18:21,183
이 단계에서 얻은 결과, 누적 합계

358
00:18:21,183 --> 00:18:24,435
보상은 우리가 일반적으로 볼 수있는 총 보상입니다.

359
00:18:24,435 --> 00:18:27,095
화면 상단에

360
00:18:27,095 --> 00:18:30,135
좋아, 그래서 우리가 우리의 네트워크에 사용할 네트워크

361
00:18:30,135 --> 00:18:32,955
Q 함수는 다음과 같이 보일 것입니다.

362
00:18:32,955 --> 00:18:37,355
맞아요, 우리는 Q-network을
가지고 있고, 체중은 theta입니다.

363
00:18:37,355 --> 00:18:41,272
그리고 나서 우리의 의견은 우리의 상태가 될 것입니다.

364
00:18:42,259 --> 00:18:43,791
우리의 현재 게임 화면.

365
00:18:43,791 --> 00:18:45,377
그리고 실제로 우리는

366
00:18:45,377 --> 00:18:49,509
마지막 네 개의 프레임이 쌓여서 역사가 있습니다.

367
00:18:49,509 --> 00:18:52,340
그래서 우리는 이러한 원시 픽셀 값을 취합니다.

368
00:18:52,340 --> 00:18:55,609
우리는 RGB에서 그레이 스케일 변환을 할 것입니다.

369
00:18:55,609 --> 00:18:57,053
일부 다운 샘플링, 일부 수확,

370
00:18:57,053 --> 00:18:58,609
그래서 일부 사전 처리.

371
00:18:58,609 --> 00:19:02,543
그리고 우리가 이것에서 빠져 나가는 것은 84 x 84

372
00:19:02,543 --> 00:19:04,631
마지막 네 프레임의 스택.

373
00:19:04,631 --> 00:19:05,464
그래, 질문.

374
00:19:05,464 --> 00:19:09,631
[청중의 들리지 않는 질문]

375
00:19:12,792 --> 00:19:14,768
좋아요, 질문은, 우리가 여기서 말하는 것입니까?

376
00:19:14,768 --> 00:19:18,067
우리의 네트워크가

377
00:19:18,067 --> 00:19:20,809
다른 상태, 액션 쌍,

378
00:19:20,809 --> 00:19:22,491
예를 들어, 이들 중 네 가지?

379
00:19:22,491 --> 00:19:24,765
그래, 맞아.

380
00:19:24,765 --> 00:19:25,598
우리는 볼 것이다,

381
00:19:25,598 --> 00:19:27,551
우리는 몇 가지 슬라이드에서 그것에 대해 이야기 할 것입니다.

382
00:19:27,551 --> 00:19:29,935
[청중의 들리지 않는 질문]

383
00:19:29,935 --> 00:19:30,768
그래서 아니야.

384
00:19:30,768 --> 00:19:32,883
그래서 우리는 연결된 후에 Softmax
레이어를 가지고 있지 않습니다.

385
00:19:32,883 --> 00:19:35,535
여기에 우리의 목표는 직접 예측하는 것이기 때문에

386
00:19:35,535 --> 00:19:36,816
우리의 Q 가치 기능.

387
00:19:36,816 --> 00:19:37,712
[청중의 들리지 않는 질문]

388
00:19:37,712 --> 00:19:38,545
Q 값.

389
00:19:38,545 --> 00:19:40,583
[청중의 들리지 않는 질문]

390
00:19:40,583 --> 00:19:44,014
예, Q 값에 대한 회귀가 더 진행되고 있습니다.

391
00:19:44,014 --> 00:19:47,549
좋아요, 그래서 우리는이 네트워크에 대한 의견을 가지고 있습니다.

392
00:19:47,549 --> 00:19:51,007
그리고 이것의 꼭대기에, 우리는

393
00:19:51,007 --> 00:19:52,847
친숙한 길쌈 층 (convolutional layers)

394
00:19:52,847 --> 00:19:54,084
및 완전히 접속 된 층,

395
00:19:54,084 --> 00:19:55,334
그래서 여기에 우리가있다.

396
00:19:56,191 --> 00:19:58,036
8 x 8의 컨볼 루션과 우리는

397
00:19:58,036 --> 00:19:59,611
4 × 4 컨볼 루션.

398
00:19:59,611 --> 00:20:01,861
그런 다음 FC 256 계층을 가지고 있습니다.

399
00:20:01,861 --> 00:20:03,458
그래서 이것은 표준 네트워크 일뿐입니다.

400
00:20:03,458 --> 00:20:05,674
전에 보았던 것.

401
00:20:05,674 --> 00:20:10,382
그런 다음 마지막으로 완전히 연결된 레이어에

402
00:20:10,382 --> 00:20:13,470
출력에 해당하는 벡터입니다.

403
00:20:13,470 --> 00:20:16,074
각 액션에 대한 Q- 값, 오른쪽, 주어진

404
00:20:16,074 --> 00:20:17,415
당신이 입력 한 상태.

405
00:20:17,415 --> 00:20:19,565
예를 들어 네 가지 조치가있는 경우

406
00:20:19,565 --> 00:20:21,770
그런 다음 여기에 우리는이 4 차원 출력을 가진다.

407
00:20:21,770 --> 00:20:25,570
전류 s의 Q에 대응하고, a-one,

408
00:20:25,570 --> 00:20:28,685
그리고 a-two, a-three, 그리고 a-four.

409
00:20:28,685 --> 00:20:30,857
맞아, 이것이 스칼라 값이 될거야.

410
00:20:30,857 --> 00:20:33,179
각각의 행동에 대해

411
00:20:33,179 --> 00:20:35,610
그리고 나서 우리가 가진 행동의 수

412
00:20:35,610 --> 00:20:36,955
사이에 다를 수 있습니다,

413
00:20:36,955 --> 00:20:41,122
예를 들어, Atari 게임에 따라 4에서 18까지.

414
00:20:43,073 --> 00:20:44,839
여기에 좋은 점 하나가 있습니다.

415
00:20:44,839 --> 00:20:46,709
이 네트워크 구조를 사용하여,

416
00:20:46,709 --> 00:20:49,931
단일 피드 포워드 패스가

417
00:20:49,931 --> 00:20:52,810
모든 기능에 대한 Q 값

418
00:20:52,810 --> 00:20:54,651
현재 상태에서.

419
00:20:54,651 --> 00:20:56,117
그래서 이것은 정말 효율적입니다.

420
00:20:56,117 --> 00:20:59,158
맞습니다. 그래서 기본적으로 우리는 현재 상태를 취합니다.

421
00:20:59,158 --> 00:21:03,121
우리가이 행동의 결과를 가지고 있기 때문에

422
00:21:03,121 --> 00:21:05,946
각 액션에 대한 각 값 또는 Q 값은 출력 레이어로,

423
00:21:05,946 --> 00:21:10,259
우리는 하나의 합격을 할 수 있고이
모든 가치를 얻을 수 있습니다.

424
00:21:10,259 --> 00:21:12,235
그리고 이것을 훈련시키기 위해서,

425
00:21:12,235 --> 00:21:15,078
이전부터 우리의 손실 함수를 사용하려고합니다.

426
00:21:15,078 --> 00:21:17,661
기억해보십시오, 우리는이 Bellman
방정식을 시행하려고합니다.

427
00:21:17,661 --> 00:21:21,329
그래서 우리의 앞으로의 패스에서, 우리의 손실 함수

428
00:21:21,329 --> 00:21:25,193
우리는 반복적으로 Q-value를
만들려고 노력할 것입니다.

429
00:21:25,193 --> 00:21:27,987
우리 목표 값에 가깝고,

430
00:21:27,987 --> 00:21:29,315
그것이 있어야합니다.

431
00:21:29,315 --> 00:21:31,281
그리고 우리의 후방 통과는 단지

432
00:21:31,281 --> 00:21:34,235
직접이 그라디언트를 복용

433
00:21:34,235 --> 00:21:37,277
우리가 가지고있는 손실 함수

434
00:21:37,277 --> 00:21:39,777
그라디언트 단계를 기반으로합니다.

435
00:21:40,706 --> 00:21:42,948
여기에 사용 된 다른 하나는 언급하고 싶습니다.

436
00:21:42,948 --> 00:21:45,639
경험 재생이라고하는 것입니다.

437
00:21:45,639 --> 00:21:49,556
그래서 이것은 단지 다음을 사용하여 문제를 해결합니다.

438
00:21:50,579 --> 00:21:53,440
방금 설명한 두 개의 일반 네트워크,

439
00:21:53,440 --> 00:21:55,416
그것은 일괄 처리에서 배우는 것입니다.

440
00:21:55,416 --> 00:21:58,134
의 연속 샘플이 나쁘다.

441
00:21:58,134 --> 00:21:58,967
그래서 이유는

442
00:21:58,967 --> 00:22:01,268
이것 때문에, 맞아, 그냥

443
00:22:01,268 --> 00:22:03,578
게임을하고, 샘플을 가져 간다.

444
00:22:03,578 --> 00:22:06,074
우리가 가진 국가 행동 보상의

445
00:22:06,074 --> 00:22:08,222
이 샘플들을 연속적으로 가져 가라.

446
00:22:08,222 --> 00:22:09,410
그리고 이것들을 가지고 훈련 시키면,

447
00:22:09,410 --> 00:22:11,814
이러한 모든 샘플은 상관 관계가 있습니다.

448
00:22:11,814 --> 00:22:14,218
그래서 이것은

449
00:22:14,218 --> 00:22:16,118
비효율적 인 학습, 우선,

450
00:22:16,118 --> 00:22:19,014
또한이 때문에 우리의 현재 Q- 네트워크

451
00:22:19,014 --> 00:22:21,456
매개 변수, 오른쪽,이 정책을 결정합니다.

452
00:22:21,456 --> 00:22:24,842
우리가 따라야 할 것이고, 그것은 우리의 다음

453
00:22:24,842 --> 00:22:25,798
샘플을 얻으 려구요.

454
00:22:25,798 --> 00:22:27,394
우리는 훈련을 위해 사용할 것입니다.

455
00:22:27,394 --> 00:22:29,578
그래서 이것은 문제를 일으킨다.

456
00:22:29,578 --> 00:22:30,832
나쁜 피드백 루프를 가질 수 있습니다.

457
00:22:30,832 --> 00:22:33,920
예를 들어, 현재 최대화

458
00:22:33,920 --> 00:22:35,468
왼쪽으로 갈 행동,

459
00:22:35,468 --> 00:22:37,588
글쎄,이게 내 모든 것을 편향시킬거야.

460
00:22:37,588 --> 00:22:39,380
다가오는 훈련 예가 지배적이다.

461
00:22:39,380 --> 00:22:42,297
왼쪽에서 샘플로.

462
00:22:43,306 --> 00:22:45,406
그래서 이것은 문제입니다, 그렇죠?

463
00:22:45,406 --> 00:22:47,875
그래서 우리는이 문제를 해결할 것입니다.

464
00:22:47,875 --> 00:22:49,808
문제는 불리는 것을 사용하는 것입니다.

465
00:22:49,808 --> 00:22:53,098
경험 재생, 우리는 이것을 계속하려고합니다.

466
00:22:53,098 --> 00:22:56,469
상태의 전이의 재생 메모리 테이블,

467
00:22:56,469 --> 00:22:59,345
국가, 행동, 보상, 다음 주,

468
00:22:59,345 --> 00:23:01,353
우리가 가진 전환과 우리는 가고있다.

469
00:23:01,353 --> 00:23:04,279
이 표를 새로운 전환으로 계속 업데이트하려면

470
00:23:04,279 --> 00:23:07,185
게임 에피소드가 진행되는 동안 우리가 얻고있는,

471
00:23:07,185 --> 00:23:08,773
우리는 더 많은 경험을 얻고 있습니다.

472
00:23:08,773 --> 00:23:10,653
맞아, 이제 우리가 할 수있는 일

473
00:23:10,653 --> 00:23:13,207
우리는 이제 무작위로 Q-network을
훈련 할 수 있으며,

474
00:23:13,207 --> 00:23:16,335
재생 메모리에서 전환의 미니 배치.

475
00:23:16,335 --> 00:23:19,261
맞아, 연속 샘플을 사용하는 대신에,

476
00:23:19,261 --> 00:23:21,815
우리는 이제 이들을 통해 샘플링 할 것입니다.

477
00:23:21,815 --> 00:23:24,827
우리가 이것들의 무작위 샘플을 축적 한 전환들,

478
00:23:24,827 --> 00:23:27,573
그리고 이것은 모든 것을 깨뜨린다.

479
00:23:27,573 --> 00:23:31,007
우리가 이전에 가지고 있던 이러한 상관 관계 문제.

480
00:23:31,007 --> 00:23:33,425
그리고 또 다른

481
00:23:33,425 --> 00:23:36,370
부수적 인 이점은 이러한 각각의 천이

482
00:23:36,370 --> 00:23:39,207
또한 잠재적으로 다중 체중 업데이
트에 기여할 수 있습니다.

483
00:23:39,207 --> 00:23:41,440
우리는이 테이블에서 샘플링을하고 있습니다.

484
00:23:41,440 --> 00:23:43,652
우리는 한 번 여러 번 샘플링 할 수 있습니다.

485
00:23:43,652 --> 00:23:44,918
그래서, 이것은 이끌어 갈 것입니다.

486
00:23:44,918 --> 00:23:47,585
또한 데이터 효율성을 향상시킵니다.

487
00:23:50,580 --> 00:23:52,442
좋아, 그럼 이걸 다 합친다.

488
00:23:52,442 --> 00:23:54,000
전체 알고리즘을 살펴 보겠습니다.

489
00:23:54,000 --> 00:23:57,583
경험 재생을 통한 심층 Q- 학습

490
00:23:59,166 --> 00:24:03,940
그래서 우리는 우리의 재생 메모리를
초기화하는 것으로 시작할 것입니다.

491
00:24:03,940 --> 00:24:07,383
우리가 선택한 어떤 역량, N, 그리고 우리는 또한

492
00:24:07,383 --> 00:24:09,703
우리를 초기화 할거야.

493
00:24:09,703 --> 00:24:13,075
임의의 가중치로 Q- 네트워크

494
00:24:13,075 --> 00:24:14,830
또는 초기 가중치.

495
00:24:14,830 --> 00:24:18,688
그리고 우리는 M 에피소드 또는 풀 게임을 할 것입니다.

496
00:24:18,688 --> 00:24:21,832
이것은 우리의 훈련 에피소드가 될 것입니다.

497
00:24:21,832 --> 00:24:22,998
그리고 우리가하려고하는 것은

498
00:24:22,998 --> 00:24:26,574
우리는 우리의 상태를 초기화 할 것인가?

499
00:24:26,574 --> 00:24:29,526
시작 게임 화면 픽셀 사용

500
00:24:29,526 --> 00:24:31,265
각 에피소드의 시작 부분.

501
00:24:31,265 --> 00:24:33,555
그리고 우리는 전처리 단계를 거친다는 것을 기억하십시오.

502
00:24:33,555 --> 00:24:37,814
우리의 실제 입력 상태가됩니다.

503
00:24:37,814 --> 00:24:39,313
그리고 나서 시간 단계마다

504
00:24:39,313 --> 00:24:41,584
우리가 현재하고있는 게임의

505
00:24:41,584 --> 00:24:44,236
우리는 작은 확률로,

506
00:24:44,236 --> 00:24:46,268
임의의 액션을 선택하고,

507
00:24:46,268 --> 00:24:49,819
그래서이 알고리즘에서 중요한 한 가지는

508
00:24:49,819 --> 00:24:53,141
충분한 탐사를하는 것이고,

509
00:24:53,141 --> 00:24:54,957
그래서 우리는

510
00:24:54,957 --> 00:24:58,559
우리는 국가 공간의 다른 부분을 샘플링하고 있습니다.

511
00:24:58,559 --> 00:25:00,353
그리고 그렇지 않으면 우리는

512
00:25:00,353 --> 00:25:02,405
탐욕스러운 행동에서 선택

513
00:25:02,405 --> 00:25:03,614
현재 정책에서

514
00:25:03,614 --> 00:25:05,564
맞아요, 그래서 대부분의 시간에 우리는
탐욕스러운 행동을 취할 것입니다.

515
00:25:05,564 --> 00:25:07,443
우리가 생각하는 것

516
00:25:07,443 --> 00:25:11,083
우리가 취하고 싶은 행동 유형에 대한 좋은 정책

517
00:25:11,083 --> 00:25:13,580
우리가보고 싶어하는 상태와 작은 확률로

518
00:25:13,580 --> 00:25:16,300
무작위로 샘플을 샘플링 해 보겠습니다.

519
00:25:16,300 --> 00:25:18,429
좋아, 그러면 우리가이 행동을 취할거야.

520
00:25:18,429 --> 00:25:23,076
a, t, 그리고 우리는 다음 보상과
다음 상태를 관찰 할 것입니다.

521
00:25:23,076 --> 00:25:26,070
그래서 r, t와 s, t는 하나 더하기.

522
00:25:26,070 --> 00:25:28,385
그런 다음이 작업을 수행하고이 전환을 저장합니다.

523
00:25:28,385 --> 00:25:32,771
우리가 만드는 우리의 리플레이 메모리에

524
00:25:32,771 --> 00:25:34,354
그리고 나서 우리는,

525
00:25:34,354 --> 00:25:35,577
우리는 네트워크를 조금 훈련시킬 것입니다.

526
00:25:35,577 --> 00:25:37,550
그래서 우리는 경험 재생을 할 것입니다.

527
00:25:37,550 --> 00:25:40,429
우리는 임의의 미니 배치 샘플을 가져옵니다.

528
00:25:40,429 --> 00:25:41,901
우리가 가진 전환의

529
00:25:41,901 --> 00:25:44,543
재생 메모리에서 가져온 다음 우리는

530
00:25:44,543 --> 00:25:47,214
그라데이션 강하 단계.

531
00:25:47,214 --> 00:25:49,635
그렇습니다. 이것이 우리의 전체 교육 과정이 될 것입니다.

532
00:25:49,635 --> 00:25:52,561
우리는 계속해서이 게임을 할 것입니다.

533
00:25:52,561 --> 00:25:55,774
샘플링도한다.

534
00:25:55,774 --> 00:25:58,431
업데이트를 위해 경험이 풍부한 재생을 사용하여 미니 바

535
00:25:58,431 --> 00:26:00,100
Q-network의 가중치와

536
00:26:00,100 --> 00:26:02,350
이 방식으로 계속됩니다.

537
00:26:03,887 --> 00:26:05,912
좋아요, 그래서 봅시다.

538
00:26:05,912 --> 00:26:07,524
내가 할 수 있는지 보자.

539
00:26:07,524 --> 00:26:09,030
이거 노는거야?

540
00:26:09,030 --> 00:26:11,852
좋아, 그럼 한번 보자.

541
00:26:11,852 --> 00:26:13,532
이 깊은 Q-learning 알고리즘

542
00:26:13,532 --> 00:26:17,699
Google DeepMind에서 Atomic
브레이크 아웃 게임에 대해 교육했습니다.

543
00:26:20,911 --> 00:26:22,316
알았어. 그래서 여기에 우리의 의견

544
00:26:22,316 --> 00:26:26,185
우리의 상태가 될 것입니다 원시 게임 픽셀입니다.

545
00:26:26,185 --> 00:26:28,385
그래서 여기서 우리는 무슨 일이
일어나고 있는지보고 있습니다.

546
00:26:28,385 --> 00:26:29,520
훈련 시작시.

547
00:26:29,520 --> 00:26:31,505
그래서 우리는 조금 훈련을 시작했습니다.

548
00:26:31,505 --> 00:26:33,159
과

549
00:26:33,159 --> 00:26:34,650
맞아, 그렇게 보일거야.

550
00:26:34,650 --> 00:26:36,824
그것은 공을 치는 종류로 배우고,

551
00:26:36,824 --> 00:26:40,303
그러나 그것을 유지하는 것은 아주 좋은 일을하지 않습니다.

552
00:26:40,303 --> 00:26:42,886
그러나 그것은 공을 찾고 있습니다.

553
00:26:50,969 --> 00:26:53,320
자, 이제는 더 많은 훈련을 한 후에,

554
00:26:53,320 --> 00:26:55,737
몇 시간이 걸릴 것 같습니다.

555
00:27:00,946 --> 00:27:05,113
좋아요. 그래서 지금 여기서 꽤 좋은
일을하는 법을 배우고 있습니다.

556
00:27:06,190 --> 00:27:08,677
따라서 지속적으로 따라갈 수 있습니다.

557
00:27:08,677 --> 00:27:10,677
이 공과

558
00:27:13,882 --> 00:27:16,593
대부분의 블록을 제거합니다.

559
00:27:16,593 --> 00:27:18,926
240 분 지나서.

560
00:27:33,248 --> 00:27:36,203
좋아, 그럼 여기에 프로 전략이 있는거야, 그렇지?

561
00:27:36,203 --> 00:27:38,225
당신은 꼭대기에 오르고 싶습니다.

562
00:27:38,225 --> 00:27:39,975
그 자체로 가자.

563
00:27:41,197 --> 00:27:42,796
좋아요.

564
00:27:42,796 --> 00:27:44,450
이것은 다음을 사용하는 예입니다.

565
00:27:44,450 --> 00:27:46,815
깊은 Q-learning을 통해

566
00:27:46,815 --> 00:27:49,501
Atari 게임을 할 수 있도록 상담원을 양성하십시오.

567
00:27:49,501 --> 00:27:51,485
많은 아타리 게임에서이 작업을 수행 할 수 있습니다.

568
00:27:51,485 --> 00:27:52,998
그래서 체크 아웃 할 수 있습니다.

569
00:27:52,998 --> 00:27:55,081
이것 좀 더 온라인으로.

570
00:27:56,419 --> 00:27:58,168
자, Q- 러닝에 대해서 이야기했습니다.

571
00:27:58,168 --> 00:28:01,149
Q-learning에는 문제가 있습니다.

572
00:28:01,149 --> 00:28:03,754
그것은 도전적 일 수 있으며 그 문제는 무엇입니까?

573
00:28:03,754 --> 00:28:05,126
글쎄, 문제는

574
00:28:05,126 --> 00:28:07,226
Q 함수는 매우 복잡합니다.

575
00:28:07,226 --> 00:28:09,344
맞아, 우리가해야 할 일은 우리가
배우고 싶어한다는 것입니다.

576
00:28:09,344 --> 00:28:12,335
모든 상태 액션 쌍의 값.

577
00:28:12,335 --> 00:28:14,854
예를 들어, 뭔가 있다고 가정 해 봅시다.

578
00:28:14,854 --> 00:28:17,275
로봇을 쥐고, 물체를 잡고 싶다.

579
00:28:17,275 --> 00:28:19,576
맞습니다. 여러분은 매우 높은
차원의 상태를 갖게 될 것입니다.

580
00:28:19,576 --> 00:28:23,033
너는 네가 가진 모든 것을 가지고 있다고 가정 해 보자.

581
00:28:23,033 --> 00:28:26,225
조인트, 조인트 위치 및 각도조차도 지원하지 않습니다.

582
00:28:26,225 --> 00:28:29,380
맞아, 모든 국가의 정확한 가치를 배워라.

583
00:28:29,380 --> 00:28:31,421
액션 쌍,

584
00:28:31,421 --> 00:28:34,171
정말, 정말 열심히 할 수 있습니다.

585
00:28:35,493 --> 00:28:38,724
하지만 다른 한편으로는 정책이 훨씬 간단해질 수 있습니다.

586
00:28:38,724 --> 00:28:40,310
좋아,이 로봇이 원하는 것처럼

587
00:28:40,310 --> 00:28:42,542
어쩌면이 단순한 움직임을 가지고있을 수도 있습니다.

588
00:28:42,542 --> 00:28:44,556
네 손을 막는 거지?

589
00:28:44,556 --> 00:28:45,952
이걸 손가락으로 움직여.

590
00:28:45,952 --> 00:28:48,252
특정 방향 및 계속.

591
00:28:48,252 --> 00:28:51,832
그래서, 그것은

592
00:28:51,832 --> 00:28:54,142
우리는이 정책을 직접 배울 수 있습니까?

593
00:28:54,142 --> 00:28:55,872
맞습니다. 아마도 최고를 찾는 것이 가능할 수도 있습니다.

594
00:28:55,872 --> 00:28:58,306
정책 모음에서 정책,

595
00:28:58,306 --> 00:28:59,988
이 과정을 거치지 않고

596
00:28:59,988 --> 00:29:02,078
당신의 Q- 가치를 추정하는 것

597
00:29:02,078 --> 00:29:05,495
그런 다음이를 사용하여 정책을 추론하십시오.

598
00:29:06,790 --> 00:29:09,288
그래서, 이것은

599
00:29:09,288 --> 00:29:10,257
오,

600
00:29:10,257 --> 00:29:13,154
그래,이 방법은

601
00:29:13,154 --> 00:29:15,938
우리는 정책 그라디언트를 호출 할 것입니다.

602
00:29:15,938 --> 00:29:18,228
그리고 공식적으로,

603
00:29:18,228 --> 00:29:20,858
매개 변수화 된 정책 클래스.

604
00:29:20,858 --> 00:29:24,146
가중치에 의해 매개 변수가 세타,

605
00:29:24,146 --> 00:29:25,889
각 정책마다

606
00:29:25,889 --> 00:29:27,791
정책의 가치를 정의합시다.

607
00:29:27,791 --> 00:29:30,859
그래서, J, 우리의 가치 J,
주어진 매개 변수 theta,

608
00:29:30,859 --> 00:29:32,437
예상되거나 예상되는

609
00:29:32,437 --> 00:29:35,723
우리가 신경 쓰는 미래 보상의 누적 합계.

610
00:29:35,723 --> 00:29:38,971
그래서 우리가 사용했던 것과 똑같은 보상입니다.

611
00:29:38,971 --> 00:29:41,879
우리의 목표는 다음과 같습니다.

612
00:29:41,879 --> 00:29:44,719
우리는 최적의 정책을 찾고자하며,

613
00:29:44,719 --> 00:29:48,243
시타 스타, 최대, 오른쪽,

614
00:29:48,243 --> 00:29:51,548
arg of theta of theta의 theta에 대한 최대 인수.

615
00:29:51,548 --> 00:29:53,946
그래서 우리는 정책, 정책 매개 변수를 찾고 싶습니다.

616
00:29:53,946 --> 00:29:56,917
최선의 보상을 제공합니다.

617
00:29:56,917 --> 00:29:58,834
그럼 어떻게 할 수 있니?

618
00:30:00,178 --> 00:30:01,011
어떤 아이디어?

619
00:30:04,993 --> 00:30:06,843
좋아, 우리가 할 수있는 일

620
00:30:06,843 --> 00:30:10,155
정책 매개 변수에 대한 동의어입니다.

621
00:30:10,155 --> 00:30:12,476
우리는 우리가 가지고있는 어떤 목표가 주어진다면,

622
00:30:12,476 --> 00:30:15,460
우리는 그라디언트 어 센트를 사용할
수있는 몇 가지 매개 변수

623
00:30:15,460 --> 00:30:17,512
및 그래디언트 동의

624
00:30:17,512 --> 00:30:20,762
지속적으로 매개 변수를 개선합니다.

625
00:30:23,202 --> 00:30:24,950
그리고 어떻게 더 구체적으로 이야기할까요?

626
00:30:24,950 --> 00:30:27,174
우리는 이것을 할 수 있습니다. 우리는 이것을 호출 할 것입니다.

627
00:30:27,174 --> 00:30:29,196
여기 강화 알고리즘.

628
00:30:29,196 --> 00:30:31,068
수학적으로, 우리는 다음과 같이 쓸 수 있습니다.

629
00:30:31,068 --> 00:30:34,375
우리가 기대하는 미래의 보상을 밖으로

630
00:30:34,375 --> 00:30:36,781
궤적을 넘어서서 우리는 샘플로 갈 것입니다.

631
00:30:36,781 --> 00:30:38,611
경험의 이러한 궤적, 바로,

632
00:30:38,611 --> 00:30:40,286
예를 들어 게임 놀이의 에피소드

633
00:30:40,286 --> 00:30:41,902
이전에 우리가 얘기했던 것.

634
00:30:41,902 --> 00:30:45,673
S-zero, a-zero, r-zero, s-one,

635
00:30:45,673 --> 00:30:47,411
a-one, r-one 등이 있습니다.

636
00:30:47,411 --> 00:30:51,723
세타의 어떤 정책을 사용합니다.

637
00:30:51,723 --> 00:30:54,139
오른쪽 궤도마다

638
00:30:54,139 --> 00:30:57,739
우리는 그 궤도에 대한 보상을 계산할 수 있습니다.

639
00:30:57,739 --> 00:30:59,135
누적 된 보상입니다.

640
00:30:59,135 --> 00:31:01,245
이 궤적을 따라 잡았습니다.

641
00:31:01,245 --> 00:31:03,733
그리고 정책의 가치,

642
00:31:03,733 --> 00:31:05,968
파이 서브 세타는 예상대로 될 것입니다.

643
00:31:05,968 --> 00:31:07,933
우리가 얻을 수있는 이러한 궤도에 대한 보상

644
00:31:07,933 --> 00:31:10,570
다음 pi 쎄타로부터.

645
00:31:10,570 --> 00:31:12,701
여기 그것이 궤적에 대한 기대입니다.

646
00:31:12,701 --> 00:31:16,868
우리가 얻을 수있는 것은 우리의
정책에서 궤적을 추출하는 것입니다.

647
00:31:18,563 --> 00:31:19,424
괜찮아.

648
00:31:19,424 --> 00:31:21,288
그래서 그라디언트 상승을 원합니다.

649
00:31:21,288 --> 00:31:22,961
그래서 이것을 차별화합시다.

650
00:31:22,961 --> 00:31:25,023
우리가 이것을 차별화하면, 우리는

651
00:31:25,023 --> 00:31:27,356
그라디언트 단계.

652
00:31:28,535 --> 00:31:30,418
그래서, 문제는 지금 우리가 노력한다면

653
00:31:30,418 --> 00:31:32,678
이것을 정확히 구별하십시오.

654
00:31:32,678 --> 00:31:34,300
이건 다루기 힘들지, 그렇지?

655
00:31:34,300 --> 00:31:37,388
따라서 기대치의 변화에 문제가 있습니다.

656
00:31:37,388 --> 00:31:41,319
여기서 p는 쎄타에 의존 할 때,
왜냐하면 여기에 있기 때문입니다.

657
00:31:41,319 --> 00:31:43,513
우리는이 그라데이션을 취하고 싶다.

658
00:31:43,513 --> 00:31:47,661
타우의 p, 주어진 쎄타,

659
00:31:47,661 --> 00:31:48,766
그러나 이것은 될 것입니다.

660
00:31:48,766 --> 00:31:50,591
우리는 타우에 대해이 적분을 취하고 싶습니다.

661
00:31:50,591 --> 00:31:53,033
맞습니다. 그래서 이것은 다루기가 어렵습니다.

662
00:31:53,033 --> 00:31:57,327
그러나이 문제를 해결하기 위해
여기에서 트릭을 사용할 수 있습니다.

663
00:31:57,327 --> 00:32:01,855
그리고이 트릭은 우리가 원하는이
그라디언트를 취하고 있습니다.

664
00:32:01,855 --> 00:32:03,203
우리는 이것을 다시 쓸 수 있습니다.

665
00:32:03,203 --> 00:32:04,941
이것을 하나씩 곱함으로써,

666
00:32:04,941 --> 00:32:07,081
위아래를 곱함으로써,

667
00:32:07,081 --> 00:32:10,286
둘 다 τ가 주어지면 쎄타가 주어진다.

668
00:32:10,286 --> 00:32:12,052
그렇다면이 용어를 보면

669
00:32:12,052 --> 00:32:14,248
우리가 지금 여기에, 내가 이것을 기록한 방식으로,

670
00:32:14,248 --> 00:32:15,958
왼쪽과 오른쪽에

671
00:32:15,958 --> 00:32:18,815
실제로는

672
00:32:18,815 --> 00:32:23,424
타우와 그라디언트의 그라디언트 배

673
00:32:23,424 --> 00:32:26,170
세타와 관련하여, 로그의, p.

674
00:32:26,170 --> 00:32:29,074
맞습니다. 왜냐하면 p의 로그의
그래디언트가 바로 가기 때문입니다.

675
00:32:29,074 --> 00:32:32,741
p의 p 배의 기울기를 갖는 것.

676
00:32:33,808 --> 00:32:36,934
좋아, 그럼 우리가 다시 주사하면

677
00:32:36,934 --> 00:32:41,385
우리가이 그라디언트에 대해 이전에 가지고 있었던 표현으로,

678
00:32:41,385 --> 00:32:43,426
우리는 이것이 실제로 어떻게 보이는지를 볼 수 있습니다.

679
00:32:43,426 --> 00:32:46,059
맞아, 이제 우리는 log p의 기울기를 갖기 때문에

680
00:32:46,059 --> 00:32:49,106
이 모든 궤도의 확률

681
00:32:49,106 --> 00:32:52,187
그런 다음 타우에 대해이 정수를 취합니다.

682
00:32:52,187 --> 00:32:54,495
이것은 이제 기대가 될 것입니다.

683
00:32:54,495 --> 00:32:58,586
우리의 궤도 tau에, 그래서 우리가 여기서 한 것은

684
00:32:58,586 --> 00:33:02,751
우리가 기대치의 그라데이션을 찍은 것입니다.

685
00:33:02,751 --> 00:33:06,823
우리는 이것을 그라디언트의 기대로 변형 시켰습니다.

686
00:33:06,823 --> 00:33:09,156
맞아, 이제 우리는

687
00:33:10,051 --> 00:33:12,404
우리가 얻을 수있는 샘플 궤도

688
00:33:12,404 --> 00:33:14,712
우리의 그라디언트를 추정하기 위해서.

689
00:33:14,712 --> 00:33:17,343
그래서 우리는 몬테카를로 샘플링을
사용하여 이것을 수행합니다.

690
00:33:17,343 --> 00:33:21,260
이것은 강화의 핵심 아이디어 중 하나입니다.

691
00:33:23,624 --> 00:33:25,846
좋아, 이걸보고있어.

692
00:33:25,846 --> 00:33:28,180
우리가 계산하고자하는 표현,

693
00:33:28,180 --> 00:33:30,421
우리가 여기있는 양을 계산할 수 있습니까?

694
00:33:30,421 --> 00:33:33,071
전이 확률을 모른 채로?

695
00:33:33,071 --> 00:33:36,643
알았어, 그래서 우리는 타우의 피가 될거야.

696
00:33:36,643 --> 00:33:38,466
탄도의 확률.

697
00:33:38,466 --> 00:33:40,387
그것은의 제품이 될 것입니다

698
00:33:40,387 --> 00:33:43,379
다음 주에 대한 모든 전이 확률

699
00:33:43,379 --> 00:33:45,821
우리의 현재 상태와 행동을 감안할 때

700
00:33:45,821 --> 00:33:49,051
행동의 확률뿐만 아니라

701
00:33:49,051 --> 00:33:52,232
우리는 우리의 정책에 따라 결정했습니다.

702
00:33:52,232 --> 00:33:54,743
맞습니다. 그래서 우리는이 모든
것들을 함께 번식 할 것입니다.

703
00:33:54,743 --> 00:33:58,441
우리의 궤도의 확률을 얻으십시오.

704
00:33:58,441 --> 00:34:03,059
그래서 우리가 계산하고자하는 타우의 p의 로그

705
00:34:03,059 --> 00:34:06,334
우리는이 기록을 가지고있을 것입니다.

706
00:34:06,334 --> 00:34:08,326
이것을 합계로 나누어 라.

707
00:34:08,326 --> 00:34:10,389
안으로 로그를 밀기.

708
00:34:10,389 --> 00:34:12,383
그리고 나서 여기에서 우리가 이것을 차별화 할 때,

709
00:34:12,384 --> 00:34:14,237
우리는 존경심으로 차별화되기를
원한다는 것을 알 수 있습니다.

710
00:34:14,237 --> 00:34:18,162
쎄타에게,하지만 우리가 여기있는 첫 번째 용어는,

711
00:34:18,163 --> 00:34:20,911
상태 전이 확률의 로그 p

712
00:34:20,911 --> 00:34:22,850
여기에 쎄타 용어가 없습니다.

713
00:34:22,850 --> 00:34:25,292
우리가 세타를 가지고있는 유일한 장소는 두 번째 용어입니다

714
00:34:25,292 --> 00:34:28,709
우리가 가지고있는 pi sub theta의 로그의

715
00:34:29,675 --> 00:34:32,914
우리의 국가에 주어진 우리의 행동에 대해

716
00:34:32,914 --> 00:34:34,139
우리가 지키는 용어

717
00:34:34,139 --> 00:34:37,368
우리의 그라디언트 추정치에서, 그래서
우리는 여기서 볼 수 있습니다

718
00:34:37,369 --> 00:34:39,670
이것은 전이 확률에 의존하지 않고,

719
00:34:39,670 --> 00:34:41,293
맞아, 우리는 실제로 알 필요가 없다.

720
00:34:41,293 --> 00:34:44,588
우리의 전이 확률이 컴퓨터에 순서대로

721
00:34:44,589 --> 00:34:46,422
그라디언트 추산.

722
00:34:47,257 --> 00:34:50,879
그리고 나서, 우리가 이것들을 샘플링 할 때,

723
00:34:50,880 --> 00:34:55,047
어떤 주어진 궤적 타우에 대해서 우리는
쎄타의 J를 추정 할 수있다.

724
00:34:56,306 --> 00:34:58,524
이 기울기 추정을 사용하여.

725
00:34:58,524 --> 00:35:00,472
이것은 여기 하나의 궤적을 보여줍니다.

726
00:35:00,472 --> 00:35:02,220
우리가 이전에 가지고 있었던 것에서,

727
00:35:02,220 --> 00:35:05,271
우리는 또한 여러 궤도를 통해
표본 추출을 할 수 있습니다.

728
00:35:05,271 --> 00:35:07,188
기대를 얻기 위해서.

729
00:35:09,248 --> 00:35:12,974
자, 우리가 도출 한이 기울기 측정기가 주어진다면,

730
00:35:12,974 --> 00:35:17,141
우리가 여기에서 만들 수있는 해석은

731
00:35:18,217 --> 00:35:21,931
궤도에 대한 보상이 높으면 보상이

732
00:35:21,931 --> 00:35:25,226
우리는 일련의 행동을 취하는 것이 좋았고,

733
00:35:25,226 --> 00:35:27,517
다음으로 모든 사람들의 확률을 높여 봅시다.

734
00:35:27,517 --> 00:35:29,434
우리가 본 행동들.

735
00:35:29,434 --> 00:35:31,458
맞아. 우린 그걸 말할거야.

736
00:35:31,458 --> 00:35:33,141
이것들은 우리가 취한 좋은 행동이었다.

737
00:35:33,141 --> 00:35:35,287
그리고 보상이 낮 으면,

738
00:35:35,287 --> 00:35:37,186
우리는 이러한 확률을 낮추기를 원합니다.

739
00:35:37,186 --> 00:35:38,629
우리는 이것이 나쁜 행동이라고 말하고 싶습니다.

740
00:35:38,629 --> 00:35:40,747
시도하고 샘플을 너무 많이 보자.

741
00:35:40,747 --> 00:35:43,568
맞습니다. 그래서 우리는 그것이 여기서
일어나는 일을 볼 수 있습니다.

742
00:35:43,568 --> 00:35:47,392
우리는 a의 pi를 가지고, s는 주어진다.

743
00:35:47,392 --> 00:35:50,980
이것이 우리가 취한 행동의 가능성입니다.

744
00:35:50,980 --> 00:35:53,163
우리는 이것을 확장 할 것이며, 우리는

745
00:35:53,163 --> 00:35:56,621
그라디언트가 얼마나 많은지 알려줄 것입니다.

746
00:35:56,621 --> 00:36:00,555
증가 시키려면 매개 변수를 변경해야합니까?

747
00:36:00,555 --> 00:36:03,575
우리의 행동에 대한 우리의 가능성, 그렇죠?

748
00:36:03,575 --> 00:36:06,501
그리고 나서 우리는 이것을 받아서

749
00:36:06,501 --> 00:36:09,019
우리가 실제로 얼마나 많은 보상을 받았는지,

750
00:36:09,019 --> 00:36:12,602
현실적으로 이러한 행동이 얼마나 좋은지.

751
00:36:14,561 --> 00:36:16,209
좋아요.

752
00:36:16,209 --> 00:36:18,454
이것은 단순하게 보일 수 있습니다.

753
00:36:18,454 --> 00:36:21,124
너도 알다시피, 궤적이 좋다면, 우리는 말하고있다.

754
00:36:21,124 --> 00:36:22,965
여기 모든 행동이 좋았다.

755
00:36:22,965 --> 00:36:23,798
권리?

756
00:36:23,798 --> 00:36:26,356
하지만, 실제로 이것은 평균적으로 아웃됩니다.

757
00:36:26,356 --> 00:36:30,125
그래서 우리는 여기에 편향된 추정자를 가지고 있습니다,

758
00:36:30,125 --> 00:36:32,580
그래서 많은 샘플을 가지고 있다면,

759
00:36:32,580 --> 00:36:35,622
그러면 우리는 그라디언트의 정확한 추정을 얻을 것입니다.

760
00:36:35,622 --> 00:36:38,510
그리고 우리가 바로 잡을 수 있기 때문에 이것은 좋은 일입니다.

761
00:36:38,510 --> 00:36:40,666
그라디언트 단계와 우리는 우리가 될 것을 알고 있습니다.

762
00:36:40,666 --> 00:36:42,994
우리의 손실 기능을 개선하고 가까이에

763
00:36:42,994 --> 00:36:45,976
~, 적어도 우리 지역의 최적 지역

764
00:36:45,976 --> 00:36:48,602
정책 매개 변수 theta.

765
00:36:48,602 --> 00:36:50,690
좋아,하지만 이것에 문제가있다.

766
00:36:50,690 --> 00:36:52,789
문제는 이것이 또한 겪는 것입니다.

767
00:36:52,789 --> 00:36:54,884
높은 분산에서.

768
00:36:54,884 --> 00:36:57,201
이 신용 할당은 정말로 어렵기 때문입니다.

769
00:36:57,201 --> 00:36:58,902
맞습니다.

770
00:36:58,902 --> 00:37:02,283
우리가 얻은 보상을 감안할 때, 우리는

771
00:37:02,283 --> 00:37:04,412
모든 행동은 좋았고, 우리는 희망을 품을뿐입니다.

772
00:37:04,412 --> 00:37:06,537
실제로 어떤 행동이 할당되었는지

773
00:37:06,537 --> 00:37:08,395
중요한 행동,

774
00:37:08,395 --> 00:37:11,080
시간이 지남에 따라 평균을 낼 것입니다.

775
00:37:11,080 --> 00:37:14,598
그래서 이것은 정말로 어렵고 많은 샘플이 필요합니다.

776
00:37:14,598 --> 00:37:17,190
좋은 견적을 얻기 위해서.

777
00:37:17,190 --> 00:37:19,406
알았어. 그래서 이것은 문제가된다.

778
00:37:19,406 --> 00:37:21,684
우리가 분산을 줄이기 위해 할 수있는 일

779
00:37:21,684 --> 00:37:23,851
견적을 향상시킬 수 있습니까?

780
00:37:26,540 --> 00:37:29,123
그래서 분산 감소는

781
00:37:30,164 --> 00:37:33,323
정책 구배의 중요한 연구 분야,

782
00:37:33,323 --> 00:37:36,467
그리고 개선하기 위해 길을 찾아야합니다.

783
00:37:36,467 --> 00:37:39,756
평가자는 더 적은 샘플을 요구한다.

784
00:37:39,756 --> 00:37:41,445
자, 이제 두 가지 아이디어를 살펴 보겠습니다.

785
00:37:41,445 --> 00:37:43,278
우리가 어떻게 할 수 있는지.

786
00:37:44,202 --> 00:37:46,764
주어진 그래디언트 추정기를 보면,

787
00:37:46,764 --> 00:37:49,017
그래서 첫 번째 아이디어는 우리가 할 수있는 것입니다.

788
00:37:49,017 --> 00:37:52,610
행동의 확률을 높인다.

789
00:37:52,610 --> 00:37:56,258
그것만이 미래의 보상에 영향을 미친다.

790
00:37:56,258 --> 00:37:57,091
그 상태에서 나온 거지?

791
00:37:57,091 --> 00:37:59,312
이제 스케일링 대신에

792
00:37:59,312 --> 00:38:02,066
이 가능성, 또는이 가능성을 높임

793
00:38:02,066 --> 00:38:04,736
이 행동의 궤적의 총 보상에 의해,

794
00:38:04,736 --> 00:38:07,320
더 자세히 살펴 보겠습니다.

795
00:38:07,320 --> 00:38:09,876
이 시간 단계에서 오는 보상

796
00:38:09,876 --> 00:38:12,108
마지막에, 맞지?

797
00:38:12,108 --> 00:38:14,224
그리고 이것은 기본적으로

798
00:38:14,224 --> 00:38:17,441
행동이 얼마나 좋은지는 얼마나 많이

799
00:38:17,441 --> 00:38:18,999
그것이 생성하는 미래의 보상.

800
00:38:18,999 --> 00:38:20,499
어떤 의미가 있습니다.

801
00:38:21,811 --> 00:38:24,251
좋아요. 우리가 사용할 수있는 또 다른 아이디어입니다.

802
00:38:24,251 --> 00:38:26,931
순서대로 할인 요소를 사용하고 있습니다.

803
00:38:26,931 --> 00:38:29,448
지연된 영향을 무시한다.

804
00:38:29,448 --> 00:38:33,133
자, 이제 우리는이 할인 요소를 다시 추가했습니다.

805
00:38:33,133 --> 00:38:36,774
우리가 전에 보았던 것, 그것은

806
00:38:36,774 --> 00:38:39,991
우리는 할인 요소가 우리에게 말할 것입니다.

807
00:38:39,991 --> 00:38:41,841
우리가 얼마나 신경을 많이 쓰는지

808
00:38:41,841 --> 00:38:44,510
곧 나올 보상들,

809
00:38:44,510 --> 00:38:47,276
나중에 많이 나온 보상에 비해

810
00:38:47,276 --> 00:38:49,462
맞았 어. 그래서 우리는 지금 갈거야.

811
00:38:49,462 --> 00:38:51,438
행동이 얼마나 좋고 나쁜지 말하라.

812
00:38:51,438 --> 00:38:54,071
이웃에 대한 더 많은 것을보고

813
00:38:54,071 --> 00:38:57,489
가까운 미래에 생성되는 액션 세트

814
00:38:57,489 --> 00:39:00,880
나중에 오는 것들을 가중치를 낮추십시오.

815
00:39:00,880 --> 00:39:02,471
좋아요.

816
00:39:02,471 --> 00:39:05,194
이것들은 몇 가지 직접적인 아이디어이다.

817
00:39:05,194 --> 00:39:07,730
실제로는 일반적으로 사용됩니다.

818
00:39:07,730 --> 00:39:11,529
그래서, 세 번째 아이디어는

819
00:39:11,529 --> 00:39:14,597
분산을 줄이기위한 기준선.

820
00:39:14,597 --> 00:39:18,273
그래서 원시 값을 사용하는 데 문제가 있습니다.

821
00:39:18,273 --> 00:39:20,690
당신의 궤적 중

822
00:39:21,675 --> 00:39:23,869
이것이 반드시 의미있는 것은 아니 겠지?

823
00:39:23,869 --> 00:39:26,653
예를 들어 보상이 모두 긍정적 인 경우,

824
00:39:26,653 --> 00:39:27,973
그러면 계속 밀고 나가겠습니다.

825
00:39:27,973 --> 00:39:29,835
모든 행동의 확률을 높입니다.

826
00:39:29,835 --> 00:39:32,039
그리고 물론, 당신은 그들을 다양한
각도로 밀어 올릴 것입니다,

827
00:39:32,039 --> 00:39:35,448
그러나 정말로 중요한 것은 보상이 더 나은지 아닌지입니다.

828
00:39:35,448 --> 00:39:39,753
또는 당신이 기대하고있는 것보다 나빠질 수 있습니다.

829
00:39:39,753 --> 00:39:42,993
좋아,이 문제를 해결하기 위해 우리는

830
00:39:42,993 --> 00:39:46,071
상태에 의존하는 기준선 함수.

831
00:39:46,071 --> 00:39:47,598
맞습니다. 따라서이 기본 기능은 우리에게

832
00:39:47,598 --> 00:39:51,219
우리의 추측은 무엇이며 우리가 기대하는 것은 무엇입니까?

833
00:39:51,219 --> 00:39:53,886
이 상태에서 벗어나고

834
00:39:55,515 --> 00:39:58,031
우리가 사용할 보상이나 스케일링 요소

835
00:39:58,031 --> 00:39:59,837
우리의 확률을 높이거나 낮추는 것,

836
00:39:59,837 --> 00:40:02,592
이제는 미래 보상의 예상 합계가 될 수 있습니다.

837
00:40:02,592 --> 00:40:05,508
이 기준선을 뺀 것이므로 이제는

838
00:40:05,508 --> 00:40:08,939
훨씬 더 좋든 나쁘 든간에 우리가 얻은 보상이다.

839
00:40:08,939 --> 00:40:10,772
우리가 기대했던 것에서.

840
00:40:11,870 --> 00:40:14,971
그렇다면이 기준선을 어떻게 선택할 수 있습니까?

841
00:40:14,971 --> 00:40:16,099
잘,

842
00:40:16,099 --> 00:40:19,168
매우 간단한 기준선, 사용할 수있는 가장 단순한 기준선,

843
00:40:19,168 --> 00:40:21,065
이동 평균을 취하고있다.

844
00:40:21,065 --> 00:40:23,013
당신이 지금까지 경험 한 보상들.

845
00:40:23,013 --> 00:40:25,027
따라서이 전체 궤적을 수행 할 수도 있습니다.

846
00:40:25,027 --> 00:40:28,863
이것은 보상의 평균에 불과합니다.

847
00:40:28,863 --> 00:40:31,431
내가 훈련을 받고있는 동안 나는보고 있었다.

848
00:40:31,431 --> 00:40:34,765
그리고 나는이 에피소드를 연주하면서?

849
00:40:34,765 --> 00:40:37,549
맞아요, 그래서 이것은

850
00:40:37,549 --> 00:40:41,716
내가 현재받는 보상은 상대적으로 좋았거나 나빴습니다.

851
00:40:42,821 --> 00:40:45,737
그래서 이것을 사용할 수있는 약간의 차이가 있습니다.

852
00:40:45,737 --> 00:40:49,215
그러나 지금까지 우리가 지금까지 보아 왔던 분산 감소

853
00:40:49,215 --> 00:40:51,588
일반적으로 일반적으로 사용되는

854
00:40:51,588 --> 00:40:54,452
"vanilla REINFORCE"알고리즘이라고합니다.

855
00:40:54,452 --> 00:40:56,787
맞아, 미래 누적 보상을 보면,

856
00:40:56,787 --> 00:41:00,954
할인 요인 및 몇 가지 간단한 기준선이 있습니다.

857
00:41:02,601 --> 00:41:05,081
이제 우리가 어떻게 할 수 있는지 이야기 해 봅시다.

858
00:41:05,081 --> 00:41:06,547
이 기본 생각에 대해 생각해보십시오.

859
00:41:06,547 --> 00:41:08,769
잠재적으로 더 나은 기준선을 선택하십시오.

860
00:41:08,769 --> 00:41:12,084
맞습니다. 그래서 우리가 더 나은 것이 무엇인지 생각한다면

861
00:41:12,084 --> 00:41:13,567
우리가 선택할 수있는 기준선,

862
00:41:13,567 --> 00:41:16,569
우리가 원하는 것은 확률을 높이기 위해서입니다.

863
00:41:16,569 --> 00:41:19,931
행동이 더 나은 경우 국가의 행동을

864
00:41:19,931 --> 00:41:24,255
우리가 그 주에서 얻는 것의 기대 가치.

865
00:41:24,255 --> 00:41:27,655
따라서 우리가 기대하는 것의 가치에 대해 생각해보십시오.

866
00:41:27,655 --> 00:41:30,163
주에서, 이것은 당신에게 무엇을 생각 나게합니까?

867
00:41:30,163 --> 00:41:31,189
이것은 당신에게 어떤 것을 생각 나게합니까?

868
00:41:31,189 --> 00:41:34,939
우리가이 강연에서 더 일찍 이야기했던 것?

869
00:41:37,023 --> 00:41:37,856
예.

870
00:41:37,856 --> 00:41:39,266
[관객으로부터 들리지 않음]

871
00:41:39,266 --> 00:41:41,297
그래, 가치 기능이 맞지?

872
00:41:41,297 --> 00:41:45,201
우리가 Q- 러닝과 이야기했던 가치 함수.

873
00:41:45,201 --> 00:41:46,034
그래서, 정확하게.

874
00:41:46,034 --> 00:41:47,871
Q 함수와 값 함수

875
00:41:47,871 --> 00:41:50,895
그래서 직관은

876
00:41:50,895 --> 00:41:52,347
잘,

877
00:41:52,347 --> 00:41:54,704
우리는 행동에 행복하다.

878
00:41:54,704 --> 00:41:58,173
상태에서 액션을 취하는 경우 if

879
00:41:58,173 --> 00:42:00,248
복용 Q-value

880
00:42:00,248 --> 00:42:04,752
이 상태의 특정 작업은

881
00:42:04,752 --> 00:42:06,999
가치 함수 또는 기대 값

882
00:42:06,999 --> 00:42:08,406
누적 미래 보상의

883
00:42:08,406 --> 00:42:09,698
우리가이 상태에서 얻을 수있는 것.

884
00:42:09,698 --> 00:42:11,842
맞아,이 말은이 행동이

885
00:42:11,842 --> 00:42:14,416
우리가 취할 수있는 다른 행동들.

886
00:42:14,416 --> 00:42:17,896
그리고 반대로, 우리는이 행동이 불만이라면,

887
00:42:17,896 --> 00:42:22,063
이 값 또는이 차이가 음이거나 작 으면.

888
00:42:23,917 --> 00:42:27,299
맞아, 이제 우리가 이것을 연결하면,

889
00:42:27,299 --> 00:42:29,269
우리가 원하는만큼의 스케일링 요소로

890
00:42:29,269 --> 00:42:32,692
위로 또는 아래로, 우리의 행동의 가능성을,

891
00:42:32,692 --> 00:42:34,868
그러면 우리는이 견적서를 여기서 얻을 수 있습니다.

892
00:42:34,868 --> 00:42:37,452
그래, 그렇게 될거야.

893
00:42:37,452 --> 00:42:40,168
이전과 정확히 동일하지만 지금은 어디에서

894
00:42:40,168 --> 00:42:43,993
우리는 누적 적으로 기대되는 보상을 받기 전에,

895
00:42:43,993 --> 00:42:46,708
우리의 다양한 감소, 분산 감소

896
00:42:46,708 --> 00:42:50,514
기술과 기준선에, 이제 우리는
지금 막 끼워 넣을 수 있습니다.

897
00:42:50,514 --> 00:42:53,297
이 차이점이

898
00:42:53,297 --> 00:42:57,113
현재의 행동은 Q-function을 기반으로했습니다.

899
00:42:57,113 --> 00:43:00,530
그 상태에서 우리의 가치 함수를 뺀 것입니다.

900
00:43:01,771 --> 00:43:04,148
맞아.하지만 우리가 지금까지 이야기 한 내용은

901
00:43:04,148 --> 00:43:06,993
REINFORCE 알고리즘, 우리는 모른다.

902
00:43:06,993 --> 00:43:09,413
Q와 V는 실제로 무엇입니까.

903
00:43:09,413 --> 00:43:11,313
그래서 우리는 이것들을 배울 수 있습니까?

904
00:43:11,313 --> 00:43:14,479
Q- 러닝을 사용하면 대답은 '예'입니다.

905
00:43:14,479 --> 00:43:16,465
우리가 이미 전에 얘기 한 것.

906
00:43:16,465 --> 00:43:19,828
따라서 정책 구배를 결합 할 수 있습니다.

907
00:43:19,828 --> 00:43:22,210
우리가 방금 Q- 러닝에 대해 이야기하고있는 동안,

908
00:43:22,210 --> 00:43:25,982
정책 인 배우와

909
00:43:25,982 --> 00:43:28,784
비평가, 권리, Q-function,

910
00:43:28,784 --> 00:43:32,366
우리가 국가가 얼마나 좋은지를 우리에게 말해 줄 것입니다.

911
00:43:32,366 --> 00:43:34,380
그리고 한 주에서의 행동.

912
00:43:34,380 --> 00:43:36,964
맞아, 접근법에서 이것을 사용하면,

913
00:43:36,964 --> 00:43:40,633
배우가 취할 조치를 결정할 것입니다.

914
00:43:40,633 --> 00:43:43,716
비평가, 즉 Q-function은
다음과 같이 말할 것입니다.

915
00:43:43,716 --> 00:43:47,708
배우의 행동이 얼마나 좋은지 그리고 어떻게 조정해야하는지.

916
00:43:47,708 --> 00:43:51,072
그래서, 그리고 이것은 또한 약간의 작업을 완화시킵니다.

917
00:43:51,072 --> 00:43:53,636
이 비평가의 Q- 학습 문제와 비교

918
00:43:53,636 --> 00:43:56,694
이전에 우리가 이것을 가지고 있어야한다고 이야기했던 것

919
00:43:56,694 --> 00:43:59,958
모든 국가, 행동 쌍,

920
00:43:59,958 --> 00:44:01,784
왜냐하면 여기 에선 이것을 배우기 만하면되기 때문입니다.

921
00:44:01,784 --> 00:44:04,762
정책에 의해 생성 된 국가 - 행동 쌍을위한

922
00:44:04,762 --> 00:44:06,103
이걸 알면 돼.

923
00:44:06,103 --> 00:44:10,512
이 스케일링 요소를 계산하는 데 중요합니다.

924
00:44:10,512 --> 00:44:12,830
맞습니다. 그리고 우리는 이것을 배울 때,

925
00:44:12,830 --> 00:44:15,196
우리가 본 Q- 학습 트릭을 모두 포함 시키십시오.

926
00:44:15,196 --> 00:44:18,188
경험 리플레이 같은 일찍.

927
00:44:18,188 --> 00:44:20,972
그리고 이제, 저는 또한

928
00:44:20,972 --> 00:44:24,610
앞에서 본 용어를 정의하십시오.

929
00:44:24,610 --> 00:44:28,248
얼마나 많은 일이 얼마나 효과적 이었습니까?

930
00:44:28,248 --> 00:44:30,831
주어진 상태에서 s의 V를 뺀 값?

931
00:44:32,199 --> 00:44:35,533
국가가 얼마나 좋은지에 대한 우리의 기대 가치

932
00:44:35,533 --> 00:44:38,172
이 용어는 우월 함수에 의한 것이다.

933
00:44:38,172 --> 00:44:41,498
맞습니다. 따라서 이점 기능은
얼마나 많은 이점이 있습니까?

934
00:44:41,498 --> 00:44:43,568
우리는이 행동을하지 않았습니까?

935
00:44:43,568 --> 00:44:48,100
그 행동이 예상보다 얼마나 나은지.

936
00:44:48,100 --> 00:44:51,709
그래서, 이것을 사용하여 우리는

937
00:44:51,709 --> 00:44:53,457
배우 비평 알고리즘.

938
00:44:53,457 --> 00:44:56,279
그리고 이것이 어떻게 생겼는지는 우리가 시작하려고합니다.

939
00:44:56,279 --> 00:45:00,326
우리의 정책 매개 변수 인 theta를 초기화함으로써

940
00:45:00,326 --> 00:45:03,689
그리고 우리가 phi라고 부르는 비평가 매개 변수.

941
00:45:03,689 --> 00:45:07,522
그리고 각각에 대해, 훈련의 반복을 위해,

942
00:45:08,401 --> 00:45:11,149
우리는 M 궤적을 샘플링하려고합니다.

943
00:45:11,149 --> 00:45:12,185
현 정책 하에서

944
00:45:12,185 --> 00:45:13,734
그렇습니다. 우리는 정책을 실행하고

945
00:45:13,734 --> 00:45:18,725
궤적을 s-zero, a-zero,
r-zero, s-one 등으로 정의합니다.

946
00:45:18,725 --> 00:45:20,359
좋아, 그럼 우리는 계산할거야.

947
00:45:20,359 --> 00:45:21,671
우리가 원하는 그라디언트.

948
00:45:21,671 --> 00:45:24,977
맞아, 그래서이 궤도마다

949
00:45:24,977 --> 00:45:26,017
매 시간마다 우리는 가고 있습니다.

950
00:45:26,017 --> 00:45:28,901
이 우세 함수를 계산하려면,

951
00:45:28,901 --> 00:45:30,818
그리고 나서 우리는

952
00:45:31,701 --> 00:45:33,465
이 우위 함수를 사용 하시겠습니까?

953
00:45:33,465 --> 00:45:37,131
그런 다음 그라디언트 추정기에서이를 사용할 것입니다.

954
00:45:37,131 --> 00:45:40,533
우리가 이전에 보여 줬던

955
00:45:40,533 --> 00:45:42,894
우리가 여기서 가지고있는 기울기 추정치.

956
00:45:42,894 --> 00:45:46,017
그리고 우리는 또한

957
00:45:46,017 --> 00:45:50,837
비평가 매개 변수 phi는 정확히 같은 방법으로,

958
00:45:50,837 --> 00:45:54,193
우리가 이전에 보았던 것처럼, 기본적으로

959
00:45:54,193 --> 00:45:57,557
이 가치 함수는 바로 우리의 가치 함수를 배우고,

960
00:45:57,557 --> 00:46:01,640
끌어 들이기 만하면됩니다.

961
00:46:02,638 --> 00:46:05,467
이 이점 기능과 이것은

962
00:46:05,467 --> 00:46:08,324
벨맨 방정식에 더 가까워 지도록 격려하십시오.

963
00:46:08,324 --> 00:46:10,347
우리가 더 일찍 보았던, 그렇지?

964
00:46:10,347 --> 00:46:14,347
그리고 이것은 기본적으로 다음과 같이 반복됩니다.

965
00:46:15,197 --> 00:46:17,733
정책 기능의 학습 및 최적화,

966
00:46:17,733 --> 00:46:20,211
뿐만 아니라 우리의 평론 기능.

967
00:46:20,211 --> 00:46:22,311
그리고 나서 우리는

968
00:46:22,311 --> 00:46:23,977
그라디언트 그리고 나서 우리는 끝까지 갈 것입니다.

969
00:46:23,977 --> 00:46:26,727
이 과정을 계속 반복하십시오.

970
00:46:29,271 --> 00:46:31,827
자, 이제 REINFORCE의 몇
가지 예를 살펴 보겠습니다.

971
00:46:31,827 --> 00:46:36,027
행동을 취하고, 먼저 여기에서 무엇인가 불리는 것을 보자.

972
00:46:36,027 --> 00:46:39,464
재발주의 모델 (Recurrent
Attention Model)은,

973
00:46:39,464 --> 00:46:42,805
그것은 하드주의라고도 불리는 모델이며,

974
00:46:42,805 --> 00:46:46,876
하지만 최근에 컴퓨터 비전에서 많이 보게 될 것입니다.

975
00:46:46,876 --> 00:46:49,146
다양한 목적을위한 작업.

976
00:46:49,146 --> 00:46:51,806
맞아요. 그래서이 아이디어는

977
00:46:51,806 --> 00:46:55,122
여기, 나는 열심히주의를 기울인
원래의 작품에 대해 이야기했다.

978
00:46:55,122 --> 00:46:59,167
이미지 분류가 이루어지며 목표는

979
00:46:59,167 --> 00:47:02,504
여전히 이미지 클래스를 예측하기 위해,

980
00:47:02,504 --> 00:47:04,822
하지만 이제는 시퀀스를 취하여이 작업을 수행 할 것입니다.

981
00:47:04,822 --> 00:47:06,494
이미지 주변의 흘끗 보임.

982
00:47:06,494 --> 00:47:10,300
당신은 이미지 주변의 지역을 볼 것입니다.

983
00:47:10,300 --> 00:47:12,754
기본적으로 이들에 선택적으로 집중하려고합니다.

984
00:47:12,754 --> 00:47:17,141
부품을 찾고 주변을 둘러 보면서 정보를 구축하십시오.

985
00:47:17,141 --> 00:47:19,382
맞아, 우리가 이걸하고 싶어하는 이유 야.

986
00:47:19,382 --> 00:47:21,638
음, 우선, 좋은 영감을 얻었습니다.

987
00:47:21,638 --> 00:47:24,551
안구 운동에서 인간의 지각으로부터.

988
00:47:24,551 --> 00:47:26,869
우리가 복잡한 이미지를보고 있다고 가정 해 봅시다.

989
00:47:26,869 --> 00:47:29,225
우리는 이미지에 무엇이 있는지를 결정하려고합니다.

990
00:47:29,225 --> 00:47:31,594
음, 저기, 우리는 아마도 저해상도를 보았을 것입니다.

991
00:47:31,594 --> 00:47:34,013
먼저 부품을 살펴본 다음 부품을 자세히 살펴보십시오.

992
00:47:34,013 --> 00:47:36,913
우리에게 단서를 줄 이미지의

993
00:47:36,913 --> 00:47:39,168
이 이미지에 무엇이 있는가.

994
00:47:39,168 --> 00:47:40,001
그리고,

995
00:47:41,160 --> 00:47:45,703
이 방법은 이미지를보고 둘러 보는 것입니다.

996
00:47:45,703 --> 00:47:48,533
지역별로도 도움을 줄 것입니다.

997
00:47:48,533 --> 00:47:50,435
전산 자원, 맞죠?

998
00:47:50,435 --> 00:47:53,293
전체 이미지를 처리 할 필요가 없습니다.

999
00:47:53,293 --> 00:47:55,366
실제로, 일반적으로 발생하는 것은

1000
00:47:55,366 --> 00:47:57,511
전체 이미지의 저해상도 이미지 먼저,

1001
00:47:57,511 --> 00:48:01,678
시작하는 방법을 결정한 다음 고해상도로 살펴 봅니다.

1002
00:48:02,773 --> 00:48:04,671
그 이후의 이미지 부분.

1003
00:48:04,671 --> 00:48:06,979
따라서 많은 계산 자원을 절약 할 수 있습니다.

1004
00:48:06,979 --> 00:48:09,725
그러면이 이점을 생각해 볼 수 있습니다.

1005
00:48:09,725 --> 00:48:11,927
확장 성, 오른쪽, 말할 수있는 것, 말하자면

1006
00:48:11,927 --> 00:48:15,177
더 큰 이미지를보다 효율적으로 처리합니다.

1007
00:48:16,164 --> 00:48:17,780
그리고 마지막으로 이것은 실제로 도움이 될 수 있습니다.

1008
00:48:17,780 --> 00:48:20,099
실제 분류 성능으로,

1009
00:48:20,099 --> 00:48:21,855
이제 당신이 할 수 있기 때문에

1010
00:48:21,855 --> 00:48:24,760
이미지의 혼란스럽고 관련없는 부분은 무시하십시오.

1011
00:48:24,760 --> 00:48:25,593
권리?

1012
00:48:25,593 --> 00:48:27,678
항상 그렇듯이

1013
00:48:27,678 --> 00:48:29,931
귀하의 ConvNet, 귀하의 이미지의 모든 부분,

1014
00:48:29,931 --> 00:48:32,846
당신은 이것을 사용해서 어쩌면 먼저

1015
00:48:32,846 --> 00:48:34,936
내가 실제로 처리하고자하는 관련 부품,

1016
00:48:34,936 --> 00:48:36,353
내 ConvNet 사용.

1017
00:48:37,237 --> 00:48:39,849
자, 강화 학습은 무엇입니까?

1018
00:48:39,849 --> 00:48:41,531
이 문제의 공식화?

1019
00:48:41,531 --> 00:48:44,711
글쎄, 우리 주정부가 될거야.

1020
00:48:44,711 --> 00:48:46,889
우리가 지금까지 보았던 엿볼은 맞습니까?

1021
00:48:46,889 --> 00:48:47,722
우리의

1022
00:48:48,881 --> 00:48:51,117
우리가 본 정보는 무엇입니까?

1023
00:48:51,117 --> 00:48:53,643
우리의 행동은

1024
00:48:53,643 --> 00:48:55,228
이미지에서 다음을 볼 수 있습니다.

1025
00:48:55,228 --> 00:48:57,090
맞아요, 실제로, 이것은 뭔가가 될 수 있습니다.

1026
00:48:57,090 --> 00:48:59,113
x, y 좌표는 어쩌면 일부를 중심으로

1027
00:48:59,113 --> 00:49:02,842
당신이 다음에보고 싶은 고정 크기의 엿볼.

1028
00:49:02,842 --> 00:49:05,664
그리고 나서 분류 문제에 대한 보상

1029
00:49:05,664 --> 00:49:08,256
마지막 시간 단계에서 하나가 될 것입니다.

1030
00:49:08,256 --> 00:49:12,423
우리의 이미지가 정확하게 분류되면,
그렇지 않으면 0이됩니다.

1031
00:49:14,495 --> 00:49:16,162
그래서,

1032
00:49:17,373 --> 00:49:20,016
이미지를 한 눈에 보면서

1033
00:49:20,016 --> 00:49:21,932
비차별적인 연산이므로,

1034
00:49:21,932 --> 00:49:24,006
이것이 왜 우리가

1035
00:49:24,006 --> 00:49:25,761
보강 학습 배합,

1036
00:49:25,761 --> 00:49:29,088
이러한 엿볼 수있는 행동을 취하는
방법에 대한 정책을 배우십시오.

1037
00:49:29,088 --> 00:49:31,792
우리는 REINFORCE를 사용하여 이것을 훈련시킬 수 있습니다.

1038
00:49:31,792 --> 00:49:35,105
그래서 지금까지 흘끗 보인 상태를 보면,

1039
00:49:35,105 --> 00:49:37,537
우리 모델의 핵심은

1040
00:49:37,537 --> 00:49:40,891
이 RNN은 우리가 국가를
모델링하기 위해 사용할 예정이며,

1041
00:49:40,891 --> 00:49:44,501
정책 매개 변수를 사용하려고합니다.

1042
00:49:44,501 --> 00:49:47,418
다음 동작을 출력합니다.

1043
00:49:49,354 --> 00:49:53,248
좋아, 그래서이 모델이 어떻게
생겼는지 우리가 취할 것입니다.

1044
00:49:53,248 --> 00:49:54,571
입력 이미지.

1045
00:49:54,571 --> 00:49:57,655
그렇습니다. 그러면 우리는이 이미지를 엿볼 것입니다.

1046
00:49:57,655 --> 00:50:00,068
여기이 빨간 상자가 여기 있습니다.

1047
00:50:00,068 --> 00:50:03,184
이것은 모두 공백, 0입니다.

1048
00:50:03,184 --> 00:50:06,966
그래서 우리는 지금까지 우리가 보았던
것을 몇 가지로 전달할 것입니다.

1049
00:50:06,966 --> 00:50:09,388
신경 네트워크, 그리고 이것은 어떤 것도 될 수 있습니다.

1050
00:50:09,388 --> 00:50:12,193
귀하의 작업에 따라 종류의 네트워크.

1051
00:50:12,193 --> 00:50:14,276
내가 여기서 보여주고있는 원래의 실험에서,

1052
00:50:14,276 --> 00:50:16,138
MNIST에서 이것은 매우 간단합니다.

1053
00:50:16,138 --> 00:50:18,758
작은 두 개의 완전히 연결된 레이어를 사용하고,

1054
00:50:18,758 --> 00:50:21,724
그러나 더 복잡한 이미지를 상상할 수 있습니다.

1055
00:50:21,724 --> 00:50:26,105
및 기타 작업을 ConvNets를
사용하기를 원할 수 있습니다.

1056
00:50:26,105 --> 00:50:28,775
맞아요. 그래서 이것을 당신이 신경망에 넣었습니다.

1057
00:50:28,775 --> 00:50:31,065
그리고 나서 우리는 또한 우리가 될
것이라고 말했던 것을 기억하십시오.

1058
00:50:31,065 --> 00:50:34,102
우리의 상태를 통합하고, 우리가 본 흘긋을

1059
00:50:34,102 --> 00:50:36,115
지금까지 재귀 네트워크를 사용했습니다.

1060
00:50:36,115 --> 00:50:38,057
그래서, 난 그냥 갈거야.

1061
00:50:38,057 --> 00:50:40,265
우리는 나중에 그것을 보게 될 것입니다,
그러나 이것은 그것을 통과 할 것입니다,

1062
00:50:40,265 --> 00:50:42,646
그런 다음 출력 할 것입니다.

1063
00:50:42,646 --> 00:50:46,094
x, y 좌표, 다음에 볼 곳.

1064
00:50:46,094 --> 00:50:48,435
실제로, 이것은 될 것입니다.

1065
00:50:48,435 --> 00:50:50,766
우리는 행동에 대한 분포를 출력하고자합니다.

1066
00:50:50,766 --> 00:50:53,385
맞아요, 그러면이게 될 일이 될거야.

1067
00:50:53,385 --> 00:50:57,282
가우시안 분포와 우리는 평균을 출력 할 것입니다.

1068
00:50:57,282 --> 00:50:59,084
평균과 분산을 출력 할 수도 있습니다.

1069
00:50:59,084 --> 00:51:00,545
실제로이 배포판의

1070
00:51:00,545 --> 00:51:03,944
분산도 고정 될 수 있습니다.

1071
00:51:03,944 --> 00:51:07,172
좋아요, 그래서 우리는 이것을 취할 것입니다.

1072
00:51:07,172 --> 00:51:08,496
우리가 지금 샘플로 할 행동

1073
00:51:08,496 --> 00:51:11,854
액션 배포의 특정 x, y 위치

1074
00:51:11,854 --> 00:51:15,457
그리고 우리는 이것을 다음에 얻기 위해 넣을 것입니다.

1075
00:51:15,457 --> 00:51:17,777
우리의 이미지에서 다음 엿볼을 추출하십시오.

1076
00:51:17,777 --> 00:51:19,297
맞아, 여기 우리가 움직였다.

1077
00:51:19,297 --> 00:51:23,385
두 사람의 끝에, 두 사람의 꼬리 부분.

1078
00:51:23,385 --> 00:51:25,465
그리고 이제 우리는 실제로 어떤 신호를 받기 시작합니다.

1079
00:51:25,465 --> 00:51:26,745
우리가보고 싶은 것, 맞죠?

1080
00:51:26,745 --> 00:51:29,065
마찬가지로 우리가 원하는 것은 관련성을 조사하는 것입니다.

1081
00:51:29,065 --> 00:51:32,724
분류에 유용한 이미지의 부분.

1082
00:51:32,724 --> 00:51:35,354
그래서 우리는 이것을 다시 우리의
신경망 층으로 통과 시키며,

1083
00:51:35,354 --> 00:51:37,104
그리고 나서

1084
00:51:38,153 --> 00:51:40,362
우리가 되풀이하는 네트워크, 맞아.

1085
00:51:40,362 --> 00:51:43,642
이 이전의 숨겨진 상태뿐 아니라

1086
00:51:43,642 --> 00:51:45,524
이것을 사용하여,

1087
00:51:45,524 --> 00:51:47,343
이것은 우리의 정책을 대표합니다.

1088
00:51:47,343 --> 00:51:49,565
우리는 이것을 출력에 사용하려고합니다.

1089
00:51:49,565 --> 00:51:51,354
우리의 다음 배포판

1090
00:51:51,354 --> 00:51:54,095
우리가 엿보고 싶은 위치.

1091
00:51:54,095 --> 00:51:55,874
그래서 우리는 이것을 계속할 수 있습니다.

1092
00:51:55,874 --> 00:51:57,303
당신은이 다음에 여기에서 엿볼 수 있습니다.

1093
00:51:57,303 --> 00:51:59,903
우리는 둘의 중심을 향해 조금 더 움직였습니다.

1094
00:51:59,903 --> 00:52:01,723
알았어. 그래서 아마 그걸 배울거야.

1095
00:52:01,723 --> 00:52:05,005
너는 알다시피, 나는이 두 꼬리 부분을 보았을 때,

1096
00:52:05,005 --> 00:52:08,093
이 모양이 어쩌면이 어퍼에서 움직일 수도 있습니다.

1097
00:52:08,093 --> 00:52:10,794
왼손 방향을 사용하면

1098
00:52:10,794 --> 00:52:12,631
가치가있는 센터,

1099
00:52:12,631 --> 00:52:14,543
귀중한 정보.

1100
00:52:14,543 --> 00:52:17,473
그런 다음 계속해서 할 수 있습니다.

1101
00:52:17,473 --> 00:52:20,612
그리고 마침내, 결국, 마지막 단계에서,

1102
00:52:20,612 --> 00:52:23,412
그래서 여기에 고정 된 시간 간격을 가질 수 있습니다.

1103
00:52:23,412 --> 00:52:26,795
실제로 6 또는 8과 같은 것입니다.

1104
00:52:26,795 --> 00:52:29,359
그리고 마지막 시간 단계에서, 우리가하고 싶기 때문에

1105
00:52:29,359 --> 00:52:33,350
분류, 우리는 우리의 표준

1106
00:52:33,350 --> 00:52:36,100
Softmax 레이어는

1107
00:52:37,376 --> 00:52:39,363
각 클래스에 대한 확률 분포.

1108
00:52:39,363 --> 00:52:42,111
그리고 나서 여기 최대 클래스는 2,

1109
00:52:42,111 --> 00:52:44,108
그래서 우리는 이것이 2라고 예측할 수 있습니다.

1110
00:52:44,108 --> 00:52:46,558
맞아요, 그리고 이것은 우리의 셋업이 될 것입니다.

1111
00:52:46,558 --> 00:52:50,428
모델과 우리의 정책을

1112
00:52:50,428 --> 00:52:53,079
우리가 가지고있는이 정책의 기울기에 대한 추정치

1113
00:52:53,079 --> 00:52:54,420
이전에 우리는

1114
00:52:54,420 --> 00:52:56,695
여기에서 궤적

1115
00:52:56,695 --> 00:52:59,569
그걸 사용하여 소품을 되 찾으십시오.

1116
00:52:59,569 --> 00:53:02,819
그래서 우리는이 모델을 훈련시키기
위해서 이것을 할 수 있습니다.

1117
00:53:02,819 --> 00:53:05,281
우리 정책의 매개 변수를 배우 죠?

1118
00:53:05,281 --> 00:53:08,698
여기에서 볼 수있는 모든 무게.

1119
00:53:09,953 --> 00:53:10,786
좋아요.

1120
00:53:12,239 --> 00:53:14,270
여기에

1121
00:53:14,270 --> 00:53:16,710
MNIST에서 교육 된 정책,

1122
00:53:16,710 --> 00:53:19,016
그래서 당신은 일반적으로,

1123
00:53:19,016 --> 00:53:20,808
어디에서 시작하든, 보통 배우게됩니다.

1124
00:53:20,808 --> 00:53:22,942
자리가 더 가까이 갈 수 있도록

1125
00:53:22,942 --> 00:53:25,260
그런 다음 해당 자릿수의 관련 부분을 살펴보십시오.

1126
00:53:25,260 --> 00:53:27,685
그래서 이것은 매우 차갑고

1127
00:53:27,685 --> 00:53:28,744
이

1128
00:53:28,744 --> 00:53:30,460
너도 알다시피, 네가 기대하는 바를 따른다. 맞다.

1129
00:53:30,460 --> 00:53:31,627
너라면

1130
00:53:33,335 --> 00:53:34,967
다음에 볼 장소를 선택하십시오.

1131
00:53:34,967 --> 00:53:38,186
가장 효율적으로 결정하기 위해

1132
00:53:38,186 --> 00:53:40,108
이게 무슨 자리 지.

1133
00:53:40,108 --> 00:53:43,491
맞아, 그래서 열심히주의를 기울이는이 아이디어는,

1134
00:53:43,491 --> 00:53:45,862
재발주의 모델의 사용, 또한 사용되었습니다

1135
00:53:45,862 --> 00:53:49,758
컴퓨터 비전의 많은 작업에서 마지막으로

1136
00:53:49,758 --> 00:53:52,687
2 년이 지나면 이것을 볼 수 있습니다. 예를 들어,

1137
00:53:52,687 --> 00:53:54,790
미세한 이미지 인식.

1138
00:53:54,790 --> 00:53:57,869
그래서 나는 이전에 언급했다.

1139
00:53:57,869 --> 00:54:00,596
이 유용한 이점 중 하나

1140
00:54:00,596 --> 00:54:01,763
~ 할 수도있다.

1141
00:54:02,975 --> 00:54:05,198
계산 효율을 줄여 준다.

1142
00:54:05,198 --> 00:54:08,009
혼란과 관련성을 무시하는 것

1143
00:54:08,009 --> 00:54:10,180
이미지의 일부분, 그리고 세분화 된 이미지

1144
00:54:10,180 --> 00:54:11,750
이미지 분류 문제,

1145
00:54:11,750 --> 00:54:13,092
당신은 대개 두 가지 모두를 원합니다.

1146
00:54:13,092 --> 00:54:17,307
당신은 고해상도를 유지하기를 원합니다.

1147
00:54:17,307 --> 00:54:19,447
중요한 차이점이 있습니다.

1148
00:54:19,447 --> 00:54:23,327
그런 다음이 차이에 초점을 맞추기를 원합니다.

1149
00:54:23,327 --> 00:54:25,777
무의미한 부분은 무시하십시오.

1150
00:54:25,777 --> 00:54:27,359
그래, 질문.

1151
00:54:27,359 --> 00:54:31,526
[청중의 들리지 않는 질문]

1152
00:54:35,061 --> 00:54:36,789
그래, 그래, 그 질문은

1153
00:54:36,789 --> 00:54:39,061
어떻게 계산 효율이 있는지,

1154
00:54:39,061 --> 00:54:41,482
왜냐하면 우리는 또한이 재발 성 신경망을
제 위치에 가지고 있기 때문입니다.

1155
00:54:41,482 --> 00:54:45,842
그렇습니다. 그것은 당신의 것이 무엇인지에 달려 있습니다.

1156
00:54:45,842 --> 00:54:47,761
당신의 문제는 무엇이고, 당신의
네트워크는 무엇입니까, 등등,

1157
00:54:47,761 --> 00:54:50,151
하지만 네가 정말로

1158
00:54:50,151 --> 00:54:52,512
고해상도 이미지

1159
00:54:52,512 --> 00:54:54,773
이 모든 부분을 처리하고 싶지는 않습니다.

1160
00:54:54,773 --> 00:54:58,477
이미지 일부 거대한 ConvNet
또는 일부 거대한, 당신도 알다시피,

1161
00:54:58,477 --> 00:55:01,900
네트워크를 통해 이제는 비용을 절약 할 수 있습니다.

1162
00:55:01,900 --> 00:55:04,669
이미지의 특정 작은 부분에 초점을 맞 춥니 다.

1163
00:55:04,669 --> 00:55:06,589
이미지의 해당 부분 만 처리합니다.

1164
00:55:06,589 --> 00:55:08,507
그러나, 당신은 옳습니다.

1165
00:55:08,507 --> 00:55:10,924
당신이 가지고있는 문제 설정.

1166
00:55:12,210 --> 00:55:14,530
이것은 또한 이미지 캡션에 사용되었습니다.

1167
00:55:14,530 --> 00:55:17,138
그래서 우리가 이미지에 대한 캡션을 생성하려고한다면,

1168
00:55:17,138 --> 00:55:20,421
우리가 선택할 수 있습니다, 당신도 알다시피,
우리는 이미지를 가질 수 있습니다.

1169
00:55:20,421 --> 00:55:23,197
이 캡션을 생성하려면이주의 모델을 사용하십시오.

1170
00:55:23,197 --> 00:55:26,120
일반적으로 학습을 끝내는 것은 이러한 정책입니다.

1171
00:55:26,120 --> 00:55:28,999
여기서 이미지의 특정 부분에 초점을 맞 춥니 다.

1172
00:55:28,999 --> 00:55:31,850
순서대로, 그리고 그것이 각 부분에 초점을 맞추면서,

1173
00:55:31,850 --> 00:55:34,629
캡션의 일부 또는 일부를 생성합니다.

1174
00:55:34,629 --> 00:55:38,341
이미지의 해당 부분을 참조하십시오.

1175
00:55:38,341 --> 00:55:40,170
그리고 나서 그것은 또한 사용되었습니다,

1176
00:55:40,170 --> 00:55:42,948
시각적 질문 응답,

1177
00:55:42,948 --> 00:55:45,509
이미지에 대한 질문을합니다.

1178
00:55:45,509 --> 00:55:48,981
모델이 어떤 대답을 출력하기를 원한다.

1179
00:55:48,981 --> 00:55:51,786
당신의 질문에, 예를 들면, 나는 모른다.

1180
00:55:51,786 --> 00:55:53,840
테이블 주위에 몇 개의 의자가 있습니까?

1181
00:55:53,840 --> 00:55:58,229
그래서 당신은이주의 메커니즘이
어떻게 보이는지 볼 수 있습니다.

1182
00:55:58,229 --> 00:55:59,457
좋은 유형의 모델 일 수 있습니다.

1183
00:55:59,457 --> 00:56:03,040
이 질문에 대답하는 법을 배우기.

1184
00:56:05,707 --> 00:56:08,475
그래, 그게 정책 구배의 예 였어.

1185
00:56:08,475 --> 00:56:10,564
이 하드 어텐션 모델에서.

1186
00:56:10,564 --> 00:56:13,524
그래서 저는 이제 하나의 예를 더 이야기 할 것입니다.

1187
00:56:13,524 --> 00:56:16,430
또한 정책 그라디언트를 사용합니다.

1188
00:56:16,430 --> 00:56:18,465
그것은 Go를하는 법을 배우고 있습니다.

1189
00:56:18,465 --> 00:56:22,006
맞아, DeepMind가이 요원을 가졌어.

1190
00:56:22,006 --> 00:56:24,497
알프 고 (AlphGo)라고 불리는 고 (Go)

1191
00:56:24,497 --> 00:56:27,297
뉴스에 많이 등장했습니다.

1192
00:56:27,297 --> 00:56:30,708
작년과 올해에.

1193
00:56:30,708 --> 00:56:31,541
죄송 해요?

1194
00:56:31,541 --> 00:56:32,374
[청중의 들리지 않는 의견]

1195
00:56:32,374 --> 00:56:35,291
그리고 어제, 맞습니다.

1196
00:56:36,172 --> 00:56:39,258
이것은 매우 흥미롭고, 최근 소식입니다.

1197
00:56:39,258 --> 00:56:40,987
그래서 작년에,

1198
00:56:40,987 --> 00:56:43,234
AlphaGo의 첫 번째 버전

1199
00:56:43,234 --> 00:56:44,817
~에 들어갔다.

1200
00:56:46,678 --> 00:56:49,539
최고의 고 (Go) 선수들과의 경쟁

1201
00:56:49,539 --> 00:56:52,609
최근 몇 년 동안, Lee Sedol과 대리인

1202
00:56:52,609 --> 00:56:54,927
그를 이길 수 있었다.

1203
00:56:54,927 --> 00:56:57,886
5 경기 동안 4 대 1 경기를 펼쳤습니다.

1204
00:56:57,886 --> 00:57:00,541
그리고 사실, 지금 당장, 그냥

1205
00:57:00,541 --> 00:57:03,788
Ke Jie와의 또 다른 경기가 있습니다.

1206
00:57:03,788 --> 00:57:07,855
세계 제일의 숫자이기 때문에 3 개 중 최고입니다.

1207
00:57:07,855 --> 00:57:09,348
지금 중국에서.

1208
00:57:09,348 --> 00:57:12,236
그래서 첫 경기가 어제였습니다.

1209
00:57:12,236 --> 00:57:13,436
AlphaGo 이겼다.

1210
00:57:13,436 --> 00:57:16,596
나는 그것이 단지 반점에 의한 것이라고 생각한다. 그래서,

1211
00:57:16,596 --> 00:57:18,606
그래서 두 가지 더 볼 게임이 있습니다.

1212
00:57:18,606 --> 00:57:20,657
이들은 모두 라이브 스트림이므로

1213
00:57:20,657 --> 00:57:24,276
너희들도 온라인에 가서이 게임들을 봐야 해.

1214
00:57:24,276 --> 00:57:28,193
해설을 듣는 것은 꽤 흥미 롭습니다.

1215
00:57:29,225 --> 00:57:32,577
그러나 DeepMind의 AlphaGo
에이전트는 무엇입니까?

1216
00:57:32,577 --> 00:57:34,868
그리고 그것은 우리가 말한 것을 많이 기반으로합니다.

1217
00:57:34,868 --> 00:57:36,466
지금까지는이 강의에서.

1218
00:57:36,466 --> 00:57:39,687
그리고 그것은 감독 학습의 혼합입니다.

1219
00:57:39,687 --> 00:57:42,045
강화 학습,

1220
00:57:42,045 --> 00:57:44,657
뿐만 아니라 이전 버전과

1221
00:57:44,657 --> 00:57:48,573
이동, 몬테카를로 트리 검색,

1222
00:57:48,573 --> 00:57:51,656
최근의 심층 RL 접근 방식에 대해서도 설명합니다.

1223
00:57:52,579 --> 00:57:56,986
그렇다면 AlphaGo는 Go
세계 챔피언을 어떻게 상회합니까?

1224
00:57:56,986 --> 00:57:59,363
음, 처음에는 무엇을합니까?

1225
00:57:59,363 --> 00:58:02,449
입력이 될 때 AlphaGo를 훈련 시키는데

1226
00:58:02,449 --> 00:58:04,089
보드의 몇 가지 특징 화.

1227
00:58:04,089 --> 00:58:06,470
기본적으로, 당신의 이사회와 직위입니다.

1228
00:58:06,470 --> 00:58:08,739
보드에 조각의.

1229
00:58:08,739 --> 00:58:10,739
그것이 당신의 자연스러운 모습입니다.

1230
00:58:10,739 --> 00:58:13,819
성과를 향상시키기 위해 그들이하는 일

1231
00:58:13,819 --> 00:58:16,638
조금은 그들이 이것을 실현하는 것입니다.

1232
00:58:16,638 --> 00:58:17,958
약간

1233
00:58:17,958 --> 00:58:20,510
하나의 더 많은 채널은 모두 다른 돌 색깔입니다,

1234
00:58:20,510 --> 00:58:21,956
그래서 이건 너 같은 종류 야.

1235
00:58:21,956 --> 00:58:23,790
보드 구성.

1236
00:58:23,790 --> 00:58:27,270
또한 일부 채널, 예를 들어, 어디서, 어떤 움직임

1237
00:58:27,270 --> 00:58:31,138
합법적 인 것, 일부 바이어스 채널, 몇 가지 다양한 것들

1238
00:58:31,138 --> 00:58:33,125
그런 다음이 상태가 주어지면,

1239
00:58:33,125 --> 00:58:35,117
그것은 첫 번째로 갈 것입니다.

1240
00:58:35,117 --> 00:58:36,518
네트워크를 훈련하다

1241
00:58:36,518 --> 00:58:38,867
감독 교육을 통해 초기화됩니다.

1242
00:58:38,867 --> 00:58:40,897
전문 이동 게임에서.

1243
00:58:40,897 --> 00:58:43,147
따라서, 현재의 보드 구성이 주어지면

1244
00:58:43,147 --> 00:58:45,627
또는 특징, 이것의 특징 화,

1245
00:58:45,627 --> 00:58:48,495
올바른 다음 조치는 무엇입니까?

1246
00:58:48,495 --> 00:58:50,678
좋아, 그렇게 주어진다.

1247
00:58:50,678 --> 00:58:52,787
전문 게임의 예,

1248
00:58:52,787 --> 00:58:55,608
알다시피, 시간이 지남에 따라 수집됩니다.

1249
00:58:55,608 --> 00:58:57,635
우리는이 모든 전문적인 움직임을 취할 수 있습니다.

1250
00:58:57,635 --> 00:58:59,815
표준, 감독지도 작성,

1251
00:58:59,815 --> 00:59:02,605
보드 상태에서 취할 조치.

1252
00:59:02,605 --> 00:59:05,365
알았어. 그래서 그들은 이것을 취한다.
이것은 꽤 좋은 시작이다.

1253
00:59:05,365 --> 00:59:07,637
그리고 나서 그들은 이것을 사용하여 초기화 할 것입니다.

1254
00:59:07,637 --> 00:59:09,227
정책 네트워크.

1255
00:59:09,227 --> 00:59:10,844
맞아요, 정책 네트워크 때문에, 그냥 취할 것입니다.

1256
00:59:10,844 --> 00:59:14,985
정확한 입력 구조는 귀하의

1257
00:59:14,985 --> 00:59:16,389
보드 상태와 출력이

1258
00:59:16,389 --> 00:59:17,778
당신이 취할 행동.

1259
00:59:17,778 --> 00:59:20,887
그리고 이것은 정책 그라디언트를위한 준비였습니다.

1260
00:59:20,887 --> 00:59:21,978
우리가 방금 본 거 맞지?

1261
00:59:21,978 --> 00:59:25,156
이제 우리는이 훈련을 계속할 것입니다.

1262
00:59:25,156 --> 00:59:27,130
정책 그라디언트 사용

1263
00:59:27,130 --> 00:59:30,831
그리고이 강화 학습 학습을 할 것입니다.

1264
00:59:30,831 --> 00:59:35,123
랜덤, 이전 반복을 위해 자체적으로 게임을합니다.

1265
00:59:35,123 --> 00:59:37,384
그래서 자기 희생과 보상을 얻을 것입니다.

1266
00:59:37,384 --> 00:59:42,243
그것이 이기면 하나이고, 잃으면 음수가됩니다.

1267
00:59:42,243 --> 00:59:44,624
그리고 우리가하려고하는 것은 또한 우리도 배울 것입니다.

1268
00:59:44,624 --> 00:59:47,573
가치 네트워크, 그래서 비평가 같은 것.

1269
00:59:47,573 --> 00:59:51,235
그리고 나서, 최종 AlphaGo가 결합 될 것입니다.

1270
00:59:51,235 --> 00:59:53,982
이 모든 것들이 함께 있으므로 정책과 가치 네트워크

1271
00:59:53,982 --> 00:59:56,075
뿐만 아니라

1272
00:59:56,075 --> 00:59:59,043
몬테카를로 트리 검색 알고리즘을 선택하려면

1273
00:59:59,043 --> 01:00:01,475
미리보기 검색을 통한 작업.

1274
01:00:01,475 --> 01:00:04,743
맞아,이 모든 것을 모아서,

1275
01:00:04,743 --> 01:00:08,853
당신이 놀고있는 노드의 값,

1276
01:00:08,853 --> 01:00:11,590
그리고 당신이 다음에하는 일은 조합이 될 것입니다.

1277
01:00:11,590 --> 01:00:13,811
당신의 가치 함수의

1278
01:00:13,811 --> 01:00:16,552
표준에서 계산 한 결과에서 롤아웃

1279
01:00:16,552 --> 01:00:19,891
Monte Carlo Tree 검색 롤 아웃.

1280
01:00:19,891 --> 01:00:22,891
그래, 근데, 근데 기본적으로

1281
01:00:24,203 --> 01:00:27,453
다양한, AlphaGo의 구성 요소.

1282
01:00:28,397 --> 01:00:30,314
이것에 대해 더 많이 읽고 싶다면,

1283
01:00:30,314 --> 01:00:33,814
2016 년에 이것에 대한 자연 종이가 있습니다.

1284
01:00:34,664 --> 01:00:37,656
그리고 그들은 이것을 훈련 시켰습니다.

1285
01:00:37,656 --> 01:00:40,765
이 버전에서 사용되는 AlphaGo 버전

1286
01:00:40,765 --> 01:00:45,016
경기는 몇 천 개의 CPU가 있다고 생각합니다.

1287
01:00:45,016 --> 01:00:47,953
몇 백 개의 GPU를 더하여이 모든 것을 하나로 모으고,

1288
01:00:47,953 --> 01:00:52,120
그래서 그것은 엄청난 양의 훈련입니다.

1289
01:00:55,659 --> 01:00:57,514
그리고 네, 그렇게해야합니다.

1290
01:00:57,514 --> 01:00:59,681
이번 주 경기에 출전하십시오.

1291
01:01:01,643 --> 01:01:03,491
그것은 꽤 흥미 롭습니다.

1292
01:01:03,491 --> 01:01:07,524
좋아요, 요약하면, 오늘 우리는

1293
01:01:07,524 --> 01:01:10,858
정책 그라디언트, 오른쪽, 일반입니다.

1294
01:01:10,858 --> 01:01:13,025
그들, 너는 바로

1295
01:01:14,456 --> 01:01:18,855
그라디언트 디센트 또는 상승을 정책 매개 변수에 적용하면

1296
01:01:18,855 --> 01:01:21,942
그래서 이것은 많은 종류의 문제들에 대해 잘 작동하며,

1297
01:01:21,942 --> 01:01:23,947
그러나 또한 높은 편차를 겪는다.

1298
01:01:23,947 --> 01:01:25,938
그래서 많은 샘플이 필요합니다.

1299
01:01:25,938 --> 01:01:28,669
여기 당신의 도전은 표본 효율입니다.

1300
01:01:28,669 --> 01:01:32,608
우리는 Q- 러닝에 대해서도
이야기했는데 항상 그렇지는 않습니다.

1301
01:01:32,608 --> 01:01:35,349
일할 때가끔 어렵다.

1302
01:01:35,349 --> 01:01:37,536
우리가 전에 말했던이 문제 때문에

1303
01:01:37,536 --> 01:01:39,702
당신은 이것을 계산하려고합니다.

1304
01:01:39,702 --> 01:01:42,285
정확한 상태, 액션 값

1305
01:01:43,324 --> 01:01:47,769
많은 사람들에게, 매우 높은 차원을
위해, 그러나 그것이 효과가있을 때,

1306
01:01:47,769 --> 01:01:51,342
예를 들어 우리가 전에 보았던 아타리 (Atari)

1307
01:01:51,342 --> 01:01:53,092
보통 더 효율적인 샘플입니다.

1308
01:01:53,092 --> 01:01:54,322
정책 기울기보다.

1309
01:01:54,322 --> 01:01:56,540
맞아요, Q-learning의 과제 중 하나는

1310
01:01:56,540 --> 01:01:57,484
당신은 당신이

1311
01:01:57,484 --> 01:01:59,902
충분한 탐사를하고있어.

1312
01:01:59,902 --> 01:02:00,735
네?

1313
01:02:00,735 --> 01:02:04,902
[청중의 들리지 않는 질문]

1314
01:02:14,313 --> 01:02:17,764
오, Q- 러닝의 경우이 과정을 어디에서 할 수 있습니까?

1315
01:02:17,764 --> 01:02:20,684
너 좋아, 너 어디서부터 시작하려고하는지.

1316
01:02:20,684 --> 01:02:21,753
일부 감독 훈련?

1317
01:02:21,753 --> 01:02:24,924
따라서 Q- 학습을위한 직접 접근 방식은 그렇지 않습니다.

1318
01:02:24,924 --> 01:02:27,532
당신이 이것들에 회귀하려고하기 때문에 그렇게하십시오.

1319
01:02:27,532 --> 01:02:29,972
이것에 대한 정책 구배 대신에 Q 값, 맞다.

1320
01:02:29,972 --> 01:02:32,732
배포판에 있지만, 나는 당신이 할
수있는 방법이 있다고 생각한다.

1321
01:02:32,732 --> 01:02:34,232
좋아, 마사지해라.

1322
01:02:35,938 --> 01:02:37,985
부츠 랏뿌도하는 타입.

1323
01:02:37,985 --> 01:02:40,393
부츠 스트랩은 일반적으로 생각하기 때문에

1324
01:02:40,393 --> 01:02:43,854
행동 복제는 좋은 방법입니다.

1325
01:02:43,854 --> 01:02:46,021
이러한 정책을 따뜻하게 시작하십시오.

1326
01:02:47,454 --> 01:02:50,284
그래, 그렇다. 그래서 우리는
정책 구배에 대해 이야기했다.

1327
01:02:50,284 --> 01:02:54,213
Q- 러닝 (Q-learning), 그리고
이들 중 일부에 대한 또 다른 시각,

1328
01:02:54,213 --> 01:02:55,413
당신이 가지고있는 보장 중 일부는

1329
01:02:55,413 --> 01:02:56,752
오른쪽, 정책 그라디언트.

1330
01:02:56,752 --> 01:02:58,622
우리가 정말 잘 알고있는 한 가지는

1331
01:02:58,622 --> 01:03:02,789
이것은 항상 세타의 J 지역의 최소치에 수렴 할 것이고,

1332
01:03:04,339 --> 01:03:06,592
그라디언트 상승을 직접하고 있기 때문에,

1333
01:03:06,592 --> 01:03:09,043
그래서 이것은 종종,

1334
01:03:09,043 --> 01:03:12,131
이 지역 최소값은 종종 꽤 좋았습니다.

1335
01:03:12,131 --> 01:03:14,931
그리고 Q-Learning에서는 다른 한편으로는

1336
01:03:14,931 --> 01:03:17,041
여기에 우리가 대략적으로하려고하기 때문에

1337
01:03:17,041 --> 01:03:20,277
복잡한 함수를 가진이 Bellman 방정식

1338
01:03:20,277 --> 01:03:23,358
approximator 및 그래서,이
경우, 이것은 문제입니다

1339
01:03:23,358 --> 01:03:25,787
Q-learning은 훈련하는
것이 조금 더 까다 롭습니다.

1340
01:03:25,787 --> 01:03:29,954
광범위한 문제에 적용 할 수있는 측면에서

1341
01:03:31,849 --> 01:03:34,737
좋아, 오늘 너는 근본적으로 아주 좋아,

1342
01:03:34,737 --> 01:03:37,907
간략한, 강화 학습의 고차원 적 개요

1343
01:03:37,907 --> 01:03:41,546
그리고 RL의 일부 주요 클래스의 알고리즘.

1344
01:03:41,546 --> 01:03:44,419
다음에 우리는

1345
01:03:44,419 --> 01:03:47,577
게스트 강사, 송 한, 많은 일을했다.

1346
01:03:47,577 --> 01:03:51,276
모델 압축에서의 개척 작업

1347
01:03:51,276 --> 01:03:52,569
에너지 효율적인 심층 학습,

1348
01:03:52,569 --> 01:03:56,459
그래서 그는 이것에 대해 몇 가지 이야기 할 것입니다.

1349
01:03:56,459 --> 01:03:58,459
고맙습니다.

