1
00:00:09,784 --> 00:00:11,867
시작하겠습니다!

2
00:00:13,038 --> 00:00:15,888
CS231n 14강에 오신 것을 환영합니다.

3
00:00:15,888 --> 00:00:20,884
이번 시간에는 강화학습을 배워보겠습니다.

4
00:00:20,884 --> 00:00:23,222
우선 공지사항을 전달하겠습니다.

5
00:00:23,222 --> 00:00:30,346
어제 저녁 중간고사 점수를 공개했습니다.
자세한 정보는 Piazza에서 확인하시기 바랍니다.

6
00:00:30,346 --> 00:00:35,402
그리고 두 번째 과제 및 마일스톤 점수는
주말 중으로 공개하도록 하겠습니다.

7
00:00:36,768 --> 00:00:40,682
프로젝트와 관련한 전달사항도 있습니다.
모든 팀은 반드시 프로젝트를 등록하셔야 합니다.

8
00:00:40,682 --> 00:00:47,580
Piazza에서 양식 확인 후 작성 바라며

9
00:00:47,580 --> 00:00:53,214
여러분이 작성하신 문서는 최종 학점에 반영되며
포스터 세션에서 활용 할 예정입니다.

10
00:00:53,214 --> 00:01:01,779
Tiny ImageNet Challenge에 쓰일 서버가 열렸습니다.

11
00:01:01,779 --> 00:01:06,193
그리고 며칠전 Piazza에 CS231n 수업평가
설문을 게시하였습니다.

12
00:01:06,193 --> 00:01:13,600
아직 진행하지 않으신 분들께서는 설문을 부탁드리며
CS231n의 성장을 위한 아주 소중한 피드백이 될 것입니다.

13
00:01:16,589 --> 00:01:19,650
자 오늘의 주제는 강화학습(reinforcement learning) 입니다.

14
00:01:19,650 --> 00:01:22,544
우리는 지금까지는
supervised learning을 배웠습니다.

15
00:01:22,544 --> 00:01:30,498
데이터인 x와 레이블인 y가 있었고, x를 y에 매핑하는
함수를 학습하는게 목표였습니다.

16
00:01:30,498 --> 00:01:35,067
가령 classification 문제가 있었습니다.

17
00:01:35,067 --> 00:01:37,753
그리고 지난 강의에서는
unsupervised learning을 배웠습니다.

18
00:01:37,753 --> 00:01:45,362
데이터는 있지만 레이블이 없는 경우였습니다. 데이터에 내재된
숨겨진 구조를 학습하는게 목표였습니다.

19
00:01:45,362 --> 00:01:50,528
가령 생성모델(generative models)이 있었습니다.

20
00:01:52,040 --> 00:01:57,370
이번에 배우는 것은 조금 다릅니다.
강화학습이라는 문제입니다.

21
00:01:57,370 --> 00:02:01,824
여기에는 에이전트(agent)가 있습니다. 환경(environment)
에서 행동(action)을 취하는 주체입니다.

22
00:02:01,824 --> 00:02:04,352
에이전트는 행동에 따른
적절한 보상(rewards)을 받습니다.

23
00:02:04,352 --> 00:02:09,959
강화학습은 에이전트의 보상을 최대화할 수 있는
행동이 무엇인지를 학습하는 것입니다.

24
00:02:09,959 --> 00:02:14,101
오늘은 이에 대해서 자세하게 다뤄볼 것입니다.

25
00:02:14,101 --> 00:02:18,116
이번 수업의 개요를 말씀드리자면
우선 강화학습이 어떤 문제인지를 다루고

26
00:02:18,116 --> 00:02:20,927
그리고 Markov Decision Processes(MDP)
에 대해서 다룰 것입니다.

27
00:02:20,927 --> 00:02:24,747
MDP는 강화학습 문제의 수식체계(formalism) 입니다.

28
00:02:24,747 --> 00:02:31,095
그리고 대표적인 강화학습 방법인 Q-Learning과
Policy Gradients에 대해서 배워보겠습니다.

29
00:02:32,876 --> 00:02:38,936
강화학습의 문제에서는 에이전트와 환경이 있습니다.

30
00:02:38,936 --> 00:02:43,268
환경에서 에이전트에게는 상태가 주어집니다.

31
00:02:43,268 --> 00:02:46,877
그리고 에이전트는 어떤 행동을 취하게 됩니다.

32
00:02:46,877 --> 00:02:52,609
그러면 환경은 행동에 따라 에이전트에게
보상을 주고 다음 상태를 부여합니다.

33
00:02:52,609 --> 00:03:00,918
이 과정은 에이전트가 종료상태(terminal state)가
될 때 까지 반복됩니다. 종료가 되면 한 에피소드가 끝나는 것이죠

34
00:03:00,918 --> 00:03:03,401
자 그럼 몇 가지 예시를 한번 살펴보겠습니다.

35
00:03:03,401 --> 00:03:05,536
우선 "cart-pole" 문제입니다.

36
00:03:05,536 --> 00:03:11,142
"cart-pole"는 아주 고전적인 문제입니다.
이미 CS229에서 접해보신 분들도 계실 것입니다.

37
00:03:11,142 --> 00:03:16,252
"cart-pole" 문제의 목표는 움직이는 카트(cart)와 카트 위에
매달려있는 막대기(pole)의 균형을 유지하는 것입니다.

38
00:03:16,252 --> 00:03:20,280
상태(state)에는 현재 시스템이 기술되어있습니다.

39
00:03:20,280 --> 00:03:28,206
가령, 막대기의 각,막대기의 각속도,
카트의 위치,카트의 수평속도가 있습니다.

40
00:03:28,206 --> 00:03:33,224
에이전트는 카트를 수평으로 미는 행동을 취할 수 있습니다.
(horizontal forces)

41
00:03:33,224 --> 00:03:38,387
우리는 카트를 밀면서 동시에
막대기의 균형을 잘 유지해야 합니다.

42
00:03:38,387 --> 00:03:43,990
환경으로 부터 받을 수 있는 보상은
"막대기가 제대로 서 있으면 1점" 입니다.

43
00:03:43,990 --> 00:03:48,143
여러분은 막대기를 가능한 똑바로 세워야 합니다.

44
00:03:49,286 --> 00:03:52,192
고전적인 RL문제에서 다뤘던 다양한 예시들을 살펴보겠습니다.

45
00:03:52,192 --> 00:03:53,998
"로봇 보행(robot locomotion)"  문제가 있습니다.

46
00:03:53,998 --> 00:03:59,670
여기 휴머노이드 로봇 모델과
개미 로봇 모델이 있습니다.

47
00:03:59,670 --> 00:04:03,128
로봇이 앞으로 나아가도록 하는 것이 목표입니다.

48
00:04:03,128 --> 00:04:10,807
이 문제에서 상태(state)는 로봇의
모든 관절들의 각과 위치입니다.

49
00:04:10,807 --> 00:04:15,887
에이전트가 취할 수 있는 행동(Action)은
각 관절들에 가해지는 토크(torques) 입니다.

50
00:04:15,887 --> 00:04:21,228
이 문제에서 하고싶은 것은 로봇이 앞으로 전진하는 것입니다.
앞으로 이동하면 보상을 받고

51
00:04:21,228 --> 00:04:31,701
휴머노이드 로봇의 경우에는 로봇에 똑바로 서 있는
경우에도 추가적인 보상을 받습니다.

52
00:04:33,521 --> 00:04:38,384
RL로 게임 문제도 풀 수 있습니다.

53
00:04:38,384 --> 00:04:40,700
가령 여기 아타리 게임(Atrari games)이 있습니다.

54
00:04:40,700 --> 00:04:44,280
깊은 강화학습(Deep reinforcement learning)가
아주 큰 성과를 이룬 종목이기도 합니다.

55
00:04:44,280 --> 00:04:48,574
아타리 게임에서는 가능한 가장 높은 점수로
게임을 끝마치는 것이 목적입니다.

56
00:04:48,574 --> 00:04:52,753
에이전트가 게임 플레이어가 되어
게임을 진행하게 됩니다.

57
00:04:52,753 --> 00:04:57,506
현재 게임 진행 화면 그대로
(raw pixels)가 상태로 주어집니다.

58
00:04:57,506 --> 00:05:02,882
우리가 게임을 할 때 보이는 화면
그 자체가 상태인 것이죠

59
00:05:02,882 --> 00:05:09,912
에이전트의 행동은, 우리가 게임 플레이할 때 처럼
위, 아래, 좌, 우로 움직일 수 있습니다.

60
00:05:09,912 --> 00:05:15,667
매 스텝마다 게임 점수를 획를할 수도, 잃을 수도 있죠

61
00:05:15,667 --> 00:05:27,572
게임 종료 시점까지 점수를 최대화 하는 것이 목표입니다.
그리고 마지막으로 살펴볼 게임은 바로 "바둑" 입니다.

62
00:05:27,573 --> 00:05:31,697
지난해 강화학습이 이룬 가장 큰 성과라고 할 수 있습니다.

63
00:05:31,697 --> 00:05:38,588
세계 최고의 바둑기사였던 이세돌을 딥마인드의
알파고가 꺽은 엄청난 사건이 있었습니다.

64
00:05:38,589 --> 00:05:50,918
그리고 알파고는 최근에 또 다른 바둑
최강자들을 상대로 대국을 준비하고 있습니다.

65
00:05:50,919 --> 00:05:56,295
바둑에서는 게임에서 이기는 것이 목표입니다.
바둑을 둘 수 있는 모든 자리가 상태가 됩니다.

66
00:05:56,295 --> 00:06:03,911
행동은 다음 수를 두는 것이며
게임을 이겼을 경우에만 보상이 주어집니다.

67
00:06:03,912 --> 00:06:08,411
이에 대해서는 잠시 후에
자세히 살펴보도록 하겠습니다.

68
00:06:08,411 --> 00:06:13,329
그렇다면 강화학습 문제를 어떤 식으로
수학적으로 나타내 볼 수 있을까요?

69
00:06:13,330 --> 00:06:18,051
앞서 보았던 것 처럼 환경은
에이전트에게 상태를 부여합니다.

70
00:06:18,051 --> 00:06:20,634
그러면 에이전트는 행동을 취합니다.

71
00:06:22,394 --> 00:06:28,512
Markov Decision Process를 통해서
강화학습 문제를 수식화 시킬 수 있습니다.

72
00:06:28,512 --> 00:06:31,447
MDP는 Markov property를 만족합니다.

73
00:06:31,447 --> 00:06:36,107
Markov property란 현재 상태만으로
전체 상태를 나타내는 성질입니다.

74
00:06:36,107 --> 00:06:40,164
그리고 MDP는 여기 보시는 것과 같이
몇 가지 속성으로 정의가 되는데

75
00:06:40,164 --> 00:06:43,170
S는 가능한 상태들의 집합입니다.

76
00:06:43,170 --> 00:06:45,762
A는 가능한 행동들의 집합입니다.

77
00:06:45,762 --> 00:06:51,694
R은 (state, action) 쌍이 주어졌을 때 받게되는
보상의 분포가 되겠습니다.

78
00:06:51,694 --> 00:06:55,323
따라서 R은 (state, action)이 보상으로 매핑되는 함수입니다.

79
00:06:55,323 --> 00:06:57,430
P는 전이확률(transition probability) 입니다.

80
00:06:57,430 --> 00:07:02,940
(state, action) 쌍이 주어졌을때 전이 될
다음 상대에 대한 분포를 나타냅니다.

81
00:07:02,940 --> 00:07:05,718
마지막으로 감마는  discount factor입니다.

82
00:07:05,718 --> 00:07:12,970
discount factor는 보상을 받는 시간에 대해서
우리가 얼마나 중요하게 생각할 것인지 말해줍니다.

83
00:07:14,203 --> 00:07:17,395
MDP가 작동하는 방식을 한번 살펴보겠습니다.

84
00:07:17,395 --> 00:07:20,053
우선 초기 time step인 t = 0입니다.

85
00:07:20,053 --> 00:07:26,362
환경은 초기 초기 상태 분포인
p(s_0)에서 상태 s_0 를 샘플링합니다.

86
00:07:26,363 --> 00:07:32,253
그리고 t=0 에서부터 완료 상태가 될 때 까지
아래를 반복합니다.

87
00:07:32,253 --> 00:07:35,797
에이전트가 행동 a_t를 선택합니다.

88
00:07:35,797 --> 00:07:38,885
환경은 어떤 분포로부터 보상을 샘플링합니다.

89
00:07:38,885 --> 00:07:44,032
보상은 우리의 상태와 우리가 택한 행동이
주어졌을때의 보상입니다.

90
00:07:44,032 --> 00:07:51,534
환경은 다음 어떤 분포에서 상태인 s_t+1도 샘플링합니다.

91
00:07:51,534 --> 00:07:58,706
그리고 에이전트는 보상과 다음 상태를 받습니다.
그리고 다음 단계를 수행하게 됩니다.

92
00:07:58,707 --> 00:08:05,542
에이전트는 에피소트가 종료될 때 까지 이런 식으로
보상과 다음 상태를 받는 과정을 되풀이합니다.

93
00:08:05,542 --> 00:08:06,989
좋습니다.

94
00:08:06,989 --> 00:08:10,724
자 이제는 정책(polity) pi를 정의할 수 있습니다.

95
00:08:10,724 --> 00:08:16,651
정책은 각 상태에서 에이전트가 어떤 행동을 취할지를
명시해주는 기능을 수행합니다.

96
00:08:16,651 --> 00:08:19,748
정책은 deterministic 할수도
stochastic 할수도 있습니다.

97
00:08:19,748 --> 00:08:27,204
이제 우리의 목적은 최적의 정책 pi^star를 찾는 것입니다. 즉,
cumulative discounted reward를 최대화시키는 것입니다.

98
00:08:27,205 --> 00:08:35,508
보상에는 미래에 얻을 보상도 포함이 되는데 이 보상은
discount factor에 의해 할인된 보상을 얻습니다.

99
00:08:35,509 --> 00:08:39,327
자 그럼 아주 간단한 MDP 예제를 살펴보겠습니다.

100
00:08:39,327 --> 00:08:44,533
여기 예시로 격자로 된 Grid World가 있습니다.
이 곳에서 테스크를 수행해 봅시다.

101
00:08:44,533 --> 00:08:50,295
여기 격자 중 어디로든 이동할 수 있습니다.
우리의 상태가 될 것입니다.

102
00:08:50,295 --> 00:08:52,613
우리는 상태에 따라서 행동을 취할 수 있습니다.

103
00:08:52,613 --> 00:08:59,298
햄동은 간단한 움직임이 될 것입니다.
상,하,좌,우로 움직이는 행동을 취할 수 있겠죠

104
00:08:59,299 --> 00:09:08,858
여러분은 한번 움직일 때 마다 음의 보상
(negative reward)를 받게 됩니다.

105
00:09:08,859 --> 00:09:11,989
가령 R = -1 이 될 수도 있겠죠

106
00:09:11,989 --> 00:09:20,054
여러분의 목표는 여기 회색으로 칠해진 "종료
상태" 에 최소한의 행동으로 도달하는 것입니다.

107
00:09:20,055 --> 00:09:27,624
종료 상태에 도달하는 시간이 길어질수록
음의 보상이 점점 쌓이게 될 것입니다.

108
00:09:27,625 --> 00:09:30,540
그럼 random policy부터 살펴보겠습니다.

109
00:09:30,540 --> 00:09:39,089
random policy 에서는 기본적으로 어떤
방향로 움직이든 무작위로 방향을 결정합니다.

110
00:09:39,090 --> 00:09:41,843
모든 방향이 동일한 확률을 갖습니다.

111
00:09:41,843 --> 00:09:46,518
하지만 우리가 일련의 학습을 거쳐서 얻게될
optimal policy의 경우에는

112
00:09:46,518 --> 00:09:51,866
우리가 점점 더 종료 상태에 가까워 지도록 만드는
적절한 방향을 선택해서 행동을 취하게 됩니다.

113
00:09:51,866 --> 00:09:59,170
가령 종료 상태 바로 주변에 위치하는 경우라면
모든 방향은 종료 상태로 바로 이동하게끔 합니다.

114
00:09:59,171 --> 00:10:09,118
종료 상태와 먼 곳에 있다고 하더라도, 종료 상태에
가장 가깝게 이동할 수 있는 방향으로 이동합니다.

115
00:10:09,119 --> 00:10:17,154
이렇게 MDP를 정의하고 나면 최적의
정책인 pi^star를 찾아야 합니다.

116
00:10:17,155 --> 00:10:20,755
최적의 정책은 보상의 합을 최대화시킵니다.

117
00:10:20,755 --> 00:10:29,730
최적의 정책은 우리가 어떤 상태에 있더라도 그 상황에서
보상을 최대화시킬 수 있는 행동을 알려줍니다.

118
00:10:29,731 --> 00:10:34,091
자 그럼 질문은 "MDP에서 발생하는 무작위성(randomness)
을 어떻게 다뤄야 할까요?"

119
00:10:34,091 --> 00:10:39,073
가령 초기 상태를 샘플링할 시에도 무작위성이 있고

120
00:10:39,073 --> 00:10:46,340
전이 확률 분포의 경우에도 다음 상태가 확률적입니다.

121
00:10:46,341 --> 00:10:51,947
이를 위해서는 보상의 합에 대한 기댓값을 최대화시키면 됩니다.

122
00:10:51,947 --> 00:11:02,956
수식적으로 써보면, 최적의 정책 pi^star는 정책 pi에
대한 미래의 보상들의 합의 기댓값을 최대화시키는 것입니다.

123
00:11:02,957 --> 00:11:05,103
여기에서 초기 상태(s_0)는 어떤 상태 분포를 따르며

124
00:11:05,103 --> 00:11:09,127
우리가 취하는 행동은 어떤 상태가 주어졌을 때
정책이 가지는 분포로부터 샘플링됩니다.

125
00:11:09,127 --> 00:11:16,423
마지막으로 다음 상태는 전이 확률
분포로부터 샘플링됩니다.

126
00:11:16,423 --> 00:11:22,142
자 그럼 최적의 정책을 찾는 방법을 다루기 이전에

127
00:11:22,143 --> 00:11:26,787
앞으로 사용하게 될 몇 가지 정의부터 살펴보겠습니다.

128
00:11:26,787 --> 00:11:31,405
가치 함수(value function)와
Q-가치 함수(Q-value function)입니다.

129
00:11:31,405 --> 00:11:37,425
우리가 정책을 따라 무언가를 수행하게 되면 결국은
모든 에피소드마다 어떤 "경로" 를 얻게 될 것입니다.

130
00:11:37,426 --> 00:11:43,611
초기 상태인 s_0, a_0, r_0 부터 시작해서
s_1, a_1, r_1 이런식으로 쭉쭉 나가겠죠

131
00:11:43,611 --> 00:11:49,331
그렇게 되면 우리가 얻을 수 있는 상태(s), 행동(a), 보상(r)
들의 하나의 경로(trajectory)가 생깁니다.

132
00:11:49,331 --> 00:11:52,613
그렇다면 우리가 현재 속해있는
상태가 얼마나 좋은 상태일까요?

133
00:11:52,613 --> 00:12:10,799
임의의 상태 s에 대한 가치함수는 상태 s와 정책
pi가 주어졌을 때 누적 보상의 기댓값 입니다.

134
00:12:10,800 --> 00:12:13,286
그렇다면 (상태, 행동) 쌍이 얼마나 좋은지는
어떻게 알 수 있을까요?

135
00:12:13,286 --> 00:12:17,370
상태 s에서 어떤 행동 a를 취하는게 좋은 것일까요?

136
00:12:17,370 --> 00:12:20,468
우리는 이를 Q-가치 함수를 통해서 정의할 수 있습니다.

137
00:12:20,468 --> 00:12:27,741
Q-가치 함수는 정책 pi, 행동 a, 상태 s가 주어졌을 때
받을 수 있는 누적 보상의 기댓값입니다.

138
00:12:29,708 --> 00:12:45,098
최적의 Q-가치 함수인 Q^star는 (상태, 행동)
쌍으로부터 얻을 수 있는 누적 보상의 기댓값을 최대화시킵니다.

139
00:12:45,099 --> 00:12:52,017
다음으로는 강화학습에서 중요한 요소 중 하나인 벨만
방정식(Bellman equation)에 대해서 알아보겠습니다.

140
00:12:52,018 --> 00:12:57,697
우선 최적의 정책으로부터 나온 Q-가치 함수인
Q^star가 있다고 가정해봅시다.

141
00:12:57,697 --> 00:13:00,911
Q^star는 벨만 방정식을 만족할 것입니다.

142
00:13:00,911 --> 00:13:05,194
이것이 의미하는 바는

143
00:13:05,194 --> 00:13:11,748
어떤 (s, a)이 주어지던 간에

144
00:13:11,748 --> 00:13:18,867
현재 (s, a)에서 받을 수 있는 r 과 에피소드가
종료될 s^prime까지의 보상을 더한 값입니다.

145
00:13:18,868 --> 00:13:24,150
여기에서는 우리가 이미 최적의 정책을 알고 있기 때문에

146
00:13:24,150 --> 00:13:28,746
따라서 s^prime에서 우리가 할 수 있는
최상의 행동을 취할 수가 있습니다.

147
00:13:28,746 --> 00:13:34,432
s^prime에서의 Q^star의 값은
우리가 현재 상태에서 취할 수 있는 모든 행동들 중에

148
00:13:34,432 --> 00:13:38,626
Q^star(s^prime, a^prime) 을 최대화시키는 값이 됩니다.

149
00:13:38,626 --> 00:13:44,119
이를 통해서 최적의 Q 값을 얻을 수 있습니다.

150
00:13:44,119 --> 00:13:54,251
그리고 우리가 어떤 상태인지에 대한 무작위성이
존재하기 때문에 여기에도 기댓값을 취합니다.

151
00:13:54,252 --> 00:14:02,487
우리는 또한 Q^star를 통해서 특정 상태에서의 최상의
행동을 취할 수 있는 최적의 정책을 구할 수 있습니다.

152
00:14:02,488 --> 00:14:08,436
Q^star는 어떤 행동을 취했을때
미래에 받을 보상의 최대치입니다.

153
00:14:08,437 --> 00:14:16,862
따라서 우리는 그저 정책을 따라 행동을
취하기만 하면 최상의 보상을 받을 수 있습니다.

154
00:14:16,863 --> 00:14:21,025
그렇다면 어떻게 최적의 정책을 구할 수 있을까요?

155
00:14:21,025 --> 00:14:25,692
우리가 이 문제를 해결할 수 있는 한 가지 방법은 바로
value iteration algorithm 입니다.

156
00:14:25,692 --> 00:14:29,527
반복적인 업데이트를 위해서 벨만 방정식을 사용할 것입니다.

157
00:14:29,527 --> 00:14:37,997
Bellman equation을 통해 각 스텝마다 Q_star를
조금씩 최적화시키면 됩니다.

158
00:14:39,347 --> 00:14:50,830
수학적으로 보면 Q_i의 i가 무한대일때
최적의 Q_star로 수렴하게 됩니다.

159
00:14:54,257 --> 00:14:58,329
이 방법이 잘 동작하긴 할 겁니다.
하지만 문제가 있습니다. 무엇일까요?

160
00:14:59,184 --> 00:15:02,720
문제는 바로 이 방법은
scalable 않다는 점입니다.

161
00:15:02,720 --> 00:15:08,596
반복적으로 업데이트하기 위해서는 모든 (상태, 행동)
마다 Q(s)를 계산해야만 합니다.

162
00:15:08,597 --> 00:15:18,932
가령 Atati 게임의 경우 스크린에
보이는 모든 픽셀이 상태가 될 수 있습니다.

163
00:15:18,933 --> 00:15:28,724
이 경우 상태 공간이 아주 크며, 기본적으로
전체 상태 공간을 계산하는 것은 불가능합니다.

164
00:15:28,725 --> 00:15:35,907
해결책은 무엇일까요? 우선, 함수 Q(s, a)
를 근사시킬 수 있습니다.

165
00:15:35,908 --> 00:15:37,620
예를 들어 neural network를 사용할 수 있겠죠

166
00:15:37,620 --> 00:15:48,471
언제나 그러 했듯이, 우리가 모르는 아주 복잡한 함수를
추정하고 싶을때는 neural network가 제격입니다.

167
00:15:48,472 --> 00:15:54,242
그러면 이제 Q-learning 수식을 살펴보겠습니다.

168
00:15:54,242 --> 00:16:02,950
여기에서는 행동 가치 함수(action value
function)을 추정하기 위한 함수 근사를 이용할 것입니다.

169
00:16:02,951 --> 00:16:08,141
함수 근사로는 최근 가장 많이 사용되는
deep neural network를 이용할 것입니다.

170
00:16:08,142 --> 00:16:10,782
이를 deep Q-learning 이라고 합니다.

171
00:16:10,782 --> 00:16:20,149
Deep Q-learning은 요즘 강화학습
하면 빠지지 않고 등장하는 방법입니다.

172
00:16:20,150 --> 00:16:30,765
여기에서는 함수 파라미터 theta가 있습니다.
theta는 neural network의 가중치입니다.

173
00:16:33,050 --> 00:16:37,970
자 그럼 함수 근사를 이용해서
어떻 방식으로 최적의 정책을 찾아낼 수 있을까요?

174
00:16:37,970 --> 00:16:44,744
Q-function은 벨만 방정식을 만족해야 한다는 점을 명심해야 합니다.

175
00:16:44,744 --> 00:16:54,712
자 이제 우리 함수가 벨만 방정식을 만족하도록 해야 합니다. 해야할
일은 neural network로 근사시킨 Q-function을

176
00:16:54,713 --> 00:17:00,239
학습시켜서 벨만 방정식의 에러가
최소가 되도록 하면 됩니다.

177
00:17:00,240 --> 00:17:03,689
손실 함수는 q(s, a)가 목적 함수와 얼마나 멀리
떨어져있는지 측정합니다.

178
00:17:03,689 --> 00:17:09,853
여기 보이는 목적함수 y_i가 바로 앞서 살펴본
벨만 방적식이 되겠습니다.

179
00:17:09,853 --> 00:17:16,928
forward pass에서는 손실 함수를 계산합니다.
손실이 최소가 되면 좋겠죠

180
00:17:16,929 --> 00:17:28,182
backward pass에서는 계산한 손실을
기반으로 파라미터 theta를 업데이트합니다.

181
00:17:28,183 --> 00:17:38,435
이런 식의 반복적인 업데이트를 총해서 우리가 가진
Q-function이 타켓과 가까워지도록 학습시킵니다.

182
00:17:38,436 --> 00:17:43,524
질문 있나요?

183
00:17:44,537 --> 00:17:53,369
Deep Q-learning 방법이
적용된 고전적인 예제를 살펴보겠습니다.

184
00:17:53,370 --> 00:18:01,745
앞서 살펴본 Atrari game 입니다. 게임의
목적은 최대한 높은 점수를 획득하는 것입니다.

185
00:18:01,746 --> 00:18:05,460
상태는 게임의 픽셀이 그대로 사용되었습니다.

186
00:18:05,460 --> 00:18:12,963
행동은 상, 하, 좌, 우 와 같이
게임에 필요한 어떤 행동이 될 것입니다.

187
00:18:12,964 --> 00:18:21,182
게임이 진행됨에 따라 매 타임 스템마다 점수가
늘어나거나 줄어듦에 따라 보상을 얻습니다.

188
00:18:21,183 --> 00:18:27,095
스코어를 기반으로 전체 누적 보상을 계산할 수 있습니다.

189
00:18:27,095 --> 00:18:32,955
Q-function에 사용한 네트워크는 다음과 같이 생겼습니다.

190
00:18:32,955 --> 00:18:37,355
Q-network는 가중치인 theta를 가지고 있습니다.

191
00:18:37,355 --> 00:18:43,791
네트워크의 입력은 상태 s 입니다.
즉 현제 게임 스크린의 픽셀들이 들어옵니다.

192
00:18:43,791 --> 00:18:49,509
실제로는 4프레임 정도 누적시켜서 사용합니다.

193
00:18:49,509 --> 00:18:58,608
입력은 RGB->grayscale변환, 다운
샘플링, 크라핑 등 전처리 과정을 거칩니다.

194
00:18:58,609 --> 00:19:04,631
전처리를 거친 입력은 최근 4 프레임을 누적시킨
84 x 84 x 4 의 형태가 됩니다.

195
00:19:04,631 --> 00:19:05,464
질문 있나요?

196
00:19:05,464 --> 00:19:09,631
[학생이 질문]

197
00:19:12,792 --> 00:19:20,808
질문은 "네트워크가 다양한 (상태, 행동) 쌍들을 구하기
위해서 Q-가치 함수를 근사시키는 것인지" 입니다.

198
00:19:20,809 --> 00:19:24,765
(가령 이 예시의 경우 4개의 (상태, 행동)쌍을 출력)
네 맞습니다.

199
00:19:24,765 --> 00:19:27,551
이에 대해서는 다음 슬라이드에서
조금 더 자세히 설명할 것입니다.

200
00:19:27,551 --> 00:19:29,935
[학생이 질문]

201
00:19:29,935 --> 00:19:36,815
여기에서는 Softmax를 사용하지는 않습니다. Q-value
functions을 직접 예측하는 것이 목적입니다.

202
00:19:36,816 --> 00:19:40,582
[학생이 질문]

203
00:19:40,583 --> 00:19:44,014
Q-values를 regression 한다고 보시면 됩니다.

204
00:19:44,014 --> 00:19:54,083
입력 다음으로 살펴볼 것은 네트워크 구조인데, 우리에게는 아주
익숙한 구조입니다. 컨볼루션, FC 레이어들로 구성됩니다.

205
00:19:54,084 --> 00:19:59,610
8 x 8 / 4 x 4 컨볼루션이 있고

206
00:19:59,611 --> 00:20:05,673
그리고 FC 256 레이어를 거칩니다. 이제는
우리에게 아주 익숙한 네트워크 구조입니다.

207
00:20:05,674 --> 00:20:17,414
FC 레이어의 출력 벡터는 네트워크의 입력인, "상태"
가 주어졌을 때 각 행동의 Q-value 입니다.

208
00:20:17,415 --> 00:20:21,770
가령 네 가지 행동이 존재한다면
출력도 4차원입니다.

209
00:20:21,770 --> 00:20:28,685
이는 현재 상태 s_t와 여기에 존재하는 행동들
a_1, a_2, a_3, a_4에 관한 Q 값들입니다.

210
00:20:28,685 --> 00:20:33,179
각 행동들 마다 하나의 스칼라 값을 얻습니다.

211
00:20:33,179 --> 00:20:43,072
참고로 행동의 수는 Atari game의 종류에
따라서 4~18가지로 변할 수도 있습니다.

212
00:20:43,073 --> 00:20:46,709
이런 방식의 네트워크 구조의 장점은

213
00:20:46,709 --> 00:20:54,650
한 번의 forward pass 만으로 현재 상태에 해당하는
모든 함수에 대한 Q-value를 계산할 수 있다는 점입니다.

214
00:20:54,651 --> 00:20:56,117
이는 정말 효율적인 방법입니다.

215
00:20:56,117 --> 00:21:05,945
현재 상태의 각 행동들에 각각 Q-value가 존재할텐데
현재 상태를 네트워크의 입력으로 넣어주기만 하면

216
00:21:05,946 --> 00:21:10,259
모든 Q-value를 한번의 forward pass로 계산할 수 있습니다.

217
00:21:10,259 --> 00:21:15,078
이 네트워크를 학습시키 위해서는
앞서 살펴본 손실 함수가 필요합니다.

218
00:21:15,078 --> 00:21:17,661
네트워크로 근사시킨 함수도
벨만 방정식을 만족해야 합니다.

219
00:21:17,661 --> 00:21:29,314
따라서 네트워크의 출력인 Q-value가 타겟
값과 가까워 지도록 반복적으로 학습시켜야 합니다.

220
00:21:29,315 --> 00:21:40,705
backward pass에서는 손실함수를 기반으로
그레디언트를 계산하고 이를 바탕으로 네트워크를 학습시킵니다.

221
00:21:40,706 --> 00:21:45,639
한 가지 알아야 할 개념이 있습니다.
"experience replay" 라는 것입니다.

222
00:21:45,639 --> 00:21:53,440
Experience Replay는 Q-network에서
발생할 수 있는 문제들을 다룹니다.

223
00:21:53,440 --> 00:21:58,134
Q-network를 학습시킬 때 하나의 배치에서
시간적으로 연속적인 샘플들로 학습하면 안좋습니다.

224
00:21:58,134 --> 00:22:09,409
가령 게임을 진행하면서 시간적으로 연속적인 샘플들을
이용해서 상태, 행동, 보상을 계산하고 학습시키면

225
00:22:09,410 --> 00:22:16,117
모든 샘플들이 상관 관계를 가집니다(correlated). 이는
학습에 아주 비효율적입니다. 이 문제가 첫 번째 문제입니다.

226
00:22:16,118 --> 00:22:24,841
그리고 두번째로는, 현재 Q-network 파라미터를 생각해보면
네트워크는 우리가 어떤 행동을 해야할지에 정책을 결정한다는 것은

227
00:22:24,842 --> 00:22:27,394
우리가 다음 샘플들도 결정하게 된다는 의미입니다.

228
00:22:27,394 --> 00:22:30,832
이는 학습에 좋지 않은 영향을 미칠 것입니다.
(bad feedback loops)

229
00:22:30,832 --> 00:22:35,468
가령 현재 상태에서 "왼쪽" 으로 이동하는 것이
보상을 최대화하는 행동이라면

230
00:22:35,468 --> 00:22:43,305
결국 다음 샘플들도 전부 "왼쪽"에서
발생할 수 있는 것들로만 편향될 것입니다.

231
00:22:43,306 --> 00:22:45,406
이는 문제가 될 수 있습니다.

232
00:22:45,406 --> 00:22:53,097
이를 해결하는 방법으로 "experience
replay" 라는 방법을 사용합니다.

233
00:22:53,098 --> 00:23:01,352
이 방법은 replay memory를 이용합니다. replay memory에는
(상태, 행동, 보상. 다음상태)로 구성된 전이 테이블이 있습니다.

234
00:23:01,353 --> 00:23:08,772
게임 에피소드를 플레이하면서 더 많은 경험을 얻음에
따라 전이 데이블을 지속적으로 업데이트시킵니다.

235
00:23:08,773 --> 00:23:16,334
여기에서는 replay memory에서의 임의의 미니
배치를 이용하여 Q-network를 학습시킵니다.

236
00:23:16,335 --> 00:23:24,826
연속적인 샘플을 사용하는 대신 전이 테이블에서
임의로 샘플링된 샘플을 사용하는 것이죠.

237
00:23:24,827 --> 00:23:31,007
이 방식을 통해서 앞서 상관관계(correlation)로
발생하는 문제를 해결할 수 있을 것입니다.

238
00:23:31,007 --> 00:23:39,206
추가적인 이점으로는 각각의 전이가 가중치 업데이트에 여러
차례(딱 한번이 아니라) 기여할 수 있다는 점입니다.

239
00:23:39,207 --> 00:23:43,652
전이 테이블로부터 샘플링을 하면
하나의 샘플도 여러번  뽑힐 수 있는 것이죠

240
00:23:43,652 --> 00:23:47,585
이를 통해 데이터 효율이 훨씬 더 증가하게 됩니다.

241
00:23:50,580 --> 00:23:59,165
자 그럼 종합해서, experience replay를
이용한 Q-learning 전체 알고리즘을 살펴보겠습니다.

242
00:23:59,166 --> 00:24:03,940
우선  replay memory을 초기화하는 것부터 시작합니다.

243
00:24:03,940 --> 00:24:14,829
우선 memory capacity인 N을 정해줍니다.
그리고 Q-network를 임의의 가중치로 초기화시킵니다.

244
00:24:14,830 --> 00:24:21,832
그리고 앞으로 M번의 에피소드를 진행할 것입니다.
따라서 학습은 총 M번의 에피소드까지 진행됩니다.

245
00:24:21,832 --> 00:24:31,264
그리고 각 에피소드마다 상태를 초기화시켜야 합니다.
상태는 게임 시작화면 픽셀이 될 것입니다.

246
00:24:31,265 --> 00:24:37,814
앞서, 입력 상태를 만들기 위한
전처리 과정은 앞서 언급했었죠

247
00:24:37,814 --> 00:24:41,584
다음은, 게임이 진행중인
매 타음 스탭마다

248
00:24:41,584 --> 00:24:46,268
적은 확률로는 임의의 행동을 취할 것입니다.
(정책을 따르지 않고)

249
00:24:46,268 --> 00:24:53,141
이는 충분한 탐사(exploration)을 위해서는
아주 중요한 부분입니다.

250
00:24:53,141 --> 00:24:58,559
이를 통해 다양한 상태 공간을 샘플링할 수 있습니다.

251
00:24:58,559 --> 00:25:03,613
낮은 확률로 임의의 행동을 취하거나, 또는(otherwise)
현재 정책에 따라 greedy action을 취합니다.

252
00:25:03,614 --> 00:25:13,579
다시 말해 대부분의 경우에는 현재 상태에 적합하다고 판단되는
greedy action을 선택하게 될 것이고, 가끔식은

253
00:25:13,580 --> 00:25:16,300
낮은 확률로 임의의 행동이 선택될 것입니다.

254
00:25:16,300 --> 00:25:26,069
이렇게 행동 a_t를 취하면 보상(r_t)과
다음 상태(s_t+1)를 얻을 수 있습니다.

255
00:25:26,070 --> 00:25:32,771
그러고 나서 전이(transition)을
replay memory에 저장합니다.

256
00:25:32,771 --> 00:25:35,577
자 이제 네트워크를 학습시킬 차례입니다.

257
00:25:35,577 --> 00:25:37,550
experience replay를 이용할 시점입니다.

258
00:25:37,550 --> 00:25:47,213
replay memory에서 임의의 미니배치 전이들(transitions)을
샘플링한 다음에 이를 이용하여 업데이트합니다.

259
00:25:47,214 --> 00:25:49,635
이것이 전체 학습의 반복과정입니다.

260
00:25:49,635 --> 00:26:03,886
게임을 진행하는 동안에 experienced replay을 이용하여
미니배치를 샘플링하고, 이를 통해 Q-network를 학습시킵니다.

261
00:26:03,887 --> 00:26:20,910
Google DeepMind에서 Atari 게임 Breakout을
Q-learning으로 학습시키는 영상을 살펴보겠습니다.

262
00:26:20,911 --> 00:26:26,185
게임화면의 픽셀 그대로가 상태입니다.

263
00:26:26,185 --> 00:26:29,520
학습 초반에 어떤 일이 발생하는지 살펴봅시다.

264
00:26:29,520 --> 00:26:40,302
학습 초반에는, 공을 치긴 하지만 그렇게 잘해보이지는 않습니다.
금방 놓치고 맙니다.

265
00:26:40,303 --> 00:26:42,886
그래도 계속 공을 찾아다니고 있습니다.

266
00:26:50,969 --> 00:26:55,737
그리고 몇 시간정도 더 학습한 결과를 살펴봅시다.

267
00:27:00,946 --> 00:27:05,113
지금은 어느정도 잘 하는 것 같습니다.

268
00:27:06,190 --> 00:27:16,592
공도 잘 따라가고 있고 블럭도 거의 다 깨는 모습입니다.

269
00:27:16,593 --> 00:27:36,203
240분이 지나면 이제는 거의
프로에 가까운 묘기를 선보입니다.

270
00:27:36,203 --> 00:27:42,795
공을 위로 올려서 알아서 블럭이 깨지게 하는 전략이죠

271
00:27:42,796 --> 00:27:49,500
지금 보여드린 예시에서는 Agent가 deep Q-learning을
통해 Atari 게임을 학습하고 플레이하였습니다.

272
00:27:49,501 --> 00:27:56,418
온라인을 통해서 더 많은 Atrari
게임에서의 예시를 확인해보실 수 있습니다.

273
00:27:56,419 --> 00:28:01,149
지금까지 Q-learning을 알아봤습니다.
하지만 Q-learning에는 문제가 있었습니다.

274
00:28:01,149 --> 00:28:07,225
아주 어려운 문제였는데요, 그 문제는 바로
Q-function이 너무나도 복잡하다는 것입니다.

275
00:28:07,226 --> 00:28:12,335
Q-learning 에서는 모든 (state, action) 쌍들을
학습해야만 합니다.

276
00:28:12,335 --> 00:28:17,275
그런데 가령, 로봇이 어떤 문체를 손에 쥐는
문제를 풀어야 한다고 생각해봅시다.

277
00:28:17,275 --> 00:28:19,576
이 경우 아주 고차원의 상태 공간이 존재합니다.

278
00:28:19,576 --> 00:28:26,225
가령 로봇의 모든 관절의 위치와 각도가 이룰 수 있는
모든 경우의 수를 생각해볼 수 있겠죠

279
00:28:26,225 --> 00:28:35,492
이 모든 (state, action)을
학습시키는것은 아주 어려운 문제입니다.

280
00:28:35,493 --> 00:28:38,724
다른 한편으로는 "정책" 자체는 훨씬 더 간단할 수 있습니다.

281
00:28:38,724 --> 00:28:44,555
정책은 간단하게 "손을 움켜쥐는 것" 같이 말이죠

282
00:28:44,556 --> 00:28:48,252
손가락을 특정 방향으로 움직이기만 하면 됩니다.

283
00:28:48,252 --> 00:28:54,142
그렇다면 정책 자체를 학습시킬 수 있을까요?

284
00:28:54,142 --> 00:28:58,306
만약 가능하다면 여러 정책들 가운데
최고의 정책을 찾아낼 수 있을 것입니다.

285
00:28:58,306 --> 00:29:06,789
정책을 결정하기에 앞서 Q-value를
추정하는 과정을 거치지 않고도 말이죠

286
00:29:06,790 --> 00:29:15,937
이러한 접근 방식이
"policy gradients" 입니다.

287
00:29:15,938 --> 00:29:20,858
자 그럼 수식적으로 매개변수화된 정책 집합
(class of parametrized policies)을 정의해봅시다.

288
00:29:20,858 --> 00:29:24,146
정책들은 가중치 theta에 의해서 매개변수화됩니다.

289
00:29:24,146 --> 00:29:27,791
자 그럼 각 정책에 대해서
정책의 값을 정의해봅시다.

290
00:29:27,791 --> 00:29:35,722
J(theta)는 미래에 받을 보상들의
누적 합의 기댓값으로 나타낼 수 있습니다.

291
00:29:35,723 --> 00:29:38,971
이는 우리가 지금까지 사용했던
보상과 동일합니다.

292
00:29:38,971 --> 00:29:41,879
이런 상황(setup)에서 우리가 하고싶은 것은

293
00:29:41,879 --> 00:29:51,547
최적의 정책인 theta^star를 찾는 것인데 이는
argmax_theta J(theta)로 나타낼 수 있습니다.

294
00:29:51,548 --> 00:29:56,917
이제는 보상의 기댓값을 최대로 하는 정책
파라미터를 찾으면 됩니다.

295
00:29:56,917 --> 00:30:01,011
이 문제를 어떻게 풀면 될까요?

296
00:30:04,993 --> 00:30:10,155
policy parameter에 대해서
gradient ascent를 수행하면 될 것입니다.

297
00:30:10,155 --> 00:30:15,460
지금까지 배웠던 것 처럼 하면 되겠죠
gradient ascent를 통해서

298
00:30:15,460 --> 00:30:20,762
parameters를 연속적으로 업데이트해주면 됩니다.

299
00:30:23,202 --> 00:30:29,196
자 그럼 조금만 더 구체적으로 살펴보겠습니다.
REINFORCE 알고리즘을 한번 알아봅시다.

300
00:30:29,196 --> 00:30:36,781
수학적으로 보면, 경로에 대한 미래 보상의
기댓값으로 나타낼 수 있습니다.

301
00:30:36,781 --> 00:30:41,902
이를 위해서 경로를 샘플링해야 하는데, 가령 이는 앞선 예제에서
게임 플레이 시의 에피소드일 수 있습니다.

302
00:30:41,902 --> 00:30:47,411
s_0, a_0, r_0, s_1 등등 이 되겠죠

303
00:30:47,411 --> 00:30:51,723
이들은 어떤 정책 pi_theta을 따라 결정될 것입니다.

304
00:30:51,723 --> 00:30:57,739
그러면 각 경로에 대해서 보상을 계산할 수 있을 것입니다.

305
00:30:57,739 --> 00:31:01,245
이 보상은 우리가 어떤 경로를 따라 얻을
수 있는 누적 보상이 될 것입니다.

306
00:31:01,245 --> 00:31:10,570
따라서 정책인 pi_theta의 값은 샘플링 된 경로로 부터
받게 될 보상의 기댓값이 될 것입니다.

307
00:31:10,570 --> 00:31:16,868
따라서 여기에 이 기댓값은 우리가 정책으로 부터
샘플링한 경로들에 대한 기댓값입니다.

308
00:31:18,563 --> 00:31:21,288
자 그럼 gradient ascent를 수행해야 합니다.

309
00:31:21,288 --> 00:31:22,961
J(theta)를 미분해봅시다.

310
00:31:22,961 --> 00:31:27,356
미분을 하고나서 이 값들을
gradient step으로 취하면 될 것입니다.

311
00:31:28,535 --> 00:31:34,300
하지만 여기에도 문제가 있습니다. 미분을 잘 하긴 했는데
이 값을 계산할 수가 없습니다(intractable).

312
00:31:34,300 --> 00:31:41,319
p가 theta에 종속되어 있는 상황에서
기댓값 안에 그레디언트가 있으면 문제가 될 수 있습니다.

313
00:31:41,319 --> 00:31:47,661
여기에서 보면 p(tau ; thera)에 대한
그레디언트를 구해야하는데

314
00:31:47,661 --> 00:31:53,033
tau에 대한 적분을 수행해야 하기 때문에
계산하기기 어렵습니다.

315
00:31:53,033 --> 00:31:57,327
하지만 이를 해결하기 위한 트릭이 있습니다.

316
00:31:57,327 --> 00:32:04,941
이 트릭은 p를 가지고 하는 것인데
어짜피 1을 곱하면 상관없으므로

317
00:32:04,941 --> 00:32:10,286
위 아래에 p(tau ; theta)를 곱해줍니다.

318
00:32:10,286 --> 00:32:26,170
자 이제 아래 쪽 두 번째, 세 번째 수식을 보시면
이 둘은 동치(equvalent)입니다.

319
00:32:26,170 --> 00:32:32,741
왜냐하면 log(p)의 그레디언트는
(1 / p) * p의 그레디언트 이기 때문입니다.

320
00:32:33,808 --> 00:32:41,385
자 그럼 이 수식을 앞에 있던
그레디언트 식에 대입해 봅시다.

321
00:32:41,385 --> 00:32:43,426
자 이제 실제로 어떻게 보이는지를 살펴봅시다.

322
00:32:43,426 --> 00:32:52,187
이제는 log(p)에 대한 그레디언트를 모든 경로에 대한
확률과 곱하는 꼴이 되고, 이를 tau에 대해서 적분하는 꼴이 되기 때문에

323
00:32:52,187 --> 00:32:58,586
이는 다시 경로 tau에 대한 기댓값의 형식으로 바꿀 수 있습니다.

324
00:32:58,586 --> 00:33:02,751
처음에는 기댓값에 대한 그레디언트를 계산했지만

325
00:33:02,751 --> 00:33:06,823
이제는 그레디언트에 대한 기댓값으로 바꾼 셈입니다.

326
00:33:06,823 --> 00:33:13,817
이로 인해, 그레디언트를 추정하기 위해서
경로들 샘플링하여 사용할 수 있게 되었습니다.

327
00:33:14,712 --> 00:33:21,260
따라서 이제는 Monte Carlo 샘플링을 사용할 수 있습니다.
이것이 바로 REINFORCE의 주요 아이디어입니다.

328
00:33:23,624 --> 00:33:28,180
자 그럼 우리가 계산하려고 하는 식을 살펴보겠습니다.

329
00:33:28,180 --> 00:33:33,071
그렇다면 우리는 p(tau ; theta)를
전이확률을 모르는 채로 계산할 수 있을까요?

330
00:33:33,071 --> 00:33:38,466
자 우선  p(tau)는 어떤 경로에 대한 확률이 될 것입니다.

331
00:33:38,466 --> 00:33:45,821
이는 현재 (상태, 행동)이 주어졌을 때, 다음에 얻게될
모든 상태에 대해서 "전이확률"과

332
00:33:45,821 --> 00:33:52,232
"정책 pi로 부터 얻은 행동에 대한 확률"의 곱의 형태로 이루어집니다.

333
00:33:52,232 --> 00:33:58,441
따라서 이를 모두 곱하면 경로에 대한
확률을 얻어낼 수 있을 것입니다.

334
00:33:58,441 --> 00:34:10,389
그렇다면 log(p ; tau)의 경우를 보면 앞서 곱했던 것들이
모두 합의 형태로 바뀌게 될 것입 니다.

335
00:34:10,389 --> 00:34:18,161
자 그럼 log(p)를 theta에 대해서 미분하는 경우를 생각해 봅시다.

336
00:34:18,163 --> 00:34:22,850
여기 보면 첫 번째 항인 전이확률은
theta와 무관한 항입니다.

337
00:34:22,850 --> 00:34:28,709
theta와 관련이 있는 항은 오직 두번째 항인
log pi_theta(a_t | s_t) 항 뿐입니다.

338
00:34:29,675 --> 00:34:39,669
우리는 이를 통해서 전이확률은 전혀 상관하지 않아도
된다는 사실을 알 수 있습니다.

339
00:34:39,670 --> 00:34:46,421
다시 말해, 그레디언트를 계산할 때
전이확률은 필요하지 않습니다.

340
00:34:47,257 --> 00:34:57,264
따라서 우리는 어떤 경로  tau에 대해서도
그레디언트를 기반으로 J(thera)을 추정해 낼 수 있을 것입니다.

341
00:34:58,524 --> 00:35:02,220
지금까지는 하나의 경로만 고려했었지만

342
00:35:02,220 --> 00:35:07,188
기댓값을 계산하기 위해서는 여러 개의 경로를
샘플링할 수도 있을 것입니다.

343
00:35:09,248 --> 00:35:17,141
그레디언트 계산을 하기 위해서 지금까지 
유도해본 것들을 한번 해석해 봅시다.

344
00:35:18,217 --> 00:35:25,226
어떤 경로로부터 얻은 보상이 크다면, 즉 
어떤 일련의 행동들이 잘 수행한 것이라면

345
00:35:25,226 --> 00:35:29,434
그 일련들 행동들을 할 확률을 높혀줍니다.

346
00:35:29,434 --> 00:35:33,141
그 행동들이 좋았다는 것을 말해주는 것입니다.

347
00:35:33,141 --> 00:35:37,186
반면 어떤 경로에 대한 보상이 낮다면
해당 확률을 낮춥니다.

348
00:35:37,186 --> 00:35:40,747
그러한 행동들은 좋지 못한 행동이었으며 따라서
그 경로가 샘플링되지 못하도록 해야 합니다.

349
00:35:40,747 --> 00:35:50,980
여기 수식을 보시면 pi(a given s)는
우리가 취한 행동들에 대한 우도(likelihood) 입니다.

350
00:35:50,980 --> 00:36:03,575
파라미터를 조정하기 위해서 그레디언트를 사용할 것이고, 
그레디언트는 우도를 높히려면 어떻게 해야 하는지를 알려줍니다.

351
00:36:03,575 --> 00:36:12,602
즉 우리가 받게될 보상, 즉 행동들이 얼마나 좋았는지에 대한 
그레디언트를 통해 파라미터를 조정하는 것입니다.

352
00:36:14,561 --> 00:36:23,798
단순하게 말해서, 어떤 경로가 좋았으면 
경로에 포함되었던 모든 행동이 좋았다는 것을 의미합니다.

353
00:36:23,798 --> 00:36:26,356
하지만 기댓값이 의해서 이 모든 것들이
averages out 됩니다.

354
00:36:26,356 --> 00:36:30,125
average out을 통해 
unbiased extimator를 얻을 수 있고

355
00:36:30,125 --> 00:36:35,622
충분히 많은 샘플링을 한다면 그레디언트를 잘 이용해서
정확하고 좋은 extimater를 얻을 수 있을 것입니다.

356
00:36:35,622 --> 00:36:42,994
이 방법이 좋은 이유는 그레디언트만 잘 계산한다면
손실 함수를 작게 만들 수 있고

357
00:36:42,994 --> 00:36:48,602
정책 파라미터 theta에 대한 (적어도) local optimum
을 구할 수 있기 때문엡니다.

358
00:36:48,602 --> 00:36:54,884
하지만 여기에도 문제가 존재합니다. 
문제의 원인은 바로 바로 높은 분산에 있습니다.

359
00:36:54,884 --> 00:36:57,201
왜냐하면 신뢰할당문제(credit assignment)가
아주 어렵기 때문입니다.

360
00:36:57,201 --> 00:37:04,412
일단 보상을 받았으면  해당 경로의 모든
행동들이 좋았다 라는 정보만 알려줄 것입니다.

361
00:37:04,412 --> 00:37:11,080
하지만 우리는 구체적으로 어떤 행동이 최선이었는지를
알고 싶을 수도 있지만, 이 정보는 average out 됩니다.

362
00:37:11,080 --> 00:37:17,190
하지만 구체적으로 어떤 행동이 좋았는지 알 길이 없으며, 
좋은 estimateor를 위해서는 샘플링을 충분히 하는 수 밖에 없습니다.

363
00:37:17,190 --> 00:37:23,851
이 문제는 결국 분산을 줄이고 estimator의 성능을 높이기 위해서는
어떻게 해야 하는지에 관한 질문으로 귀결됩니다.

364
00:37:26,540 --> 00:37:33,323
분산을 줄이는 것은 policy gradient 에서
아주 중요한 분야입니다.

365
00:37:33,323 --> 00:37:39,756
샘플링을 더 적게 하면서도 estimator의
성능을 높힐 수 있는 방법이기도 합니다.

366
00:37:39,756 --> 00:37:43,278
이를 해결할 수 있는 몇 가지 아이디어를 소개해 드리겠습니다.

367
00:37:44,202 --> 00:37:46,764
주어진 그래디언트 추정기를 보면,

368
00:37:46,764 --> 00:37:57,091
첫번째 아이디어는, 해당 상태로부터 받을 미래의 보상만을
고려하여 어떤 행동을 취할 확률을 키워주는 방법입니다.

369
00:37:57,091 --> 00:38:04,736
이 방법은 해당 경로에서 얻을 수 있는 
전체 보상을 고려하는 것 대신에

370
00:38:04,736 --> 00:38:12,108
맨 처음부터가 아닌,  현재의 time step에서 부터 종료 시점까지 
얻을 수 있는 보상의 합을 고려하는 것입니다.

371
00:38:12,108 --> 00:38:18,999
이 방법이 의도하는 바는 어떤 행동이 발생시키는 
미래의 보상이 얼마나 큰지를 고려하겠다는 것입니다.

372
00:38:18,999 --> 00:38:20,499
그럴듯한 방법이죠

373
00:38:21,811 --> 00:38:29,448
두번째 아이디어는 지연된 보상에 대해서 
할인률을 적용하는 것입니다.

374
00:38:29,448 --> 00:38:36,774
수식을 보시면 할인율이 추가되었습니다.
예전에도 봤었죠.

375
00:38:36,774 --> 00:38:47,276
할인율이 의미하는 것은. 당장 받을 수 있는 보상과 
조금은 더 늦게 받은 보상간의 차이를 구별하는 것입니다.

376
00:38:47,276 --> 00:38:57,489
두번째 방법은 어떤 행동이 좋은 행동인지 아닌지를
해당 행동에 가까운 곳에서 찾습니다.

377
00:38:57,489 --> 00:39:00,880
나중에 수행하는 행동에 대해서는
가중치를 조금 낮추는 거죠

378
00:39:00,880 --> 00:39:07,730
이 두가지 방법은 실제로도 일반적으로 많이 
사용하기도 하는 아주 쉬운 아이디어입니다.

379
00:39:07,730 --> 00:39:14,597
분산을 줄이기 위한 세번째 아이디어는
baseline이라는 방법입니다.

380
00:39:14,597 --> 00:39:20,690
경로에서부터 계산한 값을
그대로 사용하는 것은 문제가 있습니다.

381
00:39:21,675 --> 00:39:23,869
그런 값들 자체가 반드시 의미있는 값은 아닐 수도 있기 때문입니다.

382
00:39:23,869 --> 00:39:29,835
가량 보상이 모두 양수(positive)이기만 해도 
행동들에 대한 확률이 계속 커지기만 할 것입니다.

383
00:39:29,835 --> 00:39:32,039
물론 그것이 나름대로 의미가 있을 순 있지만

384
00:39:32,039 --> 00:39:39,753
정말 중요한 것은 얻은 보상이 우리가 얻을 것이라고
예상했던 것 보다 더 좋은지, 아닌지를 판단하는 것입니다.

385
00:39:39,753 --> 00:39:46,071
이를 다루기 위해서 baseline function을 사용할 수 있습니다.
이 방법은 상태를 이용하는 방법입니다.

386
00:39:46,071 --> 00:39:53,886
baseline 함수가 말하고자 하는 것은 
해당 상태에서 우리가 예상하는 보상입니다.

387
00:39:55,515 --> 00:39:59,837
그렇게 되면 확률을 키우거나 줄이는 보상 수식이 조금 바뀌게 되는데

388
00:39:59,837 --> 00:40:05,508
이를 수식에 적용하면 미래에 얻을 보상들의 합에서
baseline의 값을 빼주는 형태가 되며, 이를 통해

389
00:40:05,508 --> 00:40:10,772
우리가 기대했던 것에 비해 보상이 좋은지, 아닌지에 대한
상대적인 값을 얻을 수 있습니다.

390
00:40:11,870 --> 00:40:16,099
그렇다면 baseline을 어떻게 선택하면 좋을까요?

391
00:40:16,099 --> 00:40:19,168
매우 간단한 기준선, 사용할 수있는 가장 단순한 기준선,

392
00:40:19,168 --> 00:40:21,065
이동 평균을 취하고있다.

393
00:40:21,065 --> 00:40:23,013
당신이 지금까지 경험 한 보상들.

394
00:40:23,013 --> 00:40:25,027
따라서이 전체 궤적을 수행 할 수도 있습니다.

395
00:40:25,027 --> 00:40:28,863
이것은 보상의 평균에 불과합니다.

396
00:40:28,863 --> 00:40:31,431
내가 훈련을 받고있는 동안 나는보고 있었다.

397
00:40:31,431 --> 00:40:34,765
그리고 나는이 에피소드를 연주하면서?

398
00:40:34,765 --> 00:40:37,549
맞아요, 그래서 이것은

399
00:40:37,549 --> 00:40:41,716
내가 현재받는 보상은 상대적으로 좋았거나 나빴습니다.

400
00:40:42,821 --> 00:40:45,737
그래서 이것을 사용할 수있는 약간의 차이가 있습니다.

401
00:40:45,737 --> 00:40:49,215
그러나 지금까지 우리가 지금까지 보아 왔던 분산 감소

402
00:40:49,215 --> 00:40:51,588
일반적으로 일반적으로 사용되는

403
00:40:51,588 --> 00:40:54,452
"vanilla REINFORCE"알고리즘이라고합니다.

404
00:40:54,452 --> 00:40:56,787
맞아, 미래 누적 보상을 보면,

405
00:40:56,787 --> 00:41:00,954
할인 요인 및 몇 가지 간단한 기준선이 있습니다.

406
00:41:02,601 --> 00:41:05,081
이제 우리가 어떻게 할 수 있는지 이야기 해 봅시다.

407
00:41:05,081 --> 00:41:06,547
이 기본 생각에 대해 생각해보십시오.

408
00:41:06,547 --> 00:41:08,769
잠재적으로 더 나은 기준선을 선택하십시오.

409
00:41:08,769 --> 00:41:12,084
맞습니다. 그래서 우리가 더 나은 것이 무엇인지 생각한다면

410
00:41:12,084 --> 00:41:13,567
우리가 선택할 수있는 기준선,

411
00:41:13,567 --> 00:41:16,569
우리가 원하는 것은 확률을 높이기 위해서입니다.

412
00:41:16,569 --> 00:41:19,931
행동이 더 나은 경우 국가의 행동을

413
00:41:19,931 --> 00:41:24,255
우리가 그 주에서 얻는 것의 기대 가치.

414
00:41:24,255 --> 00:41:27,655
따라서 우리가 기대하는 것의 가치에 대해 생각해보십시오.

415
00:41:27,655 --> 00:41:30,163
주에서, 이것은 당신에게 무엇을 생각 나게합니까?

416
00:41:30,163 --> 00:41:31,189
이것은 당신에게 어떤 것을 생각 나게합니까?

417
00:41:31,189 --> 00:41:34,939
우리가이 강연에서 더 일찍 이야기했던 것?

418
00:41:37,023 --> 00:41:37,856
예.

419
00:41:37,856 --> 00:41:39,266
[관객으로부터 들리지 않음]

420
00:41:39,266 --> 00:41:41,297
그래, 가치 기능이 맞지?

421
00:41:41,297 --> 00:41:45,201
우리가 Q- 러닝과 이야기했던 가치 함수.

422
00:41:45,201 --> 00:41:46,034
그래서, 정확하게.

423
00:41:46,034 --> 00:41:47,871
Q 함수와 값 함수

424
00:41:47,871 --> 00:41:50,895
그래서 직관은

425
00:41:50,895 --> 00:41:52,347
잘,

426
00:41:52,347 --> 00:41:54,704
우리는 행동에 행복하다.

427
00:41:54,704 --> 00:41:58,173
상태에서 액션을 취하는 경우 if

428
00:41:58,173 --> 00:42:00,248
복용 Q-value

429
00:42:00,248 --> 00:42:04,752
이 상태의 특정 작업은

430
00:42:04,752 --> 00:42:06,999
가치 함수 또는 기대 값

431
00:42:06,999 --> 00:42:08,406
누적 미래 보상의

432
00:42:08,406 --> 00:42:09,698
우리가이 상태에서 얻을 수있는 것.

433
00:42:09,698 --> 00:42:11,842
맞아,이 말은이 행동이

434
00:42:11,842 --> 00:42:14,416
우리가 취할 수있는 다른 행동들.

435
00:42:14,416 --> 00:42:17,896
그리고 반대로, 우리는이 행동이 불만이라면,

436
00:42:17,896 --> 00:42:22,063
이 값 또는이 차이가 음이거나 작 으면.

437
00:42:23,917 --> 00:42:27,299
맞아, 이제 우리가 이것을 연결하면,

438
00:42:27,299 --> 00:42:29,269
우리가 원하는만큼의 스케일링 요소로

439
00:42:29,269 --> 00:42:32,692
위로 또는 아래로, 우리의 행동의 가능성을,

440
00:42:32,692 --> 00:42:34,868
그러면 우리는이 견적서를 여기서 얻을 수 있습니다.

441
00:42:34,868 --> 00:42:37,452
그래, 그렇게 될거야.

442
00:42:37,452 --> 00:42:40,168
이전과 정확히 동일하지만 지금은 어디에서

443
00:42:40,168 --> 00:42:43,993
우리는 누적 적으로 기대되는 보상을 받기 전에,

444
00:42:43,993 --> 00:42:46,708
우리의 다양한 감소, 분산 감소

445
00:42:46,708 --> 00:42:50,514
기술과 기준선에, 이제 우리는
지금 막 끼워 넣을 수 있습니다.

446
00:42:50,514 --> 00:42:53,297
이 차이점이

447
00:42:53,297 --> 00:42:57,113
현재의 행동은 Q-function을 기반으로했습니다.

448
00:42:57,113 --> 00:43:00,530
그 상태에서 우리의 가치 함수를 뺀 것입니다.

449
00:43:01,771 --> 00:43:04,148
맞아.하지만 우리가 지금까지 이야기 한 내용은

450
00:43:04,148 --> 00:43:06,993
REINFORCE 알고리즘, 우리는 모른다.

451
00:43:06,993 --> 00:43:09,413
Q와 V는 실제로 무엇입니까.

452
00:43:09,413 --> 00:43:11,313
그래서 우리는 이것들을 배울 수 있습니까?

453
00:43:11,313 --> 00:43:14,479
Q- 러닝을 사용하면 대답은 '예'입니다.

454
00:43:14,479 --> 00:43:16,465
우리가 이미 전에 얘기 한 것.

455
00:43:16,465 --> 00:43:19,828
따라서 정책 구배를 결합 할 수 있습니다.

456
00:43:19,828 --> 00:43:22,210
우리가 방금 Q- 러닝에 대해 이야기하고있는 동안,

457
00:43:22,210 --> 00:43:25,982
정책 인 배우와

458
00:43:25,982 --> 00:43:28,784
비평가, 권리, Q-function,

459
00:43:28,784 --> 00:43:32,366
우리가 국가가 얼마나 좋은지를 우리에게 말해 줄 것입니다.

460
00:43:32,366 --> 00:43:34,380
그리고 한 주에서의 행동.

461
00:43:34,380 --> 00:43:36,964
맞아, 접근법에서 이것을 사용하면,

462
00:43:36,964 --> 00:43:40,633
배우가 취할 조치를 결정할 것입니다.

463
00:43:40,633 --> 00:43:43,716
비평가, 즉 Q-function은
다음과 같이 말할 것입니다.

464
00:43:43,716 --> 00:43:47,708
배우의 행동이 얼마나 좋은지 그리고 어떻게 조정해야하는지.

465
00:43:47,708 --> 00:43:51,072
그래서, 그리고 이것은 또한 약간의 작업을 완화시킵니다.

466
00:43:51,072 --> 00:43:53,636
이 비평가의 Q- 학습 문제와 비교

467
00:43:53,636 --> 00:43:56,694
이전에 우리가 이것을 가지고 있어야한다고 이야기했던 것

468
00:43:56,694 --> 00:43:59,958
모든 국가, 행동 쌍,

469
00:43:59,958 --> 00:44:01,784
왜냐하면 여기 에선 이것을 배우기 만하면되기 때문입니다.

470
00:44:01,784 --> 00:44:04,762
정책에 의해 생성 된 국가 - 행동 쌍을위한

471
00:44:04,762 --> 00:44:06,103
이걸 알면 돼.

472
00:44:06,103 --> 00:44:10,512
이 스케일링 요소를 계산하는 데 중요합니다.

473
00:44:10,512 --> 00:44:12,830
맞습니다. 그리고 우리는 이것을 배울 때,

474
00:44:12,830 --> 00:44:15,196
우리가 본 Q- 학습 트릭을 모두 포함 시키십시오.

475
00:44:15,196 --> 00:44:18,188
경험 리플레이 같은 일찍.

476
00:44:18,188 --> 00:44:20,972
그리고 이제, 저는 또한

477
00:44:20,972 --> 00:44:24,610
앞에서 본 용어를 정의하십시오.

478
00:44:24,610 --> 00:44:28,248
얼마나 많은 일이 얼마나 효과적 이었습니까?

479
00:44:28,248 --> 00:44:30,831
주어진 상태에서 s의 V를 뺀 값?

480
00:44:32,199 --> 00:44:35,533
국가가 얼마나 좋은지에 대한 우리의 기대 가치

481
00:44:35,533 --> 00:44:38,172
이 용어는 우월 함수에 의한 것이다.

482
00:44:38,172 --> 00:44:41,498
맞습니다. 따라서 이점 기능은
얼마나 많은 이점이 있습니까?

483
00:44:41,498 --> 00:44:43,568
우리는이 행동을하지 않았습니까?

484
00:44:43,568 --> 00:44:48,100
그 행동이 예상보다 얼마나 나은지.

485
00:44:48,100 --> 00:44:51,709
그래서, 이것을 사용하여 우리는

486
00:44:51,709 --> 00:44:53,457
배우 비평 알고리즘.

487
00:44:53,457 --> 00:44:56,279
그리고 이것이 어떻게 생겼는지는 우리가 시작하려고합니다.

488
00:44:56,279 --> 00:45:00,326
우리의 정책 매개 변수 인 theta를 초기화함으로써

489
00:45:00,326 --> 00:45:03,689
그리고 우리가 phi라고 부르는 비평가 매개 변수.

490
00:45:03,689 --> 00:45:07,522
그리고 각각에 대해, 훈련의 반복을 위해,

491
00:45:08,401 --> 00:45:11,149
우리는 M 궤적을 샘플링하려고합니다.

492
00:45:11,149 --> 00:45:12,185
현 정책 하에서

493
00:45:12,185 --> 00:45:13,734
그렇습니다. 우리는 정책을 실행하고

494
00:45:13,734 --> 00:45:18,725
궤적을 s-zero, a-zero,
r-zero, s-one 등으로 정의합니다.

495
00:45:18,725 --> 00:45:20,359
좋아, 그럼 우리는 계산할거야.

496
00:45:20,359 --> 00:45:21,671
우리가 원하는 그라디언트.

497
00:45:21,671 --> 00:45:24,977
맞아, 그래서이 궤도마다

498
00:45:24,977 --> 00:45:26,017
매 시간마다 우리는 가고 있습니다.

499
00:45:26,017 --> 00:45:28,901
이 우세 함수를 계산하려면,

500
00:45:28,901 --> 00:45:30,818
그리고 나서 우리는

501
00:45:31,701 --> 00:45:33,465
이 우위 함수를 사용 하시겠습니까?

502
00:45:33,465 --> 00:45:37,131
그런 다음 그라디언트 추정기에서이를 사용할 것입니다.

503
00:45:37,131 --> 00:45:40,533
우리가 이전에 보여 줬던

504
00:45:40,533 --> 00:45:42,894
우리가 여기서 가지고있는 기울기 추정치.

505
00:45:42,894 --> 00:45:46,017
그리고 우리는 또한

506
00:45:46,017 --> 00:45:50,837
비평가 매개 변수 phi는 정확히 같은 방법으로,

507
00:45:50,837 --> 00:45:54,193
우리가 이전에 보았던 것처럼, 기본적으로

508
00:45:54,193 --> 00:45:57,557
이 가치 함수는 바로 우리의 가치 함수를 배우고,

509
00:45:57,557 --> 00:46:01,640
끌어 들이기 만하면됩니다.

510
00:46:02,638 --> 00:46:05,467
이 이점 기능과 이것은

511
00:46:05,467 --> 00:46:08,324
벨맨 방정식에 더 가까워 지도록 격려하십시오.

512
00:46:08,324 --> 00:46:10,347
우리가 더 일찍 보았던, 그렇지?

513
00:46:10,347 --> 00:46:14,347
그리고 이것은 기본적으로 다음과 같이 반복됩니다.

514
00:46:15,197 --> 00:46:17,733
정책 기능의 학습 및 최적화,

515
00:46:17,733 --> 00:46:20,211
뿐만 아니라 우리의 평론 기능.

516
00:46:20,211 --> 00:46:22,311
그리고 나서 우리는

517
00:46:22,311 --> 00:46:23,977
그라디언트 그리고 나서 우리는 끝까지 갈 것입니다.

518
00:46:23,977 --> 00:46:26,727
이 과정을 계속 반복하십시오.

519
00:46:29,271 --> 00:46:31,827
자, 이제 REINFORCE의 몇
가지 예를 살펴 보겠습니다.

520
00:46:31,827 --> 00:46:36,027
행동을 취하고, 먼저 여기에서 무엇인가 불리는 것을 보자.

521
00:46:36,027 --> 00:46:39,464
재발주의 모델 (Recurrent
Attention Model)은,

522
00:46:39,464 --> 00:46:42,805
그것은 하드주의라고도 불리는 모델이며,

523
00:46:42,805 --> 00:46:46,876
하지만 최근에 컴퓨터 비전에서 많이 보게 될 것입니다.

524
00:46:46,876 --> 00:46:49,146
다양한 목적을위한 작업.

525
00:46:49,146 --> 00:46:51,806
맞아요. 그래서이 아이디어는

526
00:46:51,806 --> 00:46:55,122
여기, 나는 열심히주의를 기울인
원래의 작품에 대해 이야기했다.

527
00:46:55,122 --> 00:46:59,167
이미지 분류가 이루어지며 목표는

528
00:46:59,167 --> 00:47:02,504
여전히 이미지 클래스를 예측하기 위해,

529
00:47:02,504 --> 00:47:04,822
하지만 이제는 시퀀스를 취하여이 작업을 수행 할 것입니다.

530
00:47:04,822 --> 00:47:06,494
이미지 주변의 흘끗 보임.

531
00:47:06,494 --> 00:47:10,300
당신은 이미지 주변의 지역을 볼 것입니다.

532
00:47:10,300 --> 00:47:12,754
기본적으로 이들에 선택적으로 집중하려고합니다.

533
00:47:12,754 --> 00:47:17,141
부품을 찾고 주변을 둘러 보면서 정보를 구축하십시오.

534
00:47:17,141 --> 00:47:19,382
맞아, 우리가 이걸하고 싶어하는 이유 야.

535
00:47:19,382 --> 00:47:21,638
음, 우선, 좋은 영감을 얻었습니다.

536
00:47:21,638 --> 00:47:24,551
안구 운동에서 인간의 지각으로부터.

537
00:47:24,551 --> 00:47:26,869
우리가 복잡한 이미지를보고 있다고 가정 해 봅시다.

538
00:47:26,869 --> 00:47:29,225
우리는 이미지에 무엇이 있는지를 결정하려고합니다.

539
00:47:29,225 --> 00:47:31,594
음, 저기, 우리는 아마도 저해상도를 보았을 것입니다.

540
00:47:31,594 --> 00:47:34,013
먼저 부품을 살펴본 다음 부품을 자세히 살펴보십시오.

541
00:47:34,013 --> 00:47:36,913
우리에게 단서를 줄 이미지의

542
00:47:36,913 --> 00:47:39,168
이 이미지에 무엇이 있는가.

543
00:47:39,168 --> 00:47:40,001
그리고,

544
00:47:41,160 --> 00:47:45,703
이 방법은 이미지를보고 둘러 보는 것입니다.

545
00:47:45,703 --> 00:47:48,533
지역별로도 도움을 줄 것입니다.

546
00:47:48,533 --> 00:47:50,435
전산 자원, 맞죠?

547
00:47:50,435 --> 00:47:53,293
전체 이미지를 처리 할 필요가 없습니다.

548
00:47:53,293 --> 00:47:55,366
실제로, 일반적으로 발생하는 것은

549
00:47:55,366 --> 00:47:57,511
전체 이미지의 저해상도 이미지 먼저,

550
00:47:57,511 --> 00:48:01,678
시작하는 방법을 결정한 다음 고해상도로 살펴 봅니다.

551
00:48:02,773 --> 00:48:04,671
그 이후의 이미지 부분.

552
00:48:04,671 --> 00:48:06,979
따라서 많은 계산 자원을 절약 할 수 있습니다.

553
00:48:06,979 --> 00:48:09,725
그러면이 이점을 생각해 볼 수 있습니다.

554
00:48:09,725 --> 00:48:11,927
확장 성, 오른쪽, 말할 수있는 것, 말하자면

555
00:48:11,927 --> 00:48:15,177
더 큰 이미지를보다 효율적으로 처리합니다.

556
00:48:16,164 --> 00:48:17,780
그리고 마지막으로 이것은 실제로 도움이 될 수 있습니다.

557
00:48:17,780 --> 00:48:20,099
실제 분류 성능으로,

558
00:48:20,099 --> 00:48:21,855
이제 당신이 할 수 있기 때문에

559
00:48:21,855 --> 00:48:24,760
이미지의 혼란스럽고 관련없는 부분은 무시하십시오.

560
00:48:24,760 --> 00:48:25,593
권리?

561
00:48:25,593 --> 00:48:27,678
항상 그렇듯이

562
00:48:27,678 --> 00:48:29,931
귀하의 ConvNet, 귀하의 이미지의 모든 부분,

563
00:48:29,931 --> 00:48:32,846
당신은 이것을 사용해서 어쩌면 먼저

564
00:48:32,846 --> 00:48:34,936
내가 실제로 처리하고자하는 관련 부품,

565
00:48:34,936 --> 00:48:36,353
내 ConvNet 사용.

566
00:48:37,237 --> 00:48:39,849
자, 강화 학습은 무엇입니까?

567
00:48:39,849 --> 00:48:41,531
이 문제의 공식화?

568
00:48:41,531 --> 00:48:44,711
글쎄, 우리 주정부가 될거야.

569
00:48:44,711 --> 00:48:46,889
우리가 지금까지 보았던 엿볼은 맞습니까?

570
00:48:46,889 --> 00:48:47,722
우리의

571
00:48:48,881 --> 00:48:51,117
우리가 본 정보는 무엇입니까?

572
00:48:51,117 --> 00:48:53,643
우리의 행동은

573
00:48:53,643 --> 00:48:55,228
이미지에서 다음을 볼 수 있습니다.

574
00:48:55,228 --> 00:48:57,090
맞아요, 실제로, 이것은 뭔가가 될 수 있습니다.

575
00:48:57,090 --> 00:48:59,113
x, y 좌표는 어쩌면 일부를 중심으로

576
00:48:59,113 --> 00:49:02,842
당신이 다음에보고 싶은 고정 크기의 엿볼.

577
00:49:02,842 --> 00:49:05,664
그리고 나서 분류 문제에 대한 보상

578
00:49:05,664 --> 00:49:08,256
마지막 시간 단계에서 하나가 될 것입니다.

579
00:49:08,256 --> 00:49:12,423
우리의 이미지가 정확하게 분류되면,
그렇지 않으면 0이됩니다.

580
00:49:14,495 --> 00:49:16,162
그래서,

581
00:49:17,373 --> 00:49:20,016
이미지를 한 눈에 보면서

582
00:49:20,016 --> 00:49:21,932
비차별적인 연산이므로,

583
00:49:21,932 --> 00:49:24,006
이것이 왜 우리가

584
00:49:24,006 --> 00:49:25,761
보강 학습 배합,

585
00:49:25,761 --> 00:49:29,088
이러한 엿볼 수있는 행동을 취하는
방법에 대한 정책을 배우십시오.

586
00:49:29,088 --> 00:49:31,792
우리는 REINFORCE를 사용하여 이것을 훈련시킬 수 있습니다.

587
00:49:31,792 --> 00:49:35,105
그래서 지금까지 흘끗 보인 상태를 보면,

588
00:49:35,105 --> 00:49:37,537
우리 모델의 핵심은

589
00:49:37,537 --> 00:49:40,891
이 RNN은 우리가 국가를
모델링하기 위해 사용할 예정이며,

590
00:49:40,891 --> 00:49:44,501
정책 매개 변수를 사용하려고합니다.

591
00:49:44,501 --> 00:49:47,418
다음 동작을 출력합니다.

592
00:49:49,354 --> 00:49:53,248
좋아, 그래서이 모델이 어떻게
생겼는지 우리가 취할 것입니다.

593
00:49:53,248 --> 00:49:54,571
입력 이미지.

594
00:49:54,571 --> 00:49:57,655
그렇습니다. 그러면 우리는이 이미지를 엿볼 것입니다.

595
00:49:57,655 --> 00:50:00,068
여기이 빨간 상자가 여기 있습니다.

596
00:50:00,068 --> 00:50:03,184
이것은 모두 공백, 0입니다.

597
00:50:03,184 --> 00:50:06,966
그래서 우리는 지금까지 우리가 보았던
것을 몇 가지로 전달할 것입니다.

598
00:50:06,966 --> 00:50:09,388
신경 네트워크, 그리고 이것은 어떤 것도 될 수 있습니다.

599
00:50:09,388 --> 00:50:12,193
귀하의 작업에 따라 종류의 네트워크.

600
00:50:12,193 --> 00:50:14,276
내가 여기서 보여주고있는 원래의 실험에서,

601
00:50:14,276 --> 00:50:16,138
MNIST에서 이것은 매우 간단합니다.

602
00:50:16,138 --> 00:50:18,758
작은 두 개의 완전히 연결된 레이어를 사용하고,

603
00:50:18,758 --> 00:50:21,724
그러나 더 복잡한 이미지를 상상할 수 있습니다.

604
00:50:21,724 --> 00:50:26,105
및 기타 작업을 ConvNets를
사용하기를 원할 수 있습니다.

605
00:50:26,105 --> 00:50:28,775
맞아요. 그래서 이것을 당신이 신경망에 넣었습니다.

606
00:50:28,775 --> 00:50:31,065
그리고 나서 우리는 또한 우리가 될
것이라고 말했던 것을 기억하십시오.

607
00:50:31,065 --> 00:50:34,102
우리의 상태를 통합하고, 우리가 본 흘긋을

608
00:50:34,102 --> 00:50:36,115
지금까지 재귀 네트워크를 사용했습니다.

609
00:50:36,115 --> 00:50:38,057
그래서, 난 그냥 갈거야.

610
00:50:38,057 --> 00:50:40,265
우리는 나중에 그것을 보게 될 것입니다,
그러나 이것은 그것을 통과 할 것입니다,

611
00:50:40,265 --> 00:50:42,646
그런 다음 출력 할 것입니다.

612
00:50:42,646 --> 00:50:46,094
x, y 좌표, 다음에 볼 곳.

613
00:50:46,094 --> 00:50:48,435
실제로, 이것은 될 것입니다.

614
00:50:48,435 --> 00:50:50,766
우리는 행동에 대한 분포를 출력하고자합니다.

615
00:50:50,766 --> 00:50:53,385
맞아요, 그러면이게 될 일이 될거야.

616
00:50:53,385 --> 00:50:57,282
가우시안 분포와 우리는 평균을 출력 할 것입니다.

617
00:50:57,282 --> 00:50:59,084
평균과 분산을 출력 할 수도 있습니다.

618
00:50:59,084 --> 00:51:00,545
실제로이 배포판의

619
00:51:00,545 --> 00:51:03,944
분산도 고정 될 수 있습니다.

620
00:51:03,944 --> 00:51:07,172
좋아요, 그래서 우리는 이것을 취할 것입니다.

621
00:51:07,172 --> 00:51:08,496
우리가 지금 샘플로 할 행동

622
00:51:08,496 --> 00:51:11,854
액션 배포의 특정 x, y 위치

623
00:51:11,854 --> 00:51:15,457
그리고 우리는 이것을 다음에 얻기 위해 넣을 것입니다.

624
00:51:15,457 --> 00:51:17,777
우리의 이미지에서 다음 엿볼을 추출하십시오.

625
00:51:17,777 --> 00:51:19,297
맞아, 여기 우리가 움직였다.

626
00:51:19,297 --> 00:51:23,385
두 사람의 끝에, 두 사람의 꼬리 부분.

627
00:51:23,385 --> 00:51:25,465
그리고 이제 우리는 실제로 어떤 신호를 받기 시작합니다.

628
00:51:25,465 --> 00:51:26,745
우리가보고 싶은 것, 맞죠?

629
00:51:26,745 --> 00:51:29,065
마찬가지로 우리가 원하는 것은 관련성을 조사하는 것입니다.

630
00:51:29,065 --> 00:51:32,724
분류에 유용한 이미지의 부분.

631
00:51:32,724 --> 00:51:35,354
그래서 우리는 이것을 다시 우리의
신경망 층으로 통과 시키며,

632
00:51:35,354 --> 00:51:37,104
그리고 나서

633
00:51:38,153 --> 00:51:40,362
우리가 되풀이하는 네트워크, 맞아.

634
00:51:40,362 --> 00:51:43,642
이 이전의 숨겨진 상태뿐 아니라

635
00:51:43,642 --> 00:51:45,524
이것을 사용하여,

636
00:51:45,524 --> 00:51:47,343
이것은 우리의 정책을 대표합니다.

637
00:51:47,343 --> 00:51:49,565
우리는 이것을 출력에 사용하려고합니다.

638
00:51:49,565 --> 00:51:51,354
우리의 다음 배포판

639
00:51:51,354 --> 00:51:54,095
우리가 엿보고 싶은 위치.

640
00:51:54,095 --> 00:51:55,874
그래서 우리는 이것을 계속할 수 있습니다.

641
00:51:55,874 --> 00:51:57,303
당신은이 다음에 여기에서 엿볼 수 있습니다.

642
00:51:57,303 --> 00:51:59,903
우리는 둘의 중심을 향해 조금 더 움직였습니다.

643
00:51:59,903 --> 00:52:01,723
알았어. 그래서 아마 그걸 배울거야.

644
00:52:01,723 --> 00:52:05,005
너는 알다시피, 나는이 두 꼬리 부분을 보았을 때,

645
00:52:05,005 --> 00:52:08,093
이 모양이 어쩌면이 어퍼에서 움직일 수도 있습니다.

646
00:52:08,093 --> 00:52:10,794
왼손 방향을 사용하면

647
00:52:10,794 --> 00:52:12,631
가치가있는 센터,

648
00:52:12,631 --> 00:52:14,543
귀중한 정보.

649
00:52:14,543 --> 00:52:17,473
그런 다음 계속해서 할 수 있습니다.

650
00:52:17,473 --> 00:52:20,612
그리고 마침내, 결국, 마지막 단계에서,

651
00:52:20,612 --> 00:52:23,412
그래서 여기에 고정 된 시간 간격을 가질 수 있습니다.

652
00:52:23,412 --> 00:52:26,795
실제로 6 또는 8과 같은 것입니다.

653
00:52:26,795 --> 00:52:29,359
그리고 마지막 시간 단계에서, 우리가하고 싶기 때문에

654
00:52:29,359 --> 00:52:33,350
분류, 우리는 우리의 표준

655
00:52:33,350 --> 00:52:36,100
Softmax 레이어는

656
00:52:37,376 --> 00:52:39,363
각 클래스에 대한 확률 분포.

657
00:52:39,363 --> 00:52:42,111
그리고 나서 여기 최대 클래스는 2,

658
00:52:42,111 --> 00:52:44,108
그래서 우리는 이것이 2라고 예측할 수 있습니다.

659
00:52:44,108 --> 00:52:46,558
맞아요, 그리고 이것은 우리의 셋업이 될 것입니다.

660
00:52:46,558 --> 00:52:50,428
모델과 우리의 정책을

661
00:52:50,428 --> 00:52:53,079
우리가 가지고있는이 정책의 기울기에 대한 추정치

662
00:52:53,079 --> 00:52:54,420
이전에 우리는

663
00:52:54,420 --> 00:52:56,695
여기에서 궤적

664
00:52:56,695 --> 00:52:59,569
그걸 사용하여 소품을 되 찾으십시오.

665
00:52:59,569 --> 00:53:02,819
그래서 우리는이 모델을 훈련시키기
위해서 이것을 할 수 있습니다.

666
00:53:02,819 --> 00:53:05,281
우리 정책의 매개 변수를 배우 죠?

667
00:53:05,281 --> 00:53:08,698
여기에서 볼 수있는 모든 무게.

668
00:53:09,953 --> 00:53:10,786
좋아요.

669
00:53:12,239 --> 00:53:14,270
여기에

670
00:53:14,270 --> 00:53:16,710
MNIST에서 교육 된 정책,

671
00:53:16,710 --> 00:53:19,016
그래서 당신은 일반적으로,

672
00:53:19,016 --> 00:53:20,808
어디에서 시작하든, 보통 배우게됩니다.

673
00:53:20,808 --> 00:53:22,942
자리가 더 가까이 갈 수 있도록

674
00:53:22,942 --> 00:53:25,260
그런 다음 해당 자릿수의 관련 부분을 살펴보십시오.

675
00:53:25,260 --> 00:53:27,685
그래서 이것은 매우 차갑고

676
00:53:27,685 --> 00:53:28,744
이

677
00:53:28,744 --> 00:53:30,460
너도 알다시피, 네가 기대하는 바를 따른다. 맞다.

678
00:53:30,460 --> 00:53:31,627
너라면

679
00:53:33,335 --> 00:53:34,967
다음에 볼 장소를 선택하십시오.

680
00:53:34,967 --> 00:53:38,186
가장 효율적으로 결정하기 위해

681
00:53:38,186 --> 00:53:40,108
이게 무슨 자리 지.

682
00:53:40,108 --> 00:53:43,491
맞아, 그래서 열심히주의를 기울이는이 아이디어는,

683
00:53:43,491 --> 00:53:45,862
재발주의 모델의 사용, 또한 사용되었습니다

684
00:53:45,862 --> 00:53:49,758
컴퓨터 비전의 많은 작업에서 마지막으로

685
00:53:49,758 --> 00:53:52,687
2 년이 지나면 이것을 볼 수 있습니다. 예를 들어,

686
00:53:52,687 --> 00:53:54,790
미세한 이미지 인식.

687
00:53:54,790 --> 00:53:57,869
그래서 나는 이전에 언급했다.

688
00:53:57,869 --> 00:54:00,596
이 유용한 이점 중 하나

689
00:54:00,596 --> 00:54:01,763
~ 할 수도있다.

690
00:54:02,975 --> 00:54:05,198
계산 효율을 줄여 준다.

691
00:54:05,198 --> 00:54:08,009
혼란과 관련성을 무시하는 것

692
00:54:08,009 --> 00:54:10,180
이미지의 일부분, 그리고 세분화 된 이미지

693
00:54:10,180 --> 00:54:11,750
이미지 분류 문제,

694
00:54:11,750 --> 00:54:13,092
당신은 대개 두 가지 모두를 원합니다.

695
00:54:13,092 --> 00:54:17,307
당신은 고해상도를 유지하기를 원합니다.

696
00:54:17,307 --> 00:54:19,447
중요한 차이점이 있습니다.

697
00:54:19,447 --> 00:54:23,327
그런 다음이 차이에 초점을 맞추기를 원합니다.

698
00:54:23,327 --> 00:54:25,777
무의미한 부분은 무시하십시오.

699
00:54:25,777 --> 00:54:27,359
그래, 질문.

700
00:54:27,359 --> 00:54:31,526
[청중의 들리지 않는 질문]

701
00:54:35,061 --> 00:54:36,789
그래, 그래, 그 질문은

702
00:54:36,789 --> 00:54:39,061
어떻게 계산 효율이 있는지,

703
00:54:39,061 --> 00:54:41,482
왜냐하면 우리는 또한이 재발 성 신경망을
제 위치에 가지고 있기 때문입니다.

704
00:54:41,482 --> 00:54:45,842
그렇습니다. 그것은 당신의 것이 무엇인지에 달려 있습니다.

705
00:54:45,842 --> 00:54:47,761
당신의 문제는 무엇이고, 당신의
네트워크는 무엇입니까, 등등,

706
00:54:47,761 --> 00:54:50,151
하지만 네가 정말로

707
00:54:50,151 --> 00:54:52,512
고해상도 이미지

708
00:54:52,512 --> 00:54:54,773
이 모든 부분을 처리하고 싶지는 않습니다.

709
00:54:54,773 --> 00:54:58,477
이미지 일부 거대한 ConvNet
또는 일부 거대한, 당신도 알다시피,

710
00:54:58,477 --> 00:55:01,900
네트워크를 통해 이제는 비용을 절약 할 수 있습니다.

711
00:55:01,900 --> 00:55:04,669
이미지의 특정 작은 부분에 초점을 맞 춥니 다.

712
00:55:04,669 --> 00:55:06,589
이미지의 해당 부분 만 처리합니다.

713
00:55:06,589 --> 00:55:08,507
그러나, 당신은 옳습니다.

714
00:55:08,507 --> 00:55:10,924
당신이 가지고있는 문제 설정.

715
00:55:12,210 --> 00:55:14,530
이것은 또한 이미지 캡션에 사용되었습니다.

716
00:55:14,530 --> 00:55:17,138
그래서 우리가 이미지에 대한 캡션을 생성하려고한다면,

717
00:55:17,138 --> 00:55:20,421
우리가 선택할 수 있습니다, 당신도 알다시피,
우리는 이미지를 가질 수 있습니다.

718
00:55:20,421 --> 00:55:23,197
이 캡션을 생성하려면이주의 모델을 사용하십시오.

719
00:55:23,197 --> 00:55:26,120
일반적으로 학습을 끝내는 것은 이러한 정책입니다.

720
00:55:26,120 --> 00:55:28,999
여기서 이미지의 특정 부분에 초점을 맞 춥니 다.

721
00:55:28,999 --> 00:55:31,850
순서대로, 그리고 그것이 각 부분에 초점을 맞추면서,

722
00:55:31,850 --> 00:55:34,629
캡션의 일부 또는 일부를 생성합니다.

723
00:55:34,629 --> 00:55:38,341
이미지의 해당 부분을 참조하십시오.

724
00:55:38,341 --> 00:55:40,170
그리고 나서 그것은 또한 사용되었습니다,

725
00:55:40,170 --> 00:55:42,948
시각적 질문 응답,

726
00:55:42,948 --> 00:55:45,509
이미지에 대한 질문을합니다.

727
00:55:45,509 --> 00:55:48,981
모델이 어떤 대답을 출력하기를 원한다.

728
00:55:48,981 --> 00:55:51,786
당신의 질문에, 예를 들면, 나는 모른다.

729
00:55:51,786 --> 00:55:53,840
테이블 주위에 몇 개의 의자가 있습니까?

730
00:55:53,840 --> 00:55:58,229
그래서 당신은이주의 메커니즘이
어떻게 보이는지 볼 수 있습니다.

731
00:55:58,229 --> 00:55:59,457
좋은 유형의 모델 일 수 있습니다.

732
00:55:59,457 --> 00:56:03,040
이 질문에 대답하는 법을 배우기.

733
00:56:05,707 --> 00:56:08,475
그래, 그게 정책 구배의 예 였어.

734
00:56:08,475 --> 00:56:10,564
이 하드 어텐션 모델에서.

735
00:56:10,564 --> 00:56:13,524
그래서 저는 이제 하나의 예를 더 이야기 할 것입니다.

736
00:56:13,524 --> 00:56:16,430
또한 정책 그라디언트를 사용합니다.

737
00:56:16,430 --> 00:56:18,465
그것은 Go를하는 법을 배우고 있습니다.

738
00:56:18,465 --> 00:56:22,006
맞아, DeepMind가이 요원을 가졌어.

739
00:56:22,006 --> 00:56:24,497
알프 고 (AlphGo)라고 불리는 고 (Go)

740
00:56:24,497 --> 00:56:27,297
뉴스에 많이 등장했습니다.

741
00:56:27,297 --> 00:56:30,708
작년과 올해에.

742
00:56:30,708 --> 00:56:31,541
죄송 해요?

743
00:56:31,541 --> 00:56:32,374
[청중의 들리지 않는 의견]

744
00:56:32,374 --> 00:56:35,291
그리고 어제, 맞습니다.

745
00:56:36,172 --> 00:56:39,258
이것은 매우 흥미롭고, 최근 소식입니다.

746
00:56:39,258 --> 00:56:40,987
그래서 작년에,

747
00:56:40,987 --> 00:56:43,234
AlphaGo의 첫 번째 버전

748
00:56:43,234 --> 00:56:44,817
~에 들어갔다.

749
00:56:46,678 --> 00:56:49,539
최고의 고 (Go) 선수들과의 경쟁

750
00:56:49,539 --> 00:56:52,609
최근 몇 년 동안, Lee Sedol과 대리인

751
00:56:52,609 --> 00:56:54,927
그를 이길 수 있었다.

752
00:56:54,927 --> 00:56:57,886
5 경기 동안 4 대 1 경기를 펼쳤습니다.

753
00:56:57,886 --> 00:57:00,541
그리고 사실, 지금 당장, 그냥

754
00:57:00,541 --> 00:57:03,788
Ke Jie와의 또 다른 경기가 있습니다.

755
00:57:03,788 --> 00:57:07,855
세계 제일의 숫자이기 때문에 3 개 중 최고입니다.

756
00:57:07,855 --> 00:57:09,348
지금 중국에서.

757
00:57:09,348 --> 00:57:12,236
그래서 첫 경기가 어제였습니다.

758
00:57:12,236 --> 00:57:13,436
AlphaGo 이겼다.

759
00:57:13,436 --> 00:57:16,596
나는 그것이 단지 반점에 의한 것이라고 생각한다. 그래서,

760
00:57:16,596 --> 00:57:18,606
그래서 두 가지 더 볼 게임이 있습니다.

761
00:57:18,606 --> 00:57:20,657
이들은 모두 라이브 스트림이므로

762
00:57:20,657 --> 00:57:24,276
너희들도 온라인에 가서이 게임들을 봐야 해.

763
00:57:24,276 --> 00:57:28,193
해설을 듣는 것은 꽤 흥미 롭습니다.

764
00:57:29,225 --> 00:57:32,577
그러나 DeepMind의 AlphaGo
에이전트는 무엇입니까?

765
00:57:32,577 --> 00:57:34,868
그리고 그것은 우리가 말한 것을 많이 기반으로합니다.

766
00:57:34,868 --> 00:57:36,466
지금까지는이 강의에서.

767
00:57:36,466 --> 00:57:39,687
그리고 그것은 감독 학습의 혼합입니다.

768
00:57:39,687 --> 00:57:42,045
강화 학습,

769
00:57:42,045 --> 00:57:44,657
뿐만 아니라 이전 버전과

770
00:57:44,657 --> 00:57:48,573
이동, 몬테카를로 트리 검색,

771
00:57:48,573 --> 00:57:51,656
최근의 심층 RL 접근 방식에 대해서도 설명합니다.

772
00:57:52,579 --> 00:57:56,986
그렇다면 AlphaGo는 Go
세계 챔피언을 어떻게 상회합니까?

773
00:57:56,986 --> 00:57:59,363
음, 처음에는 무엇을합니까?

774
00:57:59,363 --> 00:58:02,449
입력이 될 때 AlphaGo를 훈련 시키는데

775
00:58:02,449 --> 00:58:04,089
보드의 몇 가지 특징 화.

776
00:58:04,089 --> 00:58:06,470
기본적으로, 당신의 이사회와 직위입니다.

777
00:58:06,470 --> 00:58:08,739
보드에 조각의.

778
00:58:08,739 --> 00:58:10,739
그것이 당신의 자연스러운 모습입니다.

779
00:58:10,739 --> 00:58:13,819
성과를 향상시키기 위해 그들이하는 일

780
00:58:13,819 --> 00:58:16,638
조금은 그들이 이것을 실현하는 것입니다.

781
00:58:16,638 --> 00:58:17,958
약간

782
00:58:17,958 --> 00:58:20,510
하나의 더 많은 채널은 모두 다른 돌 색깔입니다,

783
00:58:20,510 --> 00:58:21,956
그래서 이건 너 같은 종류 야.

784
00:58:21,956 --> 00:58:23,790
보드 구성.

785
00:58:23,790 --> 00:58:27,270
또한 일부 채널, 예를 들어, 어디서, 어떤 움직임

786
00:58:27,270 --> 00:58:31,138
합법적 인 것, 일부 바이어스 채널, 몇 가지 다양한 것들

787
00:58:31,138 --> 00:58:33,125
그런 다음이 상태가 주어지면,

788
00:58:33,125 --> 00:58:35,117
그것은 첫 번째로 갈 것입니다.

789
00:58:35,117 --> 00:58:36,518
네트워크를 훈련하다

790
00:58:36,518 --> 00:58:38,867
감독 교육을 통해 초기화됩니다.

791
00:58:38,867 --> 00:58:40,897
전문 이동 게임에서.

792
00:58:40,897 --> 00:58:43,147
따라서, 현재의 보드 구성이 주어지면

793
00:58:43,147 --> 00:58:45,627
또는 특징, 이것의 특징 화,

794
00:58:45,627 --> 00:58:48,495
올바른 다음 조치는 무엇입니까?

795
00:58:48,495 --> 00:58:50,678
좋아, 그렇게 주어진다.

796
00:58:50,678 --> 00:58:52,787
전문 게임의 예,

797
00:58:52,787 --> 00:58:55,608
알다시피, 시간이 지남에 따라 수집됩니다.

798
00:58:55,608 --> 00:58:57,635
우리는이 모든 전문적인 움직임을 취할 수 있습니다.

799
00:58:57,635 --> 00:58:59,815
표준, 감독지도 작성,

800
00:58:59,815 --> 00:59:02,605
보드 상태에서 취할 조치.

801
00:59:02,605 --> 00:59:05,365
알았어. 그래서 그들은 이것을 취한다.
이것은 꽤 좋은 시작이다.

802
00:59:05,365 --> 00:59:07,637
그리고 나서 그들은 이것을 사용하여 초기화 할 것입니다.

803
00:59:07,637 --> 00:59:09,227
정책 네트워크.

804
00:59:09,227 --> 00:59:10,844
맞아요, 정책 네트워크 때문에, 그냥 취할 것입니다.

805
00:59:10,844 --> 00:59:14,985
정확한 입력 구조는 귀하의

806
00:59:14,985 --> 00:59:16,389
보드 상태와 출력이

807
00:59:16,389 --> 00:59:17,778
당신이 취할 행동.

808
00:59:17,778 --> 00:59:20,887
그리고 이것은 정책 그라디언트를위한 준비였습니다.

809
00:59:20,887 --> 00:59:21,978
우리가 방금 본 거 맞지?

810
00:59:21,978 --> 00:59:25,156
이제 우리는이 훈련을 계속할 것입니다.

811
00:59:25,156 --> 00:59:27,130
정책 그라디언트 사용

812
00:59:27,130 --> 00:59:30,831
그리고이 강화 학습 학습을 할 것입니다.

813
00:59:30,831 --> 00:59:35,123
랜덤, 이전 반복을 위해 자체적으로 게임을합니다.

814
00:59:35,123 --> 00:59:37,384
그래서 자기 희생과 보상을 얻을 것입니다.

815
00:59:37,384 --> 00:59:42,243
그것이 이기면 하나이고, 잃으면 음수가됩니다.

816
00:59:42,243 --> 00:59:44,624
그리고 우리가하려고하는 것은 또한 우리도 배울 것입니다.

817
00:59:44,624 --> 00:59:47,573
가치 네트워크, 그래서 비평가 같은 것.

818
00:59:47,573 --> 00:59:51,235
그리고 나서, 최종 AlphaGo가 결합 될 것입니다.

819
00:59:51,235 --> 00:59:53,982
이 모든 것들이 함께 있으므로 정책과 가치 네트워크

820
00:59:53,982 --> 00:59:56,075
뿐만 아니라

821
00:59:56,075 --> 00:59:59,043
몬테카를로 트리 검색 알고리즘을 선택하려면

822
00:59:59,043 --> 01:00:01,475
미리보기 검색을 통한 작업.

823
01:00:01,475 --> 01:00:04,743
맞아,이 모든 것을 모아서,

824
01:00:04,743 --> 01:00:08,853
당신이 놀고있는 노드의 값,

825
01:00:08,853 --> 01:00:11,590
그리고 당신이 다음에하는 일은 조합이 될 것입니다.

826
01:00:11,590 --> 01:00:13,811
당신의 가치 함수의

827
01:00:13,811 --> 01:00:16,552
표준에서 계산 한 결과에서 롤아웃

828
01:00:16,552 --> 01:00:19,891
Monte Carlo Tree 검색 롤 아웃.

829
01:00:19,891 --> 01:00:22,891
그래, 근데, 근데 기본적으로

830
01:00:24,203 --> 01:00:27,453
다양한, AlphaGo의 구성 요소.

831
01:00:28,397 --> 01:00:30,314
이것에 대해 더 많이 읽고 싶다면,

832
01:00:30,314 --> 01:00:33,814
2016 년에 이것에 대한 자연 종이가 있습니다.

833
01:00:34,664 --> 01:00:37,656
그리고 그들은 이것을 훈련 시켰습니다.

834
01:00:37,656 --> 01:00:40,765
이 버전에서 사용되는 AlphaGo 버전

835
01:00:40,765 --> 01:00:45,016
경기는 몇 천 개의 CPU가 있다고 생각합니다.

836
01:00:45,016 --> 01:00:47,953
몇 백 개의 GPU를 더하여이 모든 것을 하나로 모으고,

837
01:00:47,953 --> 01:00:52,120
그래서 그것은 엄청난 양의 훈련입니다.

838
01:00:55,659 --> 01:00:57,514
그리고 네, 그렇게해야합니다.

839
01:00:57,514 --> 01:00:59,681
이번 주 경기에 출전하십시오.

840
01:01:01,643 --> 01:01:03,491
그것은 꽤 흥미 롭습니다.

841
01:01:03,491 --> 01:01:07,524
좋아요, 요약하면, 오늘 우리는

842
01:01:07,524 --> 01:01:10,858
정책 그라디언트, 오른쪽, 일반입니다.

843
01:01:10,858 --> 01:01:13,025
그들, 너는 바로

844
01:01:14,456 --> 01:01:18,855
그라디언트 디센트 또는 상승을 정책 매개 변수에 적용하면

845
01:01:18,855 --> 01:01:21,942
그래서 이것은 많은 종류의 문제들에 대해 잘 작동하며,

846
01:01:21,942 --> 01:01:23,947
그러나 또한 높은 편차를 겪는다.

847
01:01:23,947 --> 01:01:25,938
그래서 많은 샘플이 필요합니다.

848
01:01:25,938 --> 01:01:28,669
여기 당신의 도전은 표본 효율입니다.

849
01:01:28,669 --> 01:01:32,608
우리는 Q- 러닝에 대해서도
이야기했는데 항상 그렇지는 않습니다.

850
01:01:32,608 --> 01:01:35,349
일할 때가끔 어렵다.

851
01:01:35,349 --> 01:01:37,536
우리가 전에 말했던이 문제 때문에

852
01:01:37,536 --> 01:01:39,702
당신은 이것을 계산하려고합니다.

853
01:01:39,702 --> 01:01:42,285
정확한 상태, 액션 값

854
01:01:43,324 --> 01:01:47,769
많은 사람들에게, 매우 높은 차원을
위해, 그러나 그것이 효과가있을 때,

855
01:01:47,769 --> 01:01:51,342
예를 들어 우리가 전에 보았던 아타리 (Atari)

856
01:01:51,342 --> 01:01:53,092
보통 더 효율적인 샘플입니다.

857
01:01:53,092 --> 01:01:54,322
정책 기울기보다.

858
01:01:54,322 --> 01:01:56,540
맞아요, Q-learning의 과제 중 하나는

859
01:01:56,540 --> 01:01:57,484
당신은 당신이

860
01:01:57,484 --> 01:01:59,902
충분한 탐사를하고있어.

861
01:01:59,902 --> 01:02:00,735
네?

862
01:02:00,735 --> 01:02:04,902
[청중의 들리지 않는 질문]

863
01:02:14,313 --> 01:02:17,764
오, Q- 러닝의 경우이 과정을 어디에서 할 수 있습니까?

864
01:02:17,764 --> 01:02:20,684
너 좋아, 너 어디서부터 시작하려고하는지.

865
01:02:20,684 --> 01:02:21,753
일부 감독 훈련?

866
01:02:21,753 --> 01:02:24,924
따라서 Q- 학습을위한 직접 접근 방식은 그렇지 않습니다.

867
01:02:24,924 --> 01:02:27,532
당신이 이것들에 회귀하려고하기 때문에 그렇게하십시오.

868
01:02:27,532 --> 01:02:29,972
이것에 대한 정책 구배 대신에 Q 값, 맞다.

869
01:02:29,972 --> 01:02:32,732
배포판에 있지만, 나는 당신이 할
수있는 방법이 있다고 생각한다.

870
01:02:32,732 --> 01:02:34,232
좋아, 마사지해라.

871
01:02:35,938 --> 01:02:37,985
부츠 랏뿌도하는 타입.

872
01:02:37,985 --> 01:02:40,393
부츠 스트랩은 일반적으로 생각하기 때문에

873
01:02:40,393 --> 01:02:43,854
행동 복제는 좋은 방법입니다.

874
01:02:43,854 --> 01:02:46,021
이러한 정책을 따뜻하게 시작하십시오.

875
01:02:47,454 --> 01:02:50,284
그래, 그렇다. 그래서 우리는
정책 구배에 대해 이야기했다.

876
01:02:50,284 --> 01:02:54,213
Q- 러닝 (Q-learning), 그리고
이들 중 일부에 대한 또 다른 시각,

877
01:02:54,213 --> 01:02:55,413
당신이 가지고있는 보장 중 일부는

878
01:02:55,413 --> 01:02:56,752
오른쪽, 정책 그라디언트.

879
01:02:56,752 --> 01:02:58,622
우리가 정말 잘 알고있는 한 가지는

880
01:02:58,622 --> 01:03:02,789
이것은 항상 세타의 J 지역의 최소치에 수렴 할 것이고,

881
01:03:04,339 --> 01:03:06,592
그라디언트 상승을 직접하고 있기 때문에,

882
01:03:06,592 --> 01:03:09,043
그래서 이것은 종종,

883
01:03:09,043 --> 01:03:12,131
이 지역 최소값은 종종 꽤 좋았습니다.

884
01:03:12,131 --> 01:03:14,931
그리고 Q-Learning에서는 다른 한편으로는

885
01:03:14,931 --> 01:03:17,041
여기에 우리가 대략적으로하려고하기 때문에

886
01:03:17,041 --> 01:03:20,277
복잡한 함수를 가진이 Bellman 방정식

887
01:03:20,277 --> 01:03:23,358
approximator 및 그래서,이
경우, 이것은 문제입니다

888
01:03:23,358 --> 01:03:25,787
Q-learning은 훈련하는
것이 조금 더 까다 롭습니다.

889
01:03:25,787 --> 01:03:29,954
광범위한 문제에 적용 할 수있는 측면에서

890
01:03:31,849 --> 01:03:34,737
좋아, 오늘 너는 근본적으로 아주 좋아,

891
01:03:34,737 --> 01:03:37,907
간략한, 강화 학습의 고차원 적 개요

892
01:03:37,907 --> 01:03:41,546
그리고 RL의 일부 주요 클래스의 알고리즘.

893
01:03:41,546 --> 01:03:44,419
다음에 우리는

894
01:03:44,419 --> 01:03:47,577
게스트 강사, 송 한, 많은 일을했다.

895
01:03:47,577 --> 01:03:51,276
모델 압축에서의 개척 작업

896
01:03:51,276 --> 01:03:52,569
에너지 효율적인 심층 학습,

897
01:03:52,569 --> 01:03:56,459
그래서 그는 이것에 대해 몇 가지 이야기 할 것입니다.

898
01:03:56,459 --> 01:03:58,459
고맙습니다.

