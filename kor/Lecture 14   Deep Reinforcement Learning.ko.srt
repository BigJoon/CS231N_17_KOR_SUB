1
00:00:09,784 --> 00:00:11,867
시작하겠습니다!

2
00:00:13,038 --> 00:00:15,888
CS231n 14강에 오신 것을 환영합니다.

3
00:00:15,888 --> 00:00:20,884
이번 시간에는 강화학습을 배워보겠습니다.

4
00:00:20,884 --> 00:00:23,222
우선 공지사항을 전달하겠습니다.

5
00:00:23,222 --> 00:00:30,346
어제 저녁 중간고사 점수를 공개했습니다.
자세한 정보는 Piazza에서 확인하시기 바랍니다.

6
00:00:30,346 --> 00:00:35,402
그리고 두 번째 과제 및 마일스톤 점수는
주말 중으로 공개하도록 하겠습니다.

7
00:00:36,768 --> 00:00:40,682
프로젝트와 관련한 전달사항도 있습니다.
모든 팀은 반드시 프로젝트를 등록하셔야 합니다.

8
00:00:40,682 --> 00:00:47,580
Piazza에서 양식 확인 후 작성 바라며

9
00:00:47,580 --> 00:00:53,214
여러분이 작성하신 문서는 최종 학점에 반영되며
포스터 세션에서 활용 할 예정입니다.

10
00:00:53,214 --> 00:01:01,779
Tiny ImageNet Challenge에 쓰일 서버가 열렸습니다.

11
00:01:01,779 --> 00:01:06,193
그리고 며칠전 Piazza에 CS231n 수업평가
설문을 게시하였습니다.

12
00:01:06,193 --> 00:01:13,600
아직 진행하지 않으신 분들께서는 설문을 부탁드리며
CS231n의 성장을 위한 아주 소중한 피드백이 될 것입니다.

13
00:01:16,589 --> 00:01:19,650
자 오늘의 주제는 강화학습(reinforcement learning) 입니다.

14
00:01:19,650 --> 00:01:22,544
우리는 지금까지는
supervised learning을 배웠습니다.

15
00:01:22,544 --> 00:01:30,498
데이터인 x와 레이블인 y가 있었고, x를 y에 매핑하는
함수를 학습하는게 목표였습니다.

16
00:01:30,498 --> 00:01:35,067
가령 classification 문제가 있었습니다.

17
00:01:35,067 --> 00:01:37,753
그리고 지난 강의에서는
unsupervised learning을 배웠습니다.

18
00:01:37,753 --> 00:01:45,362
데이터는 있지만 레이블이 없는 경우였습니다. 데이터에 내재된
숨겨진 구조를 학습하는게 목표였습니다.

19
00:01:45,362 --> 00:01:50,528
가령 생성모델(generative models)이 있었습니다.

20
00:01:52,040 --> 00:01:57,370
이번에 배우는 것은 조금 다릅니다.
강화학습이라는 문제입니다.

21
00:01:57,370 --> 00:02:01,824
여기에는 에이전트(agent)가 있습니다. 환경(environment)
에서 행동(action)을 취하는 주체입니다.

22
00:02:01,824 --> 00:02:04,352
에이전트는 행동에 따른
적절한 보상(rewards)을 받습니다.

23
00:02:04,352 --> 00:02:09,959
강화학습은 에이전트의 보상을 최대화할 수 있는
행동이 무엇인지를 학습하는 것입니다.

24
00:02:09,959 --> 00:02:14,101
오늘은 이에 대해서 자세하게 다뤄볼 것입니다.

25
00:02:14,101 --> 00:02:18,116
이번 수업의 개요를 말씀드리자면
우선 강화학습이 어떤 문제인지를 다루고

26
00:02:18,116 --> 00:02:20,927
그리고 Markov Decision Processes(MDP)
에 대해서 다룰 것입니다.

27
00:02:20,927 --> 00:02:24,747
MDP는 강화학습 문제의 수식체계(formalism) 입니다.

28
00:02:24,747 --> 00:02:31,095
그리고 대표적인 강화학습 방법인 Q-Learning과
Policy Gradients에 대해서 배워보겠습니다.

29
00:02:32,876 --> 00:02:38,936
강화학습의 문제에서는 에이전트와 환경이 있습니다.

30
00:02:38,936 --> 00:02:43,268
환경에서 에이전트에게는 상태가 주어집니다.

31
00:02:43,268 --> 00:02:46,877
그리고 에이전트는 어떤 행동을 취하게 됩니다.

32
00:02:46,877 --> 00:02:52,609
그러면 환경은 행동에 따라 에이전트에게
보상을 주고 다음 상태를 부여합니다.

33
00:02:52,609 --> 00:03:00,918
이 과정은 에이전트가 종료상태(terminal state)가
될 때 까지 반복됩니다. 종료가 되면 한 에피소드가 끝나는 것이죠

34
00:03:00,918 --> 00:03:03,401
자 그럼 몇 가지 예시를 한번 살펴보겠습니다.

35
00:03:03,401 --> 00:03:05,536
우선 "cart-pole" 문제입니다.

36
00:03:05,536 --> 00:03:11,142
"cart-pole"는 아주 고전적인 문제입니다.
이미 CS229에서 접해보신 분들도 계실 것입니다.

37
00:03:11,142 --> 00:03:16,252
"cart-pole" 문제의 목표는 움직이는 카트(cart)와 카트 위에
매달려있는 막대기(pole)의 균형을 유지하는 것입니다.

38
00:03:16,252 --> 00:03:20,280
상태(state)에는 현재 시스템이 기술되어있습니다.

39
00:03:20,280 --> 00:03:28,206
가령, 막대기의 각,막대기의 각속도,
카트의 위치,카트의 수평속도가 있습니다.

40
00:03:28,206 --> 00:03:33,224
에이전트는 카트를 수평으로 미는 행동을 취할 수 있습니다.
(horizontal forces)

41
00:03:33,224 --> 00:03:38,387
우리는 카트를 밀면서 동시에
막대기의 균형을 잘 유지해야 합니다.

42
00:03:38,387 --> 00:03:43,990
환경으로 부터 받을 수 있는 보상은
"막대기가 제대로 서 있으면 1점" 입니다.

43
00:03:43,990 --> 00:03:48,143
여러분은 막대기를 가능한 똑바로 세워야 합니다.

44
00:03:49,286 --> 00:03:52,192
고전적인 RL문제에서 다뤘던 다양한 예시들을 살펴보겠습니다.

45
00:03:52,192 --> 00:03:53,998
"로봇 보행(robot locomotion)"  문제가 있습니다.

46
00:03:53,998 --> 00:03:59,670
여기 휴머노이드 로봇 모델과
개미 로봇 모델이 있습니다.

47
00:03:59,670 --> 00:04:03,128
로봇이 앞으로 나아가도록 하는 것이 목표입니다.

48
00:04:03,128 --> 00:04:10,807
이 문제에서 상태(state)는 로봇의
모든 관절들의 각과 위치입니다.

49
00:04:10,807 --> 00:04:15,887
에이전트가 취할 수 있는 행동(Action)은
각 관절들에 가해지는 토크(torques) 입니다.

50
00:04:15,887 --> 00:04:21,228
이 문제에서 하고싶은 것은 로봇이 앞으로 전진하는 것입니다.
앞으로 이동하면 보상을 받고

51
00:04:21,228 --> 00:04:31,701
휴머노이드 로봇의 경우에는 로봇에 똑바로 서 있는
경우에도 추가적인 보상을 받습니다.

52
00:04:33,521 --> 00:04:38,384
RL로 게임 문제도 풀 수 있습니다.

53
00:04:38,384 --> 00:04:40,700
가령 여기 아타리 게임(Atrari games)이 있습니다.

54
00:04:40,700 --> 00:04:44,280
깊은 강화학습(Deep reinforcement learning)가
아주 큰 성과를 이룬 종목이기도 합니다.

55
00:04:44,280 --> 00:04:48,574
아타리 게임에서는 가능한 가장 높은 점수로
게임을 끝마치는 것이 목적입니다.

56
00:04:48,574 --> 00:04:52,753
에이전트가 게임 플레이어가 되어
게임을 진행하게 됩니다.

57
00:04:52,753 --> 00:04:57,506
현재 게임 진행 화면 그대로
(raw pixels)가 상태로 주어집니다.

58
00:04:57,506 --> 00:05:02,882
우리가 게임을 할 때 보이는 화면
그 자체가 상태인 것이죠

59
00:05:02,882 --> 00:05:09,912
에이전트의 행동은, 우리가 게임할 때와 동일하게
위, 아래, 좌, 우로 움직일 수 있습니다.

60
00:05:09,912 --> 00:05:15,667
매 스텝마다 게임 점수는 획득할수도 잃을수도 있습니다.

61
00:05:15,667 --> 00:05:27,572
게임 종료 시점까지 점수를 최대화 하는 것이 목표입니다.
그리고 마지막으로 살펴볼 게임은 바로 "바둑" 입니다.

62
00:05:27,573 --> 00:05:31,697
지난해 강화학습이 이룬 가장 큰 성과하고 할 수 있습니다.

63
00:05:31,697 --> 00:05:38,588
세계 최고의 바둑기사였던 이세돌을 딥마인드의
알파고가 꺽은 엄청난 사건이 있었습니다.

64
00:05:38,589 --> 00:05:50,918
그리고 알파고는 최근에 또 다른 바둑
최강자들을 상대로 대국을 준비하고 있습니다.

65
00:05:50,919 --> 00:05:56,295
바둑에서의 목표는 게임에서 이기는 것입니다.
바둑을 둘 수 있는 모든 자리가 상태입니다.

66
00:05:56,295 --> 00:06:03,911
다음 수를 두는 것이 행동이며 오직 게임을
이겼을 시에만 보상이 주어집니다.

67
00:06:03,912 --> 00:06:08,411
이에 대해서는 잠시 후에
자세히 살펴보도록 하겠습니다.

68
00:06:08,411 --> 00:06:13,329
그렇다면 강화학습 문제를 어떤 식으로
수학적으로 나타내 볼 수 있을까요?

69
00:06:13,330 --> 00:06:18,051
앞서 보았던 것 처럼 환경은
에이전트에게 상태를 부여합니다.

70
00:06:18,051 --> 00:06:20,634
그러면 에이전트는 행동을 취합니다.

71
00:06:22,394 --> 00:06:28,512
Markov Decision Process를 통해서
강화학습 문제를 수식화 시킬 수 있습니다.

72
00:06:28,512 --> 00:06:31,447
MDP는 Markov property를 만족합니다.

73
00:06:31,447 --> 00:06:36,107
Markov property란 현재 상태만으로
전체 상태를 나타내는 성질입니다.

74
00:06:36,107 --> 00:06:40,164
그리고 MDP는 여기 보시는 것과 같이
몇 가지 속성으로 정의가 되는데

75
00:06:40,164 --> 00:06:43,170
S는 가능한 상태들의 집합입니다.

76
00:06:43,170 --> 00:06:45,762
A는 가능한 행동들의 집합입니다.

77
00:06:45,762 --> 00:06:51,694
R은 (state, action) 쌍이 주어졌을 때 받게되는
보상의 분포가 되겠습니다.

78
00:06:51,694 --> 00:06:55,323
따라서 R은 (state, action)이 보상으로 매핑되는 함수입니다.

79
00:06:55,323 --> 00:06:57,430
P는 전이확률(transition probability) 입니다.

80
00:06:57,430 --> 00:07:02,940
(state, action) 쌍이 주어졌을때 전이 될
다음 상대에 대한 분포를 나타냅니다.

81
00:07:02,940 --> 00:07:05,718
마지막으로 감마는  discount factor입니다.

82
00:07:05,718 --> 00:07:12,970
 discount factor는 보상을 받는 시간에 대해서
우리가 얼마나 중요하게 생각할 것인지 말해줍니다.

83
00:07:14,203 --> 00:07:17,395
MDP가 작동하는 방식을 한번 살펴보겠습니다.

84
00:07:17,395 --> 00:07:20,053
우선 초기 time step인 t = 0입니다.

85
00:07:20,053 --> 00:07:26,363
환경은 초기 초기 상태 분포인 p(s_0)에서
상태 s_0 를 샘플링합니다.

86
00:07:26,363 --> 00:07:32,253
그리고, t=0 에서부터 완료 상태가 될 때 까지
아래를 반복합니다.

87
00:07:32,253 --> 00:07:35,797
에이전트가 행동 a_t를 선택합니다.

88
00:07:35,797 --> 00:07:38,885
환경은 어떤 분포로부터 보상을 샘플링합니다.

89
00:07:38,885 --> 00:07:44,032
보상은 우리의 상태와 우리가 택한 행동이
주어졌을때의 보상입니다.

90
00:07:44,032 --> 00:07:51,534
환경은 다음 어떤 분포에서 상태인 s_t+1도 샘플링합니다.

91
00:07:51,534 --> 00:07:58,707
그리고 에이전트는 보상과 다음 상태를 받습니다.
그리고 다음 단계를 수행하게 됩니다.

92
00:07:58,707 --> 00:08:05,542
에이전트는 에피소트가 종료될 때 까지 이런 식으로
보상과 다음 상태를 받는 과정을 되풀이합니다.

93
00:08:05,542 --> 00:08:06,989
좋습니다.

94
00:08:06,989 --> 00:08:10,724
자 이제는 정책(polity) pi를 정의할 수 있습니다.

95
00:08:10,724 --> 00:08:16,651
정책은 에이전트가 각 상태에서 어떤 행동을 취할 것인지
명시해주는 기능을 수행합니다.

96
00:08:16,651 --> 00:08:19,748
정책은 deterministic 할수도
stochastic 할수도 있습니다.

97
00:08:19,748 --> 00:08:27,205
이제 우리의 목적은 최적의 정책 pi^star를 찾는 것입니다. 즉,
cumulative discounted reward를 최대화시키는 것입니다.

98
00:08:27,205 --> 00:08:35,509
보상에는 미래에 얻을 보상도 포함이 되는데 이 보상은
discount factor에 의해 할인된 보상을 얻습니다.

99
00:08:35,509 --> 00:08:39,327
자 그럼 아주 간단한 MDP 예제를 살펴보겠습니다.

100
00:08:39,327 --> 00:08:44,533
여기 예시로 격자로 된 Grid World가 있습니다.
이 곳에서 테스크를 수행해 봅시다.

101
00:08:44,533 --> 00:08:50,295
여기 격자 중 어디로든 이동할 수 있습니다.
우리의 상태가 될 것입니다.

102
00:08:50,295 --> 00:08:52,613
우리는 상태에 따라서 행동을 취할 수 있습니다.

103
00:08:52,613 --> 00:08:59,299
햄동은 간단한 움직임이 될 것입니다.
상,하,좌,우로 움직이는 행동을 취할 수 있겠죠

104
00:08:59,299 --> 00:09:08,859
여러분은 한번 움직일 때 마다 음의 보상
(negative reward)를 받게 됩니다.

105
00:09:08,859 --> 00:09:11,989
가령 R = -1 이 될 수도 있겠죠

106
00:09:11,989 --> 00:09:20,055
여러분의 목표는 여기 회색으로 칠해진 "종료 상태" 에
최소한의 행동으로 도달하는 것입니다.

107
00:09:20,055 --> 00:09:26,522
종료 상태에 도달하는 시간이 길어질수록
음의 보상이 점점 쌓이게 될 것입니다.

108
00:09:27,625 --> 00:09:30,540
그럼 random policy부터 살펴보겠습니다.

109
00:09:30,540 --> 00:09:39,090
random policy 에서는 기본적으로 어떤 방향로 움직이든
무작위로 방향을 결정합니다.

110
00:09:39,090 --> 00:09:41,843
모든 방향이 동일한 확률을 갖습니다.

111
00:09:41,843 --> 00:09:46,518
하지만 우리가 일련의 학습을 거쳐서 얻게될
optimal policy의 경우에는

112
00:09:46,518 --> 00:09:51,866
우리가 점점 더 종료 상태에 가까워 지도록 만드는
적절한 방향을 선택해서 행동을 취하게 됩니다.

113
00:09:51,866 --> 00:09:59,171
가령 종료 상태 바로 주변에 위치하는 경우라면 모든 방향은
종료 상태로 바로 이동하게끔 합니다.

114
00:09:59,171 --> 00:10:06,405
종료 상태와 먼 곳에 있다고 하더라도, 종료 상태에 가장
가깝게 이돋할 수 있는 방향으로 이동합니다.

115
00:10:09,119 --> 00:10:17,155
이렇게 MDP를 정의하고 우리가 하고싶은 것은
최적의 정책인 pi^star를 찾고싶은 것 입니다.

116
00:10:17,155 --> 00:10:20,755
최적의 적책이라는 것은 보상의 합을
최대화시키는 정책을 말합니다.

117
00:10:20,755 --> 00:10:29,731
최적의 정책은 우리가 어떤 상태에 있더라도 그 상황에서
보상을 최대화시킬 수 있는 행동이 무엇인지를 알려줍니다.

118
00:10:29,731 --> 00:10:34,091
자 그럼 질문은 "MDP에서 발생하는 무작위성(randomness)
는 어떤 식으로 다뤄야 할까요?"

119
00:10:34,091 --> 00:10:39,073
가령 초기 상태를 샘플링할 시에도 무작위성이 있고

120
00:10:39,073 --> 00:10:46,341
전이 확률 분포의 경우에도
다음 상태가 확률 분포로 존재했습니다.

121
00:10:46,341 --> 00:10:51,947
이를 위해서는 보상의 합에 대한 기댓값을 최대화시키면 됩니다.

122
00:10:51,947 --> 00:11:02,957
수식적으로 써보면, 최적의 정책 pi^star는 정책 pi에 대한
미래의 보상들의 합의 기댓값을 최대화시키는 것입니다.

123
00:11:02,957 --> 00:11:05,103
여기에서 초기 상태(s_0)는 어떤 상태 분포를 따르며

124
00:11:05,103 --> 00:11:09,127
우리가 취하는 행동은 어떤 상태가 주어졌을 때
정책이 가지는 분포로부터 샘플링됩니다.

125
00:11:09,127 --> 00:11:16,423
마지막으로 다음 상태는 전이 확률 분포로부터
샘플링됩니다.

126
00:11:16,423 --> 00:11:21,048
자 그럼 정책을 찾는 방법을 다루기 전에

127
00:11:22,143 --> 00:11:26,787
앞으로 사용하게 될 몇 가지 정의를 짚고 넘어가야 합니다.

128
00:11:26,787 --> 00:11:31,405
가치 함수(value function)와
Q-가치 함수(Q-value function)입니다.

129
00:11:31,405 --> 00:11:37,426
우리가 정책을 따라 무언가를 수행하게 되면 결국은
모든 에피소드마다 어떤 "경로" 를 얻게 될 것입니다.

130
00:11:37,426 --> 00:11:43,611
초기 상태인 s_0, a_0, r_0 부터 시작해서
s_1, a_1, r_1 이런식으로 쭉쭉 나가겠죠

131
00:11:43,611 --> 00:11:49,331
그렇게 되면 우리가 얻을 수 있는 상태(s), 행동(a), 보상(r)
들의 하나의 경로(trajectory)가 생깁니다.

132
00:11:49,331 --> 00:11:52,613
그렇다면 우리가 현재 속해있는
상태가 얼마나 좋은 상태일까요?

133
00:11:52,613 --> 00:12:10,800
임의의 상태 s에 대한 가치함수는 상태 s와 정책 pi가
주어졌을 때 누적 보상의 기댓값 입니다.

134
00:12:10,800 --> 00:12:13,286
그렇다면 (상태, 행동) 쌍이 얼마나 좋은지는
어떻게 알 수 있을까요?

135
00:12:13,286 --> 00:12:17,370
상태 s에서 어떤 행동 a를 취하는게 좋은 것일까요?

136
00:12:17,370 --> 00:12:20,468
우리는 이를 Q-가치 함수를 통해서 정의할 수 있습니다.

137
00:12:20,468 --> 00:12:27,741
Q-가치 함수는 정책 pi, 행동 a, 상태 s가 주어졌을 때
받을 수 있는 누적 보상의 기댓값입니다.

138
00:12:29,708 --> 00:12:43,383
최적의 Q-가치 함수인 Q^star는 (상태, 행동) 쌍으로부터
얻을 수 있는 누적 보상의 기댓값을 최대화시킵니다.

139
00:12:45,099 --> 00:12:52,018
다음으로는 강화학습에서 중요한 요소 중 하나인
벨만 방정식(Bellman equation)에 대해서 알아보겠습니다.

140
00:12:52,018 --> 00:12:57,697
우선 최적의 정책으로부터 나온 Q-가치 함수인
Q^star가 있다고 가정해봅시다.

141
00:12:57,697 --> 00:13:00,911
Q^star는 벨만 방정식을 만족할 것입니다.

142
00:13:00,911 --> 00:13:05,194
이것이 의미하는 바는

143
00:13:05,194 --> 00:13:11,748
어떤 (s, a)이 주어지던 간에 (s, a)

144
00:13:11,748 --> 00:13:18,868
현제 (s, a)에서 받을 수 있는 r 과
에피소드가 종료될 s^prime까지의 보상을 더한 값입니다.

145
00:13:18,868 --> 00:13:24,150
여기에서는 우리가 이미 최적의 정책을 알고 있기 때문에

146
00:13:24,150 --> 00:13:28,746
따라서 s^prime에서 우리가 할 수 있는
최상의 행동을 취할 수가 있습니다.

147
00:13:28,746 --> 00:13:34,432
s^prime에서의 Q^star의 값은
우리가 현재 상태에서 취할 수 있는 모든 행동들 중에

148
00:13:34,432 --> 00:13:38,626
Q^star(s^prime, a^prime) 을 최대화시키는 값이 됩니다.

149
00:13:38,626 --> 00:13:44,119
이를 통해서 최적의 Q 값을 얻을 수 있습니다.

150
00:13:44,119 --> 00:13:52,380
그리고 우리가 어떤 상태인지에 대한 무작위성이 존재하기 때문에
여기에도 기댓값을 취합니다.

151
00:13:54,252 --> 00:14:02,488
우리는 또한 Q^star를 통해서 특정 상태에서의 최상의
행동을 취할 수 있는 최적의 정책을 구할 수 있습니다.

152
00:14:02,488 --> 00:14:07,359
Q^star는 어떤 행동을 취했을때 미래에 받을 보상의 최대치입니다.

153
00:14:08,437 --> 00:14:16,863
따라서 우리는 그저 정책을 따라 행동을 취하기만 하면
최상의 보상을 받을 수 있습니다.

154
00:14:16,863 --> 00:14:21,025
그렇다면 어떻게 최적의 정책을 구할 수 있을까요?

155
00:14:21,025 --> 00:14:25,692
우리가 이 문제를 해결할 수 있는 한 가지 방법은 바로
value iteration algorithm 입니다.

156
00:14:25,692 --> 00:14:29,527
반복적인 업데이트를 위해서 벨만 방정식을 사용할 것입니다.

157
00:14:29,527 --> 00:14:33,830
그래서 각 단계에서 우리는 우리의
근사치를 다듬을 것입니다.

158
00:14:33,830 --> 00:14:37,997
Bellman 방정식을 시행하려고 함으로서 Q 별의

159
00:14:39,347 --> 00:14:42,602
그리고 몇몇 수학적 조건 하에서,

160
00:14:42,602 --> 00:14:45,602
우리는 또한이 시퀀스 Q, i

161
00:14:47,008 --> 00:14:49,569
우리의 Q-function은 우리의
최적 상태로 수렴 할 것입니다

162
00:14:49,569 --> 00:14:52,236
내가 무한대에 접근 할 때 Q 별.

163
00:14:54,257 --> 00:14:55,579
그리고 이렇게, 이것은 잘 작동합니다,

164
00:14:55,579 --> 00:14:58,329
자 그럼 여기에서 문제는 무엇일까요?

165
00:14:59,184 --> 00:15:02,720
문제는 바로 이 방법은
scalable 않다는 점입니다.

166
00:15:02,720 --> 00:15:08,597
반복적으로 업데이트하기 위해서는 모든 (상태, 행동)
마다 Q(s)를 계산해야만 합니다.

167
00:15:08,597 --> 00:15:17,961
가량 Atati 게임의 경우 스크린에 보이는
모든 픽셀이 상태가 될 수 있습니다.

168
00:15:18,933 --> 00:15:27,448
이 경우 상태 공간이 아주 크며, 기본적으로 전체 상태
공간을 계산하는 것은 불가능합니다.

169
00:15:28,725 --> 00:15:35,908
해결책은 무엇일까요? 우선, Q(s, a)를 추정을 위해
함수를 근사시킬 수 있습니다.

170
00:15:35,908 --> 00:15:37,620
가령 neural network 같이 말이죠

171
00:15:37,620 --> 00:15:46,693
언제나 그러 했듯이, 우리가 모르는 아주 복잡한 함수에
대해서 추정하고 싶을때는 neural network가 제격입니다.

172
00:15:48,472 --> 00:15:54,242
그러면 이제 Q-learning 수식을 살펴보겠습니다.

173
00:15:54,242 --> 00:16:02,951
여기에서는 행동 가치 함수(action value function)을
추정하기 위한 함수 근사를 이용할 것입니다.

174
00:16:02,951 --> 00:16:08,142
함수 근사에는 최근에 가장 많이 사용되는
deep neural network를 이용할 것입니다.

175
00:16:08,142 --> 00:16:10,782
이를 deep Q-learning 이라고 합니다.

176
00:16:10,782 --> 00:16:20,150
Deep Q-learning은 요즘 강화학습 하면
빠지지 않고 등장하는 방법입니다.

177
00:16:20,150 --> 00:16:30,765
여기에서는 함수 파라미터 theta가 있습니다.
theta는 neural network의 가중치입니다.

178
00:16:33,050 --> 00:16:37,970
자 그럼 함수 근사를 이용해서
어떻 방식으로 최적의 정책을 찾아낼 수 있을까요?

179
00:16:37,970 --> 00:16:44,744
Q-function은 벨만 방정식을 만족해야 한다는 점을 명심해야 합니다.

180
00:16:44,744 --> 00:16:54,713
자 이제 우리 함수가 벨만 방정식을 만족하도록 해야 합니다.
해야할 일은 neural network로 근사시킨 Q-function을

181
00:16:54,713 --> 00:17:00,240
학습시켜서 벨만 방정식의 에러가 최소가 되도록 하면 됩니다.

182
00:17:00,240 --> 00:17:03,689
손실 함수는 q(s, a)가 목적 함수와 얼마나 멀리
떨어져있는지 측정합니다.

183
00:17:03,689 --> 00:17:09,853
여기 보이는 목적함수 y_i가 바로 앞서 살펴본
벨만 방적식이 되겠습니다.

184
00:17:09,853 --> 00:17:16,928
forward pass에서는 손실 함수를 계산하고

185
00:17:16,929 --> 00:17:28,182
backward pass에서는 계산한 손실을 기반으로
파라미터 theta를 업데이트합니다.

186
00:17:28,183 --> 00:17:38,436
이런 식의 반복적인 업데이트를 총해서 우리가 가진
Q-function이 타켓과 가까워지도록 학습시킵니다.

187
00:17:38,436 --> 00:17:43,524
질문 있나요?

188
00:17:44,537 --> 00:17:53,370
Deep Q-learning 방법이 적용된 고전적인 예제를 살펴보겠습니다.

189
00:17:53,370 --> 00:18:01,746
앞서 살펴본 Atrari game 입니다. 게임의 목적은
최대한 높은 점수를 획득하는 것입니다.

190
00:18:01,746 --> 00:18:05,460
상태는 게임의 픽셀이 그대로 사용되었습니다.

191
00:18:05,460 --> 00:18:12,964
행동은 상, 하, 좌, 우 와 같이
게임에 필요한 어떤 행동이 될 것입니다.

192
00:18:12,964 --> 00:18:21,183
게임이 진행됨에 따라 매 타임 스템마다 점수가
늘어나거나 줄어듦에 따라 보상을 얻습니다.

193
00:18:21,183 --> 00:18:27,095
스코어를 기반으로 전체 누적 보상을 계산할 수 있습니다.

194
00:18:27,095 --> 00:18:32,955
Q-function에 사용한 네트워크는 다음과 같이 생겼습니다.

195
00:18:32,955 --> 00:18:37,355
Q-network는 가중치인 theta를 가지고 있습니다.

196
00:18:37,355 --> 00:18:43,791
네트워크의 입력은 상태 s 입니다.
즉 현제 게임 스크린의 픽셀들이 들어옵니다.

197
00:18:43,791 --> 00:18:49,509
실제로는 4프레임 정도 누적시켜서 사용합니다.

198
00:18:49,509 --> 00:18:58,609
입력은 RGB->grayscale변환,
다운 샘플링, 크라핑 등 전처리 과정을 거칩니다.

199
00:18:58,609 --> 00:19:04,631
전처리를 거친 입력은 최근 4 프레임을 누적시킨
84 x 84 x 4 의 형태가 됩니다.

200
00:19:04,631 --> 00:19:05,464
질문 있나요?

201
00:19:05,464 --> 00:19:09,631
[학생이 질문]

202
00:19:12,792 --> 00:19:20,809
질문은 "네트워크가 다양한 (상태, 행동) 쌍들을 구하기 위해서
Q-가치 함수를 근사시키는 것인지" 입니다.

203
00:19:20,809 --> 00:19:24,765
(가령 이 예시의 경우 4개의 (상태, 행동)쌍을 출력)
네 맞습니다.

204
00:19:24,765 --> 00:19:27,551
이에 대해서는 다음 슬라이드에서
조금 더 자세히 설명할 것입니다.

205
00:19:27,551 --> 00:19:29,935
[학생이 질문]

206
00:19:29,935 --> 00:19:36,816
여기에서는 Softmax를 사용하지는 않습니다.
Q-value functions을 직접 예측하는 것이 목적입니다.

207
00:19:36,816 --> 00:19:40,583
[학생이 질문]

208
00:19:40,583 --> 00:19:44,014
Q-values를 regression 한다고 보시면 됩니다.

209
00:19:44,014 --> 00:19:54,084
입력 다음으로 살펴볼 것은 네트워크 구조인데, 우리에게는
아주 익숙한 구조입니다. 컨볼루션, FC 레이어들로 구성됩니다.

210
00:19:54,084 --> 00:19:58,754
8 x 8 / 4 x 4 컨볼루션이 있고

211
00:19:59,611 --> 00:20:05,674
그리고 FC 256 레이어를 거칩니다. 이제는 우리에게
아주 익숙한 네트워크 구조입니다.

212
00:20:05,674 --> 00:20:17,415
FC 레이어의 출력 벡터는 네트워크의 입력인,
"상태" 가 주어졌을 때 각 행동의 Q-value 입니다.

213
00:20:17,415 --> 00:20:21,770
가령 네 가지 행동이 존재한다면
출력도 4차원입니다.

214
00:20:21,770 --> 00:20:28,685
이는 현재 상태 s_t와 여기에 존재하는 행동들
a_1, a_2, a_3, a_4에 관한 Q 값들입니다.

215
00:20:28,685 --> 00:20:33,179
각 행동들 마다 하나의 스칼라 값을 얻습니다.

216
00:20:33,179 --> 00:20:41,122
참고로 행동의 수는 Atari game의 종류에 따라서
4~18가지로 변할 수도 있습니다.

217
00:20:43,073 --> 00:20:46,709
이런 방식의 네트워크 구조의 장점은

218
00:20:46,709 --> 00:20:54,651
한 번의 forward pass 만으로 현재 상태에 해당하는
모든 함수에 대한 Q-value를 계산할 수 있다는 점입니다.

219
00:20:54,651 --> 00:20:56,117
이는 정말 효율적인 방법입니다.

220
00:20:56,117 --> 00:21:05,946
현재 상태의 각 행동들에 각각 Q-value가 존재할텐데
현재 상태를 네트워크의 입력으로 넣어주기만 하면

221
00:21:05,946 --> 00:21:10,259
모든 Q-value를 한번의 forward pass로 계산할 수 있습니다.

222
00:21:10,259 --> 00:21:15,078
이 네트워크를 학습시키 위해서는
앞서 살펴본 손실 함수가 필요합니다.

223
00:21:15,078 --> 00:21:17,661
네트워크로 근사시킨 함수도
벨만 방정식을 만족해야 합니다.

224
00:21:17,661 --> 00:21:29,315
따라서 네트워크의 출력인 Q-value가 타겟 값과 가까워 지도록
반복적으로 학습시켜야 합니다.

225
00:21:29,315 --> 00:21:39,777
backward pass에서는 손실함수를 기반으로 그레디언트를 계산하고
이를 바탕으로 네트워크를 학습시킵니다.

226
00:21:40,706 --> 00:21:45,639
한 가지 알아야 할 개념이 있습니다.
"experience replay" 라는 것입니다.

227
00:21:45,639 --> 00:21:53,440
Experience Replay는 Q-network에서 
발생할 수 있는 문제들을 다룹니다.

228
00:21:53,440 --> 00:21:58,134
Q-network를 학습시킬 때 하나의 배치에서 
시간적으로 연속적인 샘플들로 학습하면 안좋습니다.

229
00:21:58,134 --> 00:22:09,410
가령 게임을 진행하면서 시간적으로 연속적인 샘플들을
이용해서 상태, 행동, 보상을 계산하고 학습시키면

230
00:22:09,410 --> 00:22:16,118
모든 샘플들이 상관 관계를 가집니다(correlated).
이는 학습에 아주 비효율적입니다. 이 문제가 첫 번째 문제입니다.

231
00:22:16,118 --> 00:22:24,842
그리고 두번째로는, 현재 Q-network 파라미터를 생각해보면
네트워크는 우리가 어떤 행동을 해야할지에 정책을 결정한다는 것은

232
00:22:24,842 --> 00:22:27,394
우리가 다음 샘플들도 결정하게 된다는 의미입니다.

233
00:22:27,394 --> 00:22:30,832
이는 학습에 좋지 않은 영향을 미칠 것입니다.
(bad feedback loops)

234
00:22:30,832 --> 00:22:35,468
가령 현재 상태에서 "왼쪽" 으로 이동하는 것이
보상을 최대화하는 행동이라면

235
00:22:35,468 --> 00:22:42,297
결국 다음 샘플들도 전부 "왼쪽"에서 발생할 수 있는
것들로만 편향될 것입니다.

236
00:22:43,306 --> 00:22:45,406
이는 문제가 될 수 있습니다.

237
00:22:45,406 --> 00:22:53,098
이를 해결하는 방법으로 
"experience replay" 라는 방법을 사용합니다.

238
00:22:53,098 --> 00:23:01,353
이 방법은 replay memory를 이용합니다. replay memory에는
(상태, 행동, 보상. 다음상태)로 구성된 전이 테이블이 있습니다.

239
00:23:01,353 --> 00:23:08,773
게임 에피소드를 플레이하면서 더 많은 경험을 얻음에 따라
전이 데이블을 지속적으로 업데이트시킵니다.

240
00:23:08,773 --> 00:23:16,335
여기에서는 replay memory에서의 임의의 미니 배치를 
이용하여 Q-network를 학습시킵니다.

241
00:23:16,335 --> 00:23:24,827
연속적인 샘플을 사용하는 대신 전이 테이블에서
임의로 샘플링된 샘플을 사용하는 것이죠.

242
00:23:24,827 --> 00:23:31,007
이 방식을 통해서 앞서 상관관계(correlation)로
발생하는 문제를 해결할 수 있을 것입니다.

243
00:23:31,007 --> 00:23:39,207
추가적인 이점으로는 각각의 전이가 가중치 업데이트에
여러 차례(딱 한번이 아니라) 기여할 수 있다는 점입니다.

244
00:23:39,207 --> 00:23:43,652
전이 테이블로부터 샘플링을 하면
하나의 샘플도 여러번  뽑힐 수 있는 것이죠

245
00:23:43,652 --> 00:23:47,585
이를 통해 데이터 효율이 훨씬 더 증가하게 됩니다.

246
00:23:50,580 --> 00:23:57,583
자 그럼 종합해서, experience replay를 이용한 
Q-learning 전체 알고리즘을 살펴보겠습니다.

247
00:23:59,166 --> 00:24:03,940
우선  replay memory을 초기화하는 것부터 시작합니다.

248
00:24:03,940 --> 00:24:14,830
우선 memory capacity인 N을 정해줍니다. 그리고 
Q-network를 임의의 가중치로 초기화시킵니다.

249
00:24:14,830 --> 00:24:21,832
그리고 앞으로 M번의 에피소드를 진행할 것입니다.
따라서 학습은 총 M번의 에피소드까지 진행됩니다.

250
00:24:21,832 --> 00:24:31,265
그리고 각 에피소드마다 상태를 초기화시켜야 합니다. 
상태는 게임 시작화면 픽셀이 될 것입니다.

251
00:24:31,265 --> 00:24:37,814
앞서, 입력 상태를 만들기 위한 
전처리 과정은 앞서 언급했었죠

252
00:24:37,814 --> 00:24:41,584
다음은, 게임이 진행중인
매 타음 스탭마다

253
00:24:41,584 --> 00:24:46,268
적은 확률로는 임의의 행동을 취할 것입니다. 
(정책을 따르지 않고)

254
00:24:46,268 --> 00:24:53,141
이는 충분한 탐사(exploration)을 위해서는
아주 중요한 부분입니다.

255
00:24:53,141 --> 00:24:58,559
이를 통해 다양한 상태 공간을 샘플링할 수 있습니다.

256
00:24:58,559 --> 00:25:03,614
낮은 확률로 임의의 행동을 취하거나, 또는(otherwise)
현재 정책에 따라 greedy action을 취합니다.

257
00:25:03,614 --> 00:25:13,580
다시 말해 대부분의 경우에는 현재 상태에 적합하다고 판단되는
greedy action을 선택하게 될 것이고, 가끔식은

258
00:25:13,580 --> 00:25:16,300
낮은 확률로 임의의 행동이 선택될 것입니다.

259
00:25:16,300 --> 00:25:26,070
이렇게 행동 a_t를 취하면 보상(r_t)과 
다음 상태(s_t+1)를 얻을 수 있습니다.

260
00:25:26,070 --> 00:25:32,771
그러고 나서 전이(transition)을 
replay memory에 저장합니다.

261
00:25:32,771 --> 00:25:35,577
자 이제 네트워크를 학습시킬 차례입니다.

262
00:25:35,577 --> 00:25:37,550
experience replay를 이용할 시점입니다.

263
00:25:37,550 --> 00:25:47,214
replay memory에서 임의의 미니배치 전이들(transitions)을
샘플링한 다음에 이를 이용하여 업데이트합니다.

264
00:25:47,214 --> 00:25:49,635
이것이 전체 학습의 반복과정입니다.

265
00:25:49,635 --> 00:26:02,350
게임을 진행하는 동안에 experienced replay을 이용하여
미니배치를 샘플링하고, 이를 통해 Q-network를 학습시킵니다.

266
00:26:03,887 --> 00:26:17,699
Google DeepMind에서 Atari 게임 Breakout을 
Q-learning으로 학습시키는 영상을 살펴보겠습니다.

267
00:26:20,911 --> 00:26:26,185
게임화면의 픽셀 그대로가 상태입니다.

268
00:26:26,185 --> 00:26:29,520
학습 초반에 어떤 일이 발생하는지 살펴봅시다.

269
00:26:29,520 --> 00:26:40,303
학습 초반에는, 공을 치긴 하지만 그렇게 잘해보이지는 않습니다.
금방 놓치고 맙니다.

270
00:26:40,303 --> 00:26:42,886
그래도 계속 공을 찾아다니고 있습니다.

271
00:26:50,969 --> 00:26:55,737
그리고 몇 시간정도 더 학습한 결과를 살펴봅시다.

272
00:27:00,946 --> 00:27:05,113
지금은 꾀 잘해보입니다.

273
00:27:06,190 --> 00:27:13,388
공도 잘 따라가고 있고 
블럭도 거의 다 깨는 모습입니다.

274
00:27:16,593 --> 00:27:36,203
240분이 지나면 이제는 거의
프로에 가까운 묘기를 선보입니다.

275
00:27:36,203 --> 00:27:41,574
공을 위로 올려서 알아서 블럭이 깨지게 하는 전략이죠

276
00:27:42,796 --> 00:27:49,501
지금 보여드린 예시에서는 Agent가 deep Q-learning을 통해
Atari 게임을 학습하고 플레이하였습니다.

277
00:27:49,501 --> 00:27:55,081
온라인을 통해서 더 많은 Atrari 게임에서의 예시를
확인해보실 수 있습니다.

278
00:27:56,419 --> 00:28:01,149
지금까지 Q-learning을 알아봤습니다. 하지만
Q-learning에는 문제가 있었습니다.

279
00:28:01,149 --> 00:28:07,226
아주 어려운 문제였는데요, 그 문제는 바로 
Q-function이 너무나도 복잡하다는 것입니다.

280
00:28:07,226 --> 00:28:12,335
Q-learning 에서는 모둔 (state, action) 쌍들을 
학습해야만 합니다.

281
00:28:12,335 --> 00:28:17,275
그런데 가령, 로봇이 어떤 문체를 손에 쥐는
문제를 풀어야 한다고 생각해봅시다.

282
00:28:17,275 --> 00:28:19,576
이 경우 아주 고차원의 상태 공간이 존재합니다.

283
00:28:19,576 --> 00:28:26,225
가령 모든 관절의 위치와 각도가 이루는 
모든 경우의 수를 생각해볼 수 있겠죠

284
00:28:26,225 --> 00:28:34,171
이 모든 (상태, 행동)을 학습시키는것은
아주 어려운 문제입니다.

285
00:28:35,493 --> 00:28:38,724
반면, 다른 한편으로는 정책 자체는 훨씬 더 간단할 수 있습니다.

286
00:28:38,724 --> 00:28:44,556
정책은 간단하게 
 "손을 움켜쥐는 것" 일수 있겠죠

287
00:28:44,556 --> 00:28:48,252
손가락을 특정 방향으로 움직이기만 하면 됩니다.

288
00:28:48,252 --> 00:28:54,142
그렇다면 정책을 직접적으로 배울 수 있을까요?

289
00:28:54,142 --> 00:28:58,306
만약 가능하다면 여러 정책들 가운데
최고의 정책을 찾아낼 수 있을 것입니다.

290
00:28:58,306 --> 00:29:05,495
정책을 결정하기에 앞서 Q-value를 추정하는
과정을 거치지 않고도 말이죠

291
00:29:06,790 --> 00:29:15,938
이러한 접근 방식을 "policy gradients"라고 합니다.

292
00:29:15,938 --> 00:29:20,858
자 그럼 수식적으로 매개변수화된 정책 집합
(class of parametrized policies)을 정의해봅시다.

293
00:29:20,858 --> 00:29:24,146
정책들은 가중치 theta에 의해서 매개변수화됩니다.

294
00:29:24,146 --> 00:29:27,791
자 그럼 각 정책에 대해서 
정책의 값을 정의해봅시다.

295
00:29:27,791 --> 00:29:35,723
J(theta)는 미래에 받을 보상들의 누적 합의
기댓값으로 나타낼 수 있습니다.

296
00:29:35,723 --> 00:29:38,971
이는 우리가 지금까지 사용했던
보상과 동일합니다.

297
00:29:38,971 --> 00:29:41,879
이런 상황(setup)에서 우리가 하고싶은 것은

298
00:29:41,879 --> 00:29:51,548
최적의 정책인 theta^star를 찾는 것인데 이는
argmax_theta J(theta)로 나타낼 수 있습니다.

299
00:29:51,548 --> 00:29:56,917
이제는 보상의 기댓값을 최대로 하는 정책 
파라미터를 찾으면 됩니다.

300
00:29:56,917 --> 00:30:01,011
이 문제를 어떻게 풀면 될까요?

301
00:30:04,993 --> 00:30:06,843
좋아, 우리가 할 수있는 일

302
00:30:06,843 --> 00:30:10,155
정책 매개 변수에 대한 동의어입니다.

303
00:30:10,155 --> 00:30:12,476
우리는 우리가 가지고있는 어떤 목표가 주어진다면,

304
00:30:12,476 --> 00:30:15,460
우리는 그라디언트 어 센트를 사용할
수있는 몇 가지 매개 변수

305
00:30:15,460 --> 00:30:17,512
및 그래디언트 동의

306
00:30:17,512 --> 00:30:20,762
지속적으로 매개 변수를 개선합니다.

307
00:30:23,202 --> 00:30:24,950
그리고 어떻게 더 구체적으로 이야기할까요?

308
00:30:24,950 --> 00:30:27,174
우리는 이것을 할 수 있습니다. 우리는 이것을 호출 할 것입니다.

309
00:30:27,174 --> 00:30:29,196
여기 강화 알고리즘.

310
00:30:29,196 --> 00:30:31,068
수학적으로, 우리는 다음과 같이 쓸 수 있습니다.

311
00:30:31,068 --> 00:30:34,375
우리가 기대하는 미래의 보상을 밖으로

312
00:30:34,375 --> 00:30:36,781
궤적을 넘어서서 우리는 샘플로 갈 것입니다.

313
00:30:36,781 --> 00:30:38,611
경험의 이러한 궤적, 바로,

314
00:30:38,611 --> 00:30:40,286
예를 들어 게임 놀이의 에피소드

315
00:30:40,286 --> 00:30:41,902
이전에 우리가 얘기했던 것.

316
00:30:41,902 --> 00:30:45,673
S-zero, a-zero, r-zero, s-one,

317
00:30:45,673 --> 00:30:47,411
a-one, r-one 등이 있습니다.

318
00:30:47,411 --> 00:30:51,723
세타의 어떤 정책을 사용합니다.

319
00:30:51,723 --> 00:30:54,139
오른쪽 궤도마다

320
00:30:54,139 --> 00:30:57,739
우리는 그 궤도에 대한 보상을 계산할 수 있습니다.

321
00:30:57,739 --> 00:30:59,135
누적 된 보상입니다.

322
00:30:59,135 --> 00:31:01,245
이 궤적을 따라 잡았습니다.

323
00:31:01,245 --> 00:31:03,733
그리고 정책의 가치,

324
00:31:03,733 --> 00:31:05,968
파이 서브 세타는 예상대로 될 것입니다.

325
00:31:05,968 --> 00:31:07,933
우리가 얻을 수있는 이러한 궤도에 대한 보상

326
00:31:07,933 --> 00:31:10,570
다음 pi 쎄타로부터.

327
00:31:10,570 --> 00:31:12,701
여기 그것이 궤적에 대한 기대입니다.

328
00:31:12,701 --> 00:31:16,868
우리가 얻을 수있는 것은 우리의
정책에서 궤적을 추출하는 것입니다.

329
00:31:18,563 --> 00:31:19,424
괜찮아.

330
00:31:19,424 --> 00:31:21,288
그래서 그라디언트 상승을 원합니다.

331
00:31:21,288 --> 00:31:22,961
그래서 이것을 차별화합시다.

332
00:31:22,961 --> 00:31:25,023
우리가 이것을 차별화하면, 우리는

333
00:31:25,023 --> 00:31:27,356
그라디언트 단계.

334
00:31:28,535 --> 00:31:30,418
그래서, 문제는 지금 우리가 노력한다면

335
00:31:30,418 --> 00:31:32,678
이것을 정확히 구별하십시오.

336
00:31:32,678 --> 00:31:34,300
이건 다루기 힘들지, 그렇지?

337
00:31:34,300 --> 00:31:37,388
따라서 기대치의 변화에 문제가 있습니다.

338
00:31:37,388 --> 00:31:41,319
여기서 p는 쎄타에 의존 할 때,
왜냐하면 여기에 있기 때문입니다.

339
00:31:41,319 --> 00:31:43,513
우리는이 그라데이션을 취하고 싶다.

340
00:31:43,513 --> 00:31:47,661
타우의 p, 주어진 쎄타,

341
00:31:47,661 --> 00:31:48,766
그러나 이것은 될 것입니다.

342
00:31:48,766 --> 00:31:50,591
우리는 타우에 대해이 적분을 취하고 싶습니다.

343
00:31:50,591 --> 00:31:53,033
맞습니다. 그래서 이것은 다루기가 어렵습니다.

344
00:31:53,033 --> 00:31:57,327
그러나이 문제를 해결하기 위해
여기에서 트릭을 사용할 수 있습니다.

345
00:31:57,327 --> 00:32:01,855
그리고이 트릭은 우리가 원하는이
그라디언트를 취하고 있습니다.

346
00:32:01,855 --> 00:32:03,203
우리는 이것을 다시 쓸 수 있습니다.

347
00:32:03,203 --> 00:32:04,941
이것을 하나씩 곱함으로써,

348
00:32:04,941 --> 00:32:07,081
위아래를 곱함으로써,

349
00:32:07,081 --> 00:32:10,286
둘 다 τ가 주어지면 쎄타가 주어진다.

350
00:32:10,286 --> 00:32:12,052
그렇다면이 용어를 보면

351
00:32:12,052 --> 00:32:14,248
우리가 지금 여기에, 내가 이것을 기록한 방식으로,

352
00:32:14,248 --> 00:32:15,958
왼쪽과 오른쪽에

353
00:32:15,958 --> 00:32:18,815
실제로는

354
00:32:18,815 --> 00:32:23,424
타우와 그라디언트의 그라디언트 배

355
00:32:23,424 --> 00:32:26,170
세타와 관련하여, 로그의, p.

356
00:32:26,170 --> 00:32:29,074
맞습니다. 왜냐하면 p의 로그의
그래디언트가 바로 가기 때문입니다.

357
00:32:29,074 --> 00:32:32,741
p의 p 배의 기울기를 갖는 것.

358
00:32:33,808 --> 00:32:36,934
좋아, 그럼 우리가 다시 주사하면

359
00:32:36,934 --> 00:32:41,385
우리가이 그라디언트에 대해 이전에 가지고 있었던 표현으로,

360
00:32:41,385 --> 00:32:43,426
우리는 이것이 실제로 어떻게 보이는지를 볼 수 있습니다.

361
00:32:43,426 --> 00:32:46,059
맞아, 이제 우리는 log p의 기울기를 갖기 때문에

362
00:32:46,059 --> 00:32:49,106
이 모든 궤도의 확률

363
00:32:49,106 --> 00:32:52,187
그런 다음 타우에 대해이 정수를 취합니다.

364
00:32:52,187 --> 00:32:54,495
이것은 이제 기대가 될 것입니다.

365
00:32:54,495 --> 00:32:58,586
우리의 궤도 tau에, 그래서 우리가 여기서 한 것은

366
00:32:58,586 --> 00:33:02,751
우리가 기대치의 그라데이션을 찍은 것입니다.

367
00:33:02,751 --> 00:33:06,823
우리는 이것을 그라디언트의 기대로 변형 시켰습니다.

368
00:33:06,823 --> 00:33:09,156
맞아, 이제 우리는

369
00:33:10,051 --> 00:33:12,404
우리가 얻을 수있는 샘플 궤도

370
00:33:12,404 --> 00:33:14,712
우리의 그라디언트를 추정하기 위해서.

371
00:33:14,712 --> 00:33:17,343
그래서 우리는 몬테카를로 샘플링을
사용하여 이것을 수행합니다.

372
00:33:17,343 --> 00:33:21,260
이것은 강화의 핵심 아이디어 중 하나입니다.

373
00:33:23,624 --> 00:33:25,846
좋아, 이걸보고있어.

374
00:33:25,846 --> 00:33:28,180
우리가 계산하고자하는 표현,

375
00:33:28,180 --> 00:33:30,421
우리가 여기있는 양을 계산할 수 있습니까?

376
00:33:30,421 --> 00:33:33,071
전이 확률을 모른 채로?

377
00:33:33,071 --> 00:33:36,643
알았어, 그래서 우리는 타우의 피가 될거야.

378
00:33:36,643 --> 00:33:38,466
탄도의 확률.

379
00:33:38,466 --> 00:33:40,387
그것은의 제품이 될 것입니다

380
00:33:40,387 --> 00:33:43,379
다음 주에 대한 모든 전이 확률

381
00:33:43,379 --> 00:33:45,821
우리의 현재 상태와 행동을 감안할 때

382
00:33:45,821 --> 00:33:49,051
행동의 확률뿐만 아니라

383
00:33:49,051 --> 00:33:52,232
우리는 우리의 정책에 따라 결정했습니다.

384
00:33:52,232 --> 00:33:54,743
맞습니다. 그래서 우리는이 모든
것들을 함께 번식 할 것입니다.

385
00:33:54,743 --> 00:33:58,441
우리의 궤도의 확률을 얻으십시오.

386
00:33:58,441 --> 00:34:03,059
그래서 우리가 계산하고자하는 타우의 p의 로그

387
00:34:03,059 --> 00:34:06,334
우리는이 기록을 가지고있을 것입니다.

388
00:34:06,334 --> 00:34:08,326
이것을 합계로 나누어 라.

389
00:34:08,326 --> 00:34:10,389
안으로 로그를 밀기.

390
00:34:10,389 --> 00:34:12,383
그리고 나서 여기에서 우리가 이것을 차별화 할 때,

391
00:34:12,384 --> 00:34:14,237
우리는 존경심으로 차별화되기를
원한다는 것을 알 수 있습니다.

392
00:34:14,237 --> 00:34:18,162
쎄타에게,하지만 우리가 여기있는 첫 번째 용어는,

393
00:34:18,163 --> 00:34:20,911
상태 전이 확률의 로그 p

394
00:34:20,911 --> 00:34:22,850
여기에 쎄타 용어가 없습니다.

395
00:34:22,850 --> 00:34:25,292
우리가 세타를 가지고있는 유일한 장소는 두 번째 용어입니다

396
00:34:25,292 --> 00:34:28,709
우리가 가지고있는 pi sub theta의 로그의

397
00:34:29,675 --> 00:34:32,914
우리의 국가에 주어진 우리의 행동에 대해

398
00:34:32,914 --> 00:34:34,139
우리가 지키는 용어

399
00:34:34,139 --> 00:34:37,368
우리의 그라디언트 추정치에서, 그래서
우리는 여기서 볼 수 있습니다

400
00:34:37,369 --> 00:34:39,670
이것은 전이 확률에 의존하지 않고,

401
00:34:39,670 --> 00:34:41,293
맞아, 우리는 실제로 알 필요가 없다.

402
00:34:41,293 --> 00:34:44,588
우리의 전이 확률이 컴퓨터에 순서대로

403
00:34:44,589 --> 00:34:46,422
그라디언트 추산.

404
00:34:47,257 --> 00:34:50,879
그리고 나서, 우리가 이것들을 샘플링 할 때,

405
00:34:50,880 --> 00:34:55,047
어떤 주어진 궤적 타우에 대해서 우리는
쎄타의 J를 추정 할 수있다.

406
00:34:56,306 --> 00:34:58,524
이 기울기 추정을 사용하여.

407
00:34:58,524 --> 00:35:00,472
이것은 여기 하나의 궤적을 보여줍니다.

408
00:35:00,472 --> 00:35:02,220
우리가 이전에 가지고 있었던 것에서,

409
00:35:02,220 --> 00:35:05,271
우리는 또한 여러 궤도를 통해
표본 추출을 할 수 있습니다.

410
00:35:05,271 --> 00:35:07,188
기대를 얻기 위해서.

411
00:35:09,248 --> 00:35:12,974
자, 우리가 도출 한이 기울기 측정기가 주어진다면,

412
00:35:12,974 --> 00:35:17,141
우리가 여기에서 만들 수있는 해석은

413
00:35:18,217 --> 00:35:21,931
궤도에 대한 보상이 높으면 보상이

414
00:35:21,931 --> 00:35:25,226
우리는 일련의 행동을 취하는 것이 좋았고,

415
00:35:25,226 --> 00:35:27,517
다음으로 모든 사람들의 확률을 높여 봅시다.

416
00:35:27,517 --> 00:35:29,434
우리가 본 행동들.

417
00:35:29,434 --> 00:35:31,458
맞아. 우린 그걸 말할거야.

418
00:35:31,458 --> 00:35:33,141
이것들은 우리가 취한 좋은 행동이었다.

419
00:35:33,141 --> 00:35:35,287
그리고 보상이 낮 으면,

420
00:35:35,287 --> 00:35:37,186
우리는 이러한 확률을 낮추기를 원합니다.

421
00:35:37,186 --> 00:35:38,629
우리는 이것이 나쁜 행동이라고 말하고 싶습니다.

422
00:35:38,629 --> 00:35:40,747
시도하고 샘플을 너무 많이 보자.

423
00:35:40,747 --> 00:35:43,568
맞습니다. 그래서 우리는 그것이 여기서
일어나는 일을 볼 수 있습니다.

424
00:35:43,568 --> 00:35:47,392
우리는 a의 pi를 가지고, s는 주어진다.

425
00:35:47,392 --> 00:35:50,980
이것이 우리가 취한 행동의 가능성입니다.

426
00:35:50,980 --> 00:35:53,163
우리는 이것을 확장 할 것이며, 우리는

427
00:35:53,163 --> 00:35:56,621
그라디언트가 얼마나 많은지 알려줄 것입니다.

428
00:35:56,621 --> 00:36:00,555
증가 시키려면 매개 변수를 변경해야합니까?

429
00:36:00,555 --> 00:36:03,575
우리의 행동에 대한 우리의 가능성, 그렇죠?

430
00:36:03,575 --> 00:36:06,501
그리고 나서 우리는 이것을 받아서

431
00:36:06,501 --> 00:36:09,019
우리가 실제로 얼마나 많은 보상을 받았는지,

432
00:36:09,019 --> 00:36:12,602
현실적으로 이러한 행동이 얼마나 좋은지.

433
00:36:14,561 --> 00:36:16,209
좋아요.

434
00:36:16,209 --> 00:36:18,454
이것은 단순하게 보일 수 있습니다.

435
00:36:18,454 --> 00:36:21,124
너도 알다시피, 궤적이 좋다면, 우리는 말하고있다.

436
00:36:21,124 --> 00:36:22,965
여기 모든 행동이 좋았다.

437
00:36:22,965 --> 00:36:23,798
권리?

438
00:36:23,798 --> 00:36:26,356
하지만, 실제로 이것은 평균적으로 아웃됩니다.

439
00:36:26,356 --> 00:36:30,125
그래서 우리는 여기에 편향된 추정자를 가지고 있습니다,

440
00:36:30,125 --> 00:36:32,580
그래서 많은 샘플을 가지고 있다면,

441
00:36:32,580 --> 00:36:35,622
그러면 우리는 그라디언트의 정확한 추정을 얻을 것입니다.

442
00:36:35,622 --> 00:36:38,510
그리고 우리가 바로 잡을 수 있기 때문에 이것은 좋은 일입니다.

443
00:36:38,510 --> 00:36:40,666
그라디언트 단계와 우리는 우리가 될 것을 알고 있습니다.

444
00:36:40,666 --> 00:36:42,994
우리의 손실 기능을 개선하고 가까이에

445
00:36:42,994 --> 00:36:45,976
~, 적어도 우리 지역의 최적 지역

446
00:36:45,976 --> 00:36:48,602
정책 매개 변수 theta.

447
00:36:48,602 --> 00:36:50,690
좋아,하지만 이것에 문제가있다.

448
00:36:50,690 --> 00:36:52,789
문제는 이것이 또한 겪는 것입니다.

449
00:36:52,789 --> 00:36:54,884
높은 분산에서.

450
00:36:54,884 --> 00:36:57,201
이 신용 할당은 정말로 어렵기 때문입니다.

451
00:36:57,201 --> 00:36:58,902
맞습니다.

452
00:36:58,902 --> 00:37:02,283
우리가 얻은 보상을 감안할 때, 우리는

453
00:37:02,283 --> 00:37:04,412
모든 행동은 좋았고, 우리는 희망을 품을뿐입니다.

454
00:37:04,412 --> 00:37:06,537
실제로 어떤 행동이 할당되었는지

455
00:37:06,537 --> 00:37:08,395
중요한 행동,

456
00:37:08,395 --> 00:37:11,080
시간이 지남에 따라 평균을 낼 것입니다.

457
00:37:11,080 --> 00:37:14,598
그래서 이것은 정말로 어렵고 많은 샘플이 필요합니다.

458
00:37:14,598 --> 00:37:17,190
좋은 견적을 얻기 위해서.

459
00:37:17,190 --> 00:37:19,406
알았어. 그래서 이것은 문제가된다.

460
00:37:19,406 --> 00:37:21,684
우리가 분산을 줄이기 위해 할 수있는 일

461
00:37:21,684 --> 00:37:23,851
견적을 향상시킬 수 있습니까?

462
00:37:26,540 --> 00:37:29,123
그래서 분산 감소는

463
00:37:30,164 --> 00:37:33,323
정책 구배의 중요한 연구 분야,

464
00:37:33,323 --> 00:37:36,467
그리고 개선하기 위해 길을 찾아야합니다.

465
00:37:36,467 --> 00:37:39,756
평가자는 더 적은 샘플을 요구한다.

466
00:37:39,756 --> 00:37:41,445
자, 이제 두 가지 아이디어를 살펴 보겠습니다.

467
00:37:41,445 --> 00:37:43,278
우리가 어떻게 할 수 있는지.

468
00:37:44,202 --> 00:37:46,764
주어진 그래디언트 추정기를 보면,

469
00:37:46,764 --> 00:37:49,017
그래서 첫 번째 아이디어는 우리가 할 수있는 것입니다.

470
00:37:49,017 --> 00:37:52,610
행동의 확률을 높인다.

471
00:37:52,610 --> 00:37:56,258
그것만이 미래의 보상에 영향을 미친다.

472
00:37:56,258 --> 00:37:57,091
그 상태에서 나온 거지?

473
00:37:57,091 --> 00:37:59,312
이제 스케일링 대신에

474
00:37:59,312 --> 00:38:02,066
이 가능성, 또는이 가능성을 높임

475
00:38:02,066 --> 00:38:04,736
이 행동의 궤적의 총 보상에 의해,

476
00:38:04,736 --> 00:38:07,320
더 자세히 살펴 보겠습니다.

477
00:38:07,320 --> 00:38:09,876
이 시간 단계에서 오는 보상

478
00:38:09,876 --> 00:38:12,108
마지막에, 맞지?

479
00:38:12,108 --> 00:38:14,224
그리고 이것은 기본적으로

480
00:38:14,224 --> 00:38:17,441
행동이 얼마나 좋은지는 얼마나 많이

481
00:38:17,441 --> 00:38:18,999
그것이 생성하는 미래의 보상.

482
00:38:18,999 --> 00:38:20,499
어떤 의미가 있습니다.

483
00:38:21,811 --> 00:38:24,251
좋아요. 우리가 사용할 수있는 또 다른 아이디어입니다.

484
00:38:24,251 --> 00:38:26,931
순서대로 할인 요소를 사용하고 있습니다.

485
00:38:26,931 --> 00:38:29,448
지연된 영향을 무시한다.

486
00:38:29,448 --> 00:38:33,133
자, 이제 우리는이 할인 요소를 다시 추가했습니다.

487
00:38:33,133 --> 00:38:36,774
우리가 전에 보았던 것, 그것은

488
00:38:36,774 --> 00:38:39,991
우리는 할인 요소가 우리에게 말할 것입니다.

489
00:38:39,991 --> 00:38:41,841
우리가 얼마나 신경을 많이 쓰는지

490
00:38:41,841 --> 00:38:44,510
곧 나올 보상들,

491
00:38:44,510 --> 00:38:47,276
나중에 많이 나온 보상에 비해

492
00:38:47,276 --> 00:38:49,462
맞았 어. 그래서 우리는 지금 갈거야.

493
00:38:49,462 --> 00:38:51,438
행동이 얼마나 좋고 나쁜지 말하라.

494
00:38:51,438 --> 00:38:54,071
이웃에 대한 더 많은 것을보고

495
00:38:54,071 --> 00:38:57,489
가까운 미래에 생성되는 액션 세트

496
00:38:57,489 --> 00:39:00,880
나중에 오는 것들을 가중치를 낮추십시오.

497
00:39:00,880 --> 00:39:02,471
좋아요.

498
00:39:02,471 --> 00:39:05,194
이것들은 몇 가지 직접적인 아이디어이다.

499
00:39:05,194 --> 00:39:07,730
실제로는 일반적으로 사용됩니다.

500
00:39:07,730 --> 00:39:11,529
그래서, 세 번째 아이디어는

501
00:39:11,529 --> 00:39:14,597
분산을 줄이기위한 기준선.

502
00:39:14,597 --> 00:39:18,273
그래서 원시 값을 사용하는 데 문제가 있습니다.

503
00:39:18,273 --> 00:39:20,690
당신의 궤적 중

504
00:39:21,675 --> 00:39:23,869
이것이 반드시 의미있는 것은 아니 겠지?

505
00:39:23,869 --> 00:39:26,653
예를 들어 보상이 모두 긍정적 인 경우,

506
00:39:26,653 --> 00:39:27,973
그러면 계속 밀고 나가겠습니다.

507
00:39:27,973 --> 00:39:29,835
모든 행동의 확률을 높입니다.

508
00:39:29,835 --> 00:39:32,039
그리고 물론, 당신은 그들을 다양한
각도로 밀어 올릴 것입니다,

509
00:39:32,039 --> 00:39:35,448
그러나 정말로 중요한 것은 보상이 더 나은지 아닌지입니다.

510
00:39:35,448 --> 00:39:39,753
또는 당신이 기대하고있는 것보다 나빠질 수 있습니다.

511
00:39:39,753 --> 00:39:42,993
좋아,이 문제를 해결하기 위해 우리는

512
00:39:42,993 --> 00:39:46,071
상태에 의존하는 기준선 함수.

513
00:39:46,071 --> 00:39:47,598
맞습니다. 따라서이 기본 기능은 우리에게

514
00:39:47,598 --> 00:39:51,219
우리의 추측은 무엇이며 우리가 기대하는 것은 무엇입니까?

515
00:39:51,219 --> 00:39:53,886
이 상태에서 벗어나고

516
00:39:55,515 --> 00:39:58,031
우리가 사용할 보상이나 스케일링 요소

517
00:39:58,031 --> 00:39:59,837
우리의 확률을 높이거나 낮추는 것,

518
00:39:59,837 --> 00:40:02,592
이제는 미래 보상의 예상 합계가 될 수 있습니다.

519
00:40:02,592 --> 00:40:05,508
이 기준선을 뺀 것이므로 이제는

520
00:40:05,508 --> 00:40:08,939
훨씬 더 좋든 나쁘 든간에 우리가 얻은 보상이다.

521
00:40:08,939 --> 00:40:10,772
우리가 기대했던 것에서.

522
00:40:11,870 --> 00:40:14,971
그렇다면이 기준선을 어떻게 선택할 수 있습니까?

523
00:40:14,971 --> 00:40:16,099
잘,

524
00:40:16,099 --> 00:40:19,168
매우 간단한 기준선, 사용할 수있는 가장 단순한 기준선,

525
00:40:19,168 --> 00:40:21,065
이동 평균을 취하고있다.

526
00:40:21,065 --> 00:40:23,013
당신이 지금까지 경험 한 보상들.

527
00:40:23,013 --> 00:40:25,027
따라서이 전체 궤적을 수행 할 수도 있습니다.

528
00:40:25,027 --> 00:40:28,863
이것은 보상의 평균에 불과합니다.

529
00:40:28,863 --> 00:40:31,431
내가 훈련을 받고있는 동안 나는보고 있었다.

530
00:40:31,431 --> 00:40:34,765
그리고 나는이 에피소드를 연주하면서?

531
00:40:34,765 --> 00:40:37,549
맞아요, 그래서 이것은

532
00:40:37,549 --> 00:40:41,716
내가 현재받는 보상은 상대적으로 좋았거나 나빴습니다.

533
00:40:42,821 --> 00:40:45,737
그래서 이것을 사용할 수있는 약간의 차이가 있습니다.

534
00:40:45,737 --> 00:40:49,215
그러나 지금까지 우리가 지금까지 보아 왔던 분산 감소

535
00:40:49,215 --> 00:40:51,588
일반적으로 일반적으로 사용되는

536
00:40:51,588 --> 00:40:54,452
"vanilla REINFORCE"알고리즘이라고합니다.

537
00:40:54,452 --> 00:40:56,787
맞아, 미래 누적 보상을 보면,

538
00:40:56,787 --> 00:41:00,954
할인 요인 및 몇 가지 간단한 기준선이 있습니다.

539
00:41:02,601 --> 00:41:05,081
이제 우리가 어떻게 할 수 있는지 이야기 해 봅시다.

540
00:41:05,081 --> 00:41:06,547
이 기본 생각에 대해 생각해보십시오.

541
00:41:06,547 --> 00:41:08,769
잠재적으로 더 나은 기준선을 선택하십시오.

542
00:41:08,769 --> 00:41:12,084
맞습니다. 그래서 우리가 더 나은 것이 무엇인지 생각한다면

543
00:41:12,084 --> 00:41:13,567
우리가 선택할 수있는 기준선,

544
00:41:13,567 --> 00:41:16,569
우리가 원하는 것은 확률을 높이기 위해서입니다.

545
00:41:16,569 --> 00:41:19,931
행동이 더 나은 경우 국가의 행동을

546
00:41:19,931 --> 00:41:24,255
우리가 그 주에서 얻는 것의 기대 가치.

547
00:41:24,255 --> 00:41:27,655
따라서 우리가 기대하는 것의 가치에 대해 생각해보십시오.

548
00:41:27,655 --> 00:41:30,163
주에서, 이것은 당신에게 무엇을 생각 나게합니까?

549
00:41:30,163 --> 00:41:31,189
이것은 당신에게 어떤 것을 생각 나게합니까?

550
00:41:31,189 --> 00:41:34,939
우리가이 강연에서 더 일찍 이야기했던 것?

551
00:41:37,023 --> 00:41:37,856
예.

552
00:41:37,856 --> 00:41:39,266
[관객으로부터 들리지 않음]

553
00:41:39,266 --> 00:41:41,297
그래, 가치 기능이 맞지?

554
00:41:41,297 --> 00:41:45,201
우리가 Q- 러닝과 이야기했던 가치 함수.

555
00:41:45,201 --> 00:41:46,034
그래서, 정확하게.

556
00:41:46,034 --> 00:41:47,871
Q 함수와 값 함수

557
00:41:47,871 --> 00:41:50,895
그래서 직관은

558
00:41:50,895 --> 00:41:52,347
잘,

559
00:41:52,347 --> 00:41:54,704
우리는 행동에 행복하다.

560
00:41:54,704 --> 00:41:58,173
상태에서 액션을 취하는 경우 if

561
00:41:58,173 --> 00:42:00,248
복용 Q-value

562
00:42:00,248 --> 00:42:04,752
이 상태의 특정 작업은

563
00:42:04,752 --> 00:42:06,999
가치 함수 또는 기대 값

564
00:42:06,999 --> 00:42:08,406
누적 미래 보상의

565
00:42:08,406 --> 00:42:09,698
우리가이 상태에서 얻을 수있는 것.

566
00:42:09,698 --> 00:42:11,842
맞아,이 말은이 행동이

567
00:42:11,842 --> 00:42:14,416
우리가 취할 수있는 다른 행동들.

568
00:42:14,416 --> 00:42:17,896
그리고 반대로, 우리는이 행동이 불만이라면,

569
00:42:17,896 --> 00:42:22,063
이 값 또는이 차이가 음이거나 작 으면.

570
00:42:23,917 --> 00:42:27,299
맞아, 이제 우리가 이것을 연결하면,

571
00:42:27,299 --> 00:42:29,269
우리가 원하는만큼의 스케일링 요소로

572
00:42:29,269 --> 00:42:32,692
위로 또는 아래로, 우리의 행동의 가능성을,

573
00:42:32,692 --> 00:42:34,868
그러면 우리는이 견적서를 여기서 얻을 수 있습니다.

574
00:42:34,868 --> 00:42:37,452
그래, 그렇게 될거야.

575
00:42:37,452 --> 00:42:40,168
이전과 정확히 동일하지만 지금은 어디에서

576
00:42:40,168 --> 00:42:43,993
우리는 누적 적으로 기대되는 보상을 받기 전에,

577
00:42:43,993 --> 00:42:46,708
우리의 다양한 감소, 분산 감소

578
00:42:46,708 --> 00:42:50,514
기술과 기준선에, 이제 우리는
지금 막 끼워 넣을 수 있습니다.

579
00:42:50,514 --> 00:42:53,297
이 차이점이

580
00:42:53,297 --> 00:42:57,113
현재의 행동은 Q-function을 기반으로했습니다.

581
00:42:57,113 --> 00:43:00,530
그 상태에서 우리의 가치 함수를 뺀 것입니다.

582
00:43:01,771 --> 00:43:04,148
맞아.하지만 우리가 지금까지 이야기 한 내용은

583
00:43:04,148 --> 00:43:06,993
REINFORCE 알고리즘, 우리는 모른다.

584
00:43:06,993 --> 00:43:09,413
Q와 V는 실제로 무엇입니까.

585
00:43:09,413 --> 00:43:11,313
그래서 우리는 이것들을 배울 수 있습니까?

586
00:43:11,313 --> 00:43:14,479
Q- 러닝을 사용하면 대답은 '예'입니다.

587
00:43:14,479 --> 00:43:16,465
우리가 이미 전에 얘기 한 것.

588
00:43:16,465 --> 00:43:19,828
따라서 정책 구배를 결합 할 수 있습니다.

589
00:43:19,828 --> 00:43:22,210
우리가 방금 Q- 러닝에 대해 이야기하고있는 동안,

590
00:43:22,210 --> 00:43:25,982
정책 인 배우와

591
00:43:25,982 --> 00:43:28,784
비평가, 권리, Q-function,

592
00:43:28,784 --> 00:43:32,366
우리가 국가가 얼마나 좋은지를 우리에게 말해 줄 것입니다.

593
00:43:32,366 --> 00:43:34,380
그리고 한 주에서의 행동.

594
00:43:34,380 --> 00:43:36,964
맞아, 접근법에서 이것을 사용하면,

595
00:43:36,964 --> 00:43:40,633
배우가 취할 조치를 결정할 것입니다.

596
00:43:40,633 --> 00:43:43,716
비평가, 즉 Q-function은
다음과 같이 말할 것입니다.

597
00:43:43,716 --> 00:43:47,708
배우의 행동이 얼마나 좋은지 그리고 어떻게 조정해야하는지.

598
00:43:47,708 --> 00:43:51,072
그래서, 그리고 이것은 또한 약간의 작업을 완화시킵니다.

599
00:43:51,072 --> 00:43:53,636
이 비평가의 Q- 학습 문제와 비교

600
00:43:53,636 --> 00:43:56,694
이전에 우리가 이것을 가지고 있어야한다고 이야기했던 것

601
00:43:56,694 --> 00:43:59,958
모든 국가, 행동 쌍,

602
00:43:59,958 --> 00:44:01,784
왜냐하면 여기 에선 이것을 배우기 만하면되기 때문입니다.

603
00:44:01,784 --> 00:44:04,762
정책에 의해 생성 된 국가 - 행동 쌍을위한

604
00:44:04,762 --> 00:44:06,103
이걸 알면 돼.

605
00:44:06,103 --> 00:44:10,512
이 스케일링 요소를 계산하는 데 중요합니다.

606
00:44:10,512 --> 00:44:12,830
맞습니다. 그리고 우리는 이것을 배울 때,

607
00:44:12,830 --> 00:44:15,196
우리가 본 Q- 학습 트릭을 모두 포함 시키십시오.

608
00:44:15,196 --> 00:44:18,188
경험 리플레이 같은 일찍.

609
00:44:18,188 --> 00:44:20,972
그리고 이제, 저는 또한

610
00:44:20,972 --> 00:44:24,610
앞에서 본 용어를 정의하십시오.

611
00:44:24,610 --> 00:44:28,248
얼마나 많은 일이 얼마나 효과적 이었습니까?

612
00:44:28,248 --> 00:44:30,831
주어진 상태에서 s의 V를 뺀 값?

613
00:44:32,199 --> 00:44:35,533
국가가 얼마나 좋은지에 대한 우리의 기대 가치

614
00:44:35,533 --> 00:44:38,172
이 용어는 우월 함수에 의한 것이다.

615
00:44:38,172 --> 00:44:41,498
맞습니다. 따라서 이점 기능은
얼마나 많은 이점이 있습니까?

616
00:44:41,498 --> 00:44:43,568
우리는이 행동을하지 않았습니까?

617
00:44:43,568 --> 00:44:48,100
그 행동이 예상보다 얼마나 나은지.

618
00:44:48,100 --> 00:44:51,709
그래서, 이것을 사용하여 우리는

619
00:44:51,709 --> 00:44:53,457
배우 비평 알고리즘.

620
00:44:53,457 --> 00:44:56,279
그리고 이것이 어떻게 생겼는지는 우리가 시작하려고합니다.

621
00:44:56,279 --> 00:45:00,326
우리의 정책 매개 변수 인 theta를 초기화함으로써

622
00:45:00,326 --> 00:45:03,689
그리고 우리가 phi라고 부르는 비평가 매개 변수.

623
00:45:03,689 --> 00:45:07,522
그리고 각각에 대해, 훈련의 반복을 위해,

624
00:45:08,401 --> 00:45:11,149
우리는 M 궤적을 샘플링하려고합니다.

625
00:45:11,149 --> 00:45:12,185
현 정책 하에서

626
00:45:12,185 --> 00:45:13,734
그렇습니다. 우리는 정책을 실행하고

627
00:45:13,734 --> 00:45:18,725
궤적을 s-zero, a-zero,
r-zero, s-one 등으로 정의합니다.

628
00:45:18,725 --> 00:45:20,359
좋아, 그럼 우리는 계산할거야.

629
00:45:20,359 --> 00:45:21,671
우리가 원하는 그라디언트.

630
00:45:21,671 --> 00:45:24,977
맞아, 그래서이 궤도마다

631
00:45:24,977 --> 00:45:26,017
매 시간마다 우리는 가고 있습니다.

632
00:45:26,017 --> 00:45:28,901
이 우세 함수를 계산하려면,

633
00:45:28,901 --> 00:45:30,818
그리고 나서 우리는

634
00:45:31,701 --> 00:45:33,465
이 우위 함수를 사용 하시겠습니까?

635
00:45:33,465 --> 00:45:37,131
그런 다음 그라디언트 추정기에서이를 사용할 것입니다.

636
00:45:37,131 --> 00:45:40,533
우리가 이전에 보여 줬던

637
00:45:40,533 --> 00:45:42,894
우리가 여기서 가지고있는 기울기 추정치.

638
00:45:42,894 --> 00:45:46,017
그리고 우리는 또한

639
00:45:46,017 --> 00:45:50,837
비평가 매개 변수 phi는 정확히 같은 방법으로,

640
00:45:50,837 --> 00:45:54,193
우리가 이전에 보았던 것처럼, 기본적으로

641
00:45:54,193 --> 00:45:57,557
이 가치 함수는 바로 우리의 가치 함수를 배우고,

642
00:45:57,557 --> 00:46:01,640
끌어 들이기 만하면됩니다.

643
00:46:02,638 --> 00:46:05,467
이 이점 기능과 이것은

644
00:46:05,467 --> 00:46:08,324
벨맨 방정식에 더 가까워 지도록 격려하십시오.

645
00:46:08,324 --> 00:46:10,347
우리가 더 일찍 보았던, 그렇지?

646
00:46:10,347 --> 00:46:14,347
그리고 이것은 기본적으로 다음과 같이 반복됩니다.

647
00:46:15,197 --> 00:46:17,733
정책 기능의 학습 및 최적화,

648
00:46:17,733 --> 00:46:20,211
뿐만 아니라 우리의 평론 기능.

649
00:46:20,211 --> 00:46:22,311
그리고 나서 우리는

650
00:46:22,311 --> 00:46:23,977
그라디언트 그리고 나서 우리는 끝까지 갈 것입니다.

651
00:46:23,977 --> 00:46:26,727
이 과정을 계속 반복하십시오.

652
00:46:29,271 --> 00:46:31,827
자, 이제 REINFORCE의 몇
가지 예를 살펴 보겠습니다.

653
00:46:31,827 --> 00:46:36,027
행동을 취하고, 먼저 여기에서 무엇인가 불리는 것을 보자.

654
00:46:36,027 --> 00:46:39,464
재발주의 모델 (Recurrent
Attention Model)은,

655
00:46:39,464 --> 00:46:42,805
그것은 하드주의라고도 불리는 모델이며,

656
00:46:42,805 --> 00:46:46,876
하지만 최근에 컴퓨터 비전에서 많이 보게 될 것입니다.

657
00:46:46,876 --> 00:46:49,146
다양한 목적을위한 작업.

658
00:46:49,146 --> 00:46:51,806
맞아요. 그래서이 아이디어는

659
00:46:51,806 --> 00:46:55,122
여기, 나는 열심히주의를 기울인
원래의 작품에 대해 이야기했다.

660
00:46:55,122 --> 00:46:59,167
이미지 분류가 이루어지며 목표는

661
00:46:59,167 --> 00:47:02,504
여전히 이미지 클래스를 예측하기 위해,

662
00:47:02,504 --> 00:47:04,822
하지만 이제는 시퀀스를 취하여이 작업을 수행 할 것입니다.

663
00:47:04,822 --> 00:47:06,494
이미지 주변의 흘끗 보임.

664
00:47:06,494 --> 00:47:10,300
당신은 이미지 주변의 지역을 볼 것입니다.

665
00:47:10,300 --> 00:47:12,754
기본적으로 이들에 선택적으로 집중하려고합니다.

666
00:47:12,754 --> 00:47:17,141
부품을 찾고 주변을 둘러 보면서 정보를 구축하십시오.

667
00:47:17,141 --> 00:47:19,382
맞아, 우리가 이걸하고 싶어하는 이유 야.

668
00:47:19,382 --> 00:47:21,638
음, 우선, 좋은 영감을 얻었습니다.

669
00:47:21,638 --> 00:47:24,551
안구 운동에서 인간의 지각으로부터.

670
00:47:24,551 --> 00:47:26,869
우리가 복잡한 이미지를보고 있다고 가정 해 봅시다.

671
00:47:26,869 --> 00:47:29,225
우리는 이미지에 무엇이 있는지를 결정하려고합니다.

672
00:47:29,225 --> 00:47:31,594
음, 저기, 우리는 아마도 저해상도를 보았을 것입니다.

673
00:47:31,594 --> 00:47:34,013
먼저 부품을 살펴본 다음 부품을 자세히 살펴보십시오.

674
00:47:34,013 --> 00:47:36,913
우리에게 단서를 줄 이미지의

675
00:47:36,913 --> 00:47:39,168
이 이미지에 무엇이 있는가.

676
00:47:39,168 --> 00:47:40,001
그리고,

677
00:47:41,160 --> 00:47:45,703
이 방법은 이미지를보고 둘러 보는 것입니다.

678
00:47:45,703 --> 00:47:48,533
지역별로도 도움을 줄 것입니다.

679
00:47:48,533 --> 00:47:50,435
전산 자원, 맞죠?

680
00:47:50,435 --> 00:47:53,293
전체 이미지를 처리 할 필요가 없습니다.

681
00:47:53,293 --> 00:47:55,366
실제로, 일반적으로 발생하는 것은

682
00:47:55,366 --> 00:47:57,511
전체 이미지의 저해상도 이미지 먼저,

683
00:47:57,511 --> 00:48:01,678
시작하는 방법을 결정한 다음 고해상도로 살펴 봅니다.

684
00:48:02,773 --> 00:48:04,671
그 이후의 이미지 부분.

685
00:48:04,671 --> 00:48:06,979
따라서 많은 계산 자원을 절약 할 수 있습니다.

686
00:48:06,979 --> 00:48:09,725
그러면이 이점을 생각해 볼 수 있습니다.

687
00:48:09,725 --> 00:48:11,927
확장 성, 오른쪽, 말할 수있는 것, 말하자면

688
00:48:11,927 --> 00:48:15,177
더 큰 이미지를보다 효율적으로 처리합니다.

689
00:48:16,164 --> 00:48:17,780
그리고 마지막으로 이것은 실제로 도움이 될 수 있습니다.

690
00:48:17,780 --> 00:48:20,099
실제 분류 성능으로,

691
00:48:20,099 --> 00:48:21,855
이제 당신이 할 수 있기 때문에

692
00:48:21,855 --> 00:48:24,760
이미지의 혼란스럽고 관련없는 부분은 무시하십시오.

693
00:48:24,760 --> 00:48:25,593
권리?

694
00:48:25,593 --> 00:48:27,678
항상 그렇듯이

695
00:48:27,678 --> 00:48:29,931
귀하의 ConvNet, 귀하의 이미지의 모든 부분,

696
00:48:29,931 --> 00:48:32,846
당신은 이것을 사용해서 어쩌면 먼저

697
00:48:32,846 --> 00:48:34,936
내가 실제로 처리하고자하는 관련 부품,

698
00:48:34,936 --> 00:48:36,353
내 ConvNet 사용.

699
00:48:37,237 --> 00:48:39,849
자, 강화 학습은 무엇입니까?

700
00:48:39,849 --> 00:48:41,531
이 문제의 공식화?

701
00:48:41,531 --> 00:48:44,711
글쎄, 우리 주정부가 될거야.

702
00:48:44,711 --> 00:48:46,889
우리가 지금까지 보았던 엿볼은 맞습니까?

703
00:48:46,889 --> 00:48:47,722
우리의

704
00:48:48,881 --> 00:48:51,117
우리가 본 정보는 무엇입니까?

705
00:48:51,117 --> 00:48:53,643
우리의 행동은

706
00:48:53,643 --> 00:48:55,228
이미지에서 다음을 볼 수 있습니다.

707
00:48:55,228 --> 00:48:57,090
맞아요, 실제로, 이것은 뭔가가 될 수 있습니다.

708
00:48:57,090 --> 00:48:59,113
x, y 좌표는 어쩌면 일부를 중심으로

709
00:48:59,113 --> 00:49:02,842
당신이 다음에보고 싶은 고정 크기의 엿볼.

710
00:49:02,842 --> 00:49:05,664
그리고 나서 분류 문제에 대한 보상

711
00:49:05,664 --> 00:49:08,256
마지막 시간 단계에서 하나가 될 것입니다.

712
00:49:08,256 --> 00:49:12,423
우리의 이미지가 정확하게 분류되면,
그렇지 않으면 0이됩니다.

713
00:49:14,495 --> 00:49:16,162
그래서,

714
00:49:17,373 --> 00:49:20,016
이미지를 한 눈에 보면서

715
00:49:20,016 --> 00:49:21,932
비차별적인 연산이므로,

716
00:49:21,932 --> 00:49:24,006
이것이 왜 우리가

717
00:49:24,006 --> 00:49:25,761
보강 학습 배합,

718
00:49:25,761 --> 00:49:29,088
이러한 엿볼 수있는 행동을 취하는
방법에 대한 정책을 배우십시오.

719
00:49:29,088 --> 00:49:31,792
우리는 REINFORCE를 사용하여 이것을 훈련시킬 수 있습니다.

720
00:49:31,792 --> 00:49:35,105
그래서 지금까지 흘끗 보인 상태를 보면,

721
00:49:35,105 --> 00:49:37,537
우리 모델의 핵심은

722
00:49:37,537 --> 00:49:40,891
이 RNN은 우리가 국가를
모델링하기 위해 사용할 예정이며,

723
00:49:40,891 --> 00:49:44,501
정책 매개 변수를 사용하려고합니다.

724
00:49:44,501 --> 00:49:47,418
다음 동작을 출력합니다.

725
00:49:49,354 --> 00:49:53,248
좋아, 그래서이 모델이 어떻게
생겼는지 우리가 취할 것입니다.

726
00:49:53,248 --> 00:49:54,571
입력 이미지.

727
00:49:54,571 --> 00:49:57,655
그렇습니다. 그러면 우리는이 이미지를 엿볼 것입니다.

728
00:49:57,655 --> 00:50:00,068
여기이 빨간 상자가 여기 있습니다.

729
00:50:00,068 --> 00:50:03,184
이것은 모두 공백, 0입니다.

730
00:50:03,184 --> 00:50:06,966
그래서 우리는 지금까지 우리가 보았던
것을 몇 가지로 전달할 것입니다.

731
00:50:06,966 --> 00:50:09,388
신경 네트워크, 그리고 이것은 어떤 것도 될 수 있습니다.

732
00:50:09,388 --> 00:50:12,193
귀하의 작업에 따라 종류의 네트워크.

733
00:50:12,193 --> 00:50:14,276
내가 여기서 보여주고있는 원래의 실험에서,

734
00:50:14,276 --> 00:50:16,138
MNIST에서 이것은 매우 간단합니다.

735
00:50:16,138 --> 00:50:18,758
작은 두 개의 완전히 연결된 레이어를 사용하고,

736
00:50:18,758 --> 00:50:21,724
그러나 더 복잡한 이미지를 상상할 수 있습니다.

737
00:50:21,724 --> 00:50:26,105
및 기타 작업을 ConvNets를
사용하기를 원할 수 있습니다.

738
00:50:26,105 --> 00:50:28,775
맞아요. 그래서 이것을 당신이 신경망에 넣었습니다.

739
00:50:28,775 --> 00:50:31,065
그리고 나서 우리는 또한 우리가 될
것이라고 말했던 것을 기억하십시오.

740
00:50:31,065 --> 00:50:34,102
우리의 상태를 통합하고, 우리가 본 흘긋을

741
00:50:34,102 --> 00:50:36,115
지금까지 재귀 네트워크를 사용했습니다.

742
00:50:36,115 --> 00:50:38,057
그래서, 난 그냥 갈거야.

743
00:50:38,057 --> 00:50:40,265
우리는 나중에 그것을 보게 될 것입니다,
그러나 이것은 그것을 통과 할 것입니다,

744
00:50:40,265 --> 00:50:42,646
그런 다음 출력 할 것입니다.

745
00:50:42,646 --> 00:50:46,094
x, y 좌표, 다음에 볼 곳.

746
00:50:46,094 --> 00:50:48,435
실제로, 이것은 될 것입니다.

747
00:50:48,435 --> 00:50:50,766
우리는 행동에 대한 분포를 출력하고자합니다.

748
00:50:50,766 --> 00:50:53,385
맞아요, 그러면이게 될 일이 될거야.

749
00:50:53,385 --> 00:50:57,282
가우시안 분포와 우리는 평균을 출력 할 것입니다.

750
00:50:57,282 --> 00:50:59,084
평균과 분산을 출력 할 수도 있습니다.

751
00:50:59,084 --> 00:51:00,545
실제로이 배포판의

752
00:51:00,545 --> 00:51:03,944
분산도 고정 될 수 있습니다.

753
00:51:03,944 --> 00:51:07,172
좋아요, 그래서 우리는 이것을 취할 것입니다.

754
00:51:07,172 --> 00:51:08,496
우리가 지금 샘플로 할 행동

755
00:51:08,496 --> 00:51:11,854
액션 배포의 특정 x, y 위치

756
00:51:11,854 --> 00:51:15,457
그리고 우리는 이것을 다음에 얻기 위해 넣을 것입니다.

757
00:51:15,457 --> 00:51:17,777
우리의 이미지에서 다음 엿볼을 추출하십시오.

758
00:51:17,777 --> 00:51:19,297
맞아, 여기 우리가 움직였다.

759
00:51:19,297 --> 00:51:23,385
두 사람의 끝에, 두 사람의 꼬리 부분.

760
00:51:23,385 --> 00:51:25,465
그리고 이제 우리는 실제로 어떤 신호를 받기 시작합니다.

761
00:51:25,465 --> 00:51:26,745
우리가보고 싶은 것, 맞죠?

762
00:51:26,745 --> 00:51:29,065
마찬가지로 우리가 원하는 것은 관련성을 조사하는 것입니다.

763
00:51:29,065 --> 00:51:32,724
분류에 유용한 이미지의 부분.

764
00:51:32,724 --> 00:51:35,354
그래서 우리는 이것을 다시 우리의
신경망 층으로 통과 시키며,

765
00:51:35,354 --> 00:51:37,104
그리고 나서

766
00:51:38,153 --> 00:51:40,362
우리가 되풀이하는 네트워크, 맞아.

767
00:51:40,362 --> 00:51:43,642
이 이전의 숨겨진 상태뿐 아니라

768
00:51:43,642 --> 00:51:45,524
이것을 사용하여,

769
00:51:45,524 --> 00:51:47,343
이것은 우리의 정책을 대표합니다.

770
00:51:47,343 --> 00:51:49,565
우리는 이것을 출력에 사용하려고합니다.

771
00:51:49,565 --> 00:51:51,354
우리의 다음 배포판

772
00:51:51,354 --> 00:51:54,095
우리가 엿보고 싶은 위치.

773
00:51:54,095 --> 00:51:55,874
그래서 우리는 이것을 계속할 수 있습니다.

774
00:51:55,874 --> 00:51:57,303
당신은이 다음에 여기에서 엿볼 수 있습니다.

775
00:51:57,303 --> 00:51:59,903
우리는 둘의 중심을 향해 조금 더 움직였습니다.

776
00:51:59,903 --> 00:52:01,723
알았어. 그래서 아마 그걸 배울거야.

777
00:52:01,723 --> 00:52:05,005
너는 알다시피, 나는이 두 꼬리 부분을 보았을 때,

778
00:52:05,005 --> 00:52:08,093
이 모양이 어쩌면이 어퍼에서 움직일 수도 있습니다.

779
00:52:08,093 --> 00:52:10,794
왼손 방향을 사용하면

780
00:52:10,794 --> 00:52:12,631
가치가있는 센터,

781
00:52:12,631 --> 00:52:14,543
귀중한 정보.

782
00:52:14,543 --> 00:52:17,473
그런 다음 계속해서 할 수 있습니다.

783
00:52:17,473 --> 00:52:20,612
그리고 마침내, 결국, 마지막 단계에서,

784
00:52:20,612 --> 00:52:23,412
그래서 여기에 고정 된 시간 간격을 가질 수 있습니다.

785
00:52:23,412 --> 00:52:26,795
실제로 6 또는 8과 같은 것입니다.

786
00:52:26,795 --> 00:52:29,359
그리고 마지막 시간 단계에서, 우리가하고 싶기 때문에

787
00:52:29,359 --> 00:52:33,350
분류, 우리는 우리의 표준

788
00:52:33,350 --> 00:52:36,100
Softmax 레이어는

789
00:52:37,376 --> 00:52:39,363
각 클래스에 대한 확률 분포.

790
00:52:39,363 --> 00:52:42,111
그리고 나서 여기 최대 클래스는 2,

791
00:52:42,111 --> 00:52:44,108
그래서 우리는 이것이 2라고 예측할 수 있습니다.

792
00:52:44,108 --> 00:52:46,558
맞아요, 그리고 이것은 우리의 셋업이 될 것입니다.

793
00:52:46,558 --> 00:52:50,428
모델과 우리의 정책을

794
00:52:50,428 --> 00:52:53,079
우리가 가지고있는이 정책의 기울기에 대한 추정치

795
00:52:53,079 --> 00:52:54,420
이전에 우리는

796
00:52:54,420 --> 00:52:56,695
여기에서 궤적

797
00:52:56,695 --> 00:52:59,569
그걸 사용하여 소품을 되 찾으십시오.

798
00:52:59,569 --> 00:53:02,819
그래서 우리는이 모델을 훈련시키기
위해서 이것을 할 수 있습니다.

799
00:53:02,819 --> 00:53:05,281
우리 정책의 매개 변수를 배우 죠?

800
00:53:05,281 --> 00:53:08,698
여기에서 볼 수있는 모든 무게.

801
00:53:09,953 --> 00:53:10,786
좋아요.

802
00:53:12,239 --> 00:53:14,270
여기에

803
00:53:14,270 --> 00:53:16,710
MNIST에서 교육 된 정책,

804
00:53:16,710 --> 00:53:19,016
그래서 당신은 일반적으로,

805
00:53:19,016 --> 00:53:20,808
어디에서 시작하든, 보통 배우게됩니다.

806
00:53:20,808 --> 00:53:22,942
자리가 더 가까이 갈 수 있도록

807
00:53:22,942 --> 00:53:25,260
그런 다음 해당 자릿수의 관련 부분을 살펴보십시오.

808
00:53:25,260 --> 00:53:27,685
그래서 이것은 매우 차갑고

809
00:53:27,685 --> 00:53:28,744
이

810
00:53:28,744 --> 00:53:30,460
너도 알다시피, 네가 기대하는 바를 따른다. 맞다.

811
00:53:30,460 --> 00:53:31,627
너라면

812
00:53:33,335 --> 00:53:34,967
다음에 볼 장소를 선택하십시오.

813
00:53:34,967 --> 00:53:38,186
가장 효율적으로 결정하기 위해

814
00:53:38,186 --> 00:53:40,108
이게 무슨 자리 지.

815
00:53:40,108 --> 00:53:43,491
맞아, 그래서 열심히주의를 기울이는이 아이디어는,

816
00:53:43,491 --> 00:53:45,862
재발주의 모델의 사용, 또한 사용되었습니다

817
00:53:45,862 --> 00:53:49,758
컴퓨터 비전의 많은 작업에서 마지막으로

818
00:53:49,758 --> 00:53:52,687
2 년이 지나면 이것을 볼 수 있습니다. 예를 들어,

819
00:53:52,687 --> 00:53:54,790
미세한 이미지 인식.

820
00:53:54,790 --> 00:53:57,869
그래서 나는 이전에 언급했다.

821
00:53:57,869 --> 00:54:00,596
이 유용한 이점 중 하나

822
00:54:00,596 --> 00:54:01,763
~ 할 수도있다.

823
00:54:02,975 --> 00:54:05,198
계산 효율을 줄여 준다.

824
00:54:05,198 --> 00:54:08,009
혼란과 관련성을 무시하는 것

825
00:54:08,009 --> 00:54:10,180
이미지의 일부분, 그리고 세분화 된 이미지

826
00:54:10,180 --> 00:54:11,750
이미지 분류 문제,

827
00:54:11,750 --> 00:54:13,092
당신은 대개 두 가지 모두를 원합니다.

828
00:54:13,092 --> 00:54:17,307
당신은 고해상도를 유지하기를 원합니다.

829
00:54:17,307 --> 00:54:19,447
중요한 차이점이 있습니다.

830
00:54:19,447 --> 00:54:23,327
그런 다음이 차이에 초점을 맞추기를 원합니다.

831
00:54:23,327 --> 00:54:25,777
무의미한 부분은 무시하십시오.

832
00:54:25,777 --> 00:54:27,359
그래, 질문.

833
00:54:27,359 --> 00:54:31,526
[청중의 들리지 않는 질문]

834
00:54:35,061 --> 00:54:36,789
그래, 그래, 그 질문은

835
00:54:36,789 --> 00:54:39,061
어떻게 계산 효율이 있는지,

836
00:54:39,061 --> 00:54:41,482
왜냐하면 우리는 또한이 재발 성 신경망을
제 위치에 가지고 있기 때문입니다.

837
00:54:41,482 --> 00:54:45,842
그렇습니다. 그것은 당신의 것이 무엇인지에 달려 있습니다.

838
00:54:45,842 --> 00:54:47,761
당신의 문제는 무엇이고, 당신의
네트워크는 무엇입니까, 등등,

839
00:54:47,761 --> 00:54:50,151
하지만 네가 정말로

840
00:54:50,151 --> 00:54:52,512
고해상도 이미지

841
00:54:52,512 --> 00:54:54,773
이 모든 부분을 처리하고 싶지는 않습니다.

842
00:54:54,773 --> 00:54:58,477
이미지 일부 거대한 ConvNet
또는 일부 거대한, 당신도 알다시피,

843
00:54:58,477 --> 00:55:01,900
네트워크를 통해 이제는 비용을 절약 할 수 있습니다.

844
00:55:01,900 --> 00:55:04,669
이미지의 특정 작은 부분에 초점을 맞 춥니 다.

845
00:55:04,669 --> 00:55:06,589
이미지의 해당 부분 만 처리합니다.

846
00:55:06,589 --> 00:55:08,507
그러나, 당신은 옳습니다.

847
00:55:08,507 --> 00:55:10,924
당신이 가지고있는 문제 설정.

848
00:55:12,210 --> 00:55:14,530
이것은 또한 이미지 캡션에 사용되었습니다.

849
00:55:14,530 --> 00:55:17,138
그래서 우리가 이미지에 대한 캡션을 생성하려고한다면,

850
00:55:17,138 --> 00:55:20,421
우리가 선택할 수 있습니다, 당신도 알다시피,
우리는 이미지를 가질 수 있습니다.

851
00:55:20,421 --> 00:55:23,197
이 캡션을 생성하려면이주의 모델을 사용하십시오.

852
00:55:23,197 --> 00:55:26,120
일반적으로 학습을 끝내는 것은 이러한 정책입니다.

853
00:55:26,120 --> 00:55:28,999
여기서 이미지의 특정 부분에 초점을 맞 춥니 다.

854
00:55:28,999 --> 00:55:31,850
순서대로, 그리고 그것이 각 부분에 초점을 맞추면서,

855
00:55:31,850 --> 00:55:34,629
캡션의 일부 또는 일부를 생성합니다.

856
00:55:34,629 --> 00:55:38,341
이미지의 해당 부분을 참조하십시오.

857
00:55:38,341 --> 00:55:40,170
그리고 나서 그것은 또한 사용되었습니다,

858
00:55:40,170 --> 00:55:42,948
시각적 질문 응답,

859
00:55:42,948 --> 00:55:45,509
이미지에 대한 질문을합니다.

860
00:55:45,509 --> 00:55:48,981
모델이 어떤 대답을 출력하기를 원한다.

861
00:55:48,981 --> 00:55:51,786
당신의 질문에, 예를 들면, 나는 모른다.

862
00:55:51,786 --> 00:55:53,840
테이블 주위에 몇 개의 의자가 있습니까?

863
00:55:53,840 --> 00:55:58,229
그래서 당신은이주의 메커니즘이
어떻게 보이는지 볼 수 있습니다.

864
00:55:58,229 --> 00:55:59,457
좋은 유형의 모델 일 수 있습니다.

865
00:55:59,457 --> 00:56:03,040
이 질문에 대답하는 법을 배우기.

866
00:56:05,707 --> 00:56:08,475
그래, 그게 정책 구배의 예 였어.

867
00:56:08,475 --> 00:56:10,564
이 하드 어텐션 모델에서.

868
00:56:10,564 --> 00:56:13,524
그래서 저는 이제 하나의 예를 더 이야기 할 것입니다.

869
00:56:13,524 --> 00:56:16,430
또한 정책 그라디언트를 사용합니다.

870
00:56:16,430 --> 00:56:18,465
그것은 Go를하는 법을 배우고 있습니다.

871
00:56:18,465 --> 00:56:22,006
맞아, DeepMind가이 요원을 가졌어.

872
00:56:22,006 --> 00:56:24,497
알프 고 (AlphGo)라고 불리는 고 (Go)

873
00:56:24,497 --> 00:56:27,297
뉴스에 많이 등장했습니다.

874
00:56:27,297 --> 00:56:30,708
작년과 올해에.

875
00:56:30,708 --> 00:56:31,541
죄송 해요?

876
00:56:31,541 --> 00:56:32,374
[청중의 들리지 않는 의견]

877
00:56:32,374 --> 00:56:35,291
그리고 어제, 맞습니다.

878
00:56:36,172 --> 00:56:39,258
이것은 매우 흥미롭고, 최근 소식입니다.

879
00:56:39,258 --> 00:56:40,987
그래서 작년에,

880
00:56:40,987 --> 00:56:43,234
AlphaGo의 첫 번째 버전

881
00:56:43,234 --> 00:56:44,817
~에 들어갔다.

882
00:56:46,678 --> 00:56:49,539
최고의 고 (Go) 선수들과의 경쟁

883
00:56:49,539 --> 00:56:52,609
최근 몇 년 동안, Lee Sedol과 대리인

884
00:56:52,609 --> 00:56:54,927
그를 이길 수 있었다.

885
00:56:54,927 --> 00:56:57,886
5 경기 동안 4 대 1 경기를 펼쳤습니다.

886
00:56:57,886 --> 00:57:00,541
그리고 사실, 지금 당장, 그냥

887
00:57:00,541 --> 00:57:03,788
Ke Jie와의 또 다른 경기가 있습니다.

888
00:57:03,788 --> 00:57:07,855
세계 제일의 숫자이기 때문에 3 개 중 최고입니다.

889
00:57:07,855 --> 00:57:09,348
지금 중국에서.

890
00:57:09,348 --> 00:57:12,236
그래서 첫 경기가 어제였습니다.

891
00:57:12,236 --> 00:57:13,436
AlphaGo 이겼다.

892
00:57:13,436 --> 00:57:16,596
나는 그것이 단지 반점에 의한 것이라고 생각한다. 그래서,

893
00:57:16,596 --> 00:57:18,606
그래서 두 가지 더 볼 게임이 있습니다.

894
00:57:18,606 --> 00:57:20,657
이들은 모두 라이브 스트림이므로

895
00:57:20,657 --> 00:57:24,276
너희들도 온라인에 가서이 게임들을 봐야 해.

896
00:57:24,276 --> 00:57:28,193
해설을 듣는 것은 꽤 흥미 롭습니다.

897
00:57:29,225 --> 00:57:32,577
그러나 DeepMind의 AlphaGo
에이전트는 무엇입니까?

898
00:57:32,577 --> 00:57:34,868
그리고 그것은 우리가 말한 것을 많이 기반으로합니다.

899
00:57:34,868 --> 00:57:36,466
지금까지는이 강의에서.

900
00:57:36,466 --> 00:57:39,687
그리고 그것은 감독 학습의 혼합입니다.

901
00:57:39,687 --> 00:57:42,045
강화 학습,

902
00:57:42,045 --> 00:57:44,657
뿐만 아니라 이전 버전과

903
00:57:44,657 --> 00:57:48,573
이동, 몬테카를로 트리 검색,

904
00:57:48,573 --> 00:57:51,656
최근의 심층 RL 접근 방식에 대해서도 설명합니다.

905
00:57:52,579 --> 00:57:56,986
그렇다면 AlphaGo는 Go
세계 챔피언을 어떻게 상회합니까?

906
00:57:56,986 --> 00:57:59,363
음, 처음에는 무엇을합니까?

907
00:57:59,363 --> 00:58:02,449
입력이 될 때 AlphaGo를 훈련 시키는데

908
00:58:02,449 --> 00:58:04,089
보드의 몇 가지 특징 화.

909
00:58:04,089 --> 00:58:06,470
기본적으로, 당신의 이사회와 직위입니다.

910
00:58:06,470 --> 00:58:08,739
보드에 조각의.

911
00:58:08,739 --> 00:58:10,739
그것이 당신의 자연스러운 모습입니다.

912
00:58:10,739 --> 00:58:13,819
성과를 향상시키기 위해 그들이하는 일

913
00:58:13,819 --> 00:58:16,638
조금은 그들이 이것을 실현하는 것입니다.

914
00:58:16,638 --> 00:58:17,958
약간

915
00:58:17,958 --> 00:58:20,510
하나의 더 많은 채널은 모두 다른 돌 색깔입니다,

916
00:58:20,510 --> 00:58:21,956
그래서 이건 너 같은 종류 야.

917
00:58:21,956 --> 00:58:23,790
보드 구성.

918
00:58:23,790 --> 00:58:27,270
또한 일부 채널, 예를 들어, 어디서, 어떤 움직임

919
00:58:27,270 --> 00:58:31,138
합법적 인 것, 일부 바이어스 채널, 몇 가지 다양한 것들

920
00:58:31,138 --> 00:58:33,125
그런 다음이 상태가 주어지면,

921
00:58:33,125 --> 00:58:35,117
그것은 첫 번째로 갈 것입니다.

922
00:58:35,117 --> 00:58:36,518
네트워크를 훈련하다

923
00:58:36,518 --> 00:58:38,867
감독 교육을 통해 초기화됩니다.

924
00:58:38,867 --> 00:58:40,897
전문 이동 게임에서.

925
00:58:40,897 --> 00:58:43,147
따라서, 현재의 보드 구성이 주어지면

926
00:58:43,147 --> 00:58:45,627
또는 특징, 이것의 특징 화,

927
00:58:45,627 --> 00:58:48,495
올바른 다음 조치는 무엇입니까?

928
00:58:48,495 --> 00:58:50,678
좋아, 그렇게 주어진다.

929
00:58:50,678 --> 00:58:52,787
전문 게임의 예,

930
00:58:52,787 --> 00:58:55,608
알다시피, 시간이 지남에 따라 수집됩니다.

931
00:58:55,608 --> 00:58:57,635
우리는이 모든 전문적인 움직임을 취할 수 있습니다.

932
00:58:57,635 --> 00:58:59,815
표준, 감독지도 작성,

933
00:58:59,815 --> 00:59:02,605
보드 상태에서 취할 조치.

934
00:59:02,605 --> 00:59:05,365
알았어. 그래서 그들은 이것을 취한다.
이것은 꽤 좋은 시작이다.

935
00:59:05,365 --> 00:59:07,637
그리고 나서 그들은 이것을 사용하여 초기화 할 것입니다.

936
00:59:07,637 --> 00:59:09,227
정책 네트워크.

937
00:59:09,227 --> 00:59:10,844
맞아요, 정책 네트워크 때문에, 그냥 취할 것입니다.

938
00:59:10,844 --> 00:59:14,985
정확한 입력 구조는 귀하의

939
00:59:14,985 --> 00:59:16,389
보드 상태와 출력이

940
00:59:16,389 --> 00:59:17,778
당신이 취할 행동.

941
00:59:17,778 --> 00:59:20,887
그리고 이것은 정책 그라디언트를위한 준비였습니다.

942
00:59:20,887 --> 00:59:21,978
우리가 방금 본 거 맞지?

943
00:59:21,978 --> 00:59:25,156
이제 우리는이 훈련을 계속할 것입니다.

944
00:59:25,156 --> 00:59:27,130
정책 그라디언트 사용

945
00:59:27,130 --> 00:59:30,831
그리고이 강화 학습 학습을 할 것입니다.

946
00:59:30,831 --> 00:59:35,123
랜덤, 이전 반복을 위해 자체적으로 게임을합니다.

947
00:59:35,123 --> 00:59:37,384
그래서 자기 희생과 보상을 얻을 것입니다.

948
00:59:37,384 --> 00:59:42,243
그것이 이기면 하나이고, 잃으면 음수가됩니다.

949
00:59:42,243 --> 00:59:44,624
그리고 우리가하려고하는 것은 또한 우리도 배울 것입니다.

950
00:59:44,624 --> 00:59:47,573
가치 네트워크, 그래서 비평가 같은 것.

951
00:59:47,573 --> 00:59:51,235
그리고 나서, 최종 AlphaGo가 결합 될 것입니다.

952
00:59:51,235 --> 00:59:53,982
이 모든 것들이 함께 있으므로 정책과 가치 네트워크

953
00:59:53,982 --> 00:59:56,075
뿐만 아니라

954
00:59:56,075 --> 00:59:59,043
몬테카를로 트리 검색 알고리즘을 선택하려면

955
00:59:59,043 --> 01:00:01,475
미리보기 검색을 통한 작업.

956
01:00:01,475 --> 01:00:04,743
맞아,이 모든 것을 모아서,

957
01:00:04,743 --> 01:00:08,853
당신이 놀고있는 노드의 값,

958
01:00:08,853 --> 01:00:11,590
그리고 당신이 다음에하는 일은 조합이 될 것입니다.

959
01:00:11,590 --> 01:00:13,811
당신의 가치 함수의

960
01:00:13,811 --> 01:00:16,552
표준에서 계산 한 결과에서 롤아웃

961
01:00:16,552 --> 01:00:19,891
Monte Carlo Tree 검색 롤 아웃.

962
01:00:19,891 --> 01:00:22,891
그래, 근데, 근데 기본적으로

963
01:00:24,203 --> 01:00:27,453
다양한, AlphaGo의 구성 요소.

964
01:00:28,397 --> 01:00:30,314
이것에 대해 더 많이 읽고 싶다면,

965
01:00:30,314 --> 01:00:33,814
2016 년에 이것에 대한 자연 종이가 있습니다.

966
01:00:34,664 --> 01:00:37,656
그리고 그들은 이것을 훈련 시켰습니다.

967
01:00:37,656 --> 01:00:40,765
이 버전에서 사용되는 AlphaGo 버전

968
01:00:40,765 --> 01:00:45,016
경기는 몇 천 개의 CPU가 있다고 생각합니다.

969
01:00:45,016 --> 01:00:47,953
몇 백 개의 GPU를 더하여이 모든 것을 하나로 모으고,

970
01:00:47,953 --> 01:00:52,120
그래서 그것은 엄청난 양의 훈련입니다.

971
01:00:55,659 --> 01:00:57,514
그리고 네, 그렇게해야합니다.

972
01:00:57,514 --> 01:00:59,681
이번 주 경기에 출전하십시오.

973
01:01:01,643 --> 01:01:03,491
그것은 꽤 흥미 롭습니다.

974
01:01:03,491 --> 01:01:07,524
좋아요, 요약하면, 오늘 우리는

975
01:01:07,524 --> 01:01:10,858
정책 그라디언트, 오른쪽, 일반입니다.

976
01:01:10,858 --> 01:01:13,025
그들, 너는 바로

977
01:01:14,456 --> 01:01:18,855
그라디언트 디센트 또는 상승을 정책 매개 변수에 적용하면

978
01:01:18,855 --> 01:01:21,942
그래서 이것은 많은 종류의 문제들에 대해 잘 작동하며,

979
01:01:21,942 --> 01:01:23,947
그러나 또한 높은 편차를 겪는다.

980
01:01:23,947 --> 01:01:25,938
그래서 많은 샘플이 필요합니다.

981
01:01:25,938 --> 01:01:28,669
여기 당신의 도전은 표본 효율입니다.

982
01:01:28,669 --> 01:01:32,608
우리는 Q- 러닝에 대해서도
이야기했는데 항상 그렇지는 않습니다.

983
01:01:32,608 --> 01:01:35,349
일할 때가끔 어렵다.

984
01:01:35,349 --> 01:01:37,536
우리가 전에 말했던이 문제 때문에

985
01:01:37,536 --> 01:01:39,702
당신은 이것을 계산하려고합니다.

986
01:01:39,702 --> 01:01:42,285
정확한 상태, 액션 값

987
01:01:43,324 --> 01:01:47,769
많은 사람들에게, 매우 높은 차원을
위해, 그러나 그것이 효과가있을 때,

988
01:01:47,769 --> 01:01:51,342
예를 들어 우리가 전에 보았던 아타리 (Atari)

989
01:01:51,342 --> 01:01:53,092
보통 더 효율적인 샘플입니다.

990
01:01:53,092 --> 01:01:54,322
정책 기울기보다.

991
01:01:54,322 --> 01:01:56,540
맞아요, Q-learning의 과제 중 하나는

992
01:01:56,540 --> 01:01:57,484
당신은 당신이

993
01:01:57,484 --> 01:01:59,902
충분한 탐사를하고있어.

994
01:01:59,902 --> 01:02:00,735
네?

995
01:02:00,735 --> 01:02:04,902
[청중의 들리지 않는 질문]

996
01:02:14,313 --> 01:02:17,764
오, Q- 러닝의 경우이 과정을 어디에서 할 수 있습니까?

997
01:02:17,764 --> 01:02:20,684
너 좋아, 너 어디서부터 시작하려고하는지.

998
01:02:20,684 --> 01:02:21,753
일부 감독 훈련?

999
01:02:21,753 --> 01:02:24,924
따라서 Q- 학습을위한 직접 접근 방식은 그렇지 않습니다.

1000
01:02:24,924 --> 01:02:27,532
당신이 이것들에 회귀하려고하기 때문에 그렇게하십시오.

1001
01:02:27,532 --> 01:02:29,972
이것에 대한 정책 구배 대신에 Q 값, 맞다.

1002
01:02:29,972 --> 01:02:32,732
배포판에 있지만, 나는 당신이 할
수있는 방법이 있다고 생각한다.

1003
01:02:32,732 --> 01:02:34,232
좋아, 마사지해라.

1004
01:02:35,938 --> 01:02:37,985
부츠 랏뿌도하는 타입.

1005
01:02:37,985 --> 01:02:40,393
부츠 스트랩은 일반적으로 생각하기 때문에

1006
01:02:40,393 --> 01:02:43,854
행동 복제는 좋은 방법입니다.

1007
01:02:43,854 --> 01:02:46,021
이러한 정책을 따뜻하게 시작하십시오.

1008
01:02:47,454 --> 01:02:50,284
그래, 그렇다. 그래서 우리는
정책 구배에 대해 이야기했다.

1009
01:02:50,284 --> 01:02:54,213
Q- 러닝 (Q-learning), 그리고
이들 중 일부에 대한 또 다른 시각,

1010
01:02:54,213 --> 01:02:55,413
당신이 가지고있는 보장 중 일부는

1011
01:02:55,413 --> 01:02:56,752
오른쪽, 정책 그라디언트.

1012
01:02:56,752 --> 01:02:58,622
우리가 정말 잘 알고있는 한 가지는

1013
01:02:58,622 --> 01:03:02,789
이것은 항상 세타의 J 지역의 최소치에 수렴 할 것이고,

1014
01:03:04,339 --> 01:03:06,592
그라디언트 상승을 직접하고 있기 때문에,

1015
01:03:06,592 --> 01:03:09,043
그래서 이것은 종종,

1016
01:03:09,043 --> 01:03:12,131
이 지역 최소값은 종종 꽤 좋았습니다.

1017
01:03:12,131 --> 01:03:14,931
그리고 Q-Learning에서는 다른 한편으로는

1018
01:03:14,931 --> 01:03:17,041
여기에 우리가 대략적으로하려고하기 때문에

1019
01:03:17,041 --> 01:03:20,277
복잡한 함수를 가진이 Bellman 방정식

1020
01:03:20,277 --> 01:03:23,358
approximator 및 그래서,이
경우, 이것은 문제입니다

1021
01:03:23,358 --> 01:03:25,787
Q-learning은 훈련하는
것이 조금 더 까다 롭습니다.

1022
01:03:25,787 --> 01:03:29,954
광범위한 문제에 적용 할 수있는 측면에서

1023
01:03:31,849 --> 01:03:34,737
좋아, 오늘 너는 근본적으로 아주 좋아,

1024
01:03:34,737 --> 01:03:37,907
간략한, 강화 학습의 고차원 적 개요

1025
01:03:37,907 --> 01:03:41,546
그리고 RL의 일부 주요 클래스의 알고리즘.

1026
01:03:41,546 --> 01:03:44,419
다음에 우리는

1027
01:03:44,419 --> 01:03:47,577
게스트 강사, 송 한, 많은 일을했다.

1028
01:03:47,577 --> 01:03:51,276
모델 압축에서의 개척 작업

1029
01:03:51,276 --> 01:03:52,569
에너지 효율적인 심층 학습,

1030
01:03:52,569 --> 01:03:56,459
그래서 그는 이것에 대해 몇 가지 이야기 할 것입니다.

1031
01:03:56,459 --> 01:03:58,459
고맙습니다.

