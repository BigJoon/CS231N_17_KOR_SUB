1
00:00:10,512 --> 00:00:15,376
좋은 아침입니다.
12시 3분이군요. 수업을 진행하겠습니다.

2
00:00:15,376 --> 00:00:18,014
CS231n 12강입니다.

3
00:00:18,014 --> 00:00:21,840
오늘은 CNN의 시각화와 이해에 대해서 배워볼 것입니다.

4
00:00:21,840 --> 00:00:25,270
12강은 이미지 자료가 아주 많아서
아주 재미있는 수업이 될 것입니다.

5
00:00:25,270 --> 00:00:28,375
제가 가장 좋아하는 강의입니다.

6
00:00:28,375 --> 00:00:30,354
우선 몇 가지 공지사항을 전달해 드리겠습니다.

7
00:00:30,354 --> 00:00:39,544
프로젝트는 잘 진행하고 계신가요? 오늘 밤 까지
Canvas에 마일스톤을 제출해 주셔야 합니다.

8
00:00:39,545 --> 00:00:43,590
그리고 현재 중간고사를 채점 중입니다.

9
00:00:43,590 --> 00:00:49,537
아마도 채점 결과는 Gradescope를 통해서
이번 주 이내로 확인해 보실 수 있을 것입니다.

10
00:00:49,537 --> 00:00:54,987
지난 주에 여러분께서 Gradescope 가입과
관련해서 많이 혼란스러우셨을 것입니다.

11
00:00:54,988 --> 00:00:57,372
Piazza에서 이와 관련된 질문들을 많이 받았습니다.

12
00:00:57,372 --> 00:01:02,973
중간고사 채점 결과를 Gradescope를 통해서 발표하기로
결정했습니다. 관련 메일을 받으시면 잘 확인해 주시기 바랍니다.

13
00:00:59,530 --> 00:01:02,973


14
00:01:02,973 --> 00:01:07,412
그리고 지난 주 금요일에 과제 3이 출제 되었습니다.

15
00:01:05,047 --> 00:01:07,412


16
00:01:07,412 --> 00:01:11,088
과제 3은 지난 주 금요일부터 1주일 후인 26일 까지 입니다.

17
00:01:11,088 --> 00:01:14,444
과제3은 지난 해와 비교해서 완전히 달라졌습니다.

18
00:01:12,595 --> 00:01:14,444


19
00:01:14,444 --> 00:01:18,847
과제3을 예정보다 늦게 출제해 드려서 사과의 말씀 드립니다.

20
00:01:17,152 --> 00:01:18,847


21
00:01:18,847 --> 00:01:25,283
하지만 과제3은 아주 재밌을 것입니다. 오늘 강의에서
배운 내용들을 실제로 구현하게 될 것입니다.

22
00:01:20,272 --> 00:01:22,644


23
00:01:22,644 --> 00:01:25,283


24
00:01:25,283 --> 00:01:30,921
과제3을 하시려면 PyTorch/Tensor Flow
중에서 선택하시면 됩니다.

25
00:01:27,188 --> 00:01:29,575


26
00:01:29,575 --> 00:01:30,921


27
00:01:30,921 --> 00:01:34,512
이번 과제가 여러분들에게 의미있는 경험이 되었으면 좋겠습니다.

28
00:01:34,512 --> 00:01:37,273
그리고 주말 동안 많은 분들이
HyperQuest 활동을 해주셨습니다.

29
00:01:35,822 --> 00:01:37,273


30
00:01:37,273 --> 00:01:40,549
아주 놀라웠습니다. 리더보드가 어제 생겼는데

31
00:01:39,084 --> 00:01:40,549


32
00:01:40,549 --> 00:01:47,402
많은 분들이 Deep learning/ Neural network을
학습시키는 스킬들을 마음껏 뽐내 주셨습니다. 대단합니다.

33
00:01:42,568 --> 00:01:44,227


34
00:01:44,227 --> 00:01:46,063


35
00:01:46,063 --> 00:01:47,402


36
00:01:47,402 --> 00:01:55,118
HyperQuest의 많은 분들의 성원 가운데 마일스톤 제출기한이
HyperQuest와 겹치는 부분이 있었기 때문에

37
00:01:50,087 --> 00:01:52,811


38
00:01:52,811 --> 00:01:55,118


39
00:01:55,118 --> 00:01:58,591
Extra credit을 드리는 HyperQuest 마감일을
일요일까지 연장하기로 했습니다.

40
00:01:56,808 --> 00:01:58,591


41
00:01:58,591 --> 00:02:04,773
따라서 일요일 까지 HyperQuest를 12회 이상
실행하신 모든 분들에게 extra credit을 지급할 예정입니다.

42
00:02:02,279 --> 00:02:04,773


43
00:02:04,773 --> 00:02:11,200
그리고 리더보드의 상위권에 위치하신 분들은
조금 더 많은 credit을 받게될 것입니다.

44
00:02:07,394 --> 00:02:09,175


45
00:02:09,175 --> 00:02:11,200


46
00:02:11,200 --> 00:02:15,935
HyperQuest에 많은 관심에 감사드리며
아주 뜻깊은 시간인 것 같습니다.

47
00:02:13,081 --> 00:02:15,935


48
00:02:15,935 --> 00:02:17,844
마지막 공지사항은 poster session 관련 공지입니다.

49
00:02:17,844 --> 00:02:21,445
6월 6일에 poster session이 있습니다.

50
00:02:21,445 --> 00:02:25,932
6월 6일이 거의 확정일 것입니다. 정확한 시간은
기억이 안나지만, 6월 6일은 확실합니다.

51
00:02:22,872 --> 00:02:24,940


52
00:02:25,932 --> 00:02:31,897
종강 후 여행이나 인턴쉽을 준비하고 있는 많은
분들이 poster session 일정을 문의해 주셨습니다.

53
00:02:27,141 --> 00:02:29,310


54
00:02:29,310 --> 00:02:30,297


55
00:02:30,297 --> 00:02:31,897


56
00:02:31,897 --> 00:02:35,497
poster session은 6월 6일입니다.

57
00:02:33,247 --> 00:02:35,497


58
00:02:35,497 --> 00:02:41,171
공지사항에 대한 문의사항 있으십니까?
없으면 수업 진행하겠습니다.

59
00:02:39,241 --> 00:02:41,171


60
00:02:41,171 --> 00:02:48,161
지난 시간에는 다양한
Computer Vision Tasks를 살펴보았습니다.

61
00:02:42,578 --> 00:02:44,254


62
00:02:44,254 --> 00:02:46,259


63
00:02:46,259 --> 00:02:48,161


64
00:02:48,161 --> 00:02:54,318
우선 Semantic segmentation을 배웠습니다. 입력 이미지의
모든 픽셀에 레이블링을 하는 문제였습니다.

65
00:02:49,955 --> 00:02:52,035


66
00:02:52,035 --> 00:02:54,318


67
00:02:54,318 --> 00:02:58,225
하지만 Semantic Segmentation은
이미지 내의 객체들을 구분할 수 없었습니다.

68
00:02:56,131 --> 00:02:58,225


69
00:02:58,225 --> 00:03:00,773
그리고 Classification + Localization을 배웠습니다.

70
00:03:00,773 --> 00:03:06,539
이 문제는 이미지에 레이블을 부여하는 것 뿐만 아니라
객체의 정확한 위치의 BBox도 알아내야 했습니다.

71
00:03:02,558 --> 00:03:04,059


72
00:03:04,059 --> 00:03:06,539


73
00:03:06,539 --> 00:03:12,594
Classification + Localization 문제에서는 이미지 내에
알아내야 하는 객체의 수가 정해져 있었습니다.

74
00:03:08,041 --> 00:03:10,130


75
00:03:10,130 --> 00:03:12,594


76
00:03:12,594 --> 00:03:16,785
Classification + Localization을 응용해서
Pose recognition을 할 수 있었습니다.

77
00:03:14,424 --> 00:03:16,785


78
00:03:16,785 --> 00:03:20,222
사람의 관절의 위치를 regression 문제로 푸는 방법이었죠

79
00:03:18,836 --> 00:03:20,222


80
00:03:20,222 --> 00:03:27,102
Object Detection도 배웠습니다. 개, 고양이 처럼
고정된 category labels이 있었습니다.

81
00:03:22,235 --> 00:03:23,976


82
00:03:23,976 --> 00:03:25,851


83
00:03:25,851 --> 00:03:27,102


84
00:03:27,102 --> 00:03:32,769
그리고 이미지 내에 모든 객체의 위치에
BBox를 그려 넣어야 했습니다.

85
00:03:29,460 --> 00:03:31,196


86
00:03:31,196 --> 00:03:32,769


87
00:03:32,769 --> 00:03:37,063
Object Detection은 Classification +
Localization 문제와는 다른 문제였습니다.

88
00:03:35,303 --> 00:03:37,063


89
00:03:37,063 --> 00:03:42,298
Object Detection 문제에서는 이미지 내에 객체의
수가 얼마나 있는지 미리 알 수 없었습니다.

90
00:03:38,783 --> 00:03:40,629


91
00:03:40,629 --> 00:03:42,298


92
00:03:42,298 --> 00:03:52,588
R-CNN, Fast R-CNN, Faster R-CNN 패밀리를 배웠습니다.
그리고 single shot detection 도 배웠죠(YOLO, SSD)

93
00:03:44,272 --> 00:03:48,100


94
00:03:48,100 --> 00:03:49,916


95
00:03:49,916 --> 00:03:52,588


96
00:03:52,588 --> 00:03:57,722
그리고 Instatnce Segmentation에 대해서도
간단하게 배워 보았습니다.

97
00:03:55,026 --> 00:03:57,722


98
00:03:57,722 --> 00:04:01,164
Semantic segmentation과 Object Detection을
결합시킨 방법이었습니다.

99
00:04:01,164 --> 00:04:04,934
Instance Segmentation의 목표는 우선
우리가 관심있는 카테고리에 속하는 모든 객체를 찾아내고

100
00:04:03,308 --> 00:04:04,934


101
00:04:04,934 --> 00:04:07,997
각 객체가 속하는 픽셀을 레이블링하는 문제였습니다.

102
00:04:07,997 --> 00:04:14,887
가령 이 예제에서는 2마리의 개와 1마리의 고양이가 있으며
객 객체가 속하는 픽셀을 레이블링해야 합니다.

103
00:04:11,339 --> 00:04:13,093


104
00:04:13,093 --> 00:04:14,887


105
00:04:14,887 --> 00:04:23,810
지난 시간에도 재미있는 Tasks들을 많이 다루긴 했지만
여러분이 프로젝트에 추가할만한 아주 많은 것들이 존재합니다.

106
00:04:17,437 --> 00:04:19,509


107
00:04:19,509 --> 00:04:21,284


108
00:04:21,284 --> 00:04:23,810


109
00:04:23,810 --> 00:04:27,081
오늘은 주제를 조금 바꿔보도록 하겠습니다.

110
00:04:25,645 --> 00:04:27,081


111
00:04:27,081 --> 00:04:30,578
과연 convolutional networks 내부에서는
어떤 일이 일어나고 있는 것일까요?

112
00:04:28,702 --> 00:04:30,578


113
00:04:30,578 --> 00:04:34,120
지금까지는 CNN을 어떻게 학습시킬 것인지를 배웠습니다.

114
00:04:32,445 --> 00:04:34,120


115
00:04:34,120 --> 00:04:37,503
그리고 다양한 문제를 풀기 위해서 CNN 아키텍쳐를
어떻게 설계하고 조합해야하는지를 배웠습니다.

116
00:04:35,916 --> 00:04:37,503


117
00:04:37,503 --> 00:04:42,653
자 이제 여러분이 해 볼만한 질문은 바로
CNN의 내부는 어떻게 생겼을지 입니다.

118
00:04:39,860 --> 00:04:42,653


119
00:04:42,653 --> 00:04:44,081
CNN은 이런 문제들을 어떻게 해결하는 것일까요?

120
00:04:44,081 --> 00:04:46,444
CNN이 어떤 종류의 Features를 찾고있는 것일까요?

121
00:04:46,444 --> 00:04:48,612
이와 같은 질문들 말입니다.

122
00:04:48,612 --> 00:04:53,399
지금까지는 ConvNets을 Black box라고 생각했습니다.

123
00:04:51,043 --> 00:04:53,399


124
00:04:53,399 --> 00:04:57,100
CNN의 한 쪽에서는 입력 이미지가 들어갑니다.

125
00:04:55,635 --> 00:04:57,100


126
00:04:57,100 --> 00:05:01,170
그리고 Conv Layers를 거치면서
아주 다양한 변환을 거치게 됩니다.

127
00:04:58,816 --> 00:05:01,170


128
00:05:01,170 --> 00:05:07,363
결국 CNN의 출력 값은 우리가 이해하고 해석할 수 있는
Class scores의 형태로 나오는 것입니다.

129
00:05:04,547 --> 00:05:07,363


130
00:05:07,363 --> 00:05:12,342
Class scores 뿐만 아니라 Bounding Box의 위치,
Labeled pixels 와 같은 형태의 출력도 있을 것입니다.

131
00:05:09,865 --> 00:05:12,342


132
00:05:12,342 --> 00:05:15,933
그럼 CNN의 중간 과정에서는 무슨 일이 일어나는 것일까요?

133
00:05:13,307 --> 00:05:15,933


134
00:05:15,933 --> 00:05:18,567
CNN이 입력 이미지에서 찾고있는 것은 무엇일까요?

135
00:05:18,567 --> 00:05:24,364
ConvNets은 어떻게 동작할까요? CNN이 도대체
이미지에서 어떤 종류의 것들을 찾고있는 걸까요?

136
00:05:20,857 --> 00:05:22,023


137
00:05:22,023 --> 00:05:24,364


138
00:05:24,364 --> 00:05:29,327
CNN의 내부를 분석하려면 어떤 테크닉이 필요할까요?

139
00:05:25,867 --> 00:05:29,327


140
00:05:29,327 --> 00:05:34,522
우선 가장 만만하게 접근해 볼 수 있는 것은
첫 번째 Layer 입니다.

141
00:05:32,667 --> 00:05:34,522


142
00:05:34,522 --> 00:05:41,492
우선 AlexNet의 예시를 살펴보겠습니다.

143
00:05:37,508 --> 00:05:39,819


144
00:05:39,819 --> 00:05:41,492


145
00:05:41,492 --> 00:05:45,193
AlexNet의 첫 번째 Conv Layers에는
많은 필터들이 있습니다.

146
00:05:43,262 --> 00:05:45,193


147
00:05:45,193 --> 00:05:49,230
AlexNet의 각 Conv filter는
3 x 11 x 11의 형태를 취하고 있습니다.

148
00:05:49,230 --> 00:05:52,268
Conv filters는 sliding window로
이미지를 돕니다.

149
00:05:51,228 --> 00:05:52,268


150
00:05:52,268 --> 00:05:54,947
그리고 이미지의 일부 영역과 내적을 수행합니다.

151
00:05:54,947 --> 00:06:01,729
이렇게 필터의 가중치와 내적한 결과가
첫 번째 Conv Layer의 출력입니다.

152
00:05:56,909 --> 00:05:58,689


153
00:05:58,689 --> 00:06:01,729


154
00:06:01,729 --> 00:06:05,074
AlexNet의 첫 번째 레이어에는
64개의 filter가 있습니다.

155
00:06:05,074 --> 00:06:11,682
첫 번째 Conv Layer는 입력 이미지와
직접 내적을 수행하기 때문에

156
00:06:06,947 --> 00:06:08,780


157
00:06:08,780 --> 00:06:10,175


158
00:06:10,175 --> 00:06:11,682


159
00:06:11,682 --> 00:06:19,458
이 필터를 단순히 시각화시키는 것 만으로도 이 필터가
이미지에서 무엇을 찾고 있는지 알아낼 수 있습니다.

160
00:06:14,548 --> 00:06:17,697


161
00:06:17,697 --> 00:06:19,458


162
00:06:19,458 --> 00:06:30,201
AlexNet의 11x11x3 필터는
11x11x3 rgb 이미지로 쉽게 시각화할 수 있습니다.

163
00:06:22,576 --> 00:06:25,027


164
00:06:25,027 --> 00:06:28,461


165
00:06:28,461 --> 00:06:30,201


166
00:06:30,201 --> 00:06:35,305
필터가 총 64개 이므로
64개의 11x11 이미지를 시각화시킬 수 있습니다.

167
00:06:32,051 --> 00:06:35,305


168
00:06:35,305 --> 00:06:42,509
지금 보시는 이미지는 PyTorch Model zoo에서
가져온 pretrained model의 필터들입니다.

169
00:06:38,047 --> 00:06:40,982


170
00:06:40,982 --> 00:06:42,509


171
00:06:42,509 --> 00:06:51,065
AlexNet, ResNet-18, ResNet-101,DenseNet-121
의 첫 번째 Conv filter의 가중치들 입니다.

172
00:06:44,739 --> 00:06:45,985


173
00:06:45,985 --> 00:06:48,313


174
00:06:48,313 --> 00:06:51,065


175
00:06:51,065 --> 00:06:55,553
이를 통해서 이 필터들이 무엇을
찾고있는지 알 수 있습니다.

176
00:06:53,753 --> 00:06:55,553


177
00:06:55,553 --> 00:07:01,052
우선 가장 많이 찾는 것은 엣지성분 입니다.
흰/검으로 길게 늘어선 필터들이 보이실 것입니다.

178
00:06:59,015 --> 00:07:01,052


179
00:07:01,052 --> 00:07:09,475
다양한 각도와 위치에서의 보색(oppising colors)도 보입니다.
가령 초록색과 분홍색처럼 말이죠

180
00:07:04,487 --> 00:07:07,200


181
00:07:07,200 --> 00:07:09,475


182
00:07:09,475 --> 00:07:12,732
주황색과 파랑색의 보색도 보입니다.

183
00:07:12,732 --> 00:07:17,907
학습된 필터를 살펴보면 첫 강의에서 살펴본
hubel and wiesel의 실험이 떠오릅니다.

184
00:07:14,893 --> 00:07:16,221


185
00:07:16,221 --> 00:07:17,907


186
00:07:17,907 --> 00:07:24,978
인간의 시각 체계에서도 Oriented edges를 감지한다고
알려져 있습니다. 초기 레이어에서 말이죠

187
00:07:19,716 --> 00:07:22,271


188
00:07:22,271 --> 00:07:24,978


189
00:07:24,978 --> 00:07:31,566
CNN의 첫 번째 layer에서도 비슷한 일이 일어나는 것 같습니다.

190
00:07:26,946 --> 00:07:29,136


191
00:07:29,136 --> 00:07:31,566


192
00:07:31,566 --> 00:07:46,389
흥미롭게도, CNN을 어떤 모델/데이터로 학습하건 간에
첫 번째 레이어는 전부 다 이런 식으로 생겼다는 점입니다.

193
00:07:33,153 --> 00:07:35,631


194
00:07:35,631 --> 00:07:37,920


195
00:07:37,920 --> 00:07:40,594


196
00:07:40,594 --> 00:07:42,736


197
00:07:42,736 --> 00:07:44,990


198
00:07:44,990 --> 00:07:46,389


199
00:07:46,389 --> 00:07:51,539
입력 이미지에서 oriented edges 라던지
보색(opposing colors) 같은 것들을 찾는 것입니다.

200
00:07:48,676 --> 00:07:51,539


201
00:07:51,539 --> 00:07:53,696
질문 있나요?

202
00:08:04,215 --> 00:08:07,592
네. 이 이 이미지들을 CNN Layer의
학습된 가중치 입니다.

203
00:08:06,118 --> 00:08:07,592


204
00:08:15,766 --> 00:08:21,318
질문은 "필터의 가중치를 시각화한다고 해서
필터가 무엇을 찾고있는지 알 수 있는지" 입니다.

205
00:08:16,826 --> 00:08:18,998


206
00:08:18,998 --> 00:08:21,318


207
00:08:21,318 --> 00:08:25,045
이는 Templete Matching 이나 내적을
생각하시면 됩니다.

208
00:08:23,945 --> 00:08:25,045


209
00:08:25,045 --> 00:08:28,389
가령 여러분이 Templete vecter를 가지고있다고 해봅시다.

210
00:08:28,389 --> 00:08:35,044
이 Templete Vector와 임의의 데이터를 내적해서
어떤 Scaler output을 계산합니다.

211
00:08:31,125 --> 00:08:33,272


212
00:08:33,272 --> 00:08:35,044


213
00:08:35,044 --> 00:08:43,062
필터 값을 가장 활성화시키는 입력을 생각해보면
입력 값고 필터의 가중치 값이 동일한 경우입니다.

214
00:08:38,321 --> 00:08:40,289


215
00:08:40,289 --> 00:08:43,062


216
00:08:43,062 --> 00:08:52,506
내적을 생각해보면, 내적 값을 최대화 시키려면
동일한 값 두 개를 내적하는 것입니다.

217
00:08:45,564 --> 00:08:48,066


218
00:08:48,066 --> 00:08:49,736


219
00:08:49,736 --> 00:08:52,506


220
00:08:52,506 --> 00:08:57,902
그렇게 때문에 첫 번째 레이어의 가중치를 시각화시키면
첫 번째 레이어가 무엇을 찾고 있는지 알 수 있는 것입니다.

221
00:08:55,060 --> 00:08:56,323


222
00:08:56,323 --> 00:08:57,902


223
00:09:06,008 --> 00:09:10,052
그리고 이 첫 번쨰 레이어는
Convolutional Layer 입니다.

224
00:09:08,731 --> 00:09:10,052


225
00:09:10,052 --> 00:09:18,178
이미지를 입력으로 받는 CNN은 일반적으로
첫 번째 레이어가 Conv Layer입니다.

226
00:09:12,003 --> 00:09:13,808


227
00:09:13,808 --> 00:09:15,174


228
00:09:15,174 --> 00:09:16,525


229
00:09:16,525 --> 00:09:18,178


230
00:09:28,086 --> 00:09:32,118
다음 질문은 "네트워크의 중간에도
이런 시각화를 동일하게 적용할 수 있는지" 입니다.

231
00:09:29,006 --> 00:09:30,665


232
00:09:30,665 --> 00:09:32,118


233
00:09:32,118 --> 00:09:35,104
바로 다음 슬라이드가 그 내용입니다. :D

234
00:09:33,202 --> 00:09:35,104


235
00:09:35,104 --> 00:09:41,753
앞서 했던 동일한 시각화 기법을 중간 Conv Layers에
적용하면, 우리가 보고 해석하기 더 어렵습니다.

236
00:09:37,123 --> 00:09:39,767


237
00:09:39,767 --> 00:09:41,753


238
00:09:41,753 --> 00:09:45,081
지금 보시는 것이 동일한 시각화 기법을 적용한 것입니다.

239
00:09:45,081 --> 00:09:51,890
현재 보시는 슬라이드는 우리 웹사이트에 있는
작은 ConvNets demo network입니다.

240
00:09:49,278 --> 00:09:50,474


241
00:09:50,474 --> 00:09:51,890


242
00:09:51,890 --> 00:09:55,987
이 네트워크에서 첫 번째 레이어는
7 x 7 필터 16개 입니다.

243
00:09:52,702 --> 00:09:55,987


244
00:09:55,987 --> 00:10:00,842
앞선 슬라이드에서 시각화로 보여드렸던
첫 번째 레이어와 동일합니다.

245
00:09:58,263 --> 00:10:00,842


246
00:10:00,842 --> 00:10:06,583
이제는 두 번째 레이어의 가중치들을 살펴봅시다.
두 번째 레이어까지는 수 차례 Conv & ReLU를 거쳐야 합니다.

247
00:10:02,366 --> 00:10:04,491


248
00:10:04,491 --> 00:10:06,583


249
00:10:06,583 --> 00:10:10,629
두 번째 Conv Layer는 16채널의 입력을 받습니다.

250
00:10:08,185 --> 00:10:10,629


251
00:10:10,629 --> 00:10:15,116
그리고 20개의 7x7 conv filters가 있습니다.

252
00:10:15,116 --> 00:10:20,495
이 필터들은 직접적으로 이미지의 형태로
시각화시킬 수 없다는 문제점이 있습니다.

253
00:10:16,064 --> 00:10:18,660


254
00:10:18,660 --> 00:10:20,495


255
00:10:20,495 --> 00:10:23,846
어쨌든 시도는 해볼 수 있겠죠

256
00:10:23,846 --> 00:10:28,547
이 예시에서는 입력은 16 depth를 가지고 있습니다.

257
00:10:28,547 --> 00:10:40,924
그리고 Conv filters의 사이즈는 7 x 7 x 16 입니다.
이런 필터가 20개 있습니다.

258
00:10:30,286 --> 00:10:32,542


259
00:10:32,542 --> 00:10:34,388


260
00:10:34,388 --> 00:10:35,759


261
00:10:35,759 --> 00:10:38,072


262
00:10:38,072 --> 00:10:40,924


263
00:10:40,924 --> 00:10:47,498
여기서 문제점은 이 필터들을 직접 살펴보는 것
만으로는 이해할만한 정보를 얻기 힘들다는 점입니다.

264
00:10:44,035 --> 00:10:45,128


265
00:10:45,128 --> 00:10:47,498


266
00:10:47,498 --> 00:10:53,743
여기에 (Layer 2 weights) 두 번째 레이어를 시각화 시켰습니다.
16 x 7 x 7 필터를 시각화하기 위해서는

267
00:10:49,734 --> 00:10:53,743


268
00:10:53,743 --> 00:11:01,782
16개의 7 x 7 grayscale images로 나눠서 표현해야 합니다.

269
00:10:58,192 --> 00:11:01,782


270
00:11:01,782 --> 00:11:11,852
중간에 보이시는 Gray scale 이미지들이 바로
두 번째 레이어의 각 필터들의 가중치입니다.

271
00:11:03,284 --> 00:11:07,095


272
00:11:07,095 --> 00:11:08,898


273
00:11:08,898 --> 00:11:11,852


274
00:11:11,852 --> 00:11:21,046
두 번째 레이어의 출력이 총 20개 이므로
"7 x 7 이미지 16개"가 총 20개 있습니다.

275
00:11:14,473 --> 00:11:17,534


276
00:11:17,534 --> 00:11:21,046


277
00:11:21,046 --> 00:11:28,638
두 번째 레이어의 convolutional filters를 이미지로
시각화 해보면 이런 식의 특별한 구조들을 보실 수 있습니다.

278
00:11:22,871 --> 00:11:24,307


279
00:11:24,307 --> 00:11:26,709


280
00:11:26,709 --> 00:11:28,638


281
00:11:28,638 --> 00:11:32,128
그런데 아무리 자세히 봐도 이 필터가 무엇을 찾고 있는건지
적당한 Intuition을 얻기란 힘듭니다.

282
00:11:30,897 --> 00:11:32,128


283
00:11:32,128 --> 00:11:36,644
아무리 봐도 잘 모르겠는 이유는 이 필터들은
이미지와 직접 연결되어 있지 않기 떄문입니다.

284
00:11:35,099 --> 00:11:36,644


285
00:11:36,644 --> 00:11:41,851
두 번째 레이어의 필터들은 첫 번째 레이어의
출력과 연결되어 있습니다.

286
00:11:39,493 --> 00:11:41,851


287
00:11:41,851 --> 00:11:50,646
따라서 우리가 시각화한 내용은 "두 번째 레이어의 결과를
최대화시키는 첫 번째 레이어의 출력 패턴이 무엇인지" 입니다.

288
00:11:44,189 --> 00:11:46,684


289
00:11:46,684 --> 00:11:49,331


290
00:11:49,331 --> 00:11:50,646


291
00:11:50,646 --> 00:11:58,490
하지만 이미지의 관점에서 첫 번째 레이어의 출력이
어떻게 생겼는지 감을 잡고 해석하기한 쉽지 않습니다.

292
00:11:52,423 --> 00:11:53,860


293
00:11:53,860 --> 00:11:55,966


294
00:11:55,966 --> 00:11:58,490


295
00:11:58,490 --> 00:12:03,556
따라서 네트워크의 중간 레이어의 필터들이 무엇을
찾고있는지를 알아내려면 조금 더 fancy한 기법이 필요합니다.

296
00:12:00,893 --> 00:12:02,047


297
00:12:02,047 --> 00:12:03,556


298
00:12:03,556 --> 00:12:04,819
질문있나요?

299
00:12:09,189 --> 00:12:16,552
앞서 슬라이드에 있던 이미지들은
0-255 의 범위로 가중치를 normalize한 것입니다.

300
00:12:10,489 --> 00:12:13,456


301
00:12:13,456 --> 00:12:16,552


302
00:12:16,552 --> 00:12:22,983
실제 가중치들은 범위가 정해져있지 않습니다.
하지만 시각화를 위해서라면 스케일을 조정해야 합니다.

303
00:12:18,648 --> 00:12:19,885


304
00:12:19,885 --> 00:12:22,983


305
00:12:22,983 --> 00:12:31,892
그리고 필터만 시각화한 것이지 Bias는 고려하지 않았습니다.
따라서 이 시각화 결과를 있는 그대로 믿으면 안됩니다.

306
00:12:24,685 --> 00:12:26,409


307
00:12:26,409 --> 00:12:28,162


308
00:12:28,162 --> 00:12:30,423


309
00:12:30,423 --> 00:12:31,892


310
00:12:34,180 --> 00:12:38,391
자 이제는 마지막 레이어를 살펴봅시다.
CNN의 마지막 레이어를 가지고 놀어봅시다.

311
00:12:35,237 --> 00:12:36,733


312
00:12:36,733 --> 00:12:38,391


313
00:12:38,391 --> 00:12:44,908
CNN의 마지막 레이어는 1000개의 클래스 스코어가 있습니다.
이는 학습 데이터의 predicted scores를 의미합니다.

314
00:12:40,698 --> 00:12:42,891


315
00:12:42,891 --> 00:12:44,908


316
00:12:44,908 --> 00:12:48,628
이 마지막 레이어 직전에는
Fully Connected Layer가 있습니다.

317
00:12:46,676 --> 00:12:48,628


318
00:12:48,628 --> 00:12:58,328
가령 AlesNet의 마지막 레이어는 이미지를 표현하는
4096-dim 특징벡터를 입력으로 최종 Class scores를 출력합니다.

319
00:12:49,962 --> 00:12:53,039


320
00:12:53,039 --> 00:12:55,516


321
00:12:55,516 --> 00:12:58,328


322
00:12:58,328 --> 00:13:07,967
CNN의 마지막 레이어에서 어떤 일이 일어나는 지를
알아보는 것도 시각화의 한 방법이 될 수 있습니다.

323
00:13:00,606 --> 00:13:02,787


324
00:13:02,787 --> 00:13:04,263


325
00:13:04,263 --> 00:13:06,520


326
00:13:06,520 --> 00:13:07,967


327
00:13:07,967 --> 00:13:18,687
이 방법은 많은 이미지로 CNN을 돌려서
각 이미지에서 나온 4096-dim 특징 벡터를 모두 저장힙니다.

328
00:13:09,022 --> 00:13:11,230


329
00:13:11,230 --> 00:13:13,110


330
00:13:13,110 --> 00:13:14,815


331
00:13:14,815 --> 00:13:17,174


332
00:13:17,174 --> 00:13:18,687


333
00:13:18,687 --> 00:13:26,075
앞선 방법에서는 첫 번째 Conv Layer를 시각화시켰지만
이제는 마지막 Hidden Layer를 시각화시킬 것입니다.

334
00:13:20,722 --> 00:13:23,219


335
00:13:23,219 --> 00:13:26,075


336
00:13:26,075 --> 00:13:29,791
여기에서 시도해볼 수 있는 방법 중 하나는
Nearest Neighbor를 이용한 방법입니다.

337
00:13:27,804 --> 00:13:29,791


338
00:13:29,791 --> 00:13:33,162
맨 왼쪽의 이미지는 Lecture 2에서 본 적 있으실 것입니다.

339
00:13:31,559 --> 00:13:33,162


340
00:13:33,162 --> 00:13:40,303
CiFAR-10 데이터들의  "이미지 픽셀 공간" 에서의
Nearest  neighbors 였습니다.

341
00:13:36,045 --> 00:13:37,967


342
00:13:37,967 --> 00:13:40,303


343
00:13:40,303 --> 00:13:48,660
CIFAR-10으로 한 결과를 보시면
유사한 이미지를 잘 찾아내는 것을 알 수 있습니다.

344
00:13:41,996 --> 00:13:44,765


345
00:13:44,765 --> 00:13:46,500


346
00:13:46,500 --> 00:13:48,660


347
00:13:48,660 --> 00:13:58,917
맨 왼쪽 열이 CIFAR-10 으로 학습시킨 이미지입니다.
오른쪽 5열의 이미지는 Testset이죠

348
00:13:50,777 --> 00:13:52,350


349
00:13:52,350 --> 00:13:54,987


350
00:13:54,987 --> 00:13:57,239


351
00:13:57,239 --> 00:13:58,917


352
00:13:58,917 --> 00:14:02,446
가령 두 번째 행에 있는 흰색 강아지의 예를 들어보겠습니다.

353
00:14:00,185 --> 00:14:02,446


354
00:14:02,446 --> 00:14:11,643
픽셀 공간에서의 Nearest neighbors는 흰색 덩어리가 있으면
가깝다고 생각할 것입니다. 굳이 개가 아니라도 말이죠

355
00:14:04,523 --> 00:14:06,328


356
00:14:06,328 --> 00:14:08,321


357
00:14:08,321 --> 00:14:09,885


358
00:14:09,885 --> 00:14:11,643


359
00:14:11,643 --> 00:14:16,937
이런 식으로 거리가 가까운 이미지들을 시각화해
보는 방법도 시각화 기법이 될 수 있습니다.

360
00:14:14,268 --> 00:14:16,937


361
00:14:16,937 --> 00:14:27,107
다만 픽셀 공간에서 Nearest neighbors를 계산하는 것이 아니라
CNN에서 나온 4096-dim 특징 벡터 공간에서 계산합니다.

362
00:14:17,963 --> 00:14:19,952


363
00:14:19,952 --> 00:14:21,735


364
00:14:21,735 --> 00:14:24,507


365
00:14:24,507 --> 00:14:27,107


366
00:14:27,107 --> 00:14:29,987
오른쪽에 예제가 몇 가지 있습니다.

367
00:14:28,351 --> 00:14:29,987


368
00:14:29,987 --> 00:14:38,338
맨 왼쪽 열은 ImageNet Classification
Dataset의 Test Set입니다.

369
00:14:32,069 --> 00:14:34,924


370
00:14:34,924 --> 00:14:38,338


371
00:14:38,338 --> 00:14:48,515
그리고 나머지 열 들은 AlexNet의 4096-dim
특징벡터에서 계산한 nearest neighbors 결과입니다.

372
00:14:41,253 --> 00:14:43,614


373
00:14:43,614 --> 00:14:46,863


374
00:14:46,863 --> 00:14:48,515


375
00:14:48,515 --> 00:14:52,941
이 결과를 보면 확실히 픽셀 공간에서의
nearest neighbors와는 아주 다릅니다.

376
00:14:51,010 --> 00:14:52,941


377
00:14:52,941 --> 00:14:58,375
"특징 공간에서의  nearest neighbors"의 결과를 보면
서로 픽셀 값의 차이가 큰 경우도 있습니다.

378
00:14:55,086 --> 00:14:57,111


379
00:14:57,111 --> 00:14:58,375


380
00:14:58,375 --> 00:15:03,031
서로 픽셀 값의 차이는 커도 특징 공간 내에서는
아주 유사한 특성을 지닌다는 것을 알 수 있습니다.

381
00:15:03,031 --> 00:15:10,484
가령 여기 두번째 줄에 코끼리의 예를 살펴봅시다.
코끼리는 왼쪽에 서있고 그 뒤에는 풀밭이 있습니다.

382
00:15:10,484 --> 00:15:17,307
테스트 셋의 3번째로 가까운 이미지를 살펴보면
코끼리가 오른쪽에 서 있습니다.

383
00:15:17,307 --> 00:15:26,942
아주 흥미롭습니다. 코끼리가 왼편에 서 있는 이미지와 오른편에
서 있는 이미지의 픽셀 값은 완전히 다를 것이기 때문입니다.

384
00:15:26,942 --> 00:15:32,554
하지만 네트워크가 학습한 특징 공간 내에서는
두 이미지는 아주 아까워집니다.

385
00:15:32,554 --> 00:15:37,975
이는 네트워크가 학습을 통해서 이미지의
semantic content한 특징들을 잘 포착해 낸 결과입니다.

386
00:15:37,975 --> 00:15:46,192
아주 놀라운 일이죠. 이처럼 Nearest neighbor를 통한
시각화 기법은 어떤 일이 일어나는지 살펴보기에 아주 좋습니다.

387
00:16:02,617 --> 00:16:04,630
[학생이 질문]

388
00:16:04,630 --> 00:16:13,942
기본적인 supervised learning 과정에서는 특징공간에서
서로 유사해야한다는 Loss는 없습니다.

389
00:16:13,942 --> 00:16:21,476
특징 공간에서 서로 유사해야한다고 말해주지 않았는데도
결국은 서로 가까워 진 것입니다.

390
00:16:21,476 --> 00:16:28,746
하지만 사람들이 "triplet/contrastive loss"
를 네트워크가 추가시켜서 학습하기도 합니다.

391
00:16:28,746 --> 00:16:37,253
이 losses를 통해 특징 공간에서의 계산이 가능하도록
가정과 제약조건을 마지막 레이어에 추가하는 것입니다.

392
00:16:37,253 --> 00:16:39,907
하지만 AlexNet의 경우에는 그런 장치가 없습니다.

393
00:16:44,931 --> 00:16:46,060
문제는 가장 가까운 것이 무엇인지...

394
00:16:46,060 --> 00:16:48,875
질문은 "마지막 레이어에 nearest neighbor를
적용하려면 어떻게 해야하는지" 입니다.

395
00:16:48,875 --> 00:16:51,432
우선 이미지를 네트워크에 통과시킵니다.

396
00:16:51,432 --> 00:16:57,670
뒤에서 두번째에 있는 히든 레이어는
4096-dim 벡터입니다.

397
00:16:57,670 --> 00:17:01,797
네트워크이 마지막 단에는
FC-Layer가 있었습니다.

398
00:17:01,797 --> 00:17:06,893
자 그럼 각 이미지들에 해당하는
4096-dim 벡터들을 전부 다 저장해 놓습니다.

399
00:17:06,894 --> 00:17:12,966
그리고 저장된 4096-dim 벡터들을 가지고
nearest neighbors를 계산합니다.

400
00:17:17,012 --> 00:17:19,171
수업이 끝나고 다시한번 말씀드리죠

401
00:17:19,171 --> 00:17:28,434
최종 레이어에서 어떤 일이 벌어지는지를 시각화하고자 할 때
"차원 축소"의 관점으로 볼 수도 있습니다.

402
00:17:28,435 --> 00:17:33,220
CS229를 수강하신 분들이라면
PCA에 대해서 들어본 적 있으실 것입니다.

403
00:17:33,220 --> 00:17:39,841
PCA는 4096-dim 과 같은 고차원 특징벡터들을
2-dim 으로 압축시키는 기법입니다.

404
00:17:39,841 --> 00:17:43,183
이 방법을 통해서 특징 공간을 조금 더
직접적으로 시각화시킬 수 있습니다.

405
00:17:43,183 --> 00:17:51,321
Principle Component Analysis(PCA) 로 이런
일을 할 수는 있지만 t-SNE라는 알고리즘이 더 파워풀합니다.

406
00:17:51,321 --> 00:17:54,656
t-SNE는  t-distributed stochastic neighbor
embeddings 이라는 뜻입니다.

407
00:17:54,656 --> 00:18:03,137
많은 사람들이 특징공간을 시각화하기 위해서 사용하는
PCA보다는 조금 더 강력한 방법입니다.

408
00:18:03,137 --> 00:18:07,264
여기 t-SNE의 예시가 있습니다.

409
00:18:07,264 --> 00:18:13,231
MNIST를 t-SNE dimensionality reduction
를 통해 시각화한 모습입니다.

410
00:18:13,231 --> 00:18:17,521
MNIST는 0부터 9까지로 이루어진
손글씨 숫자 데이터셋입니다.

411
00:18:17,521 --> 00:18:22,226
MNIST의 각 이미지는 Gray scale 28x28 이미지입니다.

412
00:18:22,226 --> 00:18:32,020
여기에서는 t-SNE가 MNST의 28x28-dim 데이터를 입력으로
받습니다(raw pixels). 그리고 2-dim으로 압축합니다.

413
00:18:32,020 --> 00:18:37,096
그리고 압축된 2-dim 을 이용해서
MNIST를 시각화합니다.

414
00:18:37,096 --> 00:18:42,653
t-SNE로 MNIST를 시각화해보면
이런 식으로 자연스럽게 군집화된 모습을 볼 수 있습니다.

415
00:18:42,653 --> 00:18:47,532
각 군집이 MNIST의 각 숫자를 의미합니다.

416
00:18:47,532 --> 00:18:57,348
이런 식의 시각화 기법을 ImageNet을 분류하려고
학습시킨 네트워크의 마지막 레이어에도 적용해볼 수 있습니다.

417
00:18:57,348 --> 00:19:05,073
조금 더 자세히 말씀드리자면 엄청나게 많은 이미지들을
네트워크에 통과시킵니다.

418
00:19:05,073 --> 00:19:10,865
그리고 각 이미지에 대해서
최종 단의 4096-dim 특징 벡터들을 기록합니다.

419
00:19:10,865 --> 00:19:14,756
그러면 4096-dim 특징 벡터들을 아주 많이 모을 수 있을 것입니다.

420
00:19:14,756 --> 00:19:24,277
그리고 이 특징벡터들에 t-SNE을 적용하면
4096-dim에서 2-dim으로 압축됩니다.

421
00:19:24,277 --> 00:19:36,415
이를 통해 2-dim 특징 공간의 각 grid에
압축된 2-dum 특징들이 시각화시킵니다.

422
00:19:36,415 --> 00:19:43,417
이를 통해 학습된 특징 공간의 기학적인 모습을
 어렴풋이 추측해 볼 수 있습니다.

423
00:19:43,417 --> 00:19:48,620
슬라이드의 이미지가 잘 안보이실 것입니다. 온라인에서
고해상도로 다시한번 살펴보시기 바랍니다.

424
00:19:48,620 --> 00:19:56,451
한번 살펴보자면 좌하단의 초록초록한 군집을 볼 수 있습니다.
이 곳에는 다양한 종류의 꽃들이 있습니다.

425
00:19:56,451 --> 00:20:01,800
또 다른 곳들에는 다양한 종류의 개들이 모여있고,
다양한 동물들끼리 모여있고, 다양한 지역들이 모여있습니다.

426
00:20:01,800 --> 00:20:06,192
이를 통해 우리가 학습시킨 특징 공간에는 일종의
불연속적인 의미론적 개념(semantic notion)이 존재하며

427
00:20:06,192 --> 00:20:11,597
t-SNE을 통한 dimensionality reduction version의
특징 공간을 살펴보며 그 공간을 조금이나마 느낄 수 있습니다.

428
00:20:11,597 --> 00:20:12,604
질문 있나요?

429
00:20:23,716 --> 00:20:29,793
한 이미지당 서로 다른 세 가지 정보가 존재합니다.

430
00:20:29,793 --> 00:20:31,308
1:  우선 (픽셀로 된) 원본 이미지가 있습니다.

431
00:20:31,308 --> 00:20:33,353
2: 그리고 4096-dim 벡터가 있습니다.
(fc-layer의 출력)

432
00:20:33,353 --> 00:20:38,109
3: 그리고 t-SNE를 이용해 4096-dim 벡터를
2-dim 벡터로 변환시킨 값이 있습니다.

433
00:20:38,109 --> 00:20:49,547
결국 원본 이미지를 CNN으로 4096-dim으로 줄이고
이를 다시 t-SNE로 2-dim으로 줄였다고 보시면 되겠습니다.

434
00:20:49,547 --> 00:20:50,348
질문 있나요?

435
00:20:55,864 --> 00:20:59,255
질문은 t-SNE로 만든 이 2차원 공간에서 대강
얼마나 많은 데이터들을 표현할 수 있는지 입니다.

436
00:20:59,255 --> 00:21:06,080
정확히 얼마나 표현할 수 있는지 확신하긴 어렵습니다. t-SNE는
비선형 차원축소 기법이라 상당히 복잡한 성격을 지닙니다.

437
00:21:06,080 --> 00:21:10,259
다시 한번 확인해 봐야 겠지만
정확히 얼마나 표현할 수 있을지 잘 모르겠습니다.

438
00:21:10,259 --> 00:21:14,377
질문 있나요?

439
00:21:14,377 --> 00:21:17,038
질문은 "FC-layer가 아닌 더 상위 레이어에서도
이와 같은 시각화가 가능한지" 입니다.

440
00:21:17,038 --> 00:21:21,384
네 가능합니다. 하지만 안타깝게도
우리 강의자료에는 없네요. 죄송합니다.

441
00:21:21,384 --> 00:21:24,603
질문있나요?

442
00:21:35,559 --> 00:21:39,482
질문은 "이미지들이 차원축소 과정에서
이미지들이 서로 겹치지는 않은지" 입니다.

443
00:21:39,482 --> 00:21:40,902
네 물론 겹칠 수 있습니다.

444
00:21:40,902 --> 00:21:47,537
regular grid를 기준으로 nearest neighbor, 즉
각 grid point에 가장 가까운 이미지를 뽑아냅니다.

445
00:21:47,537 --> 00:21:54,792
따라서 이 시각화 기법이 특징 공간 분포의
모양을 보여주는 것은 아닙니다.

446
00:21:54,792 --> 00:22:03,122
t-SNE는 분포 자체를 보고자 함은 아닙니다.
그러한 특성을 다루는 몇 가지 다른 시각화 기법들이 존재합니다.

447
00:22:03,122 --> 00:22:07,713
네트워크의 중간에서 뽑은 특징들을 가지고
해볼 수 있는 다른 것들도 한번 살펴보겠습니다.

448
00:22:07,713 --> 00:22:13,856
앞서, 중간 레이어에 있는 가중치를 시각화한다고 해도
이를 해석하기는 쉽지 않다고 말씀드렸습니다.

449
00:22:13,856 --> 00:22:20,846
하지만 중간 레이어의 가중치가 아니라 Activation map을 시각화
해보면 일부 해석할 수 있는 것들을 볼 수 있습니다.

450
00:22:20,846 --> 00:22:28,603
다시 AlexNet의 예시를 살펴보겠습니다.

451
00:22:28,603 --> 00:22:35,668
AlexNet의 conv5의 특징은
128 x 13 x 13-dim tensor입니다.

452
00:22:35,668 --> 00:22:42,386
이 tensor는 128개의 13x13 2-dim gird로 볼 수 있습니다.

453
00:22:42,386 --> 00:22:49,741
따라서 이 13 x 13 ( x 1) 특징 맵을
그레이스케일 이미지로 시각화해 볼 수 있습니다.

454
00:22:49,741 --> 00:22:58,501
이를 시각화해보면 conv layer가 입력에서
어떤 특징을 찾고있는지를 짐작해볼 수 있습니다.

455
00:22:58,501 --> 00:23:03,306
지금 보시고있는 툴은 Jason Yasenski가 만든
아주 멋진 툴로 여러분도 다운로드받으실 수 있습니다.

456
00:23:03,306 --> 00:23:06,598
지금 동영상으로는 보여드리지 않겠지만
이 분의 웹사이트에 접속하시면 보실 수 있습니다.

457
00:23:06,598 --> 00:23:10,059
여기에서는 웹캠으로 받은 영상을
CNN에 통과시킵니다.

458
00:23:10,059 --> 00:23:17,279
그리고 실시간으로 중간의 각 특징 맵들을 시각화시켜서
각 레이어들이 무엇을 찾고있는지를 짐작해볼 수 있습니다.

459
00:23:17,279 --> 00:23:23,931
이 예시의 입력 이미지는 카메라 앞에 사람이 있는 영상입니다.

460
00:23:23,931 --> 00:23:28,192
중간 특징들의 대부분은 noisy하고 볼만한 게 없지만

461
00:23:28,192 --> 00:23:34,277
제가 초록색으로 표시한 특징맵을 보시면,
(왼쪽에 더 크게 확대된 이미지도 있습니다.)

462
00:23:34,277 --> 00:23:41,103
이 특징맵은 사람의 얼굴에 활성화되는 것 같아 보입니다.
아주 재미있습니다.

463
00:23:41,103 --> 00:23:51,045
이 특징맵을 통해서, 분명 네트워크의 어떤 레이어에서는
사람의 얼굴을 찾고있는지 모른다는 것을 알 수 있습니다.

464
00:23:51,045 --> 00:23:54,132
아주 놀라운 발견입니다.

465
00:23:54,132 --> 00:23:55,517
질문있나요?

466
00:23:59,038 --> 00:24:04,957
질문은 "완전 새까만 특징 맵들"은
"Dead ReLU" 인지 입니다. 용어에 주의해야 합니다.

467
00:24:04,957 --> 00:24:09,539
대게 "Dead ReLU"라고 함은 모든 학습 데이터셋에
대해서 "Dead(활성화 되지 않음)" 을 의미합니다.

468
00:24:09,539 --> 00:24:14,701
이 예시의 경우에는 "특정 입력" 에 대해서
활성화되지 않았을 뿐입니다.

469
00:24:14,701 --> 00:24:15,702
질문있나요?

470
00:24:19,457 --> 00:24:22,538
질문은 "만약 ImageNet에 사람의 얼굴이 없다면
어떻게 네트워크가 사람의 얼굴을 인식할 수 있는지" 입니다.

471
00:24:22,538 --> 00:24:24,182
ImageNet의 사람은 아주 많이 등장합니다.

472
00:24:24,182 --> 00:24:29,020
물론 ImageNet Classification Chanllenge에의
1000개의 카테고리에 "사람"은 없습니다.

473
00:24:29,020 --> 00:24:34,906
많은 이미지들에 사람이 등잘하며, 다른 카테고리를 분류하더라도
사람이 등장한다는 특성은 유용한 신호일 수 있습니다.

474
00:24:34,906 --> 00:24:41,617
그리고 이는 정말 대단한 결과입니다. 분류 문제를 풀기에
유용한 특징들을 알아서 학습한 것이기 떄문입니다.

475
00:24:41,617 --> 00:24:47,483
가령 고양이만 분류하라고 했을 뿐인데, 고양이를 분류할 때
사람이 유용한 특징이라면 알아서 학습하는 것입니다. 대단한 것이죠.

476
00:24:50,346 --> 00:24:51,929
질문있나요?

477
00:24:55,192 --> 00:25:03,334
네트워크의 입력은 3 x 224 x 224 입니다.
그리고 여러 레이어를 통과할 것입니다.

478
00:25:03,334 --> 00:25:07,731
각 레이어는 3차원의 값을 반환합니다.
(width x height x depth)

479
00:25:07,731 --> 00:25:10,476
이 3차원 덩어리(chunk)가 바로 네트워크
레이어가 출력하는 값입니다.

480
00:25:10,476 --> 00:25:18,155
그리고 이 3차원 덩어리를
"activation volume" 이라고 합니다.

481
00:25:18,155 --> 00:25:22,156
그리고 이 덩어리를 하나씩 잘라내면(slice)
그것이 바로 activation map 입니다.

482
00:25:34,426 --> 00:25:38,513
앞어 질문은 "입력 이미지가 K x K 라면
activation map이 K x K 인지" 였습니다.

483
00:25:38,513 --> 00:25:42,489
그렇지 않습니다. 네트워크 중간에 pooling 과 같은
sub sampling 과정이 있을 수 있기 때문입니다.

484
00:25:42,489 --> 00:25:47,756
하지만 일반적으로 activation map의 크기는
입력 이미지와 선형적인 관계를 갖습니다.

485
00:25:50,492 --> 00:25:55,625
자 그래서 중간 특징들을 시각화시킬 수 있는
또 다른 방법이 있습니다.

486
00:25:55,625 --> 00:26:03,453
어떤 이미지가 들어와야 각 뉴런들의 활성이
최대화되는지를 시각화해보는 방법입니다.

487
00:26:03,453 --> 00:26:08,605
이 예시에서도 다시 한번 AlexNet의
conv5 layer를 사용하겠습니다.

488
00:26:08,605 --> 00:26:10,926
이 활성화 볼륨 각각을 기억하십시오.

489
00:26:10,926 --> 00:26:15,738
AlexNet의 conv5는 128 x 13 x 13 한 덩어리의
activation volume을 갖습니다.

490
00:26:15,738 --> 00:26:19,644
우리는 이제 128개의 채널 중에 하나를 뽑을 것입니다.
여기에서는 17번째 채널을 선택했군요

491
00:26:19,644 --> 00:26:23,749
그리고 많은 이미지들을 CNN에 통과시킵니다.

492
00:26:23,749 --> 00:26:27,456
그리고 각 이미지의 conv5 features를 기록해 놓습니다.

493
00:26:27,456 --> 00:26:37,925
그리고나서 어떤 이미지가 17번째 특징 맵을
최대로 활성화시키는지를 살펴봅니다.

494
00:26:37,925 --> 00:26:45,161
그리고 현재 이 뉴런은 convolutional layer 입니다.
따라서 receptive field가 작은 편이죠

495
00:26:45,161 --> 00:26:49,239
각 뉴런이 전체 이미지를 보고있지는 않습니다.
이미지의 일부만을 보고있죠

496
00:26:49,239 --> 00:27:00,731
따라서 특정 레이어의 특징을 최대화시키는
이미지의 일부분(pathes)을 시각화시킬 것입니다.

497
00:27:00,731 --> 00:27:06,177
그리고 특정 레이어의 활성화 정도를 기준으로
패치들을 정렬시키면 되겠습니다.

498
00:27:06,177 --> 00:27:12,575
자 여기 오른쪽에 예시가 있습니다. 이 네트워크의 이름은
"fully.." 사실 어떤 네트워크인지는 크게 중요하진 않습니다.

499
00:27:12,575 --> 00:27:16,380
어쨌든 여기에 지금 보이는 패치들이 바로
해당 레이어의 활성을 최대화시키는 패치들입니다.

500
00:27:16,380 --> 00:27:22,500
각 행에 있는 패치들이 하나의 뉴런에서 나온 것입니다.

501
00:27:22,500 --> 00:27:28,280
각 패치들은 데이터셋에서 나온
패치들을 정렬한 값들이고

502
00:27:28,280 --> 00:27:30,611
이 패치들이 해당 뉴런의 활성을 최대화시키는 패치들입니다.

503
00:27:30,611 --> 00:27:35,698
패치의 특징을 통해서 해당 뉴런이
무엇을 찾고있는지 짐작해 볼 수 있습니다.

504
00:27:35,698 --> 00:27:39,998
가령 맨 윗 행을 보시면
어떤 동그란 모양을 찾고있다는 것을 알 수 있습니다.

505
00:27:39,998 --> 00:27:44,621
눈 같은 것들이죠. 눈이 많네요.
그리고 파란색 동그란 패치도 있군요

506
00:27:44,621 --> 00:27:51,303
이로 미루어, 네트워크 레이어의 어떤 뉴런은 입력 영상에서
어떤 푸르스름하고 둥근 물체를 찾고있는 것 같습니다.

507
00:27:51,303 --> 00:27:56,200
그리고 여기 중간에 보시면 뉴런이 다양한 색상의
문자를 찾는 뉴런도 있습니다.

508
00:27:56,200 --> 00:28:02,201
또는 다양한 colors & orientations을 가진
휘어진 엣지를 찾는 뉴런이 있다는 것도 알 수 있습니다.

509
00:28:06,246 --> 00:28:09,199
제가 여기에서 용어를 조금 느슨하게 사용한 감이 있습니다.

510
00:28:09,199 --> 00:28:13,970
한 뉴런은 conv5 activation map의
하나의 scaler 값을 의미합니다.

511
00:28:13,970 --> 00:28:19,283
conv5 는 convolutional layer 이기 때문에
한 채널안의 모든 뉴런들은 모두 같은 가중치를 공유합니다.

512
00:28:19,283 --> 00:28:26,451
각 채널 당 하나의 conv filter가 있고 이에 상응하는
많은 뉴런들이 있습니다.(activation map)

513
00:28:26,451 --> 00:28:32,532
Convolution의 특성 상 우리는 앞서 보여드린 patch들을
이미지의 모든 곳에서 추출해 낼 수 있습니다.

514
00:28:32,532 --> 00:28:38,721
그리고 밑의 예시들은 동일 네트워크의 더 깊은 레이어에서
뉴런들을 최대로 활성화시키는 패치들입니다.

515
00:28:38,721 --> 00:28:42,294
이들은 더 깊은 레이어로부터 왔기 때문에
Receptive field가 훨씬 더 넓습니다.

516
00:28:42,294 --> 00:28:44,851
이들은 입력 영상에서 훨씬 더 큰
patch들을 기준으로 찾고 있습니다.

517
00:28:44,851 --> 00:28:49,213
실제로 예시들을 살펴보면 입력 이미지에서
더 큰 구조들(structures)을 찾고있음을 알 수 있습니다.

518
00:28:49,213 --> 00:28:56,445
두 번째 행을 살펴보면 이는 사람, 또는 사람의 얼굴을
찾고 있는것 같아 보입니다.

519
00:28:56,445 --> 00:29:06,410
세 번째 행은 카메라의 일부분을 찾고 있는듯 합니다.
이처럼 조금 더 큰 것들을 찾고 있는 것입니다.

520
00:29:06,410 --> 00:29:11,885
그리고 우리는 조금 더 재밌는 실험을 해볼 수 있습니다.
이는 Zeiler and Fergus가 ECCV'14에 발표한 논문입니다.

521
00:29:11,885 --> 00:29:14,062
이 논문은 "occlusion experiment"
에 관한 논문입니다.

522
00:29:14,062 --> 00:29:21,659
이 실험에서 알고자하는 것은 입력의 어떤 부분이
분류를 결정짓는 근거가 되는지에 관한 실험입니다.

523
00:29:21,659 --> 00:29:25,339
우선 입력 이미지를 받습니다.
가령 이 예시의 경우는 코끼리입니다.

524
00:29:25,339 --> 00:29:32,486
그리고 이미지의 일부를 가립니다. 그리고 가린 부분을
데이터셋의 평균 값으로 채워버립니다.

525
00:29:32,486 --> 00:29:39,583
그리고 가려진 이미지를 네트워크에 통과시키고
네트워크가 이 이미지를 예측한 확률을 기록합니다.

526
00:29:39,583 --> 00:29:44,752
그리고 이 가림 패치( occluded patch)를 전체 이미지에
대해 돌아가면서(slide) 같은 과정을 반복합니다.

527
00:29:44,752 --> 00:29:53,699
오른쪽의 히트맵은 이미지를 가린 patch의 위치에 따른
네트워크의 예측 확률의 변화를 의미합니다.

528
00:29:53,699 --> 00:29:59,952
이 실험의 아이디어는, 만약 이미지의 일부를 가렸는데
네트워크의 스코어의 변화가 크게 발생한다면

529
00:29:59,952 --> 00:30:04,809
가려진 바로 그 부분이 분류를 결정짓는데
아주 중요한 부분이었다는 사실을 짐작할 수 있는 것입니다.

530
00:30:04,809 --> 00:30:11,420
여기에 "occlusion experiment"을 수행한
세 가지 예시가 있습니다.

531
00:30:11,420 --> 00:30:14,456
가장 밑의 Go-kart의 예시를 살펴봅시다.

532
00:30:14,456 --> 00:30:23,077
빨간색 지역은 확률 값이 낮고,
노란색 지역은 확률 값이 높음을 의미합니다.

533
00:30:23,077 --> 00:30:30,348
앞쪽의 Go-kard를 가렸을 때 Go-kart에 대한 확률이
아주 많이 감소함을 볼 수 있습니다.

534
00:30:30,348 --> 00:30:38,419
이를 통해 네트워크가 분류를 결정할 때 실제로
go-kart를 아주 많이 고려한다는 사실을 알 수 있습니다.

535
00:30:38,419 --> 00:30:39,589
질문 있나요?

536
00:30:47,473 --> 00:30:49,780
질문은 "배경의 경우는 어떻게 된 것인지" 입니다.
(go-kart 이미지의 배경을 가려도 확률이 감소)

537
00:30:49,780 --> 00:30:56,020
현재 이미지가 너무 작아서 잘 안보이실 수도 있습니다만
뒷쪽 배경에 Go-kart 트랙과 다른 Go-kart들이 있습니다.

538
00:30:56,020 --> 00:31:00,395
제 생각에는 뒷쪽의 go-kart를 가리는 경우에도
점수에 영향을 미치는 것으로 사료됩니다.

539
00:31:00,395 --> 00:31:04,628
또는 수평선(horizon)의 영향일지도 모르겠습니다.
Go-kart를 검출하는데 수평선이 유용한 특징일지도 모릅니다.

540
00:31:04,628 --> 00:31:08,976
사실은 이렇게 해석하기 힘든 경우도 있습니다.
어쨌든 이 방법도 아주 훌륭한 시각화 방법입니다.

541
00:31:08,976 --> 00:31:10,118
질문 있나요?

542
00:31:20,486 --> 00:31:23,500
첫 번째 질문이 뭐였죠?

543
00:31:30,731 --> 00:31:36,802
이 예제의 경우에 이미지 한장에 대해서
현재 이미지의 모든 부분을 조금씩 가려본 것입니다.

544
00:31:36,802 --> 00:31:38,777
두 번 질문은 "이 시각화 방법이
학습에 어떻게 유용한지" 입니다.

545
00:31:38,777 --> 00:31:42,982
학습 하는데 직접적으로 유용하지는 않습니다. 여기에서 얻는
정보를 학습 과정에 집어 넣을 수는 없습니다.

546
00:31:42,982 --> 00:31:49,341
다만 "사람"이 네트워크가 무엇을 하고 있는지를
이해할 수 있는 도구일 뿐입니다.

547
00:31:49,341 --> 00:31:54,296
결국 이런 시각화는 네트워크의 성능을 높히려는
목적이 아니라 "여러분이 이해" 함이 목적입니다.

548
00:31:54,296 --> 00:31:57,890
그리고 이와 관련된 또 다른 아이디어가 있습니다.
"Saliency Map" 과 관련된 아이디어입니다.

549
00:31:57,890 --> 00:32:00,534
여러분이 앞으로 과제에서 보게될 것입니다.

550
00:32:00,534 --> 00:32:02,578
이 경우에도 같은 질문이 있습니다.

551
00:32:02,578 --> 00:32:07,831
입력 이미지가 들어옵니다. 이 경우에는 "개" 입니다.
그리고 이 이미지를 "개" 라고 예측했을 떄,

552
00:32:07,831 --> 00:32:11,796
우리가 알고싶은 것은 네트워크가 픽셀들을
보고서 이미지를 "개" 라고 분류했는지 입니다.

553
00:32:11,796 --> 00:32:19,452
앞서 특정 픽셀을 가리는(masking) 방법도 있었습니다만
Saliency Maps은 조금 다른 접근법을 취합니다.

554
00:32:19,452 --> 00:32:25,354
이 방법은 몇해 전 Karen Simonenian의 논문에서 나온
비교적 간단한 방법입니다.

555
00:32:25,354 --> 00:32:31,694
이 방법은 입력 이미지의 각 픽셀들에 대해서,
예측한 클래스 스코어의 그레디언트를 계산하는 방법입니다.

556
00:32:31,694 --> 00:32:36,042
이 방법은 일종의 "1차 근사적 방법"으로
어떤 픽셀이 영향력있는지를 알려줍니다.

557
00:32:36,042 --> 00:32:43,963
입력 이미지의 각 픽셀에 대해서, 우리가 그 픽셀을 조금
바꿨을 때 클래스 스코어가 얼마나 바뀔까요?

558
00:32:43,963 --> 00:32:50,496
이 질문은 어떤 픽셀이 "개"를 분류하는데 있어서 어떤 픽셀들이
필요한지알 수 있는 또 다른 방법이 될 수 있습니다.

559
00:32:50,496 --> 00:32:59,356
이 방법을 통해 "개" 이미지의 Saliency map을 만들어보면
"개"의 윤곽이 나타남을 알 수 있습니다.

560
00:32:59,356 --> 00:33:04,985
이는 네트워크가 이미지에서 어떤 픽셀들을
찾고 있는지를 짐작할 수 있습니다.

561
00:33:04,985 --> 00:33:11,675
이 방법은 다른 이미지들에도 적용해보면
네트워크가 올바른 지역을 보고있다는 것을 알 수 있습니다.

562
00:33:11,675 --> 00:33:13,360
좀 위안이 되는 것이죠

563
00:33:13,360 --> 00:33:14,462
질문 있나요?

564
00:33:17,407 --> 00:33:21,916
질문은 "사람들이 Saliency Maps을
semantic segmentation에도 사용하는지" 입니다.

565
00:33:21,916 --> 00:33:26,741
네 맞습니다. (웃음)
여러분들은 정말 최고입니다.

566
00:33:26,741 --> 00:33:29,513
semantic segmentation은 Karen의 논문에
또 다른 주제로 실려있습니다.

567
00:33:29,513 --> 00:33:38,925
아이디어, segmentation 레이블 없이 Saliency Maps 만
가지고 semantic segmentation을 수행할 수 있다는 것입니다.

568
00:33:38,925 --> 00:33:43,908
이들은 Grabcut Segmentation Algorithm을 이용합니다.
Grabcut에 대해서는 더 자세하게 말씀드리지는 않겠습니다.

569
00:33:43,908 --> 00:33:47,772
Grabcut은 간단히 말해
interactive segmentation algorithm 입니다.

570
00:33:47,772 --> 00:33:55,697
이 Saliency Map과 Grabcut을 잘 조합하면
이미지 내에서 객체를 Segmentation할 수 있습니다.

571
00:33:55,697 --> 00:34:00,326
아주 멋진 아이디어입니다.
하지만 그렇게 잘 되지는 않습니다.

572
00:34:00,326 --> 00:34:07,182
supervision을 가지고 학습을 시키는 네트워크에
비해서는 훨씬 더 안좋습니다.

573
00:34:07,182 --> 00:34:13,458
실용적인지는 잘 모르겠지만
작동한다는 것 자체가 정말 놀랍습니다.

574
00:34:13,458 --> 00:34:19,025
supervision으로 segmentation을 학습시킨
모델보다는 훨씬 안좋지만 말이죠

575
00:34:19,025 --> 00:34:23,791
또 다른 아이디어로는
guided back propagation이 있습니다.

576
00:34:23,791 --> 00:34:30,001
이번에 할 질문은, 어떤 한 이미지가 있을 떄

577
00:34:30,001 --> 00:34:37,420
이제는 클래스 스코어가 아니라
네트워크의 중간 뉴런을 하나 고릅니다.

578
00:34:37,420 --> 00:34:44,199
그리고 입력 이미지의 어떤 부분이, 내가 선택한
중간 뉴런의 값에 영향을 주는지를 찾는 것이죠

579
00:34:44,199 --> 00:34:49,059
이 경우에도 앞서 했던 방법처럼
Saliency Map을 만들어볼 수 있을 것입니다.

580
00:34:49,059 --> 00:34:53,466
이 경우에는 이미지의 각 픽셀에 대한
"클래스 스코어"의 그래디언트를 계산하는 것이 아니라

581
00:34:53,466 --> 00:34:58,815
입력 이미지의 각 픽셀에 대한 네트워크 중간
뉴런의 그레디언트를 계산합니다.

582
00:34:58,815 --> 00:35:05,832
이를 통해 어떤 픽셀이 해당 뉴런에
영향을 주는 지 알 수 있습니다.

583
00:35:05,832 --> 00:35:08,342
이 경우에도 평범한 back propagation을 이용합니다.

584
00:35:08,342 --> 00:35:15,093
하지만 back propagation 과정에서 조금의 트릭을 가미하면
조금 더 깨끗한 이미지를 얻을수 있습니다.

585
00:35:15,093 --> 00:35:21,393
이를 "guided back propagation" 이라고 합니다.
Zeilerand Fergus의 2014년 논문입니다.

586
00:35:21,393 --> 00:35:24,203
이번 시간에 이에 대해 더 깊게는 들어가지 않겠지만

587
00:35:24,203 --> 00:35:30,220
이 방법(guided backprop)은 backprop 시
ReLU를 통과할 때 조금의 변형을 가해줍니다.

588
00:35:30,220 --> 00:35:37,254
ReLU의 그레디언트의 부호가 양수 이면 그대로 통과시키고
부호가 음수이면 backprop하지 않는 방법입니다.

589
00:35:37,254 --> 00:35:46,948
이로인해 전체 네트워크가 실제 그레디언트를 이용하는
것이 아니라 "양의 부호인 그레디언트" 만을 고려하게 됩니다.

590
00:35:46,948 --> 00:35:53,614
이 방법이 왜 좋은지는 해당 논문을 각자
읽어보시기 바랍니다.

591
00:35:53,614 --> 00:36:01,649
실험 결과를 보면, guided backprop이 그냥에 비해
훨씬 더 선명하고 좋은 이미지를 얻을 수 있음을 알 수 있습니다.

592
00:36:01,649 --> 00:36:07,223
이 이미지들은 입력 이미지에 어떤 픽셀들이
특정 뉴런에 영향을 미치는지를 알려줍니다.

593
00:36:07,223 --> 00:36:12,467
앞서 슬라이드에서 "maximally activating patches"
파트에서 이와 비슷한 시각화기법을 살펴봤었습니다.

594
00:36:16,488 --> 00:36:20,174
"maximally activating patches" 기법 외에도

595
00:36:20,174 --> 00:36:27,604
"guided backpropagation" 이라는 방법을 통해서
패치의 어떤 부분이 뉴런에 영향을 미치는지 알 수 있습니다.

596
00:36:27,604 --> 00:36:37,139
맨 상당의 예를 살펴봅시다. 이미지들을 살펴보면
이 뉴런이 아마도 둥그런 것들을 찾고있음을 짐작해볼 수 있습니다.

597
00:36:37,139 --> 00:36:42,028
guided backprob 결과(왼쪽)를 살펴보면 방금 전 우리가
짐작했던 직관을 어느정도 확신할 수 있습니다.

598
00:36:42,028 --> 00:36:49,218
실제로 이미지 상의 둥근 부분들이 뉴런의 실제 값에
영향을 미치고 있음을 직접 확인할 수 있기 때문입니다.

599
00:36:49,218 --> 00:36:56,514
이렇게 guided backprob은 중간 레이어가 무엇을 찾고 있는지를
이해하기 위한 영상을 합성하는데 아주 유용합니다.

600
00:36:56,514 --> 00:37:05,108
하지만 guided backprob이나 saliency maps을 계산하는
방법들은 고정된 입력 이미지에 대한 연산을 수행할 뿐입니다.

601
00:37:05,108 --> 00:37:12,882
이 방법들은 "고정된" 입력 이미지, 또는 입력 패치의 어떤 부분이
해당 뉴런에 영향을 미치는지를 말해줍니다.

602
00:37:12,882 --> 00:37:19,110
그렇다면 입력 이미지에 의존적이지 않은 방법은 없을까요?

603
00:37:19,110 --> 00:37:24,641
해당 뉴런을 활성화시킬 수 있는
어떤 "일반적인" 입력 이미지가 있을까? 란 질문을 할 수 있습니다.

604
00:37:24,641 --> 00:37:29,118
이에 대한 질문은 "Gradient ascent" 라는
방법이 해답을 제시해줄 수 있습니다.

605
00:37:29,118 --> 00:37:34,903
우리는 지금까지 Loss를 최소화시켜 네트워크를
학습시키기 위해 Gradient decent를 사용했습니다.

606
00:37:34,903 --> 00:37:40,552
하지만 여기에서는 네트워크의
가중치들을 전부 고정시킵니다.

607
00:37:40,552 --> 00:37:50,932
그리고  Gradient ascent를 통해 중간 뉴런 혹은 클래스
스코어를 최대화 시키는 이미지의 픽셀들을 만들어냅니다.

608
00:37:50,932 --> 00:37:58,333
Gradient ascent는 네트워크의 가중치를 최적화하는
방법이 아닙니다. 가중치들은 모두 고정되어 있습니다.

609
00:37:58,333 --> 00:38:07,104
대신 뉴런, 또는 클래스 스코어가 최대화될 수 있도록
입력 이미지의 픽셀 값을 바꿔주는 방법입니다.

610
00:38:07,104 --> 00:38:10,475
이 방법에는 regularization term이 필요합니다.

611
00:38:10,475 --> 00:38:19,078
우리는 지금까지 regularization terms의 역할을 가중치들이
학습 데이터로의 과적합을 방지하기 위함으로 배웠습니다.

612
00:38:19,078 --> 00:38:27,109
이 경우에도 유사합니다. 생성된 이미지가  특정 네트워크의
특성에 완전히 과접합 되는 것을 방지하기 위함입니다.

613
00:38:27,109 --> 00:38:34,664
regularization term을 추가함으로서, 우리는 생성된 이미지가
두 가지 특성을 따르길 원하는 것입니다.

614
00:38:34,664 --> 00:38:39,269
하나는 이미지가 특정 뉴런의 값을 최대화시키는
방향으로 생성되길 원하는 것이고

615
00:38:39,269 --> 00:38:42,111
그리고 다른 하나는 이미지가 자연스러워 보여야 한다는 것입니다.

616
00:38:42,111 --> 00:38:46,485
생성된 이미지가 자연 영상에서 일반적으로 볼 수 있는
이미지이길(statistics) 원하는 것입니다.

617
00:38:46,485 --> 00:38:52,936
이런 류의 regularization term의 목적은
생성된 이미지가 비교적 자연스럽도록 강제하는 역할입니다.

618
00:38:52,936 --> 00:38:57,116
앞으로 다양한 regulaizer들을 살펴보겠습니다.

619
00:38:57,116 --> 00:39:04,371
Gradient Ascent는 비교적 간단한 방법입니다. 과제 3에서
여러분이 직접 구현해보실 수 있을 것입니다.

620
00:39:04,371 --> 00:39:10,410
Gradient Ascent를 위해서는 초기 이미지가 필요합니다.
이 이미지는 zeros, uniform, noise 등으로 초기화시켜줍니다.

621
00:39:10,410 --> 00:39:19,922
초기화를 하고나면 이미지를 네트워크에 통과시키고
여분이 관심있는 뉴런의 스코어를 계산합니다.

622
00:39:19,922 --> 00:39:26,643
그리고 이미지의 각 픽셀에 대한 해당 뉴런 스코어의
그레디언트를 계산하여 back prop을 수행합니다.

623
00:39:26,643 --> 00:39:33,897
여기에서는 Gradient Ascent를 이용해서 이미지 픽셀
자체를 업데이트합니다. 해당 스코어를 최대화시키려 하겠죠

624
00:39:33,897 --> 00:39:38,786
이 과정을 계속 반복하고나면
아주 멋진 이미지가 탄생합니다.

625
00:39:38,786 --> 00:39:42,311
여기에서도 이미지에 대한 regularizer를 언급하지 않을수 없습니다.

626
00:39:42,311 --> 00:39:49,428
여기에서는 단순하게 생성된 이미지에 대한
L2 norm을 계산해서 더해줍니다.

627
00:39:49,428 --> 00:39:51,466
사실 L2 norm을 추가하는 것 자체에 큰 의미가 있는 것은 아닙니다.

628
00:39:51,466 --> 00:40:01,764
Gradient Ascent와 같은 이미지 생성과 관련된 방법들의
초창기 문헌에서 종종 보이는 regularizer 중 하나일 뿐입니다.

629
00:40:01,764 --> 00:40:12,153
이 네트워크를 학습시켜보면, 가령 왼쪽 상당의 덤벨의
스코어를 최대화시키는 이미지가 생성됩니다.

630
00:40:12,153 --> 00:40:14,820
그리고 생성된 이미지를 살펴보면

631
00:40:14,820 --> 00:40:19,726
잘 안보이실 수도 있지만, 여러 덤벨 모양이 생성되었음을
알 수 있습니다.

632
00:40:19,726 --> 00:40:23,162
여러 덤벨들이 이곳 저곳에 중첩되어 있는 모양입니다.

633
00:40:23,162 --> 00:40:29,111
그리고 컵의 생성된 이미지를 보면 아주 다양한 컵들이
충첩되어 있는 이미지를 볼 수 있습니다.

634
00:40:29,111 --> 00:40:30,466
달마시안의 예가 정말 끝내줍니다.

635
00:40:30,466 --> 00:40:35,478
달마시안의 특징이라고 할 수 있는 검정/흰색 반점
무늬를 볼 수 있습니다.

636
00:40:35,478 --> 00:40:40,388
레몬의 경우에는 얼룩 얼룩한 노란색이 보입니다.

637
00:40:40,388 --> 00:40:43,539
여기 몇 가지 예제가 더 있습니다.
거위의 경우도 정말 놀랍습니다.

638
00:40:43,539 --> 00:40:46,514
키트 여우(kit fox)도 정말 키트 여우같아 보입니다.

639
00:40:46,514 --> 00:40:47,454
질문 있나요?

640
00:40:55,528 --> 00:40:57,929
질문은 "왜 이미지들이 무지개 색인지" 입니다.

641
00:40:57,929 --> 00:41:02,434
이 시각화 방법을 이용해서 실제 색상을
시각화하려면 상당히 까다롭습니다.

642
00:41:02,434 --> 00:41:06,693
실제 모든 이미지들은 0에서  255 사이의
값들로 이루어져야 합니다.

643
00:41:06,693 --> 00:41:10,395
이는 constrained optimization 문제입니다.

644
00:41:10,395 --> 00:41:15,721
하지만 Gradient ascent와 같은 일반적인 방법들은
제약조건이 없는(unconstrained) 경우입니다.

645
00:41:15,721 --> 00:41:21,848
따라서 여러분이 Projected gradient descent 와
같은 알고리즘을 사용하고 마지막에 rescale한 경우라면

646
00:41:21,848 --> 00:41:27,799
시각화 할 때 나타나는 색상과 관련해서는
너무 크게 신경쓸 필요 없습니다.

647
00:41:27,799 --> 00:41:28,702
질문 있나요?

648
00:41:32,801 --> 00:41:36,846
질문은 "아무 regularizer도 사용하지 않으면
어떻게 되는지" 입니다.

649
00:41:36,846 --> 00:41:44,860
그렇게 해도 클래스 스코어를 최대화시키는
어떤 이미지가 생성되기는 할 것입니다.

650
00:41:44,860 --> 00:41:48,522
하지만 그 이미지는 아무것도 아닌 것 처럼 보일 것입니다.
램덤 노이즈처럼 보일 뿐입니다.

651
00:41:48,522 --> 00:41:54,538
그렇긴 해도 그 이미지 자체가 가지는 아주 흥미로운
특징이 있습니다. 이는 나중에 더 자세히 배울 것입니다.

652
00:41:54,538 --> 00:42:00,913
어쩃든 이 이미지를 가지고는 네트워크가 어떤 것들을
찾고 있는지를 이해하기는 힘듭니다.

653
00:42:00,913 --> 00:42:09,607
따라서 이를 위해서는 regularizer를 추가해서
이미지가 조금 더 자연스럽게 생성되도록 하는 편이 좋습니다.

654
00:42:09,607 --> 00:42:10,471
질문 있나요?

655
00:42:34,416 --> 00:42:38,492
질문은 "multimodality를 다루는 다른 방법은 없는지"입니다.
(클래스 내에서도 다양한 mode가 있음.)

656
00:42:38,492 --> 00:42:44,847
당연히 있습니다. 현재 시각화에 대한 본격적인 이야기는
이제 시작입니다.

657
00:42:44,847 --> 00:42:51,517
시각화의 다른 접근법들은 regularizer를 더욱
향상시키고 이미지를 더 잘 시각화시키는 방법에 관한 것입니다.

658
00:42:51,517 --> 00:42:58,621
Jason Yesenski, et al의 논문이 있습니다. 이들은
아주 인상적인 regularizers를 추가했습니다.

659
00:42:58,621 --> 00:43:00,924
L2 norm constraint은 여전히 있습니다.

660
00:43:00,924 --> 00:43:06,213
그리고 최적화 과정에 이미지에  주기적으로
가우시안 블러를 적용합니다.

661
00:43:06,213 --> 00:43:12,441
그리고 주기적으로 값이 작은 픽셀들은 모두 0으로 만듭니다.

662
00:43:12,441 --> 00:43:14,694
낮은 기울기의 픽셀 값 중 일부는 0으로 설정됩니다.
그레디언트가 작은 값들도 모두 0으로 만듭니다.

663
00:43:14,694 --> 00:43:17,559
이는 일종의 projected Gradient descent
라고 볼 수 있습니다.

664
00:43:17,559 --> 00:43:24,555
생성된 이미지를 더 좋은 특성을 가진 이미지 집합으로
주기적으로 매핑시키는 방법입니다. (밑에 세가지 방법으로)

665
00:43:24,555 --> 00:43:28,241
가령 가우시안 블러와 같은 스무딩 연산을 통해서죠

666
00:43:28,241 --> 00:43:32,870
이 방법을 이용하면 훨씬 더 보기 좋은 이미지를
생성할 수 있습니다. 훨씬 더 깔끔합니다.

667
00:43:32,870 --> 00:43:38,553
홍학은 조금 더 홍학처럼 생겼습니다.
딱정벌레도 더 잘 보입니다.

668
00:43:38,553 --> 00:43:41,695
검은 백조도 검은 백조처럼 잘 보입니다.

669
00:43:41,695 --> 00:43:48,211
그 중에 당구대가 상당히 인상적입니다.
확실히 당구대의 구조가 선명합니다.

670
00:43:48,211 --> 00:43:55,209
이처럼 조금 괜찮은 regularizer들을 추가하게 되면
생성되는 이미지가 조금 더 깔끔해질 수 있습니다.

671
00:43:55,209 --> 00:44:01,038
이 과정은 최종 스코어에만 적용하는게 아니라
중간 뉴런에도 적용해볼 수 있습니다.

672
00:44:01,038 --> 00:44:10,111
"당구대" 클래스의 스코어를 최대화시키는 것이 아니라
중간의 뉴런을 최대화시키는 이미지를 생성해볼 수도 있습니다.

673
00:44:10,111 --> 00:44:11,118
질문 있나요?

674
00:44:16,743 --> 00:44:19,393
질문은 "이 예제의 네가지 이미지가 무엇인지" 입니다.

675
00:44:19,393 --> 00:44:21,794
우선 이 이미지는 처음에 램덤 초기화된 이미지입니다.

676
00:44:21,794 --> 00:44:25,681
따라서 이 네 개의 이미지들은 서로 다르게
램덤으로 초기화된 입력 이미지입니다.

677
00:44:28,106 --> 00:44:36,113
다시 수업으로 돌아가서, 앞서 클래스 스코어에 적용했던 과정은
네트워크의 중간 뉴런을 최대화시키는 과정에 동일하게 적용됩니다.

678
00:44:36,113 --> 00:44:40,174
이를 통해서 중간 뉴런이 무엇을 찾고있는지
짐작해볼 수 있습니다.

679
00:44:40,174 --> 00:44:44,605
가령 네번째 레이어의 경우에는 나선형의 무언가를
찾고있는 것 같습니다.

680
00:44:44,605 --> 00:44:49,703
또 어떤 뉴런은 애벌레를 찾는 것 같아 보입니다.
정확이 무엇인지는 구별하기 힘들군요

681
00:44:49,703 --> 00:44:56,585
여기 예제 이미지가 큰 이미지일수록 receptive
fileds 가 더 큰 뉴런들입니다.

682
00:44:56,585 --> 00:44:58,664
Receptive field가 클수록
이미지 패치 내에 더 큰 곳을 볼 수 있습니다.

683
00:44:58,664 --> 00:45:03,549
이런 뉴런들은 입력 이미지에서 더 큰 구조와
더 복잡한 패턴을 찾는 경향이 있습니다.

684
00:45:03,549 --> 00:45:04,802
아주 멋집니다.

685
00:45:07,499 --> 00:45:15,559
사람들이 이 시각화 방법이 이미지의 특징들을
잘 살리는 것에 아주 열광했습니다.

686
00:45:15,559 --> 00:45:23,697
방금 전 누군가가 질문해주신 multimodality 를
이 논문에서 아주 잘 다루고 있습니다.

687
00:45:23,697 --> 00:45:29,849
이 논문에서는 최적화 과정 속에 multimodality를
아주 명시적으로 다루고 있습니다.

688
00:45:29,849 --> 00:45:35,254
각 클래스마다 클러스터링 알고리즘을 수행합니다.

689
00:45:35,254 --> 00:45:42,667
한 클래스 내 서로 다른 모드들 끼리 다시 한번 클래스가 나뉩니다.
그리고 나뉜 모드들과 가까운 곳으로 초기화를 해주는 것이죠

690
00:45:42,667 --> 00:45:45,890
이 방법을 통해서 multimodality를 다룰 수 있는 것입니다.

691
00:45:45,890 --> 00:45:51,675
직관적으로 보면, 가령 여기 있는 여덟개의 이미지는
모두 식료품점(grocery store) 입니다.

692
00:45:51,675 --> 00:45:56,401
가장 상위의 이미지들은 선반 위에 전시된
물건들을 클로즈업 한 것 같아 보입니다.

693
00:45:56,401 --> 00:45:59,068
이들의 레이블은 "식료품 점" 입니다.

694
00:45:59,068 --> 00:46:04,221
그리고 하단의 이미지들은 사람들이 식료품점을
돌아다니고 있는 모슴인 것 같습니다.

695
00:46:04,221 --> 00:46:06,085
이 또한 식료품점으로 레이블링됩니다.

696
00:46:06,085 --> 00:46:08,073
하지만 이 둘은 아주 다르게 생겼습니다.

697
00:46:08,073 --> 00:46:10,988
많은 클래스들이 이렇게 multimodality를 가지고 있습니다.

698
00:46:10,988 --> 00:46:17,648
이미지를 생성할 때 이런 식으로  multimodality를
명시하게 되면 아주 좋은 (다양한) 결과를 얻을 수 있게 됩니다.

699
00:46:17,648 --> 00:46:22,569
예제를 조금 더 살펴보도록 하죠. 여러 클래스의
이미지를 합성해 보았습니다.

700
00:46:22,569 --> 00:46:31,840
피망, 카르둔, 딸기, 잭오랜턴 등을 볼 수 있습니다.
이미지가 아주 잘 생성되었습니다.

701
00:46:31,840 --> 00:46:38,177
다음 장은 깊게 들어가지는 않겠습니다만
이 방법은 훨씬 더 놀라운 방법입니다.

702
00:46:38,177 --> 00:46:43,623
이 방법은 이미지를 이쁘게 생성해 내기 위해서
훨씬 더 강력한 사전 정보(prior)를 이용합니다.

703
00:46:43,623 --> 00:46:48,921
여기 보이는 이미지들이 모두 ImageNet의 특정
클래스를 최대화하는 이미지를 생성해 낸 것입니다.

704
00:46:48,921 --> 00:46:59,020
기본 아이디어는 입력 이미지의 픽셀을 곧장 최적화하는 것
대신에 FC6를 최적화하는 것입니다.

705
00:46:59,020 --> 00:47:03,342
이를 위해서는 feature inversion network 등을 사용해야
하지만  자세히 들어가진 않겠습니다.

706
00:47:03,342 --> 00:47:05,290
관심있으신 분들은 논문을 읽어보시기 바랍니다.
아주 놀라운 방법입니다.

707
00:47:05,290 --> 00:47:11,905
요점은 이런 이미지 생성 문제에서
사전 지식(priors)를 추가하게 된다면

708
00:47:11,905 --> 00:47:16,662
아주 리얼한 이미지를 만들어낼 수 있다는 것입니다.

709
00:47:18,951 --> 00:47:23,839
이 방법도 여러분들이 시도해볼만한
아주 좋은 방법이 될 수 있습니다. 어쨋건,

710
00:47:23,839 --> 00:47:29,893
이미지 픽셀의 그레디언트를 이용해서 이렇게
이미지를 합성하는 방법은 아주 강력합니다.

711
00:47:29,893 --> 00:47:34,288
이를 통해 시도해볼 수 있는 아주 재미있는 것은 바로
네트워크를 속이는 이미지(fooling image)를 만드는 것입니다.

712
00:47:34,288 --> 00:47:43,362
우선 아무 이미지나 하나 고릅니다.
가령 코끼리 이미지를 골랐다고 해봅시다.

713
00:47:43,362 --> 00:47:49,418
그리고 네트워크가 이 이미지는 코알라 라고
분류하도록 이미지를 조금씩 바꿉니다.

714
00:47:49,418 --> 00:47:57,064
이렇게 코끼리 이미지를 조금씩 바꾸다보면
네트워크는 이 이미지를 코알라라고 분류해 버립니다.

715
00:47:57,064 --> 00:48:05,931
여러분이 혹시라도 코끼리가 갑자기 귀여운 귀를
가진 코알라로 변신하는 모습을 상상할 지 모르곘습니다만

716
00:48:05,931 --> 00:48:09,241
사실은 그런 일은 일어나지 않습니다.
이점이 아주 놀라운 점입니다.

717
00:48:09,241 --> 00:48:17,377
코끼리 사진을 가지고 코알라로 분류하도록
이미지를 바꿔보면

718
00:48:17,377 --> 00:48:24,853
실제로는 두 번째 이미지 처럼 보입니다. 네트워크는 두 번째
이미지를 코알라로 분류합니다. 우리에게는 별반 차이가 없습니다.

719
00:48:24,853 --> 00:48:28,016
아주 수상하기도(fishy) 하면서
놀랍기도 합니다.

720
00:48:28,016 --> 00:48:34,114
밑에 예제가 하나 더있습니다. 배 인것 같은데
ImageNet 클래스로는 Schooner(범선) 입니다.

721
00:48:34,114 --> 00:48:37,170
이제 네트워크가 아이팟으로 분류하도록 합니다.

722
00:48:37,170 --> 00:48:41,881
두 번째 이미지를 보면 우리에게는 똑같이 배인데
네트워크는 아이팟이라고 인식하고 있습니다.

723
00:48:41,881 --> 00:48:46,260
두 이미지 사이의 픽셀 값의 차이는 거의 없습니다.

724
00:48:46,260 --> 00:48:52,025
우리는 픽셀 값의 차이에서 코알라나 아이팟의
특징이라고는 찾아볼 수 없습니다.

725
00:48:52,025 --> 00:48:58,924
그저 랜덤한 패턴의 노이즈로 보일 뿐입니다. 그렇다면 질문은
"무슨 일이 일어난 것일까요?", "어떻게 이런게 가능한 것일까요?"

726
00:48:58,924 --> 00:49:03,635
굿조만한 우리 수업 게스트로
Ian Goodfellow를 초청할 예정입니다.

727
00:49:03,635 --> 00:49:08,068
Ian Goodfellow가 와서 이와관련한 현상을
조금 더 자세히 설명해 줄 것입니다. 아주 재밌을 것입니다.

728
00:49:08,068 --> 00:49:11,006
하지만 여기에서는 더 이상 언급하지 않겠습니다.
왜냐하면 여러분들의 과제이기 떄문이죠

729
00:49:11,006 --> 00:49:11,595
질문 있나요?

730
00:49:16,320 --> 00:49:20,050
질문은 "학습 데이터로도 네트워크를 속일 수 있는지" 입니다.

731
00:49:20,050 --> 00:49:27,214
이와 관련된 모든 것들은 Ian이 와서 소개해줄 것입니다.
Ian Goodfellow가 와서 강연 내내 다룰 것입니다.

732
00:49:27,214 --> 00:49:28,885
질문 있나요?

733
00:50:00,608 --> 00:50:03,478
질문은 "왜 우리가 이딴거(stuff)에 관심을 가져야 하는 것인지"

734
00:50:03,478 --> 00:50:08,685
제가 질문을 조금 거칠게 표현했습니다. 죄송합니다.

735
00:50:24,573 --> 00:50:32,027
질문은 "중간 뉴런을 이해하는 것이 어떻게 최종 클래스
분류를 이해하는데 도움을 줄 수 있는지" 입니다.

736
00:50:32,027 --> 00:50:38,921
사실 이런 식으로 중간 레이어를 시각화하는 방법들은
딥러닝의 비판에 대한 반응에서 유래합니다.

737
00:50:38,921 --> 00:50:43,011
딥러닝의 비판이라 함은
"딥러닝 이라는 블랙박스를 가진건 알겠어,

738
00:50:43,011 --> 00:50:47,350
- 그리고 그걸 그레디언트로 잘 학습시켜서 좋은 결과가
나오는건 알겠는데, 사실 결과를 믿진 못하겠어

739
00:50:47,350 --> 00:50:51,272
딥러닝의 결과가 어떻게 나왔는지를
당신조차 이해할수 없잖아?" 와 같은 질문이죠

740
00:50:51,272 --> 00:51:01,530
이런 많은 시각화 기법들은 왜 딥러닝이 분류 문제를
더 잘 푸는지를 사람들이 이해하기 위해 고안되었습니다.

741
00:51:01,530 --> 00:51:07,721
가령, 딥러닝이 아닌 다른 기계학습 기법들을 생각해보면

742
00:51:07,722 --> 00:51:10,493
가령 선형 모델의 경우가 일반적으로 해석하기 훨씬 더 수월합니다.

743
00:51:10,493 --> 00:51:17,457
각 가중치를 보고 입력 특징들 중 어떤 부분이 얼마나 모델이
결정을 내리는데 영향을 미치는지 해석하기 용이합니다.

744
00:51:17,458 --> 00:51:19,459
Random forest와 같은 dectision tree를 보면 말이죠.

745
00:51:19,459 --> 00:51:27,442
이런 기계학습 기법들이 블랙박스인 CNN보다는
훨씬 더 자연스럽고 해석하기도 쉽습니다.

746
00:51:27,442 --> 00:51:33,520
딥러닝에 대한 앞서 언급한 비판들에 대한 대안으로
시각화 기법들이 등장했습니다. "그래 모델이 복잡하긴 하지,

747
00:51:33,520 --> 00:51:37,263
하지만 들춰보면 해석 가능한 것들이 있다구!"
라고 말하는 것입니다.

748
00:51:37,263 --> 00:51:42,201
딥러닝 모델이 아무렇게나 분류하는 것이 아니라
의미있는 행동을 하고 있음을 증명하려는 것입니다.

749
00:51:44,891 --> 00:51:50,989
이미지에 그레디언트를 업데이트하는 방식으로 가능한
재미있는 아이디어가 하나 더 있습니다. "DeepDeram" 입니다.

750
00:51:50,989 --> 00:51:55,592
작년 Google에서 나온 아주 재미있는
블로그 포스팅이 있었습니다.

751
00:51:55,592 --> 00:52:00,859
DeepDream의 "과학적 가치"의 측면에서 보자면
그저 "재미" 만을 위한 것입니다.

752
00:52:00,859 --> 00:52:04,284
DeepDream의 목적은
"재미있는 이미지를 만드는 것" 입니다.

753
00:52:04,284 --> 00:52:10,186
부가적으로 모델이 이미지의 어떤 특징들을
찾고 있는지를 짐작할 수도 있습니다.

754
00:52:10,186 --> 00:52:15,275
DeepDeram에서는 입력 이미지를 CNN의
중간 레이어를 어느정도 통과시킵니다.

755
00:52:15,275 --> 00:52:17,035
그리고 back prop을 할 차례입니다.

756
00:52:17,035 --> 00:52:20,742
해당 레이어의 그레디언트를 
activation 값으로 설정합니다.

757
00:52:20,742 --> 00:52:25,427
그리고 back prob을 하여 이미지를 업데이트합니다.
이 과정을 계속 반복합니다.

758
00:52:25,427 --> 00:52:31,682
네트워크에 의해 검출된 해당 이미지의 특징들을
증폭시키려는 것으로 해석할 수 있습니다.

759
00:52:31,682 --> 00:52:35,875
해당 레이어에 어떤 특징들이 있던지
그 특징들을 그레디언트로 설정하면

760
00:52:35,875 --> 00:52:40,010
이는 네트워크가 이미지에서 이미 뽑아낸 특징들을
더욱 증폭시키는 역학을 하는 것입니다.

761
00:52:40,010 --> 00:52:46,918
그리고 이는 해당 레이어에서 나온 특징들의 
L2 norm을 최대화시키는 것으로 볼 수 있습니다.

762
00:52:46,918 --> 00:52:55,999
DeepDream의 코드는 아주 심플합니다. 
이는 여러분의 과제에 포함되어 있습니다.

763
00:52:55,999 --> 00:53:00,785
여러분이 앞으로 과제를 통해 접하겠지만
여기에는 몇 가지 트릭이 존재합니다.

764
00:53:00,785 --> 00:53:04,443
트릭 중 하나는 그레이언트를 계산하기에 앞서 
이미지를 조금씩 움직이는 것입니다.(jitter)

765
00:53:04,443 --> 00:53:11,187
원본 이미지를 그대로 네트워크에 통과시키는 것 대신에
이미지를 두 픽셀 정도 이동시킵니다.

766
00:53:11,187 --> 00:53:19,540
이는 regularizer 역할을 해서 자연스럽고
부드러운 이미지를 만들어줍니다.

767
00:53:19,540 --> 00:53:26,653
그리고 여기에 L1 Normalization도 들어갑니다. 
이는 이미지 합성 문제에서 아주 유용한 트릭입니다.

768
00:53:26,653 --> 00:53:33,843
그리고 픽셀 값을 한번 클리핑(Clipping) 해주기도 합니다.
이미지라면 값이 0-255 사이에 있어야만 합니다.

769
00:53:33,843 --> 00:53:39,335
이는 일종의 projected gradient decent인데  실제 
이미지가 존재할 수 있는 공간으로 매핑시키는 방법입니다.

770
00:53:39,335 --> 00:53:46,215
이렇게 하늘 이미지를 가지고 알고리즘을 수행시키면
이처럼 아주 재미있는 결과를 보실 수 있습니다.

771
00:53:46,215 --> 00:53:52,614
하늘에 조그만한 특징들이 보이실 것입니다.
이들을 앞선 과정을 통해 증폭된 것들입니다.

772
00:53:52,614 --> 00:53:59,007
돌연변이 동물같은 것들이 나타났습니다.
나선 모양의 것들도 나타났습니다.

773
00:53:59,007 --> 00:54:04,296
다양한 종류의 건축물과 자동차도 나타났습니다.
모든 것들이 아주 흥미로운 결과입니다.

774
00:54:04,296 --> 00:54:08,743
나타난 특징들 중에 사람들이 이름을 
지어낸 것들도 있습니다.

775
00:54:08,743 --> 00:54:12,133
아주 많이 나타난 것 중 하나가
"Admiral dog" 입니다.

776
00:54:12,133 --> 00:54:16,033
"돼지 달팽이 (pig snail), "낙타 새"(Camel-bird),
"개 물고기"(dog-fish) 도 있습니다.

777
00:54:16,033 --> 00:54:22,771
이 모든것들이 아주 흥미롭습니다만, 이러한 시각화에
"개" 가 아주 빈번하게 등장한다는 것은

778
00:54:22,771 --> 00:54:26,249
이 네트워크를 학습시킨 데이터와 관련이 있습니다.

779
00:54:26,249 --> 00:54:30,786
이 네트워크는 ImageNet Classification 데이터셋으로
학습시켰습니다. 1000개의 카테고리가 있습니다.

780
00:54:30,786 --> 00:54:32,915
그런데, 그 중 200개의 카테고리가 개입니다.

781
00:54:32,915 --> 00:54:44,027
따라서 시각화된 결과에 
"개"와 관련된 것들이 자주 나오는 것은 당연합니다.

782
00:54:44,027 --> 00:54:47,327
그리고 다른 레이어를 가지고 만들어보면
색다른 결과를 보실 수도 있습니다.

783
00:54:47,327 --> 00:54:52,708
이 예제의 경우 더 얕은 층의 레이어로 만든 이미지입니다.
앞서 보여드린 예제가 조금 더 깊은 층의 레이어로 만들었었죠

784
00:54:52,708 --> 00:54:57,791
더 얕은 층으로 만들어보면, 엣지나, 소용돌이 무늬와 
같은 것들을이 보입니다.

785
00:54:57,791 --> 00:55:01,766
얕은 레이어로 DeepDream을 수행하면 보이는 것들입니다.

786
00:55:01,766 --> 00:55:08,346
DeepDream에 멀티 스케일을 가미해서 오랫동안 돌려보면
다음과 같은 아주아주 놀라운 것들을 볼 수 있습니다.

787
00:55:08,346 --> 00:55:11,035
맞아요, 그래서 여기 그들은 일종의
멀티 스케일 프로세싱을하고 있습니다.

788
00:55:11,035 --> 00:55:12,599
그들은 작은 이미지로 시작합니다.

789
00:55:12,599 --> 00:55:14,631
작은 이미지에서 DeepDream을
실행 한 다음 더 크게 만듭니다.

790
00:55:14,631 --> 00:55:16,289
큰 이미지에서 DeepDream을 계속하십시오.

791
00:55:16,289 --> 00:55:18,781
이 멀티 스케일 처리로 반복되는 일종의 반복

792
00:55:18,781 --> 00:55:19,893
그리고 당신은 얻을 수 있습니다,

793
00:55:19,893 --> 00:55:22,479
그리고 아마 당신이 최종 규모를 완료 한 후에

794
00:55:22,479 --> 00:55:23,797
그런 다음 처음부터 다시 시작합니다.

795
00:55:23,797 --> 00:55:25,699
너는이 일에 사나운거야.

796
00:55:25,699 --> 00:55:28,126
그리고 당신은 정말 미친 이미지를 얻을 수 있습니다.

797
00:55:28,126 --> 00:55:30,256
그래서이 예제들은 모두 네트워크에서 나왔습니다.

798
00:55:30,256 --> 00:55:31,454
이미지 그물에서 훈련 받다

799
00:55:31,454 --> 00:55:35,216
MIT에서 MIT Places 데이터
세트라고하는 또 다른 데이터 세트가 있습니다.

800
00:55:35,216 --> 00:55:37,708
하지만 1,000 개의 객체 범주 대신

801
00:55:37,708 --> 00:55:40,224
대신 200 가지의 다른 유형의 장면이 있습니다.

802
00:55:40,224 --> 00:55:42,663
침실과 부엌 같은 것들을 좋아해요.

803
00:55:42,663 --> 00:55:45,202
그리고 지금이 DeepDream 절차를 반복한다면

804
00:55:45,202 --> 00:55:48,472
MIT 장소에서 훈련 된 네트워크를 사용합니다.

805
00:55:48,472 --> 00:55:50,868
우리는 정말 멋진 시각화를 얻습니다.

806
00:55:50,868 --> 00:55:53,251
이제 개, 민달팽이, 독수리 개 대신에

807
00:55:53,251 --> 00:55:55,610
그리고 그것은 우리가 종종 이런 것들을
얻는 것과 같은 종류의 물건입니다.

808
00:55:55,610 --> 00:55:59,491
이런 종류의 일본식 건축물의 지붕 모양

809
00:55:59,491 --> 00:56:02,104
또는 이러한 다양한 종류의 다리 또는 산맥.

810
00:56:02,104 --> 00:56:05,288
그들은 정말 멋진 멋진 시각화와 같습니다.

811
00:56:05,288 --> 00:56:08,121
DeepDream의 코드는
Google에서 온라인으로 공개합니다.

812
00:56:08,121 --> 00:56:11,685
당신은 그것을 밖으로 가서 자신의
아름다운 사진을 만들 수 있습니다.

813
00:56:11,685 --> 00:56:13,517
그래서 또 다른 종류의...

814
00:56:13,517 --> 00:56:14,535
미안 해요?

815
00:56:24,731 --> 00:56:28,252
그래서 질문은 무엇입니까?

816
00:56:28,252 --> 00:56:31,819
그래서 내가 말하는 것처럼, 만약
당신이, 왜냐하면 하나 이상의 x 제곱

817
00:56:31,819 --> 00:56:33,318
그것의 그라디언트에 x입니다.

818
00:56:33,318 --> 00:56:36,678
따라서 활성화 볼륨을 되돌려 보내면

819
00:56:36,678 --> 00:56:38,368
그 그라디언트는 max와 같습니다.

820
00:56:38,368 --> 00:56:41,082
이는 그라디언트를

821
00:56:41,082 --> 00:56:42,952
일부는 x 제곱을 한 것처럼...

822
00:56:42,952 --> 00:56:44,477
일부 값.

823
00:56:44,477 --> 00:56:46,707
따라서 표준을 극대화하는 것과 같습니다.

824
00:56:46,707 --> 00:56:49,665
그 층의 특징의 그것의.

825
00:56:49,665 --> 00:56:51,844
그러나 실제로 많은 구현

826
00:56:51,844 --> 00:56:53,118
당신은 명시 적으로 계산하지 않는 것을 볼 것입니다.

827
00:56:53,118 --> 00:56:56,511
그래디언트를 다시 보내지 마라.

828
00:56:56,511 --> 00:56:58,941
다른 종류의 유용하고 유용한 또 다른 종류의

829
00:56:58,941 --> 00:57:01,478
우리는이 기능 반전의 개념을 할 수 있습니다.

830
00:57:01,478 --> 00:57:04,024
그래서 이것은 다시 우리에게 어떤 유형의,

831
00:57:04,024 --> 00:57:06,187
캡처되는 이미지의 요소 유형

832
00:57:06,187 --> 00:57:07,687
네트워크의 여러 계층에서

833
00:57:07,687 --> 00:57:09,952
이제 우리가 할 일은 우리가하려고하는 것입니다.

834
00:57:09,952 --> 00:57:12,220
이미지를 찍고, 네트워크를 통해 그 이미지를 실행한다.

835
00:57:12,220 --> 00:57:15,832
해당 이미지 중 하나의 특징 값을 기록하십시오.

836
00:57:15,832 --> 00:57:18,423
이제 우리는 그 이미지를 재구성하려고 노력할 것입니다.

837
00:57:18,423 --> 00:57:20,283
그 특징 표현으로부터.

838
00:57:20,283 --> 00:57:22,198
그리고 지금 질문은

839
00:57:22,198 --> 00:57:23,998
얼마나 많이, 얼마나 좋아하는지에 따라

840
00:57:23,998 --> 00:57:25,885
재구성 된 이미지가 어떻게 생겼는지

841
00:57:25,885 --> 00:57:28,377
그것은 우리에게 어떤 유형의 정보에
대한 어떤 감각을 줄 것이다.

842
00:57:28,377 --> 00:57:31,074
그 이미지가 그 특징 벡터에 포착되었다.

843
00:57:31,074 --> 00:57:32,864
다시 그래디언트 상승으로이 작업을 수행 할 수 있습니다.

844
00:57:32,864 --> 00:57:34,191
일부 정규식.

845
00:57:34,191 --> 00:57:37,152
이제 점수를 최대화하는 대신

846
00:57:37,152 --> 00:57:39,611
대신 우리는 거리를 최소화하고 싶다.

847
00:57:39,611 --> 00:57:41,709
이 catch 특징 벡터 사이.

848
00:57:41,709 --> 00:57:44,397
그리고 생성 된 이미지의 계산 된 기능 사이.

849
00:57:44,397 --> 00:57:47,252
일치하는 새 이미지를 다시 시도하고 합성하려면

850
00:57:47,252 --> 00:57:50,014
이전에 계산 한 기능으로 돌아갑니다.

851
00:57:50,014 --> 00:57:53,453
그리고 여기 자주 보는 정규직의 또 다른 종류입니다.

852
00:57:53,453 --> 00:57:55,048
총 변화 정규화 기

853
00:57:55,048 --> 00:57:56,856
당신도 숙제를 볼 수 있습니다.

854
00:57:56,856 --> 00:57:59,118
여기 총합 정규화 기와 함께

855
00:57:59,118 --> 00:58:01,885
인접한 픽셀 사이의 패널링 차이점

856
00:58:01,885 --> 00:58:04,569
왼쪽과 오른쪽 모두 왼쪽과 오른쪽에

857
00:58:04,569 --> 00:58:05,954
인접한 상단과 하단.

858
00:58:05,954 --> 00:58:07,970
특별한 부드러움을 장려하려고 다시 시도하십시오.

859
00:58:07,970 --> 00:58:09,956
생성 된 이미지에서.

860
00:58:09,956 --> 00:58:12,521
이제 우리가 특징 반전에 대한이 아이디어를한다면

861
00:58:12,521 --> 00:58:14,542
여기 왼쪽에있는이 시각화

862
00:58:14,542 --> 00:58:16,369
우리는 몇 가지 원본 이미지를 보여주고 있습니다.

863
00:58:16,369 --> 00:58:18,294
코끼리 또는 과일 왼쪽에.

864
00:58:18,294 --> 00:58:19,802
그리고 우리는 그것을 실행합니다.

865
00:58:19,802 --> 00:58:22,458
우리는 VGG-16 네트워크를 통해 이미지를 실행합니다.

866
00:58:22,458 --> 00:58:25,512
해당 네트워크의 기능을 일부 레이어에 기록하십시오.

867
00:58:25,512 --> 00:58:28,036
일치하는 새 이미지를 합성하려고합니다.

868
00:58:28,036 --> 00:58:30,013
그 층의 기록 된 특징.

869
00:58:30,013 --> 00:58:32,514
그리고 이것은 우리에게 무엇에 대한 감각을 주는지입니다.

870
00:58:32,514 --> 00:58:35,599
얼마나 많은 정보가이 이미지에 저장되어 있는지.

871
00:58:35,599 --> 00:58:37,534
이러한 여러 레이어의 기능.

872
00:58:37,534 --> 00:58:39,900
예를 들어 이미지를 재구성하려고하면

873
00:58:39,900 --> 00:58:43,849
VGC-16의 relu2_2 기능을 기반으로합니다.

874
00:58:43,849 --> 00:58:46,628
이미지가 거의 완벽하게 재구성 된 것을 볼 수 있습니다.

875
00:58:46,628 --> 00:58:48,372
그 말은 우리가 정말로 버려서는 안된다는 것을 의미합니다.

876
00:58:48,372 --> 00:58:52,664
해당 레이어의 원시 픽셀 값에 대한 많은 정보

877
00:58:52,664 --> 00:58:55,155
그러나 우리가 네트워크의 더 깊은 부분으로 올라감에 따라

878
00:58:55,155 --> 00:58:58,593
relu4_3, relu5_1에서 재구성하려고합니다.

879
00:58:58,593 --> 00:59:01,311
우리는 우리의 재구성 된 이미지가 지금,

880
00:59:01,311 --> 00:59:03,435
우리는 일종의 일반적인 공간을 유지했습니다.

881
00:59:03,435 --> 00:59:05,488
이미지의 일반적인 공간 구조.

882
00:59:05,488 --> 00:59:08,322
당신은 아직도 코끼리 또는 바나나라고 말할 수 있습니다.

883
00:59:08,322 --> 00:59:09,684
또는 사과, 또는 사과.

884
00:59:09,684 --> 00:59:11,675
하지만 많은 낮은 수준의 세부 정보는 없습니다.

885
00:59:11,675 --> 00:59:13,461
픽셀 값이 정확히 무엇인지

886
00:59:13,461 --> 00:59:14,833
정확하게 색이 무엇인지,

887
00:59:14,833 --> 00:59:16,427
정확히 텍스처가 무엇인지.

888
00:59:16,427 --> 00:59:18,532
이것들은 낮은 수준의 세부 사항들입니다.

889
00:59:18,532 --> 00:59:20,923
이 네트워크의이 상위 계층에서 손실됩니다.

890
00:59:20,923 --> 00:59:22,366
그래서 그것은 우리에게

891
00:59:22,366 --> 00:59:24,756
어쩌면 우리가 네트워크의 편을 들으며 나아갈 때

892
00:59:24,756 --> 00:59:27,094
이 낮은 수준의 정보를 버리는 종류입니다.

893
00:59:27,094 --> 00:59:29,153
이미지의 정확한 픽셀에 대해

894
00:59:29,153 --> 00:59:31,408
대신에 약간의 주위를 지키려고 노력할 것입니다.

895
00:59:31,408 --> 00:59:33,888
더 많은 의미 론적 정보, 그것은 조금 불변하다.

896
00:59:33,888 --> 00:59:38,109
색상 및 질감과 같은 작은 변화에 대해서.

897
00:59:38,109 --> 00:59:41,798
그래서 우리는 스타일 이전을 위해 건물을 짓고 있습니다.

898
00:59:41,798 --> 00:59:42,835
그것은 정말로 시원하다.

899
00:59:42,835 --> 00:59:45,392
따라서 스타일 이전을 이해하는 것 외에도,

900
00:59:45,392 --> 00:59:47,185
기능 반전 외에도

901
00:59:47,185 --> 00:59:49,286
우리는 또한 관련된 문제에 대해서 이야기 할 필요가있다.

902
00:59:49,286 --> 00:59:51,029
텍스처 합성이라고합니다.

903
00:59:51,029 --> 00:59:53,790
그래서 텍스처 합성에서, 이것은 오래된 문제의 일종입니다.

904
00:59:53,790 --> 00:59:55,112
컴퓨터 그래픽에서.

905
00:59:55,112 --> 00:59:57,129
여기서 아이디어는 우리가

906
00:59:57,129 --> 00:59:58,626
텍스처의 패치입니다.

907
00:59:58,626 --> 01:00:00,465
이 작은 비늘 같은 것이 여기 있습니다.

908
01:00:00,465 --> 01:00:01,930
이제 우리는 어떤 모델을 만들고 싶다.

909
01:00:01,930 --> 01:00:05,792
그런 다음 동일한 질감의 더 큰 조각을 생성하십시오.

910
01:00:05,792 --> 01:00:08,935
예를 들어 큰 이미지를 생성하고자 할 수 있습니다.

911
01:00:08,935 --> 01:00:12,056
입력과 같은 종류의 많은 비늘이 들어 있습니다.

912
01:00:12,056 --> 01:00:15,986
그리고 이것은 다시 컴퓨터
그래픽에서 꽤 오래된 문제입니다.

913
01:00:15,986 --> 01:00:18,551
텍스트 합성에 가장 가까운 이웃 접근법이 있습니다.

914
01:00:18,551 --> 01:00:19,720
저것은 꽤 잘 작동합니다.

915
01:00:19,720 --> 01:00:21,659
여기에는 신경망이 없습니다.

916
01:00:21,659 --> 01:00:23,591
대신, 이런 종류의 간단한 알고리즘

917
01:00:23,591 --> 01:00:25,697
생성 된 이미지를 통해 우리가 행진합니다.

918
01:00:25,697 --> 01:00:27,792
스캔 라인 순서로 한 번에 하나의 픽셀.

919
01:00:27,792 --> 01:00:28,931
그런 다음 복사...

920
01:00:28,931 --> 01:00:32,000
그리고 현재 픽셀 주변의 이웃을 봅니다.

921
01:00:32,000 --> 01:00:34,742
이미 생성 한 픽셀을 기반으로

922
01:00:34,742 --> 01:00:38,229
그 근처의 가장 가까운 이웃을 계산하십시오.

923
01:00:38,229 --> 01:00:39,657
입력 이미지의 패치에서

924
01:00:39,657 --> 01:00:41,934
그런 다음 입력 이미지에서 한 픽셀을 복사합니다.

925
01:00:41,934 --> 01:00:43,968
그래서 아마도 여기에 대한 자세한
내용은 알 필요가 없습니다.

926
01:00:43,968 --> 01:00:46,902
아이디어는 고전적인 알고리즘이 많이 있다는 것입니다.

927
01:00:46,902 --> 01:00:48,889
텍스처 합성을 위해 꽤 오래된 문제입니다.

928
01:00:48,889 --> 01:00:52,749
근본적으로 신경 네트워크없이 이것을 할 수 있습니다.

929
01:00:52,749 --> 01:00:54,598
그리고 이런 종류의

930
01:00:54,598 --> 01:00:57,103
고전적인 질감 합성 알고리즘 의이 종류

931
01:00:57,103 --> 01:00:59,915
간단한 텍스처의 경우 실제로는 실제로 잘 작동합니다.

932
01:00:59,915 --> 01:01:02,117
그러나 더 복잡한 텍스처로 이동할 때

933
01:01:02,117 --> 01:01:04,855
어쩌면 픽셀을 복사하는 간단한 방법의 이러한 종류

934
01:01:04,855 --> 01:01:06,343
입력 패치에서 직접

935
01:01:06,343 --> 01:01:08,970
너무 잘 작동하지 않는 경향이 있습니다.

936
01:01:08,970 --> 01:01:11,857
그래서 2015 년에 정말 멋진 종이가있었습니다.

937
01:01:11,857 --> 01:01:14,270
그 신경 네트워크 기능을 적용하려고

938
01:01:14,270 --> 01:01:16,494
텍스처 합성의이 문제에.

939
01:01:16,494 --> 01:01:18,258
그리고 그걸 일종의 프레임으로 마무리 했어.

940
01:01:18,258 --> 01:01:19,907
기울기 상승 프로 시저,

941
01:01:19,907 --> 01:01:21,314
특징지도와 비슷한 종류의

942
01:01:21,314 --> 01:01:22,874
다양한 특징 매칭 목표들

943
01:01:22,874 --> 01:01:24,753
우리가 이미 본 것.

944
01:01:24,753 --> 01:01:28,203
그래서, 신경 텍스처 합성을 수행하기 위해

945
01:01:28,203 --> 01:01:30,558
그들은 그램 매트릭스의 개념을 사용합니다.

946
01:01:30,558 --> 01:01:31,708
그래서, 우리가 뭘하려고하는지,

947
01:01:31,708 --> 01:01:34,416
우리는 우리의 입력 텍스처를 사용할 것인가?

948
01:01:34,416 --> 01:01:36,372
이 경우 바위의 일부 사진

949
01:01:36,372 --> 01:01:37,830
그런 다음 그 입력 텍스처를 가져옵니다.

950
01:01:37,830 --> 01:01:40,351
일부 컨볼 루션 신경 네트워크를 통과시킨다.

951
01:01:40,351 --> 01:01:43,022
길쌈 기능을 끌어 낸다.

952
01:01:43,022 --> 01:01:44,347
네트워크의 일부 계층.

953
01:01:44,347 --> 01:01:47,536
그래서, 아마도이 convolutional
feature volume

954
01:01:47,536 --> 01:01:50,040
우리가 이야기 한 내용은 C에 의해
W에 의해 H 일 수 있습니다.

955
01:01:50,040 --> 01:01:53,596
또는 유감스러운, 그 네트워크의 그
층에서 W에 의해 C에 의해.

956
01:01:53,596 --> 01:01:56,515
그래서 이것을 H 차원 공간 격자로 생각할 수 있습니다.

957
01:01:56,515 --> 01:01:57,813
그리드의 각 지점에서,

958
01:01:57,813 --> 01:01:59,894
우리는이 C 차원 특징 벡터

959
01:01:59,894 --> 01:02:02,318
그 이미지의 거친 모습을 묘사

960
01:02:02,318 --> 01:02:04,347
그 시점에서.

961
01:02:04,347 --> 01:02:06,431
이제 우리는이 활성화 맵을 사용할 것입니다.

962
01:02:06,431 --> 01:02:10,179
이 입력 이미지의 텍스처의 디스크립터를 계산한다.

963
01:02:10,179 --> 01:02:12,066
그래서, 우리가 할 일은,

964
01:02:12,066 --> 01:02:14,124
이 두 가지 기능 항목 중 두 가지를 선택하십시오.

965
01:02:14,124 --> 01:02:15,294
입력 볼륨에.

966
01:02:15,294 --> 01:02:16,485
각 기능 열

967
01:02:16,485 --> 01:02:18,318
C 차원 벡터가됩니다.

968
01:02:18,318 --> 01:02:21,239
그리고 이제 그 두 벡터 사이의 외적을 취하십시오.

969
01:02:21,239 --> 01:02:23,390
우리에게 C 행렬을 제공합니다.

970
01:02:23,390 --> 01:02:24,660
이 C by C 매트릭스 지금

971
01:02:24,660 --> 01:02:26,393
공존에 대해 알려줍니다.

972
01:02:26,393 --> 01:02:30,333
이미지의 두 지점에서 서로 다른 기능을

973
01:02:30,333 --> 01:02:33,172
그렇다면, 요소라면, IJ와 같은 요소라면

974
01:02:33,172 --> 01:02:35,023
C에 의한 C 행렬은 크다.

975
01:02:35,023 --> 01:02:38,338
이는 두 개의 입력 벡터 중 두
요소 I와 J를 의미합니다.

976
01:02:38,338 --> 01:02:40,218
크고 그런 것이 었습니다.

977
01:02:40,218 --> 01:02:43,098
그래서, 이것은 어떻게 든 2 차 통계치를 포착합니다.

978
01:02:43,098 --> 01:02:46,477
해당 기능 맵에서 어떤 기능에 관한

979
01:02:46,477 --> 01:02:49,367
서로 다른 공간에서 함께 활동하는 경향이있다...

980
01:02:49,367 --> 01:02:51,572
다른 공간적 위치에서.

981
01:02:51,572 --> 01:02:53,833
이제 우리는이 과정을 반복 할 것입니다.

982
01:02:53,833 --> 01:02:55,911
모든 상이한 쌍의 특징 벡터를 사용하여

983
01:02:55,911 --> 01:02:58,071
이 H 격자의 모든 다른 점으로부터.

984
01:02:58,071 --> 01:02:59,386
그들 모두를 평균 해보면 우리에게 주어진다.

985
01:02:59,386 --> 01:03:01,664
우리 C by C 그램 행렬.

986
01:03:01,664 --> 01:03:04,323
그리고 나서 이것을 기술하기 위해 서술자로 사용됩니다.

987
01:03:04,323 --> 01:03:06,323
그 입력 이미지의 질감의 종류.

988
01:03:06,323 --> 01:03:08,308
그래서,이 그램 행렬에 대해 흥미로운 점은 무엇입니까?

989
01:03:08,308 --> 01:03:11,018
이제 모든 우주 정보를 버렸습니다.

990
01:03:11,018 --> 01:03:13,623
이 기능 볼륨에 있습니다.

991
01:03:13,623 --> 01:03:16,144
왜냐하면 우리는 모든 특징 벡터들의
쌍에 대해 평균을 구했기 때문에

992
01:03:16,144 --> 01:03:17,545
이미지의 모든 지점에서

993
01:03:17,545 --> 01:03:19,753
대신, 그것은 단지 두 번째 순서를 포착합니다.

994
01:03:19,753 --> 01:03:21,863
피처간에 동시 발생 통계.

995
01:03:21,863 --> 01:03:25,364
그리고 이것은 결국 텍스처에 대한 훌륭한 설명자가됩니다.

996
01:03:25,364 --> 01:03:27,640
그리고 그런데, 이것은 실제로 계산하는 것이 효율적입니다.

997
01:03:27,640 --> 01:03:31,922
그래서, 당신이 W에 의한 C by H를 가지고 있다면,

998
01:03:31,922 --> 01:03:34,548
당신은 W로 시간 H를보기 위해
그것을 재구성 할 수 있습니다.

999
01:03:34,548 --> 01:03:36,858
그 시대를 자신의 전치

1000
01:03:36,858 --> 01:03:38,180
이것을 한 번에 모두 계산하십시오.

1001
01:03:38,180 --> 01:03:39,682
그래서 그것은 매우 효율적입니다.

1002
01:03:39,682 --> 01:03:40,981
그러나 왜 당신은 궁금해 할 것입니다.

1003
01:03:40,981 --> 01:03:42,916
실제 공분산 행렬을 사용하지 않는다.

1004
01:03:42,916 --> 01:03:45,417
또는이 재미있는 그램 매트릭스가 아닌

1005
01:03:45,417 --> 01:03:47,298
대답은 공분산을 사용하는 것입니다...

1006
01:03:47,298 --> 01:03:50,082
실제 공분산 행렬 사용하기

1007
01:03:50,082 --> 01:03:51,845
그러나 계산하기에 조금 더 비쌉니다.

1008
01:03:51,845 --> 01:03:52,908
그래서, 실제로 많은 사람들이

1009
01:03:52,908 --> 01:03:55,203
그냥이 그램 매트릭스 디스크립터를 사용하십시오.

1010
01:03:55,203 --> 01:03:56,954
그럼 그게...이게...

1011
01:03:56,954 --> 01:04:00,146
일단 우리가 이런 종류의 텍스처의 신경 서술자를 갖게되면

1012
01:04:00,146 --> 01:04:03,168
유사한 유형의 그래디언트 상승 프로 시저를 사용합니다.

1013
01:04:03,168 --> 01:04:05,693
텍스처와 일치하는 새로운 이미지를 합성한다.

1014
01:04:05,693 --> 01:04:06,916
원본 이미지의

1015
01:04:06,916 --> 01:04:09,540
이제는이 기능 재구성과 비슷합니다.

1016
01:04:09,540 --> 01:04:10,913
우리는 전에 몇 슬라이드를 보았습니다.

1017
01:04:10,913 --> 01:04:14,052
하지만 대신 전체 지형지 물 맵을 재구성하려고합니다.

1018
01:04:14,052 --> 01:04:15,231
입력 이미지로부터.

1019
01:04:15,231 --> 01:04:17,178
대신, 우리는 단지 재구성하려고 시도하고 있습니다.

1020
01:04:17,178 --> 01:04:19,179
이 그램 매트릭스 텍스처 디스크립터

1021
01:04:19,179 --> 01:04:20,883
대신에 입력 이미지의.

1022
01:04:20,883 --> 01:04:23,077
그래서 실제로 어떤 모습일까요?

1023
01:04:23,077 --> 01:04:24,495
미리 훈련 된 모델을 다운로드하고,

1024
01:04:24,495 --> 01:04:25,969
기능 반전과 같습니다.

1025
01:04:25,969 --> 01:04:28,720
종종 사람들은 VGG 네트워크를 사용합니다.

1026
01:04:28,720 --> 01:04:31,580
당신은 당신에게 먹을 것입니다... 당신은
당신의 질감 이미지를 취할 것입니다,

1027
01:04:31,580 --> 01:04:33,008
VGG 네트워크를 통해 피드,

1028
01:04:33,008 --> 01:04:35,265
그램 행렬과 많은 다른 레이어들을 계산한다.

1029
01:04:35,265 --> 01:04:38,553
이 네트워크의

1030
01:04:38,553 --> 01:04:40,726
그런 다음 새 이미지를 초기화합니다.

1031
01:04:40,726 --> 01:04:43,019
임의의 초기화

1032
01:04:43,019 --> 01:04:45,427
다시 그라데이션 상승으로 보입니다.

1033
01:04:45,427 --> 01:04:47,414
우리가 본 다른 방법들처럼.

1034
01:04:47,414 --> 01:04:48,734
그래서, 당신은 그 이미지를 가지고 그것을 통과시킵니다.

1035
01:04:48,734 --> 01:04:50,414
동일한 VGG 네트워크,

1036
01:04:50,414 --> 01:04:52,530
다양한 레이어에서 그램 행렬을 계산합니다.

1037
01:04:52,530 --> 01:04:53,703
이제 손실 계산

1038
01:04:53,703 --> 01:04:57,213
그램 행렬들 사이의 L2 표준으로서

1039
01:04:57,213 --> 01:05:00,833
입력 텍스처와 생성 된 이미지의

1040
01:05:00,833 --> 01:05:03,012
그리고 나서 다시 소품을 찍고 픽셀을 계산합니다.

1041
01:05:03,012 --> 01:05:06,025
생성 된 이미지의 픽셀 그라디언트.

1042
01:05:06,025 --> 01:05:07,325
그리고 그라디언트 상승 단계를 만듭니다.

1043
01:05:07,325 --> 01:05:09,273
이미지의 픽셀을 조금씩 업데이트하십시오.

1044
01:05:09,273 --> 01:05:11,517
그리고 이제이 과정을 여러 번 반복하십시오.

1045
01:05:11,517 --> 01:05:13,259
앞으로 나아가고, 그램 행렬을 계산하고,

1046
01:05:13,259 --> 01:05:14,538
당신의 손실을 계산하고, 다시 ..

1047
01:05:14,538 --> 01:05:17,071
이미지에 그라디언트를 반복합니다.

1048
01:05:17,071 --> 01:05:19,342
그리고 일단 이것을하면, 결과적으로 생성을 끝낼 것입니다.

1049
01:05:19,342 --> 01:05:22,702
입력 텍스처와 아주 잘 어울리는 텍스처.

1050
01:05:22,702 --> 01:05:25,776
그래서, 이것은 Nip의 2015 년 논문에서 모두였습니다.

1051
01:05:25,776 --> 01:05:27,125
독일에있는 한 그룹.

1052
01:05:27,125 --> 01:05:30,022
그리고 텍스쳐 합성을위한 멋진 결과를 얻었습니다.

1053
01:05:30,022 --> 01:05:31,133
그래서, 정상에,

1054
01:05:31,133 --> 01:05:33,531
우리는 4 가지 다른 입력 텍스처를 보여줍니다.

1055
01:05:33,531 --> 01:05:36,362
이제 아래쪽에

1056
01:05:36,362 --> 01:05:38,562
이 텍스처 합성 접근법 수행하기

1057
01:05:38,562 --> 01:05:41,133
그램 행렬 매칭에 의해.

1058
01:05:41,133 --> 01:05:43,739
다른 레이어에서 그램 행렬을 계산하여 사용

1059
01:05:43,739 --> 01:05:45,681
이 pretrained convolutional 네트워크에서.

1060
01:05:45,681 --> 01:05:48,244
그래서 우리는이 매우 낮은 레이어를 사용하면

1061
01:05:48,244 --> 01:05:49,471
컨볼 루션 네트워크에서

1062
01:05:49,471 --> 01:05:51,273
그럼 우리는 일반적으로 일치하는 종류의...

1063
01:05:51,273 --> 01:05:53,743
우리는 일반적으로 올바른 색상의 얼룩을 얻습니다.

1064
01:05:53,743 --> 01:05:55,364
그러나 전반적인 공간 구조

1065
01:05:55,364 --> 01:05:56,965
순전히 보존되지 않습니다.

1066
01:05:56,965 --> 01:06:00,401
그리고 지금 우리가 더 큰 이미지로 나아갈 때

1067
01:06:00,401 --> 01:06:03,384
당신은 상위 계층에서이 그램 행렬을 계산합니다.

1068
01:06:03,384 --> 01:06:05,825
당신은 그들이 더 큰 패턴을
재구성하는 경향이 있음을 본다.

1069
01:06:05,825 --> 01:06:06,935
입력 이미지로부터.

1070
01:06:06,935 --> 01:06:10,107
예를 들어,이 전체 바위 또는이 전체 크랜베리.

1071
01:06:10,107 --> 01:06:11,834
그리고 지금, 이것은 꽤 잘 작동합니다.

1072
01:06:11,834 --> 01:06:14,188
이제 우리는이 새로운 이미지를 합성 할 수 있습니다.

1073
01:06:14,188 --> 01:06:16,323
그 종류의 일반 우주 통계와 일치합니다.

1074
01:06:16,323 --> 01:06:17,677
귀하의 의견을

1075
01:06:17,677 --> 01:06:19,418
그러나 그들은 픽셀 현명한 차이가 있습니다

1076
01:06:19,418 --> 01:06:21,445
실제 입력 자체에서.

1077
01:06:21,445 --> 01:06:22,528
문제?

1078
01:06:28,481 --> 01:06:30,847
그래서 문제는 어디에 손실을 계산합니까?

1079
01:06:30,847 --> 01:06:33,236
실제로, 좋은 결과를 얻고 싶습니다.

1080
01:06:33,236 --> 01:06:35,283
일반적으로 사람들은 그램 행렬을 계산할 것입니다.

1081
01:06:35,283 --> 01:06:36,421
많은 다른 층에서

1082
01:06:36,421 --> 01:06:38,684
마지막 손실은 모든 이들의 합계가 될 것입니다.

1083
01:06:38,684 --> 01:06:40,285
잠재적으로 가중 합계.

1084
01:06:40,285 --> 01:06:41,936
그러나 나는이 시각화를 위해,

1085
01:06:41,936 --> 01:06:44,369
서로 다른 층의 효과를 지적하려고 노력한다.

1086
01:06:44,369 --> 01:06:45,736
나는 이들이 재건축을하고 있었다고 생각한다.

1087
01:06:45,736 --> 01:06:47,940
하나의 레이어에서.

1088
01:06:47,940 --> 01:06:49,353
그래서, 지금은 정말로...

1089
01:06:49,353 --> 01:06:51,783
그런 다음 그들은 정말 훌륭한 아이디어를 가졌습니다.

1090
01:06:51,783 --> 01:06:52,999
이 논문 뒤의 종류

1091
01:06:52,999 --> 01:06:56,193
즉, 우리가이 텍스처 합성 접근법을한다면?

1092
01:06:56,193 --> 01:06:59,124
하지만 바위 나 크랜베리 같은 이미지를 사용하는 대신

1093
01:06:59,124 --> 01:07:01,417
우리가 그것을 작품의 조각과 동일하게 설정한다면 어떨까요?

1094
01:07:01,417 --> 01:07:03,748
그래서, 예를 들어, 당신이...

1095
01:07:03,748 --> 01:07:06,004
당신이 동일한 텍스쳐 합성 알고리즘을한다면

1096
01:07:06,004 --> 01:07:09,067
그램 행렬을 최대화하여 대신...

1097
01:07:09,067 --> 01:07:10,333
그러나 이제 우리는 예를 들어,

1098
01:07:10,333 --> 01:07:12,030
빈센트 반 고흐의 별이 빛나는 밤

1099
01:07:12,030 --> 01:07:14,656
또는 피카소의 뮤즈를 우리의 감촉으로...

1100
01:07:14,656 --> 01:07:18,175
우리의 입력 텍스처로, 그리고 나서 이것을 실행해라.

1101
01:07:18,175 --> 01:07:19,759
텍스처 합성 알고리즘.

1102
01:07:19,759 --> 01:07:21,727
그러면 생성 된 이미지가

1103
01:07:21,727 --> 01:07:23,602
흥미로운 조각을 재구성하다

1104
01:07:23,602 --> 01:07:25,683
그 작품들에서.

1105
01:07:25,683 --> 01:07:27,951
그리고 지금, 정말 재미있는 일이 일어납니다.

1106
01:07:27,951 --> 01:07:30,815
텍스처 합성의 아이디어를 결합하면

1107
01:07:30,815 --> 01:07:32,157
그램 매트릭스 매칭에 의한

1108
01:07:32,157 --> 01:07:34,616
특징 매칭에 의한 특징 반전.

1109
01:07:34,616 --> 01:07:37,151
그리고 나서 이것은 우리를이
멋진 알고리즘으로 이끌어줍니다.

1110
01:07:37,151 --> 01:07:38,988
스타일 이전이라고합니다.

1111
01:07:38,988 --> 01:07:42,716
스타일 전환에서 우리는 두 개의
이미지를 입력으로 사용하려고합니다.

1112
01:07:42,716 --> 01:07:44,688
1 개, 우리는 내용 심상을 가지고 갈 것이다

1113
01:07:44,688 --> 01:07:47,285
그것은 우리가 원하는 어떤 유형의
것과 같이 인도 할 것입니다.

1114
01:07:47,285 --> 01:07:49,813
우리는 일반적으로 출력물을 원하는 것처럼 만듭니다.

1115
01:07:49,813 --> 01:07:51,504
또한 우리에게 알려줄 스타일 이미지

1116
01:07:51,504 --> 01:07:53,220
일반적인 질감이나 스타일은 무엇입니까?

1117
01:07:53,220 --> 01:07:55,499
생성 된 이미지에

1118
01:07:55,499 --> 01:07:57,471
그리고 우리는 공동으로 기능 조정을 할 것입니다...

1119
01:07:57,471 --> 01:07:59,062
우리는 새로운 이미지를 생성 할 것입니다.

1120
01:07:59,062 --> 01:08:01,400
특징 재구성 손실을 최소화함으로써

1121
01:08:01,400 --> 01:08:02,596
콘텐츠 이미지의

1122
01:08:02,596 --> 01:08:05,661
스타일 이미지의 그램 매트릭스 손실.

1123
01:08:05,661 --> 01:08:07,064
그리고 우리가이 두 가지 일을 할 때

1124
01:08:07,064 --> 01:08:08,984
우리는 정말 멋진 이미지를 얻습니다.

1125
01:08:08,984 --> 01:08:12,261
예술적 스타일의 콘텐츠 이미지 종류를 렌더링합니다.

1126
01:08:12,261 --> 01:08:14,353
스타일 이미지의

1127
01:08:14,353 --> 01:08:16,192
그리고 지금 이것은 정말 멋지다.

1128
01:08:16,192 --> 01:08:18,317
그리고 당신은이 아름다운 인물들을 얻을 수 있습니다.

1129
01:08:18,317 --> 01:08:19,649
그래서 다시, 이런 종류의 모습

1130
01:08:19,649 --> 01:08:22,433
스타일 이미지와 콘텐츠 이미지를 가져갈 것입니다.

1131
01:08:22,433 --> 01:08:25,328
당신의 그램 행렬을 계산하기 위해
그것들을 당신의 네트워크로 보내라.

1132
01:08:25,328 --> 01:08:26,384
및 귀하의 기능.

1133
01:08:26,384 --> 01:08:28,129
이제 출력 이미지를 초기화합니다.

1134
01:08:28,129 --> 01:08:29,332
약간의 잡음이 있습니다.

1135
01:08:29,332 --> 01:08:30,937
앞으로 나아가십시오, 당신의 손실을 계산하십시오

1136
01:08:30,938 --> 01:08:33,648
뒤로 이동하여 이미지의 그라디언트를 계산하십시오.

1137
01:08:33,648 --> 01:08:35,242
반복해서이 과정을 반복하십시오.

1138
01:08:35,242 --> 01:08:38,265
생성 된 이미지의 픽셀에 그라디언트 어 센트를 적용합니다.

1139
01:08:38,265 --> 01:08:40,098
몇 백 회 반복 한 후에,

1140
01:08:40,099 --> 01:08:43,247
일반적으로 당신은 아름다운 이미지를 얻을 것이다.

1141
01:08:43,247 --> 01:08:45,950
따라서이 온라인 구현이 있습니다.

1142
01:08:45,950 --> 01:08:47,853
내 Gethub에, 많은 사람들이 사용하고 있습니다.

1143
01:08:47,853 --> 01:08:48,965
그리고 정말 멋집니다.

1144
01:08:48,965 --> 01:08:50,952
그래서, 당신은 할 수 있습니다, 이것은 일종의...

1145
01:08:50,952 --> 01:08:52,144
훨씬 더 많은 제어권을 제공합니다.

1146
01:08:52,144 --> 01:08:54,609
DeepDream과 비교하여 생성 된 이미지를

1147
01:08:54,609 --> 01:08:56,711
맞아, DeepDream에서, 당신은
많은 통제권을 가지고 있지 않습니다.

1148
01:08:56,711 --> 01:08:59,154
정확히 어떤 유형의 일이 일어날 것인가?

1149
01:08:59,154 --> 01:09:00,544
결국 나옵니다.

1150
01:09:00,544 --> 01:09:02,207
네트워크의 여러 레이어를 선택하면됩니다.

1151
01:09:02,207 --> 01:09:04,133
어쩌면 다른 수의 반복을 설정할 수 있습니다.

1152
01:09:04,133 --> 01:09:06,500
그 다음 개가 윙윙 거리며 튀어 나와 어디든 나옵니다.

1153
01:09:06,500 --> 01:09:07,624
그러나 스타일 이전과 함께,

1154
01:09:07,624 --> 01:09:09,182
너는 훨씬 더 정밀한 그레인 컨트롤을 얻는다.

1155
01:09:09,182 --> 01:09:11,228
그 결과가 어떻게 보이기를 바라는지

1156
01:09:11,228 --> 01:09:13,375
맞아, 지금까지, 다른 스타일의 이미지를 고르기.

1157
01:09:13,375 --> 01:09:14,910
동일한 콘텐츠 이미지로

1158
01:09:14,910 --> 01:09:17,729
완전히 다른 유형의 결과를 생성 할 수 있습니다.

1159
01:09:17,729 --> 01:09:19,099
그것은 정말로 시원하다.

1160
01:09:19,099 --> 01:09:21,529
또한 여기에서 하이퍼 매개 변수로 놀 수 있습니다.

1161
01:09:21,529 --> 01:09:23,355
우리가 공동으로 재구성하고 있기 때문에...

1162
01:09:23,356 --> 01:09:26,366
이 기능 재구성 손실을 최소화하고 있습니다.

1163
01:09:26,366 --> 01:09:27,497
콘텐츠 이미지의

1164
01:09:27,497 --> 01:09:30,349
그리고이 그램 매트릭스 재구성 스타일 이미지의 손실.

1165
01:09:30,350 --> 01:09:31,968
당신이 상수를 교환한다면,

1166
01:09:31,968 --> 01:09:34,346
그 두 용어와 손실 사이의 대기.

1167
01:09:34,346 --> 01:09:36,112
그럼 우리가 원하는 정도를 제어 할 수 있습니다.

1168
01:09:36,113 --> 01:09:37,322
내용과 일치하는

1169
01:09:37,322 --> 01:09:39,469
우리가 스타일에 얼마나 많이 매치시키고 싶은지.

1170
01:09:39,469 --> 01:09:41,647
다른 하이퍼 매개 변수가 많이 있습니다.

1171
01:09:41,647 --> 01:09:43,992
예를 들어 스타일 이미지의 크기를

1172
01:09:43,992 --> 01:09:45,707
그램 행렬을 계산하기 전에

1173
01:09:45,707 --> 01:09:47,840
너 한테 통제력을 줄 수있어.

1174
01:09:47,840 --> 01:09:49,660
기능의 규모가 무엇인지

1175
01:09:49,660 --> 01:09:51,162
재구성하고 싶은

1176
01:09:51,162 --> 01:09:52,344
스타일 이미지에서.

1177
01:09:52,344 --> 01:09:53,420
그래서, 당신은 여기에서 볼 수 있습니다,

1178
01:09:53,420 --> 01:09:54,919
우리는이 같은 재구성을했습니다.

1179
01:09:54,919 --> 01:09:57,314
유일한 차이점은 스타일 이미지가 얼마나 큰지입니다.

1180
01:09:57,314 --> 01:09:58,976
우리가 그램 행렬을 계산하기 전에.

1181
01:09:58,976 --> 01:10:01,304
그리고 이것은 다른 축을 넘겨줍니다.

1182
01:10:01,304 --> 01:10:04,263
너는 이걸 통제 할 수있어.

1183
01:10:04,263 --> 01:10:06,391
스타일 전송을 실제로 할 수도 있습니다.

1184
01:10:06,391 --> 01:10:07,670
여러 스타일 이미지

1185
01:10:07,670 --> 01:10:10,181
일종의 여러 그램 행렬과 일 치하는 경우

1186
01:10:10,181 --> 01:10:11,508
동시에.

1187
01:10:11,508 --> 01:10:13,431
그리고 그것은 멋진 결과입니다.

1188
01:10:13,431 --> 01:10:15,699
우리는 또한이 멀티 스케일 프로세스를 보았습니다...

1189
01:10:15,699 --> 01:10:17,569
그래서, 당신이 할 수있는 또 다른 멋진 일.

1190
01:10:17,569 --> 01:10:20,139
우리는 DeepDream을위한이 다중
스케일 처리에 대해 이야기했습니다.

1191
01:10:20,139 --> 01:10:22,454
DeepDream에서 멀티 스케일 처리 방법을 보았습니다.

1192
01:10:22,454 --> 01:10:25,105
정말 멋진 결과를 줄 수 있습니다.

1193
01:10:25,105 --> 01:10:27,440
그리고 유사한 유형의 다중 스케일
처리를 수행 할 수 있습니다.

1194
01:10:27,440 --> 01:10:29,330
스타일 이전에.

1195
01:10:29,330 --> 01:10:32,432
그러면 우리는 이와 같은 이미지를 계산할 수 있습니다.

1196
01:10:32,432 --> 01:10:36,506
저것은 초고 해상도, 이것이 4k 이미지라고 생각합니다.

1197
01:10:36,506 --> 01:10:38,101
우리가 가장 좋아하는 학교,

1198
01:10:38,101 --> 01:10:40,867
별이 빛나는 밤의 스타일로 렌더링됩니다.

1199
01:10:40,867 --> 01:10:42,652
그러나 실제로 이것은 계산하기에 너무 비싸다.

1200
01:10:42,652 --> 01:10:44,544
나는 이것이 하나의 GPU를 4 개 가지고 있다고 생각한다.

1201
01:10:44,544 --> 01:10:47,074
그래서 조금 비싸요.

1202
01:10:47,074 --> 01:10:49,253
우리는 다른 스타일, 다른 스타일의
이미지도 사용할 수 있습니다.

1203
01:10:49,253 --> 01:10:50,531
그리고 정말 멋진 결과를 얻으십시오.

1204
01:10:50,531 --> 01:10:51,800
동일한 콘텐츠 이미지에서

1205
01:10:51,800 --> 01:10:53,666
다시 말하지만, 고해상도.

1206
01:10:53,666 --> 01:10:56,451
당신이 할 수있는 또 다른 재미있는 일은

1207
01:10:56,451 --> 01:10:59,129
아시다시피, 실제로 공동 스타일 전송을 할 수 있습니다.

1208
01:10:59,129 --> 01:11:01,168
동시에 DeepDream을 사용합니다.

1209
01:11:01,168 --> 01:11:03,919
이제 우리는 세 가지 손실, 즉 콘텐츠 손실

1210
01:11:03,919 --> 01:11:05,606
스타일 손실과 이것...

1211
01:11:05,606 --> 01:11:09,017
그리고 DeepDream 손실은
규범을 극대화하려고 시도합니다.

1212
01:11:09,017 --> 01:11:11,623
그리고 이와 비슷한 것을 얻으십시오.

1213
01:11:11,623 --> 01:11:12,766
그래서 지금은 반 고흐입니다.

1214
01:11:12,766 --> 01:11:14,286
개가 쏜 총알이 사방에 나옵니다.

1215
01:11:14,286 --> 01:11:15,858
[웃음]

1216
01:11:15,858 --> 01:11:18,466
그래서, 정말 멋집니다.

1217
01:11:18,466 --> 01:11:19,867
하지만 문제가 있습니다.

1218
01:11:19,867 --> 01:11:21,399
이 스타일 전송 알고리즘

1219
01:11:21,399 --> 01:11:23,012
그것은 그들이 꽤 느리다는 것입니다.

1220
01:11:23,012 --> 01:11:24,140
맞아, 너는 생산해야 해.

1221
01:11:24,140 --> 01:11:26,633
정방향 및 역방향 패스를 많이 계산해야합니다.

1222
01:11:26,633 --> 01:11:28,482
미리 훈련 된 네트워크를 통해

1223
01:11:28,482 --> 01:11:30,164
이러한 이미지를 완성하기 위해.

1224
01:11:30,164 --> 01:11:32,489
특히 우리가 본 고해상도 결과

1225
01:11:32,489 --> 01:11:33,770
이전 슬라이드

1226
01:11:33,770 --> 01:11:36,160
4k 이미지의 각 전방 및 후방 통과

1227
01:11:36,160 --> 01:11:38,200
많은 계산과 많은 메모리를 사용할 것입니다.

1228
01:11:38,200 --> 01:11:40,878
그리고 당신이 그 반복의 수백을 할 필요가있는 경우에

1229
01:11:40,878 --> 01:11:42,569
이러한 이미지를 생성하는 것은 많은,

1230
01:11:42,569 --> 01:11:43,662
몇 분만에

1231
01:11:43,662 --> 01:11:46,340
강력한 GPU에서도.

1232
01:11:46,340 --> 01:11:48,371
따라서, 이러한 것들을 적용하는 것은
실제로 그렇게 실용적이지 않습니다.

1233
01:11:48,371 --> 01:11:50,320
실제로.

1234
01:11:50,320 --> 01:11:52,852
해결책은 이제 다른 신경 네트워크를 훈련시키는 것입니다.

1235
01:11:52,852 --> 01:11:54,874
우리를 위해 스타일 이전을 할 수 있습니다.

1236
01:11:54,874 --> 01:11:57,536
그래서 나는 작년에 관한 논문을 가지고 있었다.

1237
01:11:57,536 --> 01:12:00,129
아이디어는 우리가 어떤 스타일을 고쳐야한다는 것입니다.

1238
01:12:00,129 --> 01:12:01,615
처음에는 우리가 염려하는 것입니다.

1239
01:12:01,615 --> 01:12:03,164
이 경우, 별이 빛나는 밤.

1240
01:12:03,164 --> 01:12:04,241
그리고 이제는 달리기보다는

1241
01:12:04,241 --> 01:12:05,988
별도의 최적화 절차

1242
01:12:05,988 --> 01:12:08,034
우리가 합성하고 싶은 각 이미지에 대해

1243
01:12:08,034 --> 01:12:10,783
대신 우리는 하나의 피드 포워드
네트워크를 훈련 할 것입니다.

1244
01:12:10,783 --> 01:12:12,978
콘텐츠 이미지를 입력 할 수있는

1245
01:12:12,978 --> 01:12:15,748
양식 된 결과를 직접 출력 할 수 있습니다.

1246
01:12:15,748 --> 01:12:17,433
그리고 지금 우리가이 네트워크를 훈련시키는 방법

1247
01:12:17,433 --> 01:12:20,106
동일한 콘텐츠를 계산한다는 것입니다.

1248
01:12:20,106 --> 01:12:22,906
피드 포워드 네트워크 교육 중 스타일 손실

1249
01:12:22,906 --> 01:12:25,491
동일한 그라디언트를 사용하여 가중치를 업데이트하십시오.

1250
01:12:25,491 --> 01:12:26,848
피드 포워드 네트워크의

1251
01:12:26,848 --> 01:12:29,785
이제이 일이 기차에 몇 시간 걸릴 것입니다.

1252
01:12:29,785 --> 01:12:30,998
그러나 일단 훈련되면,

1253
01:12:30,998 --> 01:12:32,994
그런 다음 양식화 된 이미지를 만들기 위해

1254
01:12:32,994 --> 01:12:34,586
하나의 순방향 패스 만하면됩니다.

1255
01:12:34,586 --> 01:12:36,148
훈련 된 네트워크를 통해

1256
01:12:36,148 --> 01:12:37,957
따라서이 온라인 코드를 가지고 있습니다.

1257
01:12:37,957 --> 01:12:40,501
당신은 그것이 끝나는 것을 볼 수 있습니다...

1258
01:12:40,501 --> 01:12:43,513
일부 경우 상대적으로 비교 가능한 품질

1259
01:12:43,513 --> 01:12:45,864
이 매우 느린 최적화 기본 방법

1260
01:12:45,864 --> 01:12:47,101
하지만 지금은 실시간으로 실행됩니다.

1261
01:12:47,101 --> 01:12:49,880
그것은 천 배나 더 빠릅니다.

1262
01:12:49,880 --> 01:12:53,101
그래서, 여기에서 볼 수 있습니다.이
라이브 데모 데모와 같습니다.

1263
01:12:53,101 --> 01:12:54,990
내 웹캠 끄기.

1264
01:12:54,990 --> 01:12:57,347
그래서, 지금 당장 라이브로 실행되지 않습니다.

1265
01:12:57,347 --> 01:12:59,523
하지만 큰 GPU를 가지고 있다면

1266
01:12:59,523 --> 01:13:01,528
네 가지 스타일을 쉽게 실행할 수 있습니다.

1267
01:13:01,528 --> 01:13:03,391
실시간으로 동시에

1268
01:13:03,391 --> 01:13:05,476
왜냐하면 그것은 매우 효율적이기 때문입니다.

1269
01:13:05,476 --> 01:13:07,191
저기... 러시아에서 다른 그룹이 있었어.

1270
01:13:07,191 --> 01:13:08,569
그게 아주 비슷 했어.

1271
01:13:08,569 --> 01:13:10,809
그것은 매우 유사한 종이를 동시에 가지고있었습니다.

1272
01:13:10,809 --> 01:13:12,650
그 결과는 좋았습니다.

1273
01:13:12,650 --> 01:13:15,392
그들은 또한이 종류의 알고리즘을 가지고있었습니다.

1274
01:13:15,392 --> 01:13:19,140
따라서 우리가 훈련하고있는이 피드 포워드 네트워크

1275
01:13:19,140 --> 01:13:20,872
끝내주는 이들이 많이 보입니다.

1276
01:13:20,872 --> 01:13:22,962
우리가 본 세분화 모델.

1277
01:13:22,962 --> 01:13:25,450
따라서 이러한 세분화 네트워크,

1278
01:13:25,450 --> 01:13:27,906
시멘틱 세분화를 위해 우리는 다운 샘플링을하고있다.

1279
01:13:27,906 --> 01:13:29,711
그 다음에 많은 층, 그리고 많은 층

1280
01:13:29,711 --> 01:13:32,551
그리고 나서 약간의 샘플링 [mumbling]

1281
01:13:32,551 --> 01:13:35,654
전이 경련이있는 상태에서

1282
01:13:35,654 --> 01:13:37,678
보다 효율적인 샘플 업 샘플 다운.

1283
01:13:37,678 --> 01:13:39,735
유일한 차이점은이 마지막 레이어

1284
01:13:39,735 --> 01:13:41,547
3 채널 출력을 생성합니다.

1285
01:13:41,547 --> 01:13:45,244
최종 이미지의 RGB에 대해.

1286
01:13:45,244 --> 01:13:47,247
이 네트워크 내부에는 일괄 정규화가 있습니다.

1287
01:13:47,247 --> 01:13:48,540
다양 한 레이어에서.

1288
01:13:48,540 --> 01:13:50,275
그러나이 신문에서, 그들은 소개...

1289
01:13:50,275 --> 01:13:52,003
배치 정규화를 바꿉니다.

1290
01:13:52,003 --> 01:13:53,693
인스턴스 정규화라고하는 다른 것을 위해

1291
01:13:53,693 --> 01:13:56,027
당신에게 훨씬 더 나은 결과를주는 경향이 있습니다.

1292
01:13:56,027 --> 01:13:59,191
따라서 이러한 유형의 메소드의 단점은

1293
01:13:59,191 --> 01:14:02,384
우리는 이제 하나의 새로운 스타일 전송
네트워크를 훈련하고 있습니다...

1294
01:14:02,384 --> 01:14:05,500
각자를 위해... 우리가 적용하고 싶은 작풍을 위해.

1295
01:14:05,500 --> 01:14:07,038
그래서 비싸 질 수 있습니다.

1296
01:14:07,038 --> 01:14:08,454
지금 당신이 많이 지킬 필요가 있다면

1297
01:14:08,454 --> 01:14:10,433
주위에 다른 훈련 된 네트워크의.

1298
01:14:10,433 --> 01:14:13,534
Google에서 온 종이가 방금 도착했습니다.

1299
01:14:13,534 --> 01:14:16,047
꽤 최근에이 문제를 해결했습니다.

1300
01:14:16,047 --> 01:14:18,450
하나의 피드 포워드 훈련 된 네트워크 사용

1301
01:14:18,450 --> 01:14:21,178
입력 이미지에 다양한 스타일을 적용 할 수 있습니다.

1302
01:14:21,178 --> 01:14:22,884
이제는 한 네트워크를 교육 할 수 있습니다.

1303
01:14:22,884 --> 01:14:26,887
테스트 시간에 다양한 스타일 적용

1304
01:14:26,887 --> 01:14:28,034
하나의 숙련 된 네트워크를 사용합니다.

1305
01:14:28,034 --> 01:14:30,839
이제 콘텐츠 이미지 입력을 가져 오려고합니다.

1306
01:14:30,839 --> 01:14:33,397
적용 할 스타일의 ID뿐만 아니라

1307
01:14:33,397 --> 01:14:34,805
그런 다음 하나의 네트워크를 사용하고 있습니다.

1308
01:14:34,805 --> 01:14:36,477
다양한 스타일을 적용 할 수 있습니다.

1309
01:14:36,477 --> 01:14:39,365
그리고 다시 실시간으로 실행됩니다.

1310
01:14:39,365 --> 01:14:42,346
같은 알고리즘이 이런 종류의 스타일
블렌딩을 할 수 있습니다.

1311
01:14:42,346 --> 01:14:44,442
하나의 숙련 된 네트워크에서 실시간으로

1312
01:14:44,442 --> 01:14:45,935
자 이제이 네트워크를 훈련하면

1313
01:14:45,935 --> 01:14:47,453
이 네 가지 스타일

1314
01:14:47,453 --> 01:14:49,668
실제로 이러한 스타일의 혼합을 지정할 수 있습니다.

1315
01:14:49,668 --> 01:14:52,458
정말 시원한 테스트 시간에 적용해야합니다.

1316
01:14:52,458 --> 01:14:55,759
그래서 이러한 종류의 실시간 스타일 전송 방법

1317
01:14:55,759 --> 01:14:58,552
다양한 앱에 있습니다.이 앱을 볼 수 있습니다.

1318
01:14:58,552 --> 01:15:01,976
요즘은 요즘 많이 요

1319
01:15:01,976 --> 01:15:04,071
오늘날 우리가 본 것을 요약 해줍니다.

1320
01:15:04,071 --> 01:15:05,896
우리는 많은 다른 방법들에 대해서 이야기 해왔다.

1321
01:15:05,896 --> 01:15:08,113
CNN 대표를 이해하기 위해

1322
01:15:08,113 --> 01:15:10,190
우리는 이러한 활성화 기반 방법
중 일부에 대해 이야기했습니다.

1323
01:15:10,190 --> 01:15:12,254
가장 가까운 이웃과 같은, 차원 감소,

1324
01:15:12,254 --> 01:15:14,220
최대 패치, 교합 이미지

1325
01:15:14,220 --> 01:15:16,676
활성화 값을 기반으로 이해하려고 시도합니다.

1326
01:15:16,676 --> 01:15:18,316
기능이 무엇을 찾고 있는지 알려줍니다.

1327
01:15:18,316 --> 01:15:20,461
우리는 또한 그라디언트 기반 방법에 대해 이야기했다.

1328
01:15:20,461 --> 01:15:23,754
그라데이션을 사용하여 새 이미지 합성

1329
01:15:23,754 --> 01:15:27,127
돌출지도 같은 기능 이해하기

1330
01:15:27,127 --> 01:15:29,382
클래스 시각화, 속이는 이미지,

1331
01:15:29,382 --> 01:15:30,417
피쳐 반전.

1332
01:15:30,417 --> 01:15:31,620
그리고 우리는 또한 재미있는 시간을 보았습니다.

1333
01:15:31,620 --> 01:15:33,110
이 비슷한 아이디어가 얼마나 많은지

1334
01:15:33,110 --> 01:15:35,528
Style Transfer와 DeepDream
같은 것들에 적용될 수 있습니다.

1335
01:15:35,528 --> 01:15:37,997
정말 멋진 이미지를 생성합니다.

1336
01:15:37,997 --> 01:15:40,397
다음에 우리는 감독되지 않은 학습에
대해서 이야기 할 것입니다.

1337
01:15:40,397 --> 01:15:42,380
자동 인코딩, 변이 자동 인코딩

1338
01:15:42,380 --> 01:15:43,917
생성 적 공격 네트워크

1339
01:15:43,917 --> 01:15:45,834
그래서 재미있는 강의가되어야합니다.

