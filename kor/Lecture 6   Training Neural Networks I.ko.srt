1
00:00:10,729 --> 00:00:12,896
시작하겠습니다.

2
00:00:16,381 --> 00:00:21,529
이번 시간에는 nerural network를 학습시키는 방법을
심도깊게 살펴보겠습니다.

3
00:00:23,166 --> 00:00:28,785
우선 공지사항먼저 전달드리자면 
첫 번째 과제는 오늘까지입니다.

4
00:00:28,785 --> 00:00:36,521
금일 11:59 p.m 까지 Canvas에 제출해주시기 바라며, 
더불어 과제 2가 새롭게 나갈 것입니다.

5
00:00:36,521 --> 00:00:40,082
그리고 여러분의 프로젝트 제안서는
25일까지 제출해 주시기 바랍니다.

6
00:00:40,082 --> 00:00:46,591
아직 준비가 안되신 분들이면 서둘러 해주시기 바랍니다.

7
00:00:46,591 --> 00:00:54,804
어느정도 구상한 분들이 계십니까?
몇 분정도 계시는군요

8
00:00:54,804 --> 00:01:03,937
어떻게 해야할지 모르겠다 싶으면 
TA 사무실로 방문해 주시면 도와드리겠습니다.

9
00:01:05,657 --> 00:01:18,121
Piazza에 다른 연계 프로젝트도 있으니 확인해 주시기 바랍니다.

10
00:01:19,604 --> 00:01:28,004
그리고 Justin이 만든 "Linear Layer에서의 Backporp", 
"vector/tensor 미분" 관련자료가 있으니 참고해주세요.

11
00:01:28,004 --> 00:01:33,964
이 자료를 보시면 vector/matrix에서 backporp이 어떻게
동작하는지 이해하실 수 있을 것입니다.

12
00:01:33,964 --> 00:01:40,484
Syllabus에 Lecture 4에 링크되어 있으니 보시면 됩니다.

13
00:01:45,110 --> 00:01:57,124
지난 시간에 함수를 computational graph로 표현하는
방법을 배웠습니다. 어떤 함수든 이렇게 표현할 수 있습니다.

14
00:01:57,124 --> 00:02:03,751
Neural network에 대해서 자세히 배웠습니다.
여기 보이는 것 처럼 Linear Layer들이 있었죠

15
00:02:03,751 --> 00:02:08,360
비 선형 연산자들을 끼워넣으면서 여러 층으로 쌓았습니다.

16
00:02:09,456 --> 00:02:13,360
지난 시간에 CNN도 배웠습니다.

17
00:02:13,360 --> 00:02:24,936
CNN은 "Spatial structure" 를 사용하기 위해 
Conv Layer를 사용하는 NN의 특수한 형태이입니다.

18
00:02:24,936 --> 00:02:38,056
Conv 필터가 입력 이미지를 슬라이딩해서 계산한 값들이 모여 
출력 Activation map을 만들게 됩니다.

19
00:02:38,056 --> 00:02:45,456
그리고 Conlayer에서는 보통 여러개의 필터를 사용했습니다.
각 필터는 서로 다른 Activation map을 생성합니다.

20
00:02:45,456 --> 00:02:50,655
여기 그림은 보면 어떤 입력이 있고 Depth가 있습니다. 
이제 Activation map 을 만들어야 하는데

21
00:02:50,655 --> 00:02:58,771
Activation map는 필터의 갯수만큼 존재하며 각 map은 
입력의 공간적인 정보를 보존하고 있습니다.

22
00:02:59,695 --> 00:03:05,895
자 이제 우리가 하고싶은 것은 가중치, 즉 파라미터를
업데이트 하고 싶은 것입니다.

23
00:03:05,895 --> 00:03:12,507
지난 시간에 배웠듯이 Optimization을 통해서 
네트워크의 파라미터를 학습할 수 있었습니다.

24
00:03:12,507 --> 00:03:17,254
그리고 Loss 라는 산에서 Loss가 줄어드는 방향으로
이동하고 싶은 것입니다.

25
00:03:17,254 --> 00:03:23,053
그리고 이는 gradient의 반대 방향으로 이동하는 것과 같습니다.

26
00:03:23,053 --> 00:03:27,614
이를 Mini-batch Stochastic Gradient Descent
하고 했습니다.

27
00:03:27,614 --> 00:03:38,585
Mini-batch SGD는 우선 데이터의 일부만 가지고
Forword pass를 수행한 뒤에 Loss를 계산합니다.

28
00:03:38,585 --> 00:03:41,960
그리고 gradient를 계산하기 위해서 
backprop를 수행합니다.

29
00:03:41,960 --> 00:03:47,986
그리고 그 gradient를 이용해서 파마미터를 업데이트합니다.

30
00:03:49,980 --> 00:03:58,321
향 후 몇번의 강의는 NN의 학습에 대해 다룰 것입니다.

31
00:03:58,321 --> 00:04:02,441
우선 NN 학습을 처음 시작할때 필요한 기본 설정에 대해 알아볼 것입니다.

32
00:04:02,441 --> 00:04:11,015
활성함수 선택, 데이터 전처리, 가중치 초기화
Regularization, gradient checking 등이 이에 속하죠

33
00:04:11,015 --> 00:04:16,118
또한 Training dynamics에 대해서도 다룰 것입니다.
학습이 잘 되고 있는지 확인하는 법에 대해 배울 것입니다.

34
00:04:16,118 --> 00:04:21,294
어떤 방식으로 파라미터를 업데이트할 것인지에 대해서도 배울 것이고

35
00:04:21,294 --> 00:04:26,241
가장 적절한 하이퍼파라미터를 찾기 위해
하이퍼파라미터를 Optimization 하는 방법도 배울 것입니다.

36
00:04:26,241 --> 00:04:28,281
그리고 평가에 대해서도 이야기 할 것입니다.

37
00:04:28,281 --> 00:04:29,948
그리고 Model ensemble에 대해서도 배웁니다.

38
00:04:33,000 --> 00:04:41,015
오늘은 Part 1 으로 활성함수, 데이터 전처리,
가중치 초기화, Batch Normalization

39
00:04:41,015 --> 00:04:45,412
학습 과정 다루기, 하이퍼 파라미터 Optimization 을
배워 보도록 하겠습니다.

40
00:04:47,348 --> 00:04:50,348
자 Activation Function 부터 시작해봅시다.

41
00:04:51,708 --> 00:04:55,095
자 먼저 지난시간에 봤던 Layer를 살펴보면

42
00:04:55,095 --> 00:05:01,481
데이터 입력이 들어오면 가중치와 곱합니다.
FC 나 CNN이 될 수 있겠죠

43
00:05:01,481 --> 00:05:06,388
그 다음 활성함수, 즉 비선형 연산을 거치게 됩니다.

44
00:05:06,388 --> 00:05:08,027
활성함수의 예시를 보겠습니다.

45
00:05:08,027 --> 00:05:13,295
지난 시간에 Sigmoid를 본적이 있죠 
ReLU도 봤습니다.

46
00:05:13,295 --> 00:05:20,479
이번 시간에는 좀 더 다양한 종류의 활성함수와 
그들간의 Trade-off에 대해 다뤄보도록 하겠습니다.

47
00:05:22,228 --> 00:05:27,241
자 우선 Sigmoid부터 보겠습니다. 
우리한테 제일 만만한 놈이죠

48
00:05:27,241 --> 00:05:32,572
Sigmoid는 이렇게 생겼습니다. 
1/(1+e^-x) 이죠

49
00:05:32,572 --> 00:05:45,201
얘가 하는 일은 각 입력을 받아서 그 입력을 [0, 1] 사이의
값이 되도록 해줍니다.

50
00:05:45,201 --> 00:05:50,427
입력의 값이 크면 Sigmoid의 출력은 1에 가까울 것이고

51
00:05:50,427 --> 00:05:55,321
값이 작으면 0에 가까울 것입니다.

52
00:05:55,321 --> 00:06:02,481
0 근처 구간(rigime)을 보면 선형스럽습니다 (/ 모양) 
그 쪽은 선형함수 같아 보입니다.

53
00:06:02,481 --> 00:06:05,374
Sigmoid는 역사적으로 아주 유명했습니다.

54
00:06:05,374 --> 00:06:11,530
왜냐하면 Sigmoid가 일종의, 뉴런의 firing rate를 
saturation 시키는 것으로 해석할 수 있기 때문입니다.

55
00:06:11,530 --> 00:06:15,455
어떤 값이 0에서 1 사이의 값을 가지면 
이를 fireing rate라고 생각할 수 있을 것입니다.

56
00:06:15,455 --> 00:06:23,588
나중에 ReLU 같은 것을 배울 것인데, 실제로 ReLU가 
생물학적 타당성이 더 크다는게 밝혀졌지만

57
00:06:23,588 --> 00:06:27,402
Sigmoid 또한 이런 식으로 해석할 수 있다는 점을
알고 넘어가면 될 것 같습니다.

58
00:06:30,015 --> 00:06:36,492
요 녀석을 한번 자세히 살펴보면
문제점이 몇 가지 있습니다.

59
00:06:36,492 --> 00:06:44,065
우선 Saturation되는게 gradient를 없앱니다.
이게 무슨 뜻일까요?

60
00:06:44,988 --> 00:06:48,801
이 Sigmoid gate를 Computraional graph에서
한번 살펴보겠습니다.

61
00:06:48,801 --> 00:06:54,566
여기 데이터 X가 있고 출력이 있습니다.

62
00:06:54,566 --> 00:06:59,236
Backprop에서 Gradient는 어떻게될까요?

63
00:06:59,236 --> 00:07:08,441
우선 dL/dSigma 가 있습니다. dL/dSigma가 밑으로
내려가죠 그리고 dX/dSigma를 곱하게 될 것입니다.

64
00:07:08,441 --> 00:07:11,081
이 값이 local sigmoid function의 
gradient가 될 것입니다.

65
00:07:11,081 --> 00:07:16,495
그리고 이런 값들이 계속 연쇄적으로 
Backporp될 것입니다.

66
00:07:16,495 --> 00:07:24,708
그렇다면 질문입니다. X가 -10이면 어떻게 될까요?
gradient는 어떻게 생기게 될까요?

67
00:07:24,708 --> 00:07:28,868
네 맞습니다. 0이 되겠죠. 
gradient는 0이 됩니다.

68
00:07:28,868 --> 00:07:37,348
Sigmoid에서 음의 큰 값이면 sigmoid가
flat하게 되고 gradient가 0이 되겠죠

69
00:07:37,348 --> 00:07:40,001
그리고 이 값이 밑으로 내려갈 것입니다.

70
00:07:40,001 --> 00:07:46,501
거의 0에 가까운 값이 backprob 될 것입니다.

71
00:07:46,501 --> 00:07:55,381
이 부분에서 gradient가 죽어버리게 되고
밑으로 0이 계속 전달되게 됩니다.

72
00:07:58,869 --> 00:08:10,015
그렇다면 X가 0이면 어떻까요? 
맞습니다. 이 구간에서는 잘 동작할 것입니다.

73
00:08:10,015 --> 00:08:15,135
아주 그럴싸한 gradient를 얻게 될 것이고 
backprop 이 잘 될 것입니다.

74
00:08:15,135 --> 00:08:20,055
그러면 X가 10이면 어떻까요?

75
00:08:20,055 --> 00:08:31,108
X가 양의 큰 값일 경우에도 sigmoid가 flat하기 때문에
gradient들 다 죽이게 됩니다.

76
00:08:31,108 --> 00:08:35,275
그러면 gradient가 잘 흐르지 않겠죠

77
00:08:37,055 --> 00:08:42,454
두 번째 문제는 바로 
sigmoid 의 출력이 zero centered 하지 않다는 것입니다.

78
00:08:42,454 --> 00:08:46,415
이게 왜 문제인지 한번 알아보도록 하겠습니다.

79
00:08:46,415 --> 00:08:51,892
자 그럼 뉴런의 입력이 항상 양수일 때 
어떤 일이 벌어지는지 한번 살펴보겠습니다.

80
00:08:51,892 --> 00:08:54,948
이 경우에 모든 X가 양수일 때 입니다.

81
00:08:54,948 --> 00:09:04,348
이 x는 어떤 가중치랑 곱해지고 활성함수를 통과하겠죠

82
00:09:04,348 --> 00:09:08,015
그럼 W에 대한 gradient를 한번 생각해볼까요?

83
00:09:12,375 --> 00:09:18,135
이 Layer에서의 local gradient가 어떻게 될지
한번 생각해 보세요

84
00:09:18,135 --> 00:09:24,214
우선 dL/d(활성함수) 를 계산하겠죠 그렇게 Loss가 내려가고

85
00:09:24,214 --> 00:09:29,834
그리고 Local gradient가 있을텐데, 
기본적으로 그냥 X입니다.

86
00:09:29,834 --> 00:09:34,001
그렇다면 모든 X가 양수라는 것은 무슨뜻일까요?

87
00:09:36,253 --> 00:09:44,401
누군가가 gradient가 전부 "양수" 가 된다고 하셨는데요. 
적확히 말하면 "전부 양수" 또는 "전부 음수" 입니다.

88
00:09:44,401 --> 00:09:53,588
위에서 dL/df(활성함수) 가 넘어오겠죠
이 값이 음수 또는 양수가 될 것입니다.

89
00:09:53,588 --> 00:09:55,815
어떤 임의의 gradient가 내려온다고 생각해 봅시다.

90
00:09:55,815 --> 00:10:06,619
우선 local gradient는 이 값이랑 곱해질 것이고
dF(활성함수)/dW는, 그냥 X가 될 것입니다.

91
00:10:07,880 --> 00:10:20,800
그렇게 되면 gradient의 부호는 그저 
위에서 내려온 gradient의 부호과 같아 질 것입니다.

92
00:10:20,800 --> 00:10:28,520
이것이 의미하는 바는 W가 모두 같은 방향으로만 움직일 것임을
의미하게 됩니다.

93
00:10:28,520 --> 00:10:42,467
파라미터를 업데이트 할 때 다 같이 증가하거나
다 같이 감소하거나 할 수 밖에 없습니다.

94
00:10:42,467 --> 00:10:48,867
여기서의 문제는 이런 gradient 업데이트는
아주 비효율적이라는 것입니다.

95
00:10:48,867 --> 00:10:59,507
여기 W가 이차원인 예제가 있습니다.
W에 대한 두개의 축으로 이루어져 있습니다.

96
00:10:59,507 --> 00:11:04,796
전부 양수 또는 음수로 업데이트된다는 것을 해석해보면

97
00:11:04,796 --> 00:11:12,400
이렇게 되면 gradient가 이동할 수 있는 방향은 4분면 중
이 두 영역뿐이 안 될 것입니다.

98
00:11:12,400 --> 00:11:17,213
이 두 방향으로 밖에 gradient가 업데이트되지 않습니다.

99
00:11:17,213 --> 00:11:25,399
그래서 이론적으로 가장 최적의 W 업데이트가
저 파란색 화살표라고 했을때

100
00:11:25,399 --> 00:11:30,773
우리가 어떤 시작점부터 내려간다고 했을 때, 
여기에서 시작점은 빨간 화살표의 맨 처음이 되겠죠

101
00:11:30,773 --> 00:11:38,946
우리는 파란색 화살표 방향으로 gradient를 이동시킬 수 없습니다.
저 방향으로는 움직일 수 없기 때문이죠

102
00:11:38,946 --> 00:11:43,479
그렇기 때문에 우리는 여러번 gradient 업데이트를
수행해 줘야 합니다.

103
00:11:43,479 --> 00:11:51,953
가령 이렇게 빨간 화살표 방향와 같이, gradient가 이동 가능한
방향으로만 이동을 할 수 있게 됩니다.

104
00:11:53,039 --> 00:11:58,479
이것이 바로 우리가 일반적으로 
zero-mean data을 원하는 이유입니다.

105
00:11:58,479 --> 00:12:11,893
입력 X가 양수/음수를 모두 가지고 있으면  전부 같은 방향으로 움직이는
일은 발생하지는 않을 것입니다.

106
00:12:11,893 --> 00:12:17,819
이해 하셨습니까? 질문 있으신가요?
좋습니다.

107
00:12:21,453 --> 00:12:24,930
자 우리는 지금까지 sigmoid가 가진 문제에 대해서 배웠습니다.

108
00:12:24,930 --> 00:12:30,586
Saturation이 gradient를 죽일 수 있었습니다. 
양/음의 방햐으로 너무 큰 값을 가진다면 말이죠

109
00:12:30,586 --> 00:12:36,586
그리고 zero-centered 가 아니기 때문에 
gradient 업데이트가 효율적이지 않았습니다.

110
00:12:36,586 --> 00:12:43,146
그리고 세 번째 문제는 exp() 가 있어서 
계산이 비싸다는 것입니다.

111
00:12:43,146 --> 00:12:46,837
이 세 번째 문제는 그렇게 큰 문제는 아닙니다.

112
00:12:46,837 --> 00:12:51,186
큰 그림으로 봤을 때 다른 연산들, 가령 
내적의 더 계산이 비쌉니다.

113
00:12:51,186 --> 00:12:55,103
하지만 굳이 문제로 뽑자면 이렇다는 것입니다.

114
00:12:58,986 --> 00:13:03,166
이제 두번 째 활성함수를 한번 보겠습니다. 
tanh입니다.

115
00:13:03,166 --> 00:13:10,999
sigmoid 랑 유사하죠. 하지만 범위가 [-1 , 1] 입니다.

116
00:13:10,999 --> 00:13:12,466
그래서 여기에, 주요 차이점

117
00:13:12,466 --> 00:13:15,573
그것은 이제 제로 중심이고,

118
00:13:15,573 --> 00:13:18,253
그래서 우리는 우리가 가진 두 번째 문제를 제거했습니다.

119
00:13:18,253 --> 00:13:21,306
그러나 포화 상태 일 때는 그래디언트를 죽입니다.

120
00:13:21,306 --> 00:13:23,452
그래서, 당신은 여전히이 정권을 가지고 있습니다.

121
00:13:23,452 --> 00:13:26,386
그라디언트가 본질적으로 평면 인 곳

122
00:13:26,386 --> 00:13:29,264
그래디언트 흐름을 없앨 것입니다.

123
00:13:29,264 --> 00:13:31,426
이것은 S 자보다 조금 나아졌습니다.

124
00:13:31,426 --> 00:13:34,009
하지만 여전히 문제가 있습니다.

125
00:13:36,586 --> 00:13:40,104
이제는 ReLU 활성화 기능을 살펴 보겠습니다.

126
00:13:40,104 --> 00:13:44,026
그리고 이것은 우리가 지난 강의에서 본 마지막 강의입니다

127
00:13:44,026 --> 00:13:45,426
우리가 이야기 할 때

128
00:13:45,426 --> 00:13:47,573
길쌈 신경 네트워크.

129
00:13:47,573 --> 00:13:50,946
그리고 우리는 우리가 ReLU 비선형
성을 산재시킨 것을 보았습니다

130
00:13:50,946 --> 00:13:53,279
길쌈 층의 많은 것 사이에서.

131
00:13:53,279 --> 00:13:58,253
그리고이 함수는 x의 f와 0과 x의 최대 값을 같습니다.

132
00:13:58,253 --> 00:14:02,626
그래서 당신의 입력에 elementwise
연산이 필요합니다.

133
00:14:02,626 --> 00:14:04,853
기본적으로 입력이 음수이면

134
00:14:04,853 --> 00:14:06,573
그것을 0으로 놓을 것입니다.

135
00:14:06,573 --> 00:14:09,053
그리고 나서 그것이 긍정적이면,

136
00:14:09,053 --> 00:14:11,346
그것은 방금 통과하게 될 것입니다.

137
00:14:11,346 --> 00:14:13,264
그것은 정체성입니다.

138
00:14:13,264 --> 00:14:16,266
그리고 이것은 꽤 일반적으로 사용되는 것입니다,

139
00:14:16,266 --> 00:14:18,706
우리가 이걸 본다면

140
00:14:18,706 --> 00:14:20,199
문제를보고 생각해보십시오.

141
00:14:20,199 --> 00:14:22,892
이전에 시그 모이 드와 탄으로 보았던 것,

142
00:14:22,892 --> 00:14:25,053
우리는 그것이 포화 상태가 아님을 알 수 있습니다.

143
00:14:25,053 --> 00:14:26,746
긍정적 인 지역에서.

144
00:14:26,746 --> 00:14:29,253
입력 공간의 절반이 전부입니다.

145
00:14:29,253 --> 00:14:31,893
그것이 포화 상태가되지 않는 곳에,

146
00:14:31,893 --> 00:14:34,465
그래서 이것은 큰 이점입니다.

147
00:14:34,465 --> 00:14:36,959
따라서 이것은 계산 상 매우 효율적입니다.

148
00:14:36,959 --> 00:14:39,475
우리는 앞서 sigmoid

149
00:14:39,475 --> 00:14:42,466
이 지수는 지수 적입니다.

150
00:14:42,466 --> 00:14:45,986
그래서 ReLU는이 단순한 최대 값입니다.

151
00:14:45,986 --> 00:14:48,968
그리고 그것은 매우 빠르다.

152
00:14:48,968 --> 00:14:51,198
실제로,이 ReLU를 사용하여,

153
00:14:51,198 --> 00:14:54,343
그것은 시그 모이 드 (sigmoid)와 탄
(tanh)보다 훨씬 빠르게 수렴하며,

154
00:14:54,343 --> 00:14:57,063
그래서 약 6 배 더 빠릅니다.

155
00:14:57,063 --> 00:14:59,078
그리고 더 많은 것으로 밝혀졌습니다.

156
00:14:59,078 --> 00:15:01,090
S 형보다 생물학적으로 그럴듯 해.

157
00:15:01,090 --> 00:15:02,741
그래서 당신이 뉴런을 본다면

158
00:15:02,741 --> 00:15:04,330
여러분은 입력 내용이 어떻게 보이는지 살펴 봅니다.

159
00:15:04,330 --> 00:15:06,823
당신은 출력이 어떻게 보이는지 살펴 봅니다.

160
00:15:06,823 --> 00:15:11,450
당신은 이것을 신경 과학 실험에서 측정하려고합니다.

161
00:15:11,450 --> 00:15:12,414
너는 이걸 보게 될거야.

162
00:15:12,414 --> 00:15:15,410
실제로 더 가까운 근사치입니다.

163
00:15:15,410 --> 00:15:18,303
Sigmoids보다 무슨 일이 일어나는지.

164
00:15:18,303 --> 00:15:21,136
그래서 ReLUs가 사용되기 시작했습니다.

165
00:15:21,136 --> 00:15:24,935
우리가 AlexNet을 가졌던 2012 년 즈음에,

166
00:15:24,935 --> 00:15:27,188
첫 번째 주요 길쌈 신경망

167
00:15:27,188 --> 00:15:28,345
잘 할 수 있었던

168
00:15:28,345 --> 00:15:30,548
ImageNet 및 대규모 데이터

169
00:15:30,548 --> 00:15:33,798
그들은 실험에서 ReLU를 사용했습니다.

170
00:15:36,775 --> 00:15:38,628
그러나 문제는 ReLU와 함께,

171
00:15:38,628 --> 00:15:42,082
여전히 그것이 영 중심에 있지 않다는 것입니다.

172
00:15:42,082 --> 00:15:44,586
그래서 우리는 시그 모이 드가 제로
중심이 아니라는 것을 알았습니다.

173
00:15:44,586 --> 00:15:49,228
Tanh가 이것을 고쳤으며 ReLU는이
문제를 다시 가지고 있습니다.

174
00:15:49,228 --> 00:15:52,122
그래서 이것은 ReLU의 문제 중 하나입니다.

175
00:15:52,122 --> 00:15:55,357
그리고 우리는 또한이 더 성가심을 가지고 있습니다.

176
00:15:55,357 --> 00:15:58,682
다시 우리는 입력의 긍정적 인 절반에서,

177
00:15:58,682 --> 00:16:00,334
우리는 채도가 없다.

178
00:16:00,334 --> 00:16:04,222
그러나 이것은 음의 절반의 경우가 아닙니다.

179
00:16:04,222 --> 00:16:05,282
맞아, 이거 생각하고있어.

180
00:16:05,282 --> 00:16:06,882
조금 더 정확하게.

181
00:16:06,882 --> 00:16:11,255
그래서 X가 음수 10 일 때 여기서
무슨 일이 일어나고 있습니까?

182
00:16:11,255 --> 00:16:12,855
그래서 제로 그라디언트, 맞습니다.

183
00:16:12,855 --> 00:16:16,522
X가 양수 10 일 때 어떻게됩니까?

184
00:16:17,455 --> 00:16:18,295
좋습니다.

185
00:16:18,295 --> 00:16:20,175
그래서, 우리는 선형 체제에 있습니다.

186
00:16:20,175 --> 00:16:24,008
그러면 X가 0 일 때 어떻게됩니까?

187
00:16:25,975 --> 00:16:27,362
예, 정의되지 않았습니다.

188
00:16:27,362 --> 00:16:30,442
실제로, 우리는 알다시피, 0, 맞습니다.

189
00:16:30,442 --> 00:16:33,241
그리고 기본적으로 그라디언트를 죽이고 있습니다.

190
00:16:33,241 --> 00:16:35,074
정권의 절반.

191
00:16:37,948 --> 00:16:40,228
그래서 우리는이 현상을 얻을 수 있습니다.

192
00:16:40,228 --> 00:16:42,322
기본적으로 죽은 ReLUs의,

193
00:16:42,322 --> 00:16:45,708
우리가 정권의 나쁜 부분에있을 때.

194
00:16:45,708 --> 00:16:48,668
그래서 거기에서 볼 수 있습니다.

195
00:16:48,668 --> 00:16:51,212
몇 가지 잠재적 인 이유에서 오는 것처럼

196
00:16:51,212 --> 00:16:54,442
따라서 여기에 데이터 클라우드를 살펴보면,

197
00:16:54,442 --> 00:16:57,192
이것은 우리의 모든 훈련 데이터입니다.

198
00:16:59,033 --> 00:17:01,873
우리가 ReLUs가 떨어질 수있는 곳을 보면,

199
00:17:01,873 --> 00:17:06,040
그래서 ReLUs 수 있습니다, 각각의 이들은 기본적으로

200
00:17:08,896 --> 00:17:11,948
그것이 활성화 될 비행기의 절반.

201
00:17:11,948 --> 00:17:13,292
그리고 이것들은 각각 비행기입니다.

202
00:17:13,292 --> 00:17:15,640
이 ReLUs 각각을 정의하는,

203
00:17:15,640 --> 00:17:18,842
우리는 당신이 죽은 ReLUs를 가질
수 있다는 것을 알 수 있습니다.

204
00:17:18,842 --> 00:17:21,201
기본적으로 데이터 클라우드와 다릅니다.

205
00:17:21,201 --> 00:17:25,041
이 경우 결코 활성화되지 않고 업데이트되지 않습니다.

206
00:17:25,042 --> 00:17:26,588
활성 ReLU에 비해

207
00:17:26,588 --> 00:17:27,628
일부 데이터

208
00:17:27,628 --> 00:17:30,121
긍정적이 될 것이고 통과 할 것입니다.

209
00:17:30,121 --> 00:17:31,732
일부는 그렇지 않을 것입니다.

210
00:17:31,732 --> 00:17:33,480
그래서 여기에는 몇 가지 이유가 있습니다.

211
00:17:33,480 --> 00:17:34,948
첫 번째는 일어날 수 있다는 것입니다.

212
00:17:34,948 --> 00:17:37,201
나쁜 초기화가있을 때.

213
00:17:37,201 --> 00:17:39,946
그래서 만약 당신이 무게가 불행한 일이 있다면

214
00:17:39,946 --> 00:17:41,868
데이터 클라우드에서 벗어난 경우,

215
00:17:41,868 --> 00:17:45,015
그래서 그들은이 나쁜 ReLU를 여기에 지정하게됩니다.

216
00:17:45,015 --> 00:17:47,573
그런 다음 그들은 결코 얻을 수 없습니다.

217
00:17:47,573 --> 00:17:50,534
데이터 입력이 활성화되게하는 데이터 입력,

218
00:17:50,534 --> 00:17:53,367
그래서 그들은 절대 가지 않을거야.

219
00:17:54,406 --> 00:17:56,108
좋은 그라디언트 흐름이 다시 나타납니다.

220
00:17:56,108 --> 00:17:59,321
따라서 결코 업데이트되지 않고 활성화되지 않습니다.

221
00:17:59,321 --> 00:18:01,228
더 일반적인 경우는 무엇입니까?

222
00:18:01,228 --> 00:18:03,880
학습률이 너무 높을 때.

223
00:18:03,880 --> 00:18:06,628
그래서이 사건은 당신이 괜찮은 ReLU로 시작 했어요.

224
00:18:06,628 --> 00:18:09,240
그러나 당신이이 거대한 갱신을 만들고 있기 때문에,

225
00:18:09,240 --> 00:18:11,561
가중치가 주위에 뛰어 오른다.

226
00:18:11,561 --> 00:18:13,148
그리고 나서 당신의 ReLU 유닛은 어떤 의미에서,

227
00:18:13,148 --> 00:18:15,401
데이터 매니 폴드에서 빠져 나간다.

228
00:18:15,401 --> 00:18:18,028
그리고 이것은 훈련을 통해 발생합니다.

229
00:18:18,028 --> 00:18:19,388
그래서 처음에는 괜찮 았어.

230
00:18:19,388 --> 00:18:22,975
그리고 나서 어떤 시점에서, 그것은
나 빠지고 그것이 죽었습니다.

231
00:18:22,975 --> 00:18:24,108
그리고 실제로,

232
00:18:24,108 --> 00:18:26,828
당신이 훈련 한 네트워크를 멈 추면

233
00:18:26,828 --> 00:18:27,961
당신은 데이터를 통과시킵니다.

234
00:18:27,961 --> 00:18:30,881
당신은 그것이 실제로 10 ~ 20
% 정도로 많이 볼 수 있습니다.

235
00:18:30,881 --> 00:18:33,361
이 죽은 ReLUs는 네트워크의

236
00:18:33,361 --> 00:18:35,841
그래서 당신은 그것이 문제라는 것을 압니다.

237
00:18:35,841 --> 00:18:38,535
또한 대부분의 네트워크에는

238
00:18:38,535 --> 00:18:40,001
ReLUs를 사용할 때 이러한 유형의 문제가 발생합니다.

239
00:18:40,001 --> 00:18:41,413
그들 중 일부는 죽을 것이다.

240
00:18:41,413 --> 00:18:44,681
실제로, 사람들은 이것을 조사하고,

241
00:18:44,681 --> 00:18:46,255
그것은 연구 문제이다.

242
00:18:46,255 --> 00:18:49,467
그러나 그것은 여전히 훈련
네트워크에 대해 잘하고 있습니다.

243
00:18:49,467 --> 00:18:51,268
그래, 질문 있니?

244
00:18:51,268 --> 00:18:54,851
[마이크 끄기 학생]

245
00:19:01,908 --> 00:19:02,741
권리.

246
00:19:02,741 --> 00:19:03,574
문제는 그래,

247
00:19:03,574 --> 00:19:05,335
데이터 클라우드는 교육 데이터 일뿐입니다.

248
00:19:05,335 --> 00:19:08,918
[마이크 끄기 학생]

249
00:19:17,641 --> 00:19:20,127
그래, 질문은 언제,

250
00:19:20,127 --> 00:19:23,428
ReLU가 언제 죽을 것인지 어떻게 알 수 있습니까?

251
00:19:23,428 --> 00:19:25,708
데이터 클라우드와 관련하여?

252
00:19:25,708 --> 00:19:27,841
그리고 만약에 당신이 보면,

253
00:19:27,841 --> 00:19:30,988
이것은 단순한 2 차원의 경우와 같은 예입니다.

254
00:19:30,988 --> 00:19:34,771
그리고 우리의 ReLU, 우리는 ReLU에
대한 우리의 의견을 얻을 것입니다.

255
00:19:34,771 --> 00:19:38,266
기본적으로 당신이 알고있을 것입니다,

256
00:19:38,266 --> 00:19:42,278
W1 X1과 W2 X2를 합한 것입니다.

257
00:19:42,278 --> 00:19:46,080
그래서 이것은 이것을 여기에서
분리하는 초평면을 정의합니다,

258
00:19:46,080 --> 00:19:47,773
그리고 나서 우리는 그것의 절반을 가져갈 것입니다.

259
00:19:47,773 --> 00:19:49,466
그것은 긍정적이 될 것입니다.

260
00:19:49,466 --> 00:19:51,453
그 중 절반은 죽을거야.

261
00:19:51,453 --> 00:19:55,051
그리고 네, 그래서 당신, 당신은 당신을 알고 있습니다.

262
00:19:55,051 --> 00:19:57,117
무게가 무엇이든간에 그것은 일어났습니다.

263
00:19:57,117 --> 00:20:00,706
데이터가있는 곳은 어디서,

264
00:20:00,706 --> 00:20:03,789
이 초평면이 떨어지는 곳,

265
00:20:05,560 --> 00:20:08,293
그래서 그래, 그래서 훈련의 과정을 통해,

266
00:20:08,293 --> 00:20:11,746
일부 ReLUs가 다른 장소에있을 것입니다.

267
00:20:11,746 --> 00:20:14,329
데이터 클라우드와 관련하여

268
00:20:16,480 --> 00:20:18,050
오, 질문.

269
00:20:18,050 --> 00:20:21,633
[마이크 끄기 학생]

270
00:20:23,380 --> 00:20:24,213
네.

271
00:20:27,780 --> 00:20:29,780
그럼, 질문은

272
00:20:29,780 --> 00:20:32,247
Sigmoid에 대해 우리는 두
가지 단점에 대해 이야기했습니다.

273
00:20:32,247 --> 00:20:37,045
그리고 그 중 하나는 뉴런이 포화 상태에
빠질 수 있다는 것이 었습니다.

274
00:20:37,045 --> 00:20:40,500
그러니 여기서 다시 Sigmoid로 돌아 가자.

275
00:20:40,500 --> 00:20:43,140
그리고 이것이 문제가 아닌가하는 질문이있었습니다.

276
00:20:43,140 --> 00:20:45,820
모든 입력이 긍정적 인 경우.

277
00:20:45,820 --> 00:20:48,180
모든 입력이 양수일 때,

278
00:20:48,180 --> 00:20:49,727
그들은 모두 들어올 예정이다.

279
00:20:49,727 --> 00:20:51,971
이 제로 플러스 영역에서,

280
00:20:51,971 --> 00:20:54,464
그래서 당신은 여전히 포화 뉴런을 얻을 수 있습니다,

281
00:20:54,464 --> 00:20:58,624
당신이이 긍정적 인 지역에서 보았 기 때문에,

282
00:20:58,624 --> 00:21:00,544
그것도 하나에 평원,

283
00:21:00,544 --> 00:21:02,622
그래서 긍정적 인 가치가 큰 때입니다.

284
00:21:02,622 --> 00:21:05,179
입력으로 당신은 또한 0의 그라데이션을 얻습니다.

285
00:21:05,179 --> 00:21:08,846
여기 경사가 평평해서.

286
00:21:10,715 --> 00:21:11,548
괜찮아.

287
00:21:16,355 --> 00:21:20,522
좋아, 실제로 사람들은 ReLUs를
초기화하는 것을 좋아한다.

288
00:21:21,822 --> 00:21:24,528
약간 긍정적 인 편견을 가지고,

289
00:21:24,528 --> 00:21:27,102
그럴 가능성을 높이기 위해

290
00:21:27,102 --> 00:21:30,721
초기화시 활성화되고 일부 업데이트가 발생합니다.

291
00:21:30,721 --> 00:21:34,195
맞아요. 그래서 이것은 기본적으로 더
많은 ReLUs쪽으로 편향됩니다.

292
00:21:34,195 --> 00:21:35,755
처음에 발사,

293
00:21:35,755 --> 00:21:37,808
실제로 어떤 사람들은 그것이 도움이된다고 말합니다.

294
00:21:37,808 --> 00:21:40,430
어떤 사람들은 그렇지 않다고 말한다.

295
00:21:40,430 --> 00:21:42,887
일반적으로 사람들은 항상 이것을 사용하지는 않습니다.

296
00:21:42,887 --> 00:21:46,155
예, 많은 사람들이 방금 초기화했습니다.

297
00:21:46,155 --> 00:21:48,072
여전히 0으로 편향되어있다.

298
00:21:49,483 --> 00:21:51,926
좋아, 이제 우리는 약간의 수정을 볼 수있다.

299
00:21:51,926 --> 00:21:54,777
그 이후로 나온 ReLU에,

300
00:21:54,777 --> 00:21:57,768
한 가지 예가이 새는 ReLU입니다.

301
00:21:57,768 --> 00:22:00,035
그리고 이것은 원래의 ReLU와 매우 유사합니다.

302
00:22:00,035 --> 00:22:03,088
그리고 유일한 차이점은 이제 평평하지 않고

303
00:22:03,088 --> 00:22:04,429
부정적인 정권에서,

304
00:22:04,429 --> 00:22:07,408
우리는 여기에 약간의 음의 기울기를 주려고합니다.

305
00:22:07,408 --> 00:22:10,595
그래서 이것은 많은 문제를 해결합니다.

306
00:22:10,595 --> 00:22:11,955
우리가 이전에 언급 한.

307
00:22:11,955 --> 00:22:14,648
바로 여기에 우리는 포화 상태를 가지고 있지 않습니다.

308
00:22:14,648 --> 00:22:17,142
부정적인 공간에서도.

309
00:22:17,142 --> 00:22:19,782
여전히 계산 상 효율적입니다.

310
00:22:19,782 --> 00:22:22,088
그것은 여전히 시그 모이 드
(sigmoid)와 탄 (tanh)

311
00:22:22,088 --> 00:22:23,968
ReLU와 매우 유사합니다.

312
00:22:23,968 --> 00:22:27,218
그리고 그것은이 죽어가는 문제를 가지고 있지 않습니다.

313
00:22:28,923 --> 00:22:31,966
또 다른 예가 있습니다.

314
00:22:31,966 --> 00:22:35,380
파라 메트릭 정류기이므로 PRULU입니다.

315
00:22:35,380 --> 00:22:37,862
그리고이 경우 그것은 마치 새는 ReLU와 같습니다.

316
00:22:37,862 --> 00:22:40,593
우리는 다시이 경 사진 지역을 가지고 있습니다.

317
00:22:40,593 --> 00:22:42,195
음의 공간에서,

318
00:22:42,195 --> 00:22:44,195
그러나 지금 부정적인 정권에서의이 경사

319
00:22:44,195 --> 00:22:47,088
이 알파 매개 변수를 통해 결정되며,

320
00:22:47,088 --> 00:22:48,848
그래서 우리는 지정하지 않습니다. 우리는
그것을 하드 코딩하지 않습니다.

321
00:22:48,848 --> 00:22:50,822
그러나 우리는 그것을 매개 변수로 취급합니다.

322
00:22:50,822 --> 00:22:52,982
우리가 역행하고 배울 수있는 것.

323
00:22:52,982 --> 00:22:57,555
그래서 이것은 약간의 융통성을줍니다.

324
00:22:57,555 --> 00:22:59,262
그리고 우리는 또한

325
00:22:59,262 --> 00:23:02,342
지수 선형 유닛, ELU,

326
00:23:02,342 --> 00:23:06,087
그래서 우리는 기본적으로이 모든
다른 LU들을 가지고 있습니다.

327
00:23:06,087 --> 00:23:08,295
이거 다시, 너도 알다시피,

328
00:23:08,295 --> 00:23:10,341
그것은 ReLu의 모든 이점을 가지고 있습니다.

329
00:23:10,341 --> 00:23:14,508
하지만 이제 당신은 0 점에 가깝습니다.

330
00:23:16,181 --> 00:23:19,464
그래서, 그것은 실제로 유익한 ReLU,

331
00:23:19,464 --> 00:23:22,151
파라 메릭 ReLU,이 많은 것들이 당신을 허용합니다.

332
00:23:22,151 --> 00:23:24,901
평균을 제로에 가깝게하려면

333
00:23:26,699 --> 00:23:28,606
그러나 새는 ReLU와 비교해 보면,

334
00:23:28,606 --> 00:23:32,632
그것이 부정적인 정권에서 경 사진되는 대신에,

335
00:23:32,632 --> 00:23:34,227
여기 당신은 실제로 다시 건축하고 있습니다.

336
00:23:34,227 --> 00:23:36,538
음의 포화 영역에서,

337
00:23:36,538 --> 00:23:40,406
기본적으로 이것이 당신을 허용한다는 주장이 있습니다.

338
00:23:40,406 --> 00:23:43,029
소음에 좀 더 견고 함을주기 위해서,

339
00:23:43,029 --> 00:23:46,299
기본적으로 이러한 비활성화 상태를 얻습니다.

340
00:23:46,299 --> 00:23:48,566
더 강력해질 수 있습니다.

341
00:23:48,566 --> 00:23:51,667
그리고 당신은이 논문을 볼 수 있습니다.

342
00:23:51,667 --> 00:23:53,711
더 많은 정당성이있다.

343
00:23:53,711 --> 00:23:55,885
왜 그런가?

344
00:23:55,885 --> 00:23:58,177
그리고 어떤 의미에서 이것은 일종의 것입니다.

345
00:23:58,177 --> 00:24:01,111
ReLUs와 새는 ReLUs 사이에서,

346
00:24:01,111 --> 00:24:02,944
이 모양의 일부는 어디에 있습니까?

347
00:24:02,944 --> 00:24:05,405
Leaky ReLU가하는,

348
00:24:05,405 --> 00:24:07,672
평균 출력이 0에 가까워지며,

349
00:24:07,672 --> 00:24:09,925
그러나 그 후에도 여전히 이것의 일부를 가지고 있습니다.

350
00:24:09,925 --> 00:24:13,267
ReLU가 가지고있는 더 포화적인 행동.

351
00:24:13,267 --> 00:24:14,350
질문 하나?

352
00:24:14,350 --> 00:24:17,933
[마이크 끄기 학생]

353
00:24:19,952 --> 00:24:21,858
따라서이 매개 변수 alpha

354
00:24:21,858 --> 00:24:24,365
각 신경 세포에 특유 할 것입니다.

355
00:24:24,365 --> 00:24:27,512
그래서 저는 그것이 종종 지정되어 있다고 믿습니다.

356
00:24:27,512 --> 00:24:29,392
그러나 나는 실제로 정확하게 기억할 수 없다.

357
00:24:29,392 --> 00:24:31,923
그래서 당신은 종이를 정확하게 볼 수 있습니다.

358
00:24:31,923 --> 00:24:34,090
예, 이것이 어떻게 정의되는지,

359
00:24:35,578 --> 00:24:39,325
하지만 그래,이 기능은 기본적으로

360
00:24:39,325 --> 00:24:42,231
매우 신중하게 설계해야한다.

361
00:24:42,231 --> 00:24:45,050
좋은 바람직한 속성.

362
00:24:45,050 --> 00:24:46,765
좋아, 기본적으로이 모든 것이있다.

363
00:24:46,765 --> 00:24:49,992
ReLU의 변종들.

364
00:24:49,992 --> 00:24:53,165
그래서 여러분은 이것들이 모두
종류라는 것을 알 수 있습니다.

365
00:24:53,165 --> 00:24:55,765
당신은 각자가 특정 이익을 가질 수
있다고 주장 할 수 있습니다.

366
00:24:55,765 --> 00:24:58,192
실제로는 단점이 있습니다.

367
00:24:58,192 --> 00:25:00,232
사람들은 단지 실험을 모두하고 싶어합니다.

368
00:25:00,232 --> 00:25:02,218
더 잘 작동하는 것이 경험적으로 보입니다.

369
00:25:02,218 --> 00:25:04,950
그것을 정당화하고 새로운 것들을 생각해 내고,

370
00:25:04,950 --> 00:25:05,862
그러나 그들은 모두 다른 것들이다.

371
00:25:05,862 --> 00:25:08,612
그것들은 실험되고 있습니다.

372
00:25:10,135 --> 00:25:12,855
그래서 한 가지 더 언급 해 봅시다.

373
00:25:12,855 --> 00:25:14,744
이것은 Maxout Neuron입니다.

374
00:25:14,744 --> 00:25:16,517
그래서 이건 조금 달라 보이는구나.

375
00:25:16,517 --> 00:25:20,214
다른 사람들과 똑같은 형식을 가지고 있지 않다는 점에서

376
00:25:20,214 --> 00:25:22,762
당신의 기본적인 내적을 가지고 가기의,

377
00:25:22,762 --> 00:25:24,023
이 요소를 현명하게

378
00:25:24,023 --> 00:25:25,969
그것 앞에 비선형 성이있다.

379
00:25:25,969 --> 00:25:27,423
대신, 그것은 이렇게 보입니다.

380
00:25:27,423 --> 00:25:30,590
X와 B의 W 곱의이 최대 값,

381
00:25:31,990 --> 00:25:33,570
및 제 2 세트의 웨이트를 포함하며,

382
00:25:33,570 --> 00:25:36,070
W2 점 제품 (X + B2).

383
00:25:38,230 --> 00:25:40,352
그리고 이것은 무엇입니까, 이것이 최대를 취하는 것입니까?

384
00:25:40,352 --> 00:25:43,185
이 두 가지 기능 중 어느 것을 의미합니다.

385
00:25:44,870 --> 00:25:47,909
그래서 ReLU가 일반화됩니다.

386
00:25:47,909 --> 00:25:48,949
그리고 새는 ReLu,

387
00:25:48,949 --> 00:25:52,362
왜냐하면 당신은 당신이이 두
가지를 극대화하고 있기 때문에,

388
00:25:52,362 --> 00:25:54,112
2 개의 선형 함수.

389
00:25:55,023 --> 00:25:56,645
그리고 이것이 우리에게주는 것,

390
00:25:56,645 --> 00:26:00,458
그것은 다시 당신이 선형 정권에서 움직이고 있습니다.

391
00:26:00,458 --> 00:26:02,927
그것은 포화 상태가 아니며 죽지 않습니다.

392
00:26:02,927 --> 00:26:04,480
문제는 여기서,

393
00:26:04,480 --> 00:26:06,893
당신은 뉴런 당 매개 변수의
수를 두 배로 늘리고 있습니다.

394
00:26:06,893 --> 00:26:11,817
그래서, 각각의 뉴런은 이제 원래의
가중치 세트 W를 갖습니다.

395
00:26:11,817 --> 00:26:15,984
하지만 지금은 W1과 W2를 가지고
있으므로 두번 가지고 있습니다.

396
00:26:17,765 --> 00:26:18,955
그래서 실제로,

397
00:26:18,955 --> 00:26:21,190
우리가 이러한 모든 활성화 함수를 볼 때,

398
00:26:21,190 --> 00:26:24,560
좋은 경험 법칙의 종류는 ReLU를 사용합니다.

399
00:26:24,560 --> 00:26:26,806
이것은 가장 표준적인 것입니다.

400
00:26:26,806 --> 00:26:29,389
그것은 일반적으로 잘 작동합니다.

401
00:26:30,231 --> 00:26:32,963
그리고 당신은 당신이 일반적으로 조심하기를
원한다는 것을 알고 있습니다.

402
00:26:32,963 --> 00:26:35,611
귀하의 학습 속도와 함께 그들을 기반으로 조정,

403
00:26:35,611 --> 00:26:36,497
상황이 어떻게되는지보십시오.

404
00:26:36,497 --> 00:26:37,952
학습 속도 조정에 대해 더 자세히 이야기하겠습니다.

405
00:26:37,952 --> 00:26:40,091
이 강연의 뒷부분에서,

406
00:26:40,091 --> 00:26:42,532
하지만이 중 일부를 시도해 볼 수도 있습니다.

407
00:26:42,532 --> 00:26:45,347
더 활발한 활성화 기능,

408
00:26:45,347 --> 00:26:47,680
누출 ReLU, Maxout, ELU,

409
00:26:49,190 --> 00:26:51,573
그러나 이들은 일반적으로,

410
00:26:51,573 --> 00:26:53,828
그들은 여전히 좀 더 실험적입니다.

411
00:26:53,828 --> 00:26:56,643
그래서, 당신은 그들이 당신의 문제에
어떻게 작용하는지 볼 수 있습니다.

412
00:26:56,643 --> 00:26:59,348
당신은 또한 tanh를 시도 할 수있다.

413
00:26:59,348 --> 00:27:01,706
그러나 아마도이 ReLU 중 일부는

414
00:27:01,706 --> 00:27:04,035
ReLU 변종이 더 좋아질 것입니다.

415
00:27:04,035 --> 00:27:06,712
그리고 일반적으로 S 자형을 사용하지 마십시오.

416
00:27:06,712 --> 00:27:09,928
이것은 가장 초기의 정품 인증 기능 중 하나이며,

417
00:27:09,928 --> 00:27:11,910
및 ReLU 및 이들 다른 변형

418
00:27:11,910 --> 00:27:15,243
그 이후로 일반적으로 더 효과적이었습니다.

419
00:27:17,361 --> 00:27:20,034
좋아, 이제 조금 얘기하자.

420
00:27:20,034 --> 00:27:21,517
데이터 전처리에 대해.

421
00:27:21,517 --> 00:27:22,944
맞아, 활성화 기능,

422
00:27:22,944 --> 00:27:24,602
우리는 이것이 우리 네트워크의 일부라고 디자인합니다.

423
00:27:24,602 --> 00:27:25,961
이제 우리는 네트워크를 훈련시키고 자합니다.

424
00:27:25,961 --> 00:27:27,361
우리는 입력 데이터를 가지고있다.

425
00:27:27,361 --> 00:27:30,361
우리는 훈련을 시작하고 싶습니다.

426
00:27:31,424 --> 00:27:33,892
따라서 일반적으로 우리는 항상
데이터를 사전 처리하기를 원합니다.

427
00:27:33,892 --> 00:27:36,375
이것은 이전에 보았던 것입니다.

428
00:27:36,375 --> 00:27:39,495
당신이 그것들을 가져간다면 기계 학습 수업에서.

429
00:27:39,495 --> 00:27:42,349
그리고 몇 가지 표준 유형의 전처리는,

430
00:27:42,349 --> 00:27:44,098
너는 너의 원래 데이터를 가져 간다.

431
00:27:44,098 --> 00:27:46,054
당신은 0을 의미하고 싶습니다.

432
00:27:46,054 --> 00:27:49,366
그리고 나서 당신은 아마 그것을
정상화하기를 원할 것입니다.

433
00:27:49,366 --> 00:27:52,699
표준 편차로 정규화 된

434
00:27:55,400 --> 00:27:57,367
그래서 우리는 왜 이것을하고 싶습니까?

435
00:27:57,367 --> 00:28:00,669
제로 센터링의 경우, 이전에 기억할 수 있습니다.

436
00:28:00,669 --> 00:28:03,232
우리가 모든 입력이 언제

437
00:28:03,232 --> 00:28:04,979
예를 들어,

438
00:28:04,979 --> 00:28:06,710
우리는 모든 그라디언트를 얻습니다.

439
00:28:06,710 --> 00:28:08,165
무게에 긍정적 인 것을,

440
00:28:08,165 --> 00:28:12,772
우리는 기본적으로 차선 최적화를 얻습니다.

441
00:28:12,772 --> 00:28:17,543
그리고 일반적으로 모두 0이
아니거나 모두 음수가 아니더라도,

442
00:28:17,543 --> 00:28:21,710
어떤 종류의 바이어스라도 이러한
유형의 문제를 일으킬 수 있습니다.

443
00:28:23,770 --> 00:28:27,194
그리고 나서 데이터를 표준화하는 측면에서,

444
00:28:27,194 --> 00:28:29,579
이것은 기본적으로 데이터를 정규화하려는 것입니다.

445
00:28:29,579 --> 00:28:31,268
일반적으로 기계 학습 문제에서,

446
00:28:31,268 --> 00:28:33,666
모든 피처가 동일한 범위에 있도록,

447
00:28:33,666 --> 00:28:36,440
그것들이 똑같이 기여할 수 있도록.

448
00:28:36,440 --> 00:28:39,032
실제로는 이미지의 경우

449
00:28:39,032 --> 00:28:43,790
우리는이 과정에서 대부분을 다루고 있습니다.

450
00:28:43,790 --> 00:28:45,866
우리는 제로 센터링을합니다,

451
00:28:45,866 --> 00:28:48,226
실제로 우리는 실제로 정상화하지 않습니다.

452
00:28:48,226 --> 00:28:51,416
일반적으로 이미지의 경우 픽셀 값이 너무 큽니다.

453
00:28:51,416 --> 00:28:53,828
이미 가지고있는 각 위치에서 바로

454
00:28:53,828 --> 00:28:56,616
상대적으로 비교 가능한 규모와 분포,

455
00:28:56,616 --> 00:28:59,153
그래서 우리는 그렇게 정상화 할 필요가 없습니다.

456
00:28:59,153 --> 00:29:03,020
보다 일반적인 기계 학습 문제에 비해,

457
00:29:03,020 --> 00:29:05,172
다른 기능을 사용할 수도 있습니다.

458
00:29:05,172 --> 00:29:09,339
그것들은 매우 다르며 매우 다른 비늘입니다.

459
00:29:11,037 --> 00:29:12,540
그리고 기계 학습에서,

460
00:29:12,540 --> 00:29:15,686
당신은 또한 더 복잡한 것들을 볼 수도 있습니다.

461
00:29:15,686 --> 00:29:19,983
PCA 또는 미백과 같지만 이미지로 다시 나타납니다.

462
00:29:19,983 --> 00:29:22,877
우리는 일반적으로 제로 평균을 고수합니다.

463
00:29:22,877 --> 00:29:24,499
우리는 정상화하지 않습니다.

464
00:29:24,499 --> 00:29:26,011
우리는 또한 이들 중 일부를하지 않습니다.

465
00:29:26,011 --> 00:29:28,678
더 복잡한 전처리.

466
00:29:29,519 --> 00:29:31,889
그리고 한 가지 이유는 일반적으로 이미지가있는 것입니다.

467
00:29:31,889 --> 00:29:34,495
우리는 실제로 우리의 모든 의견을 듣고 싶지는 않습니다.

468
00:29:34,495 --> 00:29:36,536
픽셀 값을 말하고 이것을 계획 해 봅시다.

469
00:29:36,536 --> 00:29:38,253
보다 낮은 차원 공간

470
00:29:38,253 --> 00:29:40,876
우리가 다루고있는 새로운 종류의 기능들.

471
00:29:40,876 --> 00:29:42,193
우리는 일반적으로

472
00:29:42,193 --> 00:29:44,565
공간적으로 길쌈 회선 네트워크

473
00:29:44,565 --> 00:29:48,184
원래 이미지 위에 우리의 공간 구조를 가지고 있습니다.

474
00:29:48,184 --> 00:29:49,595
그래, 질문.

475
00:29:49,595 --> 00:29:53,178
[마이크 끄기 학생]

476
00:29:58,858 --> 00:30:00,674
따라서 문제는이 전처리를 수행하는 것입니다.

477
00:30:00,674 --> 00:30:02,075
훈련 단계에서,

478
00:30:02,075 --> 00:30:05,181
테스트 단계에서 우리도 같은 종류의 일을합니까?

479
00:30:05,181 --> 00:30:06,968
대답은 '예'입니다.

480
00:30:06,968 --> 00:30:10,238
그럼, 다음 슬라이드로 이동하겠습니다.

481
00:30:10,238 --> 00:30:11,860
따라서 일반적으로 교육 단계에 있습니다.

482
00:30:11,860 --> 00:30:14,730
우리가 결정하는 곳입니다.

483
00:30:14,730 --> 00:30:18,897
이 똑같은 평균을 테스트 데이터에 적용합니다.

484
00:30:19,886 --> 00:30:22,523
그래서, 우리는 같은 것으로 정규화 할 것입니다.

485
00:30:22,523 --> 00:30:24,839
훈련 데이터의 경험적 평균.

486
00:30:24,839 --> 00:30:28,422
좋아, 기본적으로 이미지를 요약하기 위해,

487
00:30:29,857 --> 00:30:33,174
우리는 일반적으로 제로 평균 사전 처리를 수행합니다.

488
00:30:33,174 --> 00:30:37,257
전체 평균 이미지를 뺄 수 있습니다.

489
00:30:38,151 --> 00:30:39,566
따라서, 훈련 데이터로부터,

490
00:30:39,566 --> 00:30:41,354
당신은 평균 이미지를 계산하고,

491
00:30:41,354 --> 00:30:44,598
각 이미지와 동일한 크기가 될 것입니다.

492
00:30:44,598 --> 00:30:47,026
따라서, 예를 들어 32 x 32,

493
00:30:47,026 --> 00:30:48,911
당신은 숫자의 배열을 얻을 것이다,

494
00:30:48,911 --> 00:30:53,086
그런 다음 각 이미지에서 그 값을 뺍니다

495
00:30:53,086 --> 00:30:54,777
당신이 곧 네트워크를 통과하려하고 있다는 것을

496
00:30:54,777 --> 00:30:57,024
시험 시간에 똑같은 일을 할거야.

497
00:30:57,024 --> 00:31:00,532
당신이 훈련 시간에 결정한이 배열 때문입니다.

498
00:31:00,532 --> 00:31:04,791
실제로 일부 네트워크의 경우,

499
00:31:04,791 --> 00:31:06,574
우리는 또한 이것을 빼기 만하면됩니다.

500
00:31:06,574 --> 00:31:07,885
채널당 평균,

501
00:31:07,885 --> 00:31:10,132
따라서 전체 평균 이미지를 갖는 대신

502
00:31:10,132 --> 00:31:13,099
에 의해 제로 중심으로 갈 예정이었던

503
00:31:13,099 --> 00:31:14,916
우리는 채널별로 평균을 취합니다.

504
00:31:14,916 --> 00:31:17,967
그리고 이것은 단지 그것이

505
00:31:17,967 --> 00:31:20,893
그것은 전체 이미지에서 충분히 유사했으며,

506
00:31:20,893 --> 00:31:22,459
그렇게 큰 차이를 만들지는 않았다.

507
00:31:22,459 --> 00:31:23,915
평균 이미지를 빼는 것

508
00:31:23,915 --> 00:31:25,718
채널당 가치와 비교합니다.

509
00:31:25,718 --> 00:31:28,422
그리고 이것은 그냥 지나치고 다루기가 더 쉽습니다.

510
00:31:28,422 --> 00:31:32,179
예를 들어, VGG 네트워크에서 이것을 볼 수 있습니다.

511
00:31:32,179 --> 00:31:34,898
이는 AlexNet 이후에 나온 네트워크입니다.

512
00:31:34,898 --> 00:31:36,936
우리는 나중에 그것에 대해서 이야기 할 것입니다.

513
00:31:36,936 --> 00:31:38,545
문제.

514
00:31:38,545 --> 00:31:42,128
[마이크 끄기 학생]

515
00:31:45,215 --> 00:31:46,712
좋아요, 그래서 두 가지 질문이 있습니다.

516
00:31:46,712 --> 00:31:49,165
첫 번째는 채널이 무엇인지,이 경우,

517
00:31:49,165 --> 00:31:52,049
채널당 평균을 뺄 때?

518
00:31:52,049 --> 00:31:55,365
그리고 이것은 RGB입니다. 그래서 우리 배열은,

519
00:31:55,365 --> 00:31:59,455
우리의 이미지는 일반적으로 예를
들어 32 x 32 x 3입니다.

520
00:31:59,455 --> 00:32:01,547
그래서 너비, 높이, 각각 32,

521
00:32:01,547 --> 00:32:04,198
우리의 깊이, 우리는 3 개의 채널 RGB,

522
00:32:04,198 --> 00:32:07,096
그래서 우리는 빨간 채널에 대해
하나의 의미를 가질 것입니다.

523
00:32:07,096 --> 00:32:09,786
하나는 녹색을 의미하고 다른 하나는 파란색을 의미합니다.

524
00:32:09,786 --> 00:32:14,529
두 번째 질문은 두 번째 질문 이었습니까?

525
00:32:14,529 --> 00:32:18,112
[마이크 끄기 학생]

526
00:32:21,349 --> 00:32:22,182
오.

527
00:32:23,237 --> 00:32:24,970
그래, 질문은

528
00:32:24,970 --> 00:32:26,246
우리는 평균 이미지를 뺀다.

529
00:32:26,246 --> 00:32:27,882
점령 된 평균은 무엇입니까?

530
00:32:27,882 --> 00:32:31,474
그리고 그 의미는 모든 교육 이미지를 인계받습니다.

531
00:32:31,474 --> 00:32:33,651
따라서 모든 교육 이미지를

532
00:32:33,651 --> 00:32:37,631
모든 이들의 평균을 계산하면됩니다.

533
00:32:37,631 --> 00:32:39,114
말이 돼?

534
00:32:39,114 --> 00:32:42,697
[마이크 끄기 학생]

535
00:32:48,432 --> 00:32:50,041
그래, 문제는

536
00:32:50,041 --> 00:32:51,858
우리는 전체 훈련 세트에 대해 이것을 수행합니다.

537
00:32:51,858 --> 00:32:53,480
일단 우리가 훈련을 시작하기 전에.

538
00:32:53,480 --> 00:32:55,255
우리는 배치별로 이것을하지 않습니다.

539
00:32:55,255 --> 00:32:57,904
그리고 맞습니다. 정확히 맞습니다.

540
00:32:57,904 --> 00:33:01,453
그래서 우리는 단지 좋은 표본을 갖고 싶습니다.

541
00:33:01,453 --> 00:33:03,984
우리가 가진 경험적 평균.

542
00:33:03,984 --> 00:33:06,368
그리고 만약 당신이 배치 당 그것을 가져 가면,

543
00:33:06,368 --> 00:33:08,810
합리적인 배치를 샘플링하는 경우,

544
00:33:08,810 --> 00:33:11,279
그것은 기본적으로,

545
00:33:11,279 --> 00:33:12,125
너는

546
00:33:12,125 --> 00:33:13,983
어쨌든 평균에 대한 동일한 값,

547
00:33:13,983 --> 00:33:17,380
그래서 더 효율적이고 쉬워졌습니다.

548
00:33:17,380 --> 00:33:19,126
처음에 한 번 해보십시오.

549
00:33:19,126 --> 00:33:21,289
너는 정말로 그것을 가져 가지 않아도 될지도 모른다.

550
00:33:21,289 --> 00:33:22,622
전체 교육 데이터에 대해

551
00:33:22,622 --> 00:33:25,296
당신은 또한 충분한 훈련 이미지를 샘플링 할 수 있습니다.

552
00:33:25,296 --> 00:33:28,296
너의 평균의 좋은 견적을 얻기 위해서.

553
00:33:30,734 --> 00:33:34,602
데이터 전처리에 대한 다른 질문이 있습니까?

554
00:33:34,602 --> 00:33:35,560
예.

555
00:33:35,560 --> 00:33:38,654
[마이크 끄기 학생]

556
00:33:38,654 --> 00:33:40,540
그래서 문제는 데이터를 전처리하는 것입니다.

557
00:33:40,540 --> 00:33:42,187
Sigmoid 문제를 해결 하시겠습니까?

558
00:33:42,187 --> 00:33:46,354
데이터 사전 처리가 제로 평균을하고 있습니까?

559
00:33:47,540 --> 00:33:49,092
그리고 우리는 sigmoid,

560
00:33:49,092 --> 00:33:50,535
우리는 제로 평균을 원합니다.

561
00:33:50,535 --> 00:33:54,195
그래서 첫 번째 레이어에서이 문제를 해결합니다.

562
00:33:54,195 --> 00:33:56,262
우리는 그것을 통과시켰다.

563
00:33:56,262 --> 00:33:58,257
이제 첫 번째 레이어에 대한 입력

564
00:33:58,257 --> 00:34:00,263
우리 네트워크의 평균은 0이 될 것입니다.

565
00:34:00,263 --> 00:34:03,006
그러나 우리는 나중에 우리가 실제로

566
00:34:03,006 --> 00:34:07,030
이 문제는 훨씬 더 나쁜 형태로 나타납니다.

567
00:34:07,030 --> 00:34:08,472
우리는 깊은 네트워크를 가지고 있습니다.

568
00:34:08,472 --> 00:34:09,481
너는 많은 것을 얻을 것이다.

569
00:34:09,481 --> 00:34:12,437
나중에 0이 아닌 평균 문제가 발생합니다.

570
00:34:12,438 --> 00:34:15,183
그래서이 경우 충분하지 않을 것입니다.

571
00:34:15,183 --> 00:34:19,350
따라서 이것은 네트워크의 첫 번째 계층에서만 도움이됩니다.

572
00:34:21,784 --> 00:34:24,501
좋아, 이제 우리가 어떻게해야하는지 얘기하자.

573
00:34:24,501 --> 00:34:28,203
네트워크의 가중치를 초기화 하시겠습니까?

574
00:34:28,204 --> 00:34:29,577
그래서 우리는

575
00:34:29,577 --> 00:34:31,822
우리의 표준 2 층 신경 네트워크

576
00:34:31,822 --> 00:34:34,471
우리는 우리가 배우고 자하는 모든 무게를 가지고 있습니다.

577
00:34:34,472 --> 00:34:38,090
하지만 우리는 그것들을 어떤
가치로 시작해야합니다, 그렇죠?

578
00:34:38,090 --> 00:34:40,364
그리고 우리는 그들을 업데이트 할 것입니다.

579
00:34:40,365 --> 00:34:43,510
거기에서 그라데이션 업데이트를 사용합니다.

580
00:34:43,510 --> 00:34:44,982
그래서 첫 번째 질문입니다.

581
00:34:44,983 --> 00:34:46,273
우리가 사용하면 어떻게 될까?

582
00:34:46,273 --> 00:34:48,893
W의 초기화는 0입니까?

583
00:34:48,893 --> 00:34:53,026
우리는 모든 매개 변수를 0으로 설정합니다.

584
00:34:53,026 --> 00:34:56,157
이것의 문제점은 무엇입니까?

585
00:34:56,157 --> 00:34:58,683
[마이크 끄기 학생]

586
00:34:58,683 --> 00:35:00,766
미안, 다시 말해봐.

587
00:35:02,039 --> 00:35:05,562
그래서 나는 모든 뉴런들이 죽을 것이라고 들었다.

588
00:35:05,562 --> 00:35:07,070
업데이트가 없습니다.

589
00:35:07,070 --> 00:35:08,320
정확하지는 않습니다.

590
00:35:11,035 --> 00:35:13,337
그래서 그 부분은 정확합니다.

591
00:35:13,337 --> 00:35:15,372
모든 뉴런들은 똑같은 일을 할 것입니다.

592
00:35:15,372 --> 00:35:16,995
그래서 그들은 모두 죽은 것은 아닙니다.

593
00:35:16,995 --> 00:35:19,272
입력 값에 따라,

594
00:35:19,272 --> 00:35:22,128
당신은 당신의 뉴런의 정권에있을 수 있습니다,

595
00:35:22,128 --> 00:35:23,321
그래서 그들은 죽지 않을지도 모릅니다.

596
00:35:23,321 --> 00:35:27,869
그러나 중요한 것은 그들이 모두
똑같은 일을 할 것이라는 것입니다.

597
00:35:27,869 --> 00:35:29,781
그래서, 당신의 무게가 제로이기 때문에,

598
00:35:29,781 --> 00:35:33,151
입력이 주어지면 모든 뉴런이 될 것입니다.

599
00:35:33,151 --> 00:35:36,577
기본적으로 입력과 동일한 작업을 수행해야합니다.

600
00:35:36,577 --> 00:35:40,363
그래서 그들은 모두 똑같은 것을 산출 할 것이기 때문에,

601
00:35:40,363 --> 00:35:43,621
그들은 또한 모두 동일한 그라디언트를 얻을 것입니다.

602
00:35:43,621 --> 00:35:44,924
그래서, 그 때문에,

603
00:35:44,924 --> 00:35:47,571
그들은 모두 같은 방식으로 업데이트 할 것입니다.

604
00:35:47,571 --> 00:35:48,807
그리고 이제 막 너는 얻을 것이다.

605
00:35:48,807 --> 00:35:50,957
정확히 동일한 모든 뉴런

606
00:35:50,957 --> 00:35:51,983
그건 네가 원하는게 아니야.

607
00:35:51,983 --> 00:35:54,075
뉴런들이 다른 것들을 배우기를 원합니다.

608
00:35:54,075 --> 00:35:55,824
그래서, 그것이 문제입니다.

609
00:35:55,824 --> 00:35:58,514
모든 것을 똑같이 초기화하면

610
00:35:58,514 --> 00:36:02,730
기본적으로 여기에 대칭이 없습니다.

611
00:36:02,730 --> 00:36:05,961
그래서, 첫 번째 질문은 무엇입니까?

612
00:36:05,961 --> 00:36:09,544
[마이크 끄기 학생]

613
00:36:19,699 --> 00:36:22,611
그래서 질문은, 왜냐하면,

614
00:36:22,611 --> 00:36:25,828
그라디언트 또한 우리의 손실에 달려 있기 때문에,

615
00:36:25,828 --> 00:36:29,961
하나는 다른 것과 비교하여 다르게 역행하지 않을 것인가?

616
00:36:29,961 --> 00:36:32,544
그래서 마지막 레이어에서 예와 같이,

617
00:36:34,828 --> 00:36:37,449
당신은 기본적으로 이것의 일부를 가지고 있습니다.

618
00:36:37,449 --> 00:36:40,790
그라디언트가 동일해질 것입니다.

619
00:36:40,790 --> 00:36:43,716
죄송합니다, 각 특정 뉴런에 대해 다른 손실을 가져옵니다

620
00:36:43,716 --> 00:36:46,072
그것이 연결된 클래스에 기초하여,

621
00:36:46,072 --> 00:36:47,416
하지만 모든 뉴런을 보면

622
00:36:47,416 --> 00:36:50,456
일반적으로 네트워크 전반에 걸쳐,

623
00:36:50,456 --> 00:36:52,411
당신은 기본적으로 많은 뉴런들을 가지고 있습니다.

624
00:36:52,411 --> 00:36:54,352
이들은 정확히 동일한 방식으로 연결됩니다.

625
00:36:54,352 --> 00:36:55,600
그들은 같은 업데이트를했다.

626
00:36:55,600 --> 00:36:59,885
기본적으로 문제가 될 것입니다.

627
00:36:59,885 --> 00:37:01,812
좋아, 우리가 가질 수있는 첫 번째 아이디어

628
00:37:01,812 --> 00:37:04,160
시도하고 개선하기 위해

629
00:37:04,160 --> 00:37:08,096
모든 가중치를 작은 난수로 설정하는 것입니다.

630
00:37:08,096 --> 00:37:10,885
우리는 배포판에서 샘플링 할 수 있습니다.

631
00:37:10,885 --> 00:37:14,241
그래서,이 경우, 우리는

632
00:37:14,241 --> 00:37:16,002
기본적으로 표준 가우스,

633
00:37:16,002 --> 00:37:17,442
하지만 우리는 그것을 확장하려고합니다.

634
00:37:17,442 --> 00:37:18,706
표준 편차

635
00:37:18,706 --> 00:37:22,450
실제로 하나의 E 음수 2, 0.01입니다.

636
00:37:22,450 --> 00:37:25,640
그래서,이 작은 무작위 가중치를 많이 주면됩니다.

637
00:37:25,640 --> 00:37:28,358
따라서 소규모 네트워크에서도 정상적으로 작동합니다.

638
00:37:28,358 --> 00:37:30,729
이제 우리는 대칭을 깨뜨 렸습니다.

639
00:37:30,729 --> 00:37:34,896
그러나 더 깊은 네트워크에는 문제가있을 것입니다.

640
00:37:35,970 --> 00:37:39,354
그래서 이것이 왜 그런지 살펴 보겠습니다.

641
00:37:39,354 --> 00:37:43,070
여기, 이것은 기본적으로 우리가 할 수있는 실험입니다.

642
00:37:43,070 --> 00:37:45,341
어디에서 더 깊은 네트워크를 취해야합니까?

643
00:37:45,341 --> 00:37:49,268
그래서이 경우에는 10 층 신경망을 초기화 해 봅시다.

644
00:37:49,268 --> 00:37:53,622
이 10 개의 층 각각에 500 개의 뉴런이 있어야합니다.

645
00:37:53,622 --> 00:37:56,437
좋아, 우리는이 경우에 tanh
비선형 성을 사용할 것이다.

646
00:37:56,437 --> 00:38:00,639
작은 난수로 초기화 할 것입니다.

647
00:38:00,639 --> 00:38:02,995
마지막 슬라이드에서 설명했듯이

648
00:38:02,995 --> 00:38:04,563
그래서 여기에, 우리는 기본적으로

649
00:38:04,563 --> 00:38:06,116
이 네트워크를 초기화하십시오.

650
00:38:06,116 --> 00:38:09,264
우리는 우리가 취할 임의의 데이터를 가지고 있습니다.

651
00:38:09,264 --> 00:38:12,356
이제는 전체 네트워크를 통과시켜 보겠습니다.

652
00:38:12,356 --> 00:38:14,808
각 계층에서 통계를 살펴보십시오.

653
00:38:14,808 --> 00:38:18,725
그 레이어에서 나오는 활성화 중 하나입니다.

654
00:38:22,476 --> 00:38:24,140
그래서 우리가 볼 수있는 것은 아마도

655
00:38:24,140 --> 00:38:25,485
상단을 읽기가 힘들어.

656
00:38:25,485 --> 00:38:27,995
그러나 우리가 평균을 계산한다면

657
00:38:27,995 --> 00:38:31,156
각 층에서의 표준 편차,

658
00:38:31,156 --> 00:38:34,573
잘 첫 번째 레이어에서 이것은,

659
00:38:36,660 --> 00:38:39,410
수단은 언제나 0에 가깝다.

660
00:38:40,267 --> 00:38:42,767
재미있는 소리가 여기에 있습니다.

661
00:38:45,052 --> 00:38:48,219
흥미 롭군요, 잘 됐네요.

662
00:38:49,613 --> 00:38:52,980
그래서, 우리가 살펴 본다면, 여기서 나온 결과를 보면,

663
00:38:52,980 --> 00:38:56,294
평균은 항상 0에 가까워 질 것입니다.

664
00:38:56,294 --> 00:38:58,153
그것은 의미가 있습니다.

665
00:38:58,153 --> 00:39:01,175
그래서, 우리가 여기서 보게되면, 보자.

666
00:39:01,175 --> 00:39:05,342
우리가 이것을 취하면, 우리는
X의 내적을 W로 보았습니다.

667
00:39:06,237 --> 00:39:09,496
그리고 나서 우리는 선형에 선형을 취했습니다.

668
00:39:09,496 --> 00:39:12,315
그리고 우리는이 값들을 저장합니다. 그래서,

669
00:39:12,315 --> 00:39:15,116
그것은 tanh가 0 주위에 집중되기 때문에,

670
00:39:15,116 --> 00:39:16,780
이것은 말이 될 것입니다,

671
00:39:16,780 --> 00:39:20,371
표준 편차는 줄어들지 만,

672
00:39:20,371 --> 00:39:22,450
그리고 그것은 빠르게 0으로 붕괴됩니다.

673
00:39:22,450 --> 00:39:24,572
우리가 이것을 계획한다면,

674
00:39:24,572 --> 00:39:26,195
여기 두 번째 행의 그림

675
00:39:26,195 --> 00:39:29,523
평균과 표준 편차를 보여주고있다.

676
00:39:29,523 --> 00:39:32,019
레이어별로 시간이 지남에 따라,

677
00:39:32,019 --> 00:39:35,056
플롯의 시퀀스는 각 레이어에 대해 표시됩니다.

678
00:39:35,056 --> 00:39:38,592
우리가 가진 활동의 분포는 무엇입니까?

679
00:39:38,592 --> 00:39:40,269
그리고 우리는 첫 번째 레이어에서,

680
00:39:40,269 --> 00:39:43,362
우리는 여전히 합리적인 가우스 모습을 가지고 있습니다.

681
00:39:43,362 --> 00:39:45,206
좋은 배포판입니다.

682
00:39:45,206 --> 00:39:48,424
그러나 문제는 우리가 번식 할 때

683
00:39:48,424 --> 00:39:52,084
이 W에 의해, 각 층에서의 이러한 작은 수,

684
00:39:52,084 --> 00:39:55,638
이렇게하면 이러한 모든 값이 빠르게 줄어들고 축소됩니다.

685
00:39:55,638 --> 00:39:58,591
우리가 이것을 반복해서 반복 할 때.

686
00:39:58,591 --> 00:40:02,191
그리고 결국, 우리는이 모든 0을 얻습니다.

687
00:40:02,191 --> 00:40:04,262
우리가 원하는 것이 아닙니다.

688
00:40:04,262 --> 00:40:07,457
그래서 모든 활성화가 0이됩니다.

689
00:40:07,457 --> 00:40:10,420
이제는 거꾸로 생각해 봅시다.

690
00:40:10,420 --> 00:40:12,112
그래서 우리가 역으로 통과한다면,

691
00:40:12,112 --> 00:40:13,984
이제 이것이 우리의 순방향 패스라고 가정합니다.

692
00:40:13,984 --> 00:40:16,144
이제 우리는 그라디언트를 계산하려고합니다.

693
00:40:16,144 --> 00:40:17,941
먼저 그라디언트는 무엇입니까?

694
00:40:17,941 --> 00:40:20,024
가중치 같아?

695
00:40:24,155 --> 00:40:26,238
누가 추측을합니까?

696
00:40:28,571 --> 00:40:30,960
우리가 이것에 대해 생각한다면,

697
00:40:30,960 --> 00:40:34,806
우리는 우리의 입력 값이 매우 작습니다

698
00:40:34,806 --> 00:40:36,531
각 레이어 오른쪽에,

699
00:40:36,531 --> 00:40:39,529
그들은이 근처에서 모두 무너 졌기 때문에,

700
00:40:39,529 --> 00:40:40,638
그리고 나서 각 층,

701
00:40:40,638 --> 00:40:43,273
우리는 우리의 업스트림 그라디언트가 흘러 내리고,

702
00:40:43,273 --> 00:40:46,513
그리고 나서 가중치에 그라데이션을 얻기 위해서

703
00:40:46,513 --> 00:40:50,140
우리의 업스트림 그라디언트가 로컬
그라디언트 인 것을 기억하십시오.

704
00:40:50,140 --> 00:40:53,483
이 점 제품은 W 배 X를하고있었습니다.

705
00:40:53,483 --> 00:40:56,985
기본적으로 X가 될 것입니다. 이것은 우리의 입력입니다.

706
00:40:56,985 --> 00:40:59,318
그래서, 그것은 다시 비슷한 종류의 문제입니다.

707
00:40:59,318 --> 00:41:00,571
우리가 이전에 보았던,

708
00:41:00,571 --> 00:41:03,062
왜냐하면 X가 작기 때문에 지금 여기서부터, 여기에서,

709
00:41:03,062 --> 00:41:05,636
우리의 가중치는 매우 작은 그래디언트를 얻고 있습니다.

710
00:41:05,636 --> 00:41:07,058
기본적으로 업데이트되지 않습니다.

711
00:41:07,058 --> 00:41:10,665
그래서, 이것은 당신이 기본적으로
시도하고 생각할 수있는 방법입니다.

712
00:41:10,665 --> 00:41:13,488
그라디언트 효과가 네트워크를 통해 흐릅니다.

713
00:41:13,488 --> 00:41:16,859
앞으로 패스가 무엇을하고 있는지 항상 생각할 수 있습니다.

714
00:41:16,859 --> 00:41:18,644
무슨 일이 일어나고 있는지 생각해보십시오.

715
00:41:18,644 --> 00:41:20,329
그래디언트 흐름이 내려 오면

716
00:41:20,329 --> 00:41:22,374
및 다양한 유형의 입력,

717
00:41:22,374 --> 00:41:25,825
이것이 실제로 우리의 무게에 미치는 영향

718
00:41:25,825 --> 00:41:28,562
그리고 그들에 그라디언트.

719
00:41:28,562 --> 00:41:31,645
그리고 또한 그렇게 생각한다면, 지금 생각해 보면

720
00:41:33,624 --> 00:41:36,604
다시 흐르게 될 그라데이션은 무엇입니까?

721
00:41:36,604 --> 00:41:40,004
이 모든 그라디언트를 체인으로 묶을 때 각 레이어에서

722
00:41:40,004 --> 00:41:41,358
좋아, 이렇게 될거야.

723
00:41:41,358 --> 00:41:43,233
우리가 지금 가지고있는 뒤집기

724
00:41:43,233 --> 00:41:45,712
다시 흐르는 그라디언트는 업스트림 그라디언트입니다.

725
00:41:45,712 --> 00:41:50,291
이 경우 로컬 그라디언트는 입력 X에서 W입니다.

726
00:41:50,291 --> 00:41:53,085
그리고 다시 한번, 이것이 내적
(dot product)이기 때문에,

727
00:41:53,085 --> 00:41:56,944
이제는 실제로 각 레이어에서 뒤로 이동합니다.

728
00:41:56,944 --> 00:41:59,262
우리는 기본적으로 곱셈을하고있다.

729
00:41:59,262 --> 00:42:02,041
우리의 가중치에 의한 업스트림 그래디언트

730
00:42:02,041 --> 00:42:06,208
다음 그라디언트가 아래쪽으로 흐르도록합니다.

731
00:42:07,283 --> 00:42:08,651
그리고 여기 때문에,

732
00:42:08,651 --> 00:42:11,027
우리는 W에 계속해서 다시 곱합니다.

733
00:42:11,027 --> 00:42:13,416
당신은 기본적으로 동일한 현상을 보이고 있습니다.

734
00:42:13,416 --> 00:42:15,160
우리가 포워드 패스에서했던 것처럼

735
00:42:15,160 --> 00:42:18,198
모든 것이 점점 작아지고 있습니다.

736
00:42:18,198 --> 00:42:20,776
이제 그라디언트, 업스트림 그래디언트

737
00:42:20,776 --> 00:42:23,541
또한 0으로 축소됩니다.

738
00:42:23,541 --> 00:42:24,869
문제?

739
00:42:24,869 --> 00:42:28,452
[마이크 끄기 학생]

740
00:42:30,731 --> 00:42:33,493
네, 업스트림과 다운 스트림이 같아요.

741
00:42:33,493 --> 00:42:35,080
다르게 해석 될 수 있지만,

742
00:42:35,080 --> 00:42:37,945
당신이 앞뒤로 가고 있는지에 따라,

743
00:42:37,945 --> 00:42:41,387
그러나이 경우에 우리는 가고 있습니다, 우리는하고 있습니다,

744
00:42:41,387 --> 00:42:42,465
우린 거꾸로 가고있어, 그렇지?

745
00:42:42,465 --> 00:42:43,907
우리는 역 전파를하고 있습니다.

746
00:42:43,907 --> 00:42:47,376
그리고 업스트림은 흐르는 그라디언트입니다.

747
00:42:47,376 --> 00:42:49,441
당신은 당신의 손실로부터 흐름을 생각할 수 있습니다,

748
00:42:49,441 --> 00:42:51,409
다시 귀하의 의견을 기다립니다.

749
00:42:51,409 --> 00:42:53,238
그리고 업스트림은 무엇이 왔는지입니다.

750
00:42:53,238 --> 00:42:54,851
당신이 이미 한 일부터

751
00:42:54,851 --> 00:42:58,684
여러분이 현재 노드로 내려가는 것을 알 수 있습니다.

752
00:43:00,270 --> 00:43:02,310
맞아, 그래서 우리는 아래쪽으로 흐르기 위해서,

753
00:43:02,310 --> 00:43:05,521
그리고 우리는 역행을 통해 노드에 들어갈 수 있습니다.

754
00:43:05,521 --> 00:43:07,521
업스트림에서 왔습니다.

755
00:43:13,888 --> 00:43:17,861
자, 이제는 언제 어떻게되는지 생각해 봅시다.

756
00:43:17,861 --> 00:43:19,317
우리는 이것이 문제라는 것을 알았습니다.

757
00:43:19,317 --> 00:43:21,102
우리의 무게가 꽤 작았을 때, 맞습니까?

758
00:43:21,102 --> 00:43:22,767
그래서 우리는 잘 생각할 수 있습니다.

759
00:43:22,767 --> 00:43:23,943
우리가이 문제를 해결하려고한다면 어떨까요?

760
00:43:23,943 --> 00:43:26,133
우리의 무게를 크게함으로써?

761
00:43:26,133 --> 00:43:29,773
그래서이 표준 가우스에서 샘플을 봅시다.

762
00:43:29,773 --> 00:43:33,940
0.01 대신 표준 편차 1을 사용합니다.

763
00:43:35,133 --> 00:43:37,383
그러면 여기서 어떤 문제가 발생합니까?

764
00:43:39,227 --> 00:43:41,310
누가 추측을합니까?

765
00:43:44,558 --> 00:43:49,134
우리의 무게가 모두 커지면 우리는
그들을 지나가고 있습니다.

766
00:43:49,134 --> 00:43:51,180
우리는 W times X의 이러한 출력을 취하고 있습니다.

767
00:43:51,180 --> 00:43:54,750
그것들을 tanh 비선형 성을 통과 시키면,

768
00:43:54,750 --> 00:43:57,718
우리는 무슨 일이 일어나고 있는지
이야기하고 있었다는 것을 기억하십시오.

769
00:43:57,718 --> 00:43:59,373
탄 (tanh)에 대한 투입물의 다른 값에서,

770
00:43:59,373 --> 00:44:01,883
그래서 문제가 무엇입니까?

771
00:44:01,883 --> 00:44:04,402
그래, 그래, 나는 그것이 포화
상태가 될 것이라고 들었다.

772
00:44:04,402 --> 00:44:06,289
그래서 맞습니다.

773
00:44:06,289 --> 00:44:09,846
기본적으로 지금, 우리의 무게가 커질 것이기 때문에,

774
00:44:09,846 --> 00:44:12,941
우리는 항상 포화 상태에있게 될 것입니다.

775
00:44:12,941 --> 00:44:15,966
매우 부정적인 또는 매우 긍정적 인 tanh.

776
00:44:15,966 --> 00:44:18,701
그리고 실제로, 당신이 여기에 올 것입니다.

777
00:44:18,701 --> 00:44:23,385
우리가 활성화의 분포를 보면 지금이다.

778
00:44:23,385 --> 00:44:25,528
여기 바닥의 각 층에는

779
00:44:25,528 --> 00:44:29,695
그들은 기본적으로 모두 부정적인 것
또는 더하기 하나가 될 것입니다.

780
00:44:30,855 --> 00:44:32,613
맞아. 그러면 문제가 생길거야.

781
00:44:32,613 --> 00:44:33,535
우리가 얘기 한

782
00:44:33,535 --> 00:44:35,421
이전에 tanh와 함께, 그들은 포화 때,

783
00:44:35,421 --> 00:44:37,697
모든 그라디언트는 0이 될 것이고,

784
00:44:37,697 --> 00:44:40,447
우리의 무게는 업데이트되지 않습니다.

785
00:44:41,397 --> 00:44:43,543
그래서 기본적으로, 그것은 얻기가 정말로 어렵습니다.

786
00:44:43,543 --> 00:44:46,363
체중 초기화 권리.

787
00:44:46,363 --> 00:44:48,309
너무 작 으면 모두 붕괴됩니다.

788
00:44:48,309 --> 00:44:50,296
너무 크면 포화 상태가됩니다.

789
00:44:50,296 --> 00:44:53,147
그래서, 알아 내려는 시도가있었습니다.

790
00:44:53,147 --> 00:44:55,553
음,이 가중치를 초기화하는 올바른 방법은 무엇입니까?

791
00:44:55,553 --> 00:45:00,175
그래서, 당신이 사용할 수있는 엄지 손가락의 좋은 법칙

792
00:45:00,175 --> 00:45:02,507
Xavier 초기화입니다.

793
00:45:02,507 --> 00:45:07,388
그리고 이것은 2010 년 Glorot가
작성한이 논문의 내용입니다.

794
00:45:07,388 --> 00:45:09,721
그리고이 공식이 무엇인지,

795
00:45:11,162 --> 00:45:13,826
우리가 여기서 W를 보면,

796
00:45:13,826 --> 00:45:17,403
우리는 이들을 초기화 할 수 있음을 알 수 있습니다.

797
00:45:17,403 --> 00:45:19,654
우리는 우리의 표준 가우스 (gaussian)

798
00:45:19,654 --> 00:45:20,767
우리는 규모를 늘릴 것입니다.

799
00:45:20,767 --> 00:45:22,653
우리가 가지고있는 인풋의 수만큼

800
00:45:22,653 --> 00:45:24,507
그리고 당신은 수학을 통해 갈 수 있습니다.

801
00:45:24,507 --> 00:45:25,705
강의 노트에서 볼 수 있습니다.

802
00:45:25,705 --> 00:45:27,018
이 논문에서도

803
00:45:27,018 --> 00:45:28,599
정확히 어떻게 작동하는지,

804
00:45:28,599 --> 00:45:31,064
하지만 기본적으로 우리가하는 방식은

805
00:45:31,064 --> 00:45:33,027
우리는 입력의 분산을 원한다.

806
00:45:33,027 --> 00:45:35,789
출력의 분산과 같도록,

807
00:45:35,789 --> 00:45:38,382
체중이 어떻게되어야하는지 파생한다면

808
00:45:38,382 --> 00:45:40,195
당신은이 공식을 얻을 것이고,

809
00:45:40,195 --> 00:45:42,789
직관적으로 이런 종류의 수단으로

810
00:45:42,789 --> 00:45:46,015
당신이 적은 수의 입력이 있다면,

811
00:45:46,015 --> 00:45:48,161
우리는 더 작은 숫자로 나눌 것입니다.

812
00:45:48,161 --> 00:45:50,566
더 큰 가중치를 얻으려면 더 큰 가중치가 필요합니다.

813
00:45:50,566 --> 00:45:52,654
작은 입력으로,

814
00:45:52,654 --> 00:45:55,174
그리고 당신은 이들 각각에 무게를 곱하고 있습니다.

815
00:45:55,174 --> 00:45:56,153
더 큰 무게가 필요해.

816
00:45:56,153 --> 00:45:58,993
출력에서 동일한 더 큰 분산을 얻으려면,

817
00:45:58,993 --> 00:46:02,339
그리고 그 반대도 마찬가지입니다.
우리가 많은 의견을 가지고 있다면,

818
00:46:02,339 --> 00:46:05,465
그러면 우리는 더 작은 가중치를 얻기 위해서

819
00:46:05,465 --> 00:46:08,505
출력에서 동일한 스프레드.

820
00:46:08,505 --> 00:46:10,795
따라서 이에 대한 자세한 내용은 메모를 참조하십시오.

821
00:46:10,795 --> 00:46:14,697
그리고 근본적으로 지금 우리가
단위 가우시안을 갖고 싶다면,

822
00:46:14,697 --> 00:46:16,886
각 레이어에 입력으로,

823
00:46:16,886 --> 00:46:20,860
우리는 이런 종류의 초기화를 사용할 수 있습니다.

824
00:46:20,860 --> 00:46:23,150
훈련 시간에, 이것을 초기화 할 수 있고,

825
00:46:23,150 --> 00:46:25,169
그래서 대략

826
00:46:25,169 --> 00:46:27,669
각 계층에서 가우스 단위.

827
00:46:29,057 --> 00:46:31,822
좋아, 그럼에도 불구하고 한 가지는

828
00:46:31,822 --> 00:46:35,032
선형 활성화가 있다고 가정합니다.

829
00:46:35,032 --> 00:46:37,250
그래서 그것은 우리가 활성화되어 있다고 가정합니다.

830
00:46:37,250 --> 00:46:40,837
예를 들어, tanh의 활성 영역에 있습니다.

831
00:46:40,837 --> 00:46:43,402
그리고 다시, 당신은 노트를 볼 수 있습니다.

832
00:46:43,402 --> 00:46:46,051
그 파생을 실제로 시도하고 이해하기 위해서,

833
00:46:46,051 --> 00:46:48,194
그러나 문제는 이것이 깨어진 것입니다.

834
00:46:48,194 --> 00:46:51,255
지금 당신은 ReLU와 같은 것을 사용합니다.

835
00:46:51,255 --> 00:46:54,849
맞아요, 그리고 ReLU와 함께 일어나는 일은,

836
00:46:54,849 --> 00:46:57,674
그것은 당신 유닛의 절반을 죽이고 있기 때문에,

837
00:46:57,674 --> 00:46:59,382
그것들의 약 절반을 설정하고있다.

838
00:46:59,382 --> 00:47:01,014
매번 0으로,

839
00:47:01,014 --> 00:47:04,685
그것은 실제로 당신이 이것에서
벗어나는 분산을 반으로 줄입니다.

840
00:47:04,685 --> 00:47:07,985
그리고 지금, 당신이 똑같은 가정을한다면

841
00:47:07,985 --> 00:47:11,090
귀하의 파생물로 일찍이 실제로는

842
00:47:11,090 --> 00:47:14,393
올바른 분산이 나오도록하십시오.

843
00:47:14,393 --> 00:47:16,193
그것은 너무 작을 것입니다.

844
00:47:16,193 --> 00:47:19,994
그리고 여러분이 보는 것은 이런 종류의 현상입니다.

845
00:47:19,994 --> 00:47:23,323
분포가 붕괴되기 시작하자.

846
00:47:23,323 --> 00:47:25,769
이 경우에 당신은 점점 더 0으로 정점을 이룹니다.

847
00:47:25,769 --> 00:47:28,019
더 많은 유닛이 비활성화되었습니다.

848
00:47:29,541 --> 00:47:31,370
이 문제를 해결하는 방법

849
00:47:31,370 --> 00:47:35,661
몇몇 종이에서 지적 된 무언가로,

850
00:47:35,661 --> 00:47:38,685
이것은 당신이 이것을 설명하려고 할 수 있다는 것입니다.

851
00:47:38,685 --> 00:47:41,580
여분으로, 2로 나눈.

852
00:47:41,580 --> 00:47:44,273
이제, 당신은 기본적으로 사실을 조정하고 있습니다.

853
00:47:44,273 --> 00:47:47,023
뉴런의 절반이 죽게됩니다.

854
00:47:48,636 --> 00:47:51,803
그래서 당신은 동등한 입력입니다.

855
00:47:53,013 --> 00:47:55,116
실제로이 입력의 수는 절반입니다.

856
00:47:55,116 --> 00:47:57,549
그래서 이것을 두 개의 인수로 나눈 값을 더합니다.

857
00:47:57,549 --> 00:47:59,332
이것은 훨씬 더 잘 작동합니다.

858
00:47:59,332 --> 00:48:02,265
그리고 여러분은 그 분포가 꽤
좋다는 것을 알 수 있습니다.

859
00:48:02,265 --> 00:48:05,348
네트워크의 모든 계층에서

860
00:48:06,959 --> 00:48:09,594
실제로 실제로 이것은 실제로 중요합니다.

861
00:48:09,594 --> 00:48:11,855
이런 종류의 작은 것들을 훈련하기 위해서,

862
00:48:11,855 --> 00:48:14,692
당신의 무게가 어떻게에 있는지
진짜로주의를 기울이기 위하여,

863
00:48:14,692 --> 00:48:16,161
큰 변화를 가져라.

864
00:48:16,161 --> 00:48:19,818
그리고 예를 들어, 몇몇 논문에서 볼 수 있습니다.

865
00:48:19,818 --> 00:48:23,591
이것이 실제로 네트워크 간의 차이라고

866
00:48:23,591 --> 00:48:26,226
심지어 전혀 훈련을하고 잘 수행한다.

867
00:48:26,226 --> 00:48:28,309
대 아무것도 일어나지 않는다.

868
00:48:32,548 --> 00:48:34,909
따라서 적절한 초기화가 여전히 있습니다.

869
00:48:34,909 --> 00:48:36,321
연구의 활발한 영역.

870
00:48:36,321 --> 00:48:37,429
그리고 만약 당신이 이것에 관심이 있다면,

871
00:48:37,429 --> 00:48:40,281
당신은이 서류들과 자료들을 많이 볼 수 있습니다.

872
00:48:40,281 --> 00:48:43,319
좋은 경험 법칙은 기본적으로

873
00:48:43,319 --> 00:48:46,225
Xavier Initialization을
사용하여 시작합니다.

874
00:48:46,225 --> 00:48:48,618
그런 다음에 대해서도 생각해 볼 수 있습니다.

875
00:48:48,618 --> 00:48:51,701
이러한 다른 종류의 메소드 중 일부.

876
00:48:53,871 --> 00:48:56,049
그리고 이제 우리는 이것에 관련된
아이디어에 대해서 이야기 할 것입니다,

877
00:48:56,049 --> 00:48:58,655
그래서 활성화를 유지하려는이 아이디어

878
00:48:58,655 --> 00:49:01,405
우리가 원하는 가우스 범위에서.

879
00:49:03,330 --> 00:49:04,967
맞아, 그리고이 아이디어 뒤에

880
00:49:04,967 --> 00:49:06,533
우리가 일괄 정규화라고 부르는 것은,

881
00:49:06,533 --> 00:49:09,672
우리는 단위 가우시안 활성화를 원합니다.

882
00:49:09,672 --> 00:49:11,370
그냥 그렇게 만들자.

883
00:49:11,370 --> 00:49:14,240
그 사람들을 그렇게 강요합시다.

884
00:49:14,240 --> 00:49:15,834
그리고 이것이 어떻게 작동합니까?

885
00:49:15,834 --> 00:49:19,376
그럼, 어떤 레이어에서 활성화 배치를 고려해 봅시다.

886
00:49:19,376 --> 00:49:22,631
이제 우리는 모든 활성화가 이루어집니다.

887
00:49:22,631 --> 00:49:25,640
이 단위를 가우시안으로 만들고 싶다면,

888
00:49:25,640 --> 00:49:29,368
우리는 실제로 경험적으로 이것을 바로 할 수 있습니다.

889
00:49:29,368 --> 00:49:33,861
우리는 지금까지 가지고있는 배치의
평균을 취할 수 있습니다.

890
00:49:33,861 --> 00:49:37,562
현재 일괄 처리의 값은 단지 현재 일괄 처리의 값이며,

891
00:49:37,562 --> 00:49:39,392
우리는 이것으로 정상화 할 수 있습니다.

892
00:49:39,392 --> 00:49:41,408
맞아, 근본적으로,

893
00:49:41,408 --> 00:49:44,000
무게 초기화 대신에,

894
00:49:44,000 --> 00:49:46,319
우리는 훈련 시작시 이것을 설정하고 있습니다.

895
00:49:46,319 --> 00:49:48,594
그래서 우리는 그것을 좋은 곳으로 가져 가려고 노력한다.

896
00:49:48,594 --> 00:49:50,867
우리는 모든 계층에서 단위 가우시안을 가질 수 있으며,

897
00:49:50,867 --> 00:49:53,096
그리고 훈련 중에 희망을 유지할 것입니다.

898
00:49:53,096 --> 00:49:55,661
이제 우리는 명시 적으로 그렇게 할 것입니다.

899
00:49:55,661 --> 00:49:58,336
네트워크를 통한 모든 순방향 통과시.

900
00:49:58,336 --> 00:50:00,833
우리는 이것이 기능적으로 일어날 것입니다,

901
00:50:00,833 --> 00:50:04,166
기본적으로 평균에 의해 정규화함으로써

902
00:50:05,518 --> 00:50:08,139
및 각 뉴런의 분산,

903
00:50:08,139 --> 00:50:11,607
우리는 그것에 들어오는 모든 입력을 봅니다.

904
00:50:11,607 --> 00:50:14,185
해당 배치의 평균 및 분산을 계산합니다.

905
00:50:14,185 --> 00:50:15,754
그것을 정상화하십시오.

906
00:50:15,754 --> 00:50:18,084
그리고 이것은, 이것은,

907
00:50:18,084 --> 00:50:19,928
이것은 단지 차별화 된 기능입니다.

908
00:50:19,928 --> 00:50:23,256
우리가 평균과 분산을 상수로 가지고 있다면,

909
00:50:23,256 --> 00:50:26,931
이것은 단지 연산 연산의 연속이다.

910
00:50:26,931 --> 00:50:31,098
우리가 차별화하고이를 통해 다시 소품을 만들 수 있습니다.

911
00:50:33,115 --> 00:50:36,512
그래, 내가 이전에 말한 것처럼,

912
00:50:36,512 --> 00:50:40,172
우리가 입력 데이터를 보면, 우리는
이것을 다음과 같이 생각합니다.

913
00:50:40,172 --> 00:50:43,695
우리는 현재 배치에서 N 개의
훈련 예제를 가지고 있습니다.

914
00:50:43,695 --> 00:50:47,065
각 배치는 차원 D를 가지며,

915
00:50:47,065 --> 00:50:49,616
우리는 경험적 평균을 계산할 것입니다.

916
00:50:49,616 --> 00:50:52,999
및 각 차원에 대해 독립적으로 분산,

917
00:50:52,999 --> 00:50:56,063
그래서 각각의 기본적으로 기능 요소,

918
00:50:56,063 --> 00:50:58,893
우리는이 일괄 처리를 통해 이것을 계산합니다.

919
00:50:58,893 --> 00:51:00,323
우리의 현재 미니 배치

920
00:51:00,323 --> 00:51:02,406
우리는 이것으로 정상화합니다.

921
00:51:05,786 --> 00:51:08,629
그리고 이것은 일반적으로 완전히 연결된 후에 삽입됩니다.

922
00:51:08,629 --> 00:51:09,988
또는 길쌈 층.

923
00:51:09,988 --> 00:51:12,191
우리는 그것이 우리가

924
00:51:12,191 --> 00:51:15,590
우리가 반복해서하는이 층에서 W에 의해,

925
00:51:15,590 --> 00:51:18,932
그러면 우리는 각각이 나쁜 스케일링
효과를 가질 수 있습니다.

926
00:51:18,932 --> 00:51:22,731
따라서 기본적으로이 효과를 실행 취소 할 수 있습니다.

927
00:51:22,731 --> 00:51:26,584
맞습니다. 그리고 기본적으로
입력에 의해 스케일링되기 때문에

928
00:51:26,584 --> 00:51:29,748
각 뉴런에 연결, 각 활성화,

929
00:51:29,748 --> 00:51:32,632
우리는이 방법을 완전히 연결해도 적용 할 수 있습니다.

930
00:51:32,632 --> 00:51:37,132
길쌈 층 (convolutional
layers)과 유일한 차이점은,

931
00:51:37,132 --> 00:51:38,879
길쌈 레이어로, 우리는 정상화하고 싶다.

932
00:51:38,879 --> 00:51:41,861
모든 교육 사례 에서뿐만 아니라,

933
00:51:41,861 --> 00:51:45,895
각 피처 치수에 대해 독립적으로,

934
00:51:45,895 --> 00:51:48,527
그러나 우리는 실제로 공동으로 정상화하기를 원합니다.

935
00:51:48,527 --> 00:51:52,285
두 가지 기능 차원 모두에서

936
00:51:52,285 --> 00:51:53,488
모든 공간 위치

937
00:51:53,488 --> 00:51:55,527
우리가 활성화 맵에 가지고있는 것,

938
00:51:55,527 --> 00:51:58,895
모든 교육 사례를 제공합니다.

939
00:51:58,895 --> 00:51:59,839
그리고 우리는 이것을합니다.

940
00:51:59,839 --> 00:52:01,607
우리는 길쌈 재산에 복종하기를 원하기 때문에,

941
00:52:01,607 --> 00:52:03,285
근처 위치를 원한다.

942
00:52:03,285 --> 00:52:05,903
똑같은 방식으로 정상화되는 거지?

943
00:52:05,903 --> 00:52:07,417
그래서 컨볼 루션 레이어 (convolutional layer)

944
00:52:07,417 --> 00:52:09,913
우리는 기본적으로 하나의 의미를 가지려고합니다.

945
00:52:09,913 --> 00:52:11,410
하나의 표준 편차,

946
00:52:11,410 --> 00:52:13,489
우리가 가지고있는 활성화 맵마다,

947
00:52:13,489 --> 00:52:15,432
우리는 이것을 정상화하려고합니다.

948
00:52:15,432 --> 00:52:18,094
배치의 모든 예제에서

949
00:52:18,094 --> 00:52:21,103
그래서 이것은 당신들이하는 것입니다.

950
00:52:21,103 --> 00:52:23,098
당신의 다음 과제에서 구현할 것입니다.

951
00:52:23,098 --> 00:52:27,586
그래서 모든 세부 사항은 매우 명확하게 설명됩니다.

952
00:52:27,586 --> 00:52:29,367
이 신문에서 2015.

953
00:52:29,367 --> 00:52:31,932
그리고 이것은 매우 유용합니다.

954
00:52:31,932 --> 00:52:35,621
실용적으로 많이 사용하고 싶은 유용한 기술.

955
00:52:35,621 --> 00:52:37,823
이러한 일괄 정규화 레이어가 필요합니다.

956
00:52:37,823 --> 00:52:40,970
그래서이 신문을 읽어야합니다.

957
00:52:40,970 --> 00:52:43,369
모든 파생어를 살펴보고,

958
00:52:43,369 --> 00:52:46,129
또한 파생어를 통과합니다.

959
00:52:46,129 --> 00:52:50,212
주어진 그라데이션을 계산하는 방법에 대해

960
00:52:51,301 --> 00:52:53,718
이 정규화 연산.

961
00:52:56,626 --> 00:52:59,993
그래, 내가 지적하고자하는 한 가지는,

962
00:52:59,993 --> 00:53:01,632
너도 알다시피,

963
00:53:01,632 --> 00:53:03,584
우리는이 일괄 정규화를하고 있습니다.

964
00:53:03,584 --> 00:53:05,930
모든 완전히 연결된 레이어 다음에,

965
00:53:05,930 --> 00:53:08,676
우리가 반드시 원한다는 것이 명확하지 않다.

966
00:53:08,676 --> 00:53:12,031
이러한 tanh 비선형성에 대한 단위 가우시안 입력,

967
00:53:12,031 --> 00:53:14,679
이게 뭘하고 있는지는 이것이 당신을
제약하고 있기 때문입니다.

968
00:53:14,679 --> 00:53:17,107
이 비선형 성의 선형 정권에,

969
00:53:17,107 --> 00:53:20,047
우리는 실제로는 아니며, 기본적으로 말하려고합니다.

970
00:53:20,047 --> 00:53:21,974
이 채도를 갖지 말자.

971
00:53:21,974 --> 00:53:24,734
어쩌면이게 조금 좋을 수도 있지, 그렇지?

972
00:53:24,734 --> 00:53:27,321
당신은 당신이 원하는 것을 통제 할 수 있기를 원합니다.

973
00:53:27,321 --> 00:53:30,821
얼마나 포화 상태인지 알고 싶습니다.

974
00:53:31,845 --> 00:53:34,462
그래서 우리가이 문제를 해결하는 방법은 무엇입니까?

975
00:53:34,462 --> 00:53:36,166
일괄 정규화를 수행 할 때

976
00:53:36,166 --> 00:53:39,512
우리가 정규화 연산을 수행한다는 것입니다.

977
00:53:39,512 --> 00:53:42,577
그러나 그 후에 우리는이 추가적인

978
00:53:42,577 --> 00:53:44,453
스쿼시 및 스케일링 작업.

979
00:53:44,453 --> 00:53:45,922
그래서, 우리는 우리의 정규화를 수행합니다.

980
00:53:45,922 --> 00:53:49,098
그런 다음 우리는 일정한 감마에 의해 축척 할 것입니다,

981
00:53:49,098 --> 00:53:52,515
베타의 또 다른 요인으로 이동하십시오.

982
00:53:53,349 --> 00:53:55,914
맞습니다. 그래서 실제로 이것이하는 일은

983
00:53:55,914 --> 00:54:00,546
이렇게하면 신원 기능을 복구 할 수 있습니다

984
00:54:00,546 --> 00:54:02,071
네가 원한다면.

985
00:54:02,071 --> 00:54:03,499
따라서 네트워크가 원한다면,

986
00:54:03,499 --> 00:54:06,381
그것은 귀하의 스케일링 요소 감마를 배울 수

987
00:54:06,381 --> 00:54:07,868
너의 차이 일 뿐이다.

988
00:54:07,868 --> 00:54:10,613
베타가 당신의 평균임을 알 수 있습니다.

989
00:54:10,613 --> 00:54:13,844
이 경우 ID 매핑을 복구 할 수 있습니다.

990
00:54:13,844 --> 00:54:16,659
배치 정규화가없는 것처럼

991
00:54:16,659 --> 00:54:18,323
이제는 유연성이 있습니다.

992
00:54:18,323 --> 00:54:21,249
사이에 모든 일을하는

993
00:54:21,249 --> 00:54:24,563
네트워크 학습 만들기

994
00:54:24,563 --> 00:54:27,142
당신의 tanh를 더 많이 또는 덜 포화시키는 법,

995
00:54:27,142 --> 00:54:30,392
그리고 그렇게하기 위해 얼마나해야하는지,

996
00:54:32,065 --> 00:54:33,898
좋은 훈련을받는 것.

997
00:54:38,166 --> 00:54:39,969
좋아, 그럼 그냥 요약해라.

998
00:54:39,969 --> 00:54:42,285
배치 정규화 아이디어.

999
00:54:42,285 --> 00:54:44,585
맞습니다.

1000
00:54:44,585 --> 00:54:48,455
우리는 미니 배치 평균을 계산할 것입니다.

1001
00:54:48,455 --> 00:54:50,868
그래서 우리는 들어오는 모든
미니 배치에 대해 이렇게합니다.

1002
00:54:50,868 --> 00:54:52,906
우리는 우리의 분산을 계산합니다.

1003
00:54:52,906 --> 00:54:54,984
우리는 평균과 분산으로 정규화하고,

1004
00:54:54,984 --> 00:54:58,342
이 추가 확장 및 이동 요소가 있습니다.

1005
00:54:58,342 --> 00:55:02,793
따라서 네트워크를 통한 그라디언트 흐름이 향상됩니다.

1006
00:55:02,793 --> 00:55:05,484
결과적으로 더 견고합니다.

1007
00:55:05,484 --> 00:55:08,460
더 많은 학습 속도 범위에서 작동하며,

1008
00:55:08,460 --> 00:55:10,562
및 다른 종류의 초기화,

1009
00:55:10,562 --> 00:55:11,727
사람들은

1010
00:55:11,727 --> 00:55:13,183
배치 정규화를 입력하면

1011
00:55:13,183 --> 00:55:14,750
그리고 훈련하기가 더 쉽습니다.

1012
00:55:14,750 --> 00:55:16,955
그래서 이것이 당신이해야하는 이유입니다.

1013
00:55:16,955 --> 00:55:21,850
그리고 내가 지적하고 싶은 한가지

1014
00:55:21,850 --> 00:55:24,724
당신은 또한 이런 식으로 생각할 수 있다는 것입니다.

1015
00:55:24,724 --> 00:55:27,162
또한 일부 정규화를 수행합니다.

1016
00:55:27,162 --> 00:55:31,329
이제는 각 레이어의 출력물에 있기 때문에,

1017
00:55:32,189 --> 00:55:35,706
이러한 각각의 활성화들, 이들 각각의 출력들,

1018
00:55:35,706 --> 00:55:38,282
귀하의 입력 X,

1019
00:55:38,282 --> 00:55:41,165
배치의 다른 예제들

1020
00:55:41,165 --> 00:55:42,733
그것은, 오른쪽으로,

1021
00:55:42,733 --> 00:55:45,713
각 입력 데이터를 정규화하려고하기 때문에

1022
00:55:45,713 --> 00:55:48,266
그 배치를 통해 경험적 평균에 의해.

1023
00:55:48,266 --> 00:55:50,525
그래서 그 때문에 더 이상 생산하지 않습니다.

1024
00:55:50,525 --> 00:55:54,021
주어진 트레이닝 예제에 대한 결정 론적 값,

1025
00:55:54,021 --> 00:55:57,543
이 모든 입력을 한 묶음으로 묶어 두는 것입니다.

1026
00:55:57,543 --> 00:56:00,330
그리고 이것은 기본적으로 더 이상 결정적이지 않기 때문에,

1027
00:56:00,330 --> 00:56:03,048
약간의 불안감은 X의 표현을 조금씩,

1028
00:56:03,048 --> 00:56:07,215
그리고 어떤 의미에서는 일종의 정규화 효과를냅니다.

1029
00:56:08,941 --> 00:56:10,490
그래, 질문 있니?

1030
00:56:10,490 --> 00:56:13,401
[학생이 카메라에서 말하기]

1031
00:56:13,401 --> 00:56:15,621
문제는 감마와 베타가 학습 된 매개 변수이며,

1032
00:56:15,621 --> 00:56:17,354
그리고 그렇습니다.

1033
00:56:17,354 --> 00:56:20,937
[마이크 끄기 학생]

1034
00:56:27,754 --> 00:56:29,682
네, 그렇다면 왜 우리가 배우고 싶은가요?

1035
00:56:29,682 --> 00:56:31,429
이 감마와 베타가 가능한 베타

1036
00:56:31,429 --> 00:56:34,618
다시 신원 기능을 배우려면,

1037
00:56:34,618 --> 00:56:36,484
그 이유는

1038
00:56:36,484 --> 00:56:38,481
당신은 그것에게 유연성을주고 싶다.

1039
00:56:38,481 --> 00:56:41,518
맞아, 일괄 정규화가하고있는 일,

1040
00:56:41,518 --> 00:56:46,080
우리 데이터가이 단위 가우시안이되도록 강요하고 있습니까?

1041
00:56:46,080 --> 00:56:48,381
우리의 입력은 단위 가우스,

1042
00:56:48,381 --> 00:56:50,890
그러나 일반적으로 이것은 좋은 생각이지만,

1043
00:56:50,890 --> 00:56:54,232
이것이 항상 최선의 방법은 아닙니다.

1044
00:56:54,232 --> 00:56:57,240
그리고 우리는 특히 tanh와 같은 것을 보았습니다.

1045
00:56:57,240 --> 00:56:58,379
너는 통제하고 싶을지도 모른다.

1046
00:56:58,379 --> 00:57:00,279
당신이 가진 채도의 정도.

1047
00:57:00,279 --> 00:57:02,720
그리고 이것이하는 일은 유연성을 제공합니다.

1048
00:57:02,720 --> 00:57:07,018
단위 가우스 정규화와 똑같이하는 것,

1049
00:57:07,018 --> 00:57:09,979
원한다면 어쩌면 그걸 배우 겠지.

1050
00:57:09,979 --> 00:57:12,101
네트워크의이 특정 부분에서,

1051
00:57:12,101 --> 00:57:14,195
어쩌면 그렇게하는 것이 최선의 방법이 아닙니다.

1052
00:57:14,195 --> 00:57:16,926
어쩌면 우리는이 일반적인 생각에
여전히 어떤 것을 원할 것입니다.

1053
00:57:16,926 --> 00:57:19,838
약간 다른 권리, 약간의 규모 또는 이동.

1054
00:57:19,838 --> 00:57:23,125
따라서 이러한 매개 변수는 유연성을 제공합니다.

1055
00:57:23,125 --> 00:57:25,968
원한다면 배우기.

1056
00:57:25,968 --> 00:57:28,184
그리고 나서 네가 할 수있는 최선의 일이라면

1057
00:57:28,184 --> 00:57:31,712
그냥 일괄 정규화 다음 배울 것입니다

1058
00:57:31,712 --> 00:57:34,457
그것에 대한 올바른 매개 변수.

1059
00:57:34,457 --> 00:57:35,665
네?

1060
00:57:35,665 --> 00:57:39,710
[마이크 끄기 학생]

1061
00:57:39,710 --> 00:57:42,877
그래, 기본적으로 각 신경 출력.

1062
00:57:44,164 --> 00:57:46,424
그래서, 우리는 완전히 연결된
레이어의 출력을 가지고 있습니다.

1063
00:57:46,424 --> 00:57:48,366
우리는 W 곱하기 X를가집니다.

1064
00:57:48,366 --> 00:57:51,347
그래서 우리는이 각각의 산출 값을 가지고 있습니다,

1065
00:57:51,347 --> 00:57:52,650
그런 다음 우리는 신청할 것입니다.

1066
00:57:52,650 --> 00:57:57,365
각 뉴런에 개별적으로 일괄 정규화.

1067
00:57:57,365 --> 00:57:58,835
문제?

1068
00:57:58,835 --> 00:58:02,418
[마이크 끄기 학생]

1069
00:58:10,031 --> 00:58:11,445
그래, 문제는

1070
00:58:11,445 --> 00:58:13,023
강화 학습,

1071
00:58:13,023 --> 00:58:14,825
당신은 정말로 작은 배치 크기를 가질 것입니다.

1072
00:58:14,825 --> 00:58:17,517
이걸 어떻게 처리하니?

1073
00:58:17,517 --> 00:58:21,149
그래서 실제로, 배치 정규화가 사용 된 것 같습니다.

1074
00:58:21,149 --> 00:58:24,324
표준 길쌈 신경 네트워크와 같은 많은 것들,

1075
00:58:24,324 --> 00:58:27,595
우리가 어떻게하고 싶은지에 대한 논문이 실제로 있습니다.

1076
00:58:27,595 --> 00:58:30,900
여러 종류의 반복 네트워크에 대한 정규화,

1077
00:58:30,900 --> 00:58:32,203
또는이 네트워크 중 일부를 알고 있습니다.

1078
00:58:32,203 --> 00:58:34,520
그것은 또한 보강 학습에있을 수도 있습니다.

1079
00:58:34,520 --> 00:58:36,240
그래서 다른 고려 사항이 있습니다.

1080
00:58:36,240 --> 00:58:37,543
네가 거기서 생각하고 싶어 할지도 몰라.

1081
00:58:37,543 --> 00:58:40,532
그리고 이것은 여전히 활발한 연구 분야입니다.

1082
00:58:40,532 --> 00:58:42,681
이것에 대한 논문이 있습니다.

1083
00:58:42,681 --> 00:58:44,442
나중에 이것에 대해 좀 더 이야기 해보십시오.

1084
00:58:44,442 --> 00:58:47,783
그러나 전형적인 길쌈 신경 네트워크를 위해

1085
00:58:47,783 --> 00:58:49,490
이 일반적으로 잘 작동합니다.

1086
00:58:49,490 --> 00:58:52,042
그리고 작은 배치 크기가 있다면,

1087
00:58:52,042 --> 00:58:55,657
어쩌면 이것이 조금 덜 정확해질 수 있습니다.

1088
00:58:55,657 --> 00:58:57,741
그러나 당신은 여전히 같은 효과를 얻습니다.

1089
00:58:57,741 --> 00:58:59,599
그리고 가능하다는 것도 알고 있습니다.

1090
00:58:59,599 --> 00:59:02,840
당신은 당신의 평균과 분산을 설계 할 수 있습니다.

1091
00:59:02,840 --> 00:59:06,088
아마도 더 많은 예제를 통해 계산 될 수 있습니다.

1092
00:59:06,088 --> 00:59:08,514
실제로 나는 보통 괜찮다고 생각합니다.

1093
00:59:08,514 --> 00:59:09,942
그래서 너는 이것을 너무 많이 보지 않는다.

1094
00:59:09,942 --> 00:59:12,994
하지만 이것은 아마도 도움이 될만한 것입니다.

1095
00:59:12,994 --> 00:59:14,755
그게 문제 였다면.

1096
00:59:14,755 --> 00:59:16,128
그래, 질문 있니?

1097
00:59:16,128 --> 00:59:19,711
[마이크 끄기 학생]

1098
00:59:24,947 --> 00:59:27,180
그래서 질문,

1099
00:59:27,180 --> 00:59:30,896
그래서 질문은 우리가 입력을 가우스로 강제한다면,

1100
00:59:30,896 --> 00:59:32,979
우리가 그 구조를 잃어 버렸습니까?

1101
00:59:35,211 --> 00:59:39,056
그래서, 어떤 의미에서 당신이
생각할 수있는 것은 아닙니다.

1102
00:59:39,056 --> 00:59:41,608
모든 기능을 배포했다면

1103
00:59:41,608 --> 00:59:42,898
예를 들어 가우스 (gaussian)

1104
00:59:42,898 --> 00:59:45,221
데이터 사전 처리 만하는 경우에도,

1105
00:59:45,221 --> 00:59:47,925
이 가우스는 당신에게 어떤 구조도 잃지 않습니다.

1106
00:59:47,925 --> 00:59:50,879
모든 것, 그것은 단지 변화하고있다.

1107
00:59:50,879 --> 00:59:52,778
데이터를 정권으로 확장하고,

1108
00:59:52,778 --> 00:59:55,689
운영에 잘 작동하는

1109
00:59:55,689 --> 00:59:57,913
당신이 그것을 수행 할 것입니다.

1110
00:59:57,913 --> 01:00:00,604
길쌈 계층에서, 당신은 어떤 구조를 가지고 있습니다,

1111
01:00:00,604 --> 01:00:03,169
당신이 공간적으로 보존하기를 원하는 것입니다.

1112
01:00:03,169 --> 01:00:05,967
활성화 맵을 보는 것처럼,

1113
01:00:05,967 --> 01:00:09,156
당신은 그들 모두가 상대적으로
모든 것을 이해하기를 원합니다.

1114
01:00:09,156 --> 01:00:11,581
그래서이 경우에는

1115
01:00:11,581 --> 01:00:12,899
그것을 고려하십시오.

1116
01:00:12,899 --> 01:00:15,077
이제는 정상화 할 것입니다.

1117
01:00:15,077 --> 01:00:17,823
전체 정품 인증 맵에 대해 하나의 의미를 찾고,

1118
01:00:17,823 --> 01:00:20,749
그래서 우리는 경험적 평균과 분산만을 발견 할 수있다.

1119
01:00:20,749 --> 01:00:22,815
over 훈련 예.

1120
01:00:22,815 --> 01:00:25,022
그리고 그것은 뭔가입니다.

1121
01:00:25,022 --> 01:00:27,340
네가 네 숙제에서 할 수있는 일,

1122
01:00:27,340 --> 01:00:30,405
또한이 논문에서도 설명했다.

1123
01:00:30,405 --> 01:00:32,455
그래서, 당신은 그것을 참조해야합니다.

1124
01:00:32,455 --> 01:00:33,288
예.

1125
01:00:34,287 --> 01:00:37,870
[마이크 끄기 학생]

1126
01:00:43,065 --> 01:00:45,922
그래서 문제는 우리가 체중을 표준화하는 것입니까?

1127
01:00:45,922 --> 01:00:47,849
그래서 가우스가된다.

1128
01:00:47,849 --> 01:00:49,665
따라서, 귀하의 질문을 정확하게 이해한다면,

1129
01:00:49,665 --> 01:00:51,634
그 때 대답은,

1130
01:00:51,634 --> 01:00:54,560
우리는 각 레이어의 입력을 정규화하고 있습니다.

1131
01:00:54,560 --> 01:00:58,727
그래서 우리는이 과정에서 가중치를 변경하지 않습니다.

1132
01:01:00,895 --> 01:01:04,562
[마이크 끄기 학생]

1133
01:01:15,208 --> 01:01:18,023
그래, 문제는 일단 평균을 빼면

1134
01:01:18,023 --> 01:01:20,033
표준 편차로 나누면,

1135
01:01:20,033 --> 01:01:24,512
이것이 가우스 적 (gaussian)이되고
대답은 '예'입니다.

1136
01:01:24,512 --> 01:01:28,605
따라서 현재 진행중인 작업에 대해 생각해 보면,

1137
01:01:28,605 --> 01:01:30,960
기본적으로 당신은 평균에 의해 바뀌고 있습니다.

1138
01:01:30,960 --> 01:01:33,843
그리고 이러한 변화는 제로 중심으로 진행됩니다.

1139
01:01:33,843 --> 01:01:36,410
그런 다음 표준 편차로 비율을 조정합니다.

1140
01:01:36,410 --> 01:01:40,243
이제 이것을 단위 가우스로 변환합니다.

1141
01:01:41,249 --> 01:01:45,229
그리고 만약 당신이 그것에 대해 더 많이보고 싶다면,

1142
01:01:45,229 --> 01:01:46,324
너가 볼 수 있다고 생각해.

1143
01:01:46,324 --> 01:01:48,630
기계 학습에 관한 많은 설명이 있습니다.

1144
01:01:48,630 --> 01:01:51,139
이게 정확히 무슨 일인지,

1145
01:01:51,139 --> 01:01:52,942
이 작업으로 시각화하면,

1146
01:01:52,942 --> 01:01:54,980
하지만 이건 기본적으로 데이터를 필요로합니다.

1147
01:01:54,980 --> 01:01:58,563
이것을 가우시안 분포로 바꾼다.

1148
01:02:00,458 --> 01:02:02,375
그래, 그래 그래?

1149
01:02:03,436 --> 01:02:07,019
[마이크 끄기 학생]

1150
01:02:08,262 --> 01:02:09,095
어 허.

1151
01:02:26,194 --> 01:02:28,008
그래서 질문은,

1152
01:02:28,008 --> 01:02:29,880
우리가 변화와 규모를 수행하려고한다면,

1153
01:02:29,880 --> 01:02:32,944
그리고 이것들을 배우는 것은 일괄 정규화가 중복되어,

1154
01:02:32,944 --> 01:02:35,634
ID 매핑을 복구 할 수 있기 때문에?

1155
01:02:35,634 --> 01:02:38,324
따라서 네트워크가 학습하는 경우

1156
01:02:38,324 --> 01:02:40,571
그 신원 매핑은 항상 최고입니다,

1157
01:02:40,571 --> 01:02:41,985
그리고이 매개 변수들을 배우고,

1158
01:02:41,985 --> 01:02:44,523
예, 일괄 정규화에 대한 아무런 의미가 없을 것입니다.

1159
01:02:44,523 --> 01:02:46,215
그러나 실제로 이것은 일어나지 않습니다.

1160
01:02:46,215 --> 01:02:49,818
그래서 실제로, 우리는이 감마와 베타를 배울 것입니다.

1161
01:02:49,818 --> 01:02:52,579
이는 ID 매핑과 동일하지 않습니다.

1162
01:02:52,579 --> 01:02:55,242
그래서, 그것은 약간의 금액만큼 이동하고 확장 할 것이고,

1163
01:02:55,242 --> 01:02:56,157
금액은 아니지만

1164
01:02:56,157 --> 01:02:58,858
그것은 당신에게 신원 매핑을 줄 것입니다.

1165
01:02:58,858 --> 01:02:59,873
그래서 당신이 얻는 것은

1166
01:02:59,873 --> 01:03:03,201
당신은 여전히이 일괄 정규화 효과를 얻습니다.

1167
01:03:03,201 --> 01:03:05,753
맞아요, 그래서이 정체성 매핑을 거기에 가지고있어서,

1168
01:03:05,753 --> 01:03:09,938
나는 극단적으로 말하기 위해서 이것을 여기에두고있다.

1169
01:03:09,938 --> 01:03:12,033
그것은 신원 매핑을 배울 수 있었고,

1170
01:03:12,033 --> 01:03:14,266
그러나 실제로는 그렇지 않습니다.

1171
01:03:14,266 --> 01:03:15,970
그래, 질문.

1172
01:03:15,970 --> 01:03:19,553
[마이크 끄기 학생]

1173
01:03:21,368 --> 01:03:22,561
네.

1174
01:03:22,561 --> 01:03:26,144
[마이크 끄기 학생]

1175
01:03:30,825 --> 01:03:32,242
아 맞다.

1176
01:03:33,709 --> 01:03:36,177
네, 죄송합니다, 이것에 대해서는 명확하지 않았습니다.

1177
01:03:36,177 --> 01:03:37,405
하지만 그래,이게 관련이 있다고 생각해.

1178
01:03:37,405 --> 01:03:38,972
이전에 다른 질문에,

1179
01:03:38,972 --> 01:03:40,886
우리가이 일을 할 때 예.

1180
01:03:40,886 --> 01:03:43,285
우리는 실제로 제로 평균과 단위 가우스를 얻고 있습니다.

1181
01:03:43,285 --> 01:03:45,240
이것을 멋진 모양으로 만들었습니다.

1182
01:03:45,240 --> 01:03:49,814
실제로 가우스가 될 필요는 없습니다.

1183
01:03:49,814 --> 01:03:51,438
그래서 그래, 나는 이상적으로,

1184
01:03:51,438 --> 01:03:56,001
우리가 입력하는 것과 같은 입력을보고 있다면, 아시다시피,

1185
01:03:56,001 --> 01:03:57,830
일종의 가우스 (gaussian)

1186
01:03:57,830 --> 01:03:59,925
우리는 이런 종류의 효과를 원합니다.

1187
01:03:59,925 --> 01:04:03,592
하지만 그래, 실제로 그것은 필요하지 않습니다.

1188
01:04:06,658 --> 01:04:07,741
좋아, 그럼...

1189
01:04:09,438 --> 01:04:11,547
좋아요, 그래서 이것에 대해 제가 언급하고 싶은 마지막 것

1190
01:04:11,547 --> 01:04:15,714
그렇기 때문에 테스트 시간에 일괄 정규화 레이어,

1191
01:04:17,064 --> 01:04:19,564
우리는 이제 경험적 평균을 취한다.

1192
01:04:21,464 --> 01:04:24,839
및 훈련 데이터로부터의 분산.

1193
01:04:24,839 --> 01:04:26,932
따라서 우리는 테스트시 이것을 다시 계산하지 않습니다.

1194
01:04:26,932 --> 01:04:29,010
우리는 훈련 시간에 이것을 추정합니다.

1195
01:04:29,010 --> 01:04:31,189
예를 들어, 러닝 평균 (running average)

1196
01:04:31,189 --> 01:04:35,128
그리고 나서 우리는 테스트 시간에 이것을 사용할 것입니다.

1197
01:04:35,128 --> 01:04:38,295
그래서, 우리는 그것으로 확장 할 것입니다.

1198
01:04:40,078 --> 01:04:41,684
좋아, 이제 나는 계속 나아갈거야.

1199
01:04:41,684 --> 01:04:43,725
학습 과정을 보육하는 것.

1200
01:04:43,725 --> 01:04:46,110
그렇습니다. 이제 우리는 네트워크 아키텍처를 정의했습니다.

1201
01:04:46,110 --> 01:04:49,784
우리는 어떻게 우리가 훈련을 모니터 할 지,

1202
01:04:49,784 --> 01:04:54,264
우리가가는 동안 하이퍼 파라미터를 조정하는 방법,

1203
01:04:54,264 --> 01:04:56,681
좋은 학습 결과를 얻으려면?

1204
01:04:58,091 --> 01:05:00,184
항상 그렇듯이 우리가하고 싶은 첫 번째 단계는

1205
01:05:00,184 --> 01:05:02,251
우리는 데이터를 전처리하고 싶습니다.

1206
01:05:02,251 --> 01:05:04,192
맞아, 우리는 0을 의미한다.

1207
01:05:04,192 --> 01:05:05,773
우리가 이전에 말했던 것처럼.

1208
01:05:05,773 --> 01:05:07,381
그런 다음 우리는 아키텍처를 선택하고자합니다.

1209
01:05:07,381 --> 01:05:11,458
여기서 우리는 하나의 숨겨진 레이어로 시작합니다.

1210
01:05:11,458 --> 01:05:13,455
예를 들어, 50 개의 뉴런 중에서,

1211
01:05:13,455 --> 01:05:16,700
하지만 우리는 기본적으로 모든
아키텍처를 선택할 수 있습니다.

1212
01:05:16,700 --> 01:05:18,950
우리가 시작하고 싶습니다.

1213
01:05:20,223 --> 01:05:21,863
그리고 나서 우리가하고 싶은 첫 번째 일

1214
01:05:21,863 --> 01:05:23,934
우리는 네트워크를 초기화하고 있습니까?

1215
01:05:23,934 --> 01:05:25,858
우리는 그것을 통해 전달 전달,

1216
01:05:25,858 --> 01:05:28,600
우리는 우리의 손실이 합리적임을 확인하기를 원합니다.

1217
01:05:28,600 --> 01:05:30,773
그래서 우리는 몇 가지 강의를 전에 이야기했습니다.

1218
01:05:30,773 --> 01:05:33,106
우리는 기본적으로,

1219
01:05:34,902 --> 01:05:37,493
여기 Softmax 분류기가 있다고 가정 해 봅시다.

1220
01:05:37,493 --> 01:05:39,480
우리는 우리의 손실이 무엇인지 알고 있습니다.

1221
01:05:39,480 --> 01:05:41,013
우리의 무게가 작을 때,

1222
01:05:41,013 --> 01:05:44,012
일반적으로 확산 분포가 있습니다.

1223
01:05:44,012 --> 01:05:48,133
그런 다음 Softmax 분류기가 손실되기를 원합니다.

1224
01:05:48,133 --> 01:05:50,293
귀하의 부정적인 대립 가능성이 될 것입니다.

1225
01:05:50,293 --> 01:05:51,920
우리가 10 개의 과목을 가지고 있다면,

1226
01:05:51,920 --> 01:05:54,826
그것은 10 이상의 하나의 부정적인
로그와 같은 것이 될 것입니다.

1227
01:05:54,826 --> 01:05:58,986
여기 2.3 정도입니다. 그래서 우리는

1228
01:05:58,986 --> 01:06:03,213
우리의 상실은 우리가 기대하는 바입니다.

1229
01:06:03,213 --> 01:06:07,629
그래서, 이것은 우리가 원했던 좋은 정신 체크입니다.

1230
01:06:07,629 --> 01:06:09,453
항상, 항상 그래야합니다.

1231
01:06:09,453 --> 01:06:12,253
자, 이제 우리가 원래의 손실이 좋았다는 것을 알게되면,

1232
01:06:12,253 --> 01:06:13,503
이제 우리가 원한다.

1233
01:06:14,853 --> 01:06:17,946
그래서 우리는 먼저 정규화가 0 인 이것을하고 싶습니다.

1234
01:06:17,946 --> 01:06:20,039
그래서 우리가 정규화를 불가능하게 할 때,

1235
01:06:20,039 --> 01:06:22,613
이제 우리의 유일한 손실 기간은이 데이터 손실입니다.

1236
01:06:22,613 --> 01:06:25,463
여기에 2.3을 줄 예정입니다.

1237
01:06:25,463 --> 01:06:28,893
그리고 여기에서 우리는 이제 정규화를 시작하려고합니다.

1238
01:06:28,893 --> 01:06:33,039
우리가 그렇게 할 때, 우리는 우리의
손실이 증가한다는 것을보고 싶습니다.

1239
01:06:33,039 --> 01:06:36,226
이 추가 정규화 용어를 추가했기 때문입니다.

1240
01:06:36,226 --> 01:06:37,693
그래서, 이것은 좋은 다음 단계입니다.

1241
01:06:37,693 --> 01:06:40,879
당신의 정신 건강 체크를 위해 할 수있는 일.

1242
01:06:40,879 --> 01:06:43,559
그리고 이제 우리는 훈련을 시작할 수 있습니다.

1243
01:06:43,559 --> 01:06:46,309
이제 우리는 훈련을 시작합니다.

1244
01:06:47,331 --> 01:06:49,719
우리가하는 일은 좋은 방법입니다.

1245
01:06:49,719 --> 01:06:53,026
매우 적은 양의 데이터로 시작하는 것입니다.

1246
01:06:53,026 --> 01:06:55,491
아주 작은 훈련 세트를 가지고 있다면,

1247
01:06:55,491 --> 01:06:57,678
너는 이것에 아주 잘 맞을 수 있어야한다.

1248
01:06:57,678 --> 01:07:00,944
여기에 아주 좋은 훈련 손실을.

1249
01:07:00,944 --> 01:07:03,527
이 경우에는

1250
01:07:04,554 --> 01:07:06,530
우리의 정규화를 다시 끄십시오,

1251
01:07:06,530 --> 01:07:10,697
손실을 0으로 만들 수 있는지 확인하십시오.

1252
01:07:12,199 --> 01:07:14,293
그래서 우리는 우리의 손실이 어떻게
변하는지를 볼 수 있습니다.

1253
01:07:14,293 --> 01:07:16,346
우리는이 모든 신 (新) 시대를 가지고 있기 때문입니다.

1254
01:07:16,346 --> 01:07:19,138
우리는 각 시대마다 우리의 손실을 계산합니다.

1255
01:07:19,138 --> 01:07:21,961
그리고 우리는 이것이 0으로 내려가는 것을보고 싶습니다.

1256
01:07:21,961 --> 01:07:23,151
맞아. 여기 우리가 볼 수있어.

1257
01:07:23,151 --> 01:07:24,406
우리의 훈련 정확도

1258
01:07:24,406 --> 01:07:27,124
모든 것이 하나가 될 것이고, 이것은 당연한 것입니다.

1259
01:07:27,124 --> 01:07:28,980
데이터의 수가 매우 적은 경우,

1260
01:07:28,980 --> 01:07:32,813
당신은 이것에 완벽하게 맞을 수 있어야합니다.

1261
01:07:34,726 --> 01:07:36,033
자, 이제 일단 그렇게하면,

1262
01:07:36,033 --> 01:07:38,700
이들은 모두 온 전성 검사입니다.

1263
01:07:38,700 --> 01:07:40,366
이제 정말로 훈련을 시작할 수 있습니다.

1264
01:07:40,366 --> 01:07:43,540
이제 전체 교육 데이터를 가져올 수 있습니다.

1265
01:07:43,540 --> 01:07:46,460
이제는 작은 양의 정규화로 시작합니다.

1266
01:07:46,460 --> 01:07:49,480
우선 좋은 학습 속도가 무엇인지 알아 봅시다.

1267
01:07:49,480 --> 01:07:50,813
따라서 학습률은 가장

1268
01:07:50,813 --> 01:07:52,184
중요한 하이퍼 매개 변수,

1269
01:07:52,184 --> 01:07:54,942
그리고 그것은 당신이 먼저 조정하기를 원하는 것입니다.

1270
01:07:54,942 --> 01:07:58,196
그래서, 당신은 학습 속도의 가치를 시험하고 싶습니다.

1271
01:07:58,196 --> 01:08:00,954
그리고 여기 나는 하나의 E negative 6을 시도했다.

1272
01:08:00,954 --> 01:08:04,096
손실이 거의 변하지 않았 음을 알 수 있습니다.

1273
01:08:04,096 --> 01:08:06,076
맞아요. 그리고 이것이 간신히 변하는 이유입니다.

1274
01:08:06,076 --> 01:08:10,244
학습 속도가 너무 낮기 때문에 보통입니다.

1275
01:08:10,244 --> 01:08:11,380
그래서 너무 작 으면,

1276
01:08:11,380 --> 01:08:12,862
그래디언트 업데이트로는 충분하지 않습니다.

1277
01:08:12,862 --> 01:08:16,362
비용은 기본적으로 거의 같습니다.

1278
01:08:17,423 --> 01:08:18,256
그래,

1279
01:08:21,151 --> 01:08:24,020
제가 여기서 지적하고자하는 한가지,

1280
01:08:24,020 --> 01:08:25,699
비록 우리가

1281
01:08:25,699 --> 01:08:27,239
간신히 변화하면서 우리의 손실,

1282
01:08:27,239 --> 01:08:29,694
훈련 및 검증 정확성

1283
01:08:29,694 --> 01:08:32,701
매우 빠르게 20 %까지 뛰었습니다.

1284
01:08:32,701 --> 01:08:35,568
그리고 누구도 아이디어를 가지고 있지 않습니다.

1285
01:08:35,569 --> 01:08:38,152
왜 이것이 사실일까요?

1286
01:08:40,089 --> 01:08:42,460
왜, Softmax 기능이 있다는 것을 기억하십시오.

1287
01:08:42,460 --> 01:08:43,736
우리의 손실은 실제로 변하지 않았습니다.

1288
01:08:43,737 --> 01:08:46,404
그러나 우리의 정확성은 많이 향상되었습니다.

1289
01:08:50,263 --> 01:08:53,820
좋아, 그 이유는

1290
01:08:53,820 --> 01:08:56,857
여기서 확률은 여전히 꽤 분산되어 있습니다.

1291
01:08:56,857 --> 01:08:59,727
그래서 우리의 손실 기간은 여전히 꽤 유사합니다.

1292
01:08:59,727 --> 01:09:03,368
그러나이 모든 확률을 바꿀 때

1293
01:09:03,368 --> 01:09:05,076
약간 오른쪽 방향으로,

1294
01:09:05,076 --> 01:09:06,183
우리가 올바르게 배우기 때문에?

1295
01:09:06,183 --> 01:09:08,210
우리의 무게는 올바른 방향으로 변화하고 있습니다.

1296
01:09:08,210 --> 01:09:11,954
이제 갑작스런 정확성이 뛰어 날 수 있습니다.

1297
01:09:11,954 --> 01:09:14,866
우리는 최대 정확한 값을 취하고 있기 때문에,

1298
01:09:14,866 --> 01:09:17,902
그래서 우리는 정확도가 크게 향상 될 것입니다.

1299
01:09:17,902 --> 01:09:21,985
비록 우리의 손실은 여전히 상대적으로 확산되어 있습니다.

1300
01:09:23,588 --> 01:09:25,995
자, 이제 우리가 다른 학습 속도를 시도한다면,

1301
01:09:25,995 --> 01:09:28,122
지금 나는 여기에서 다른 극단에 뛰어 들고있다.

1302
01:09:28,123 --> 01:09:31,326
매우 큰 학습 속도, 한 E 부정 6.

1303
01:09:31,326 --> 01:09:36,007
현재 비용은 우리에게 NaN을 제공하고 있습니다.

1304
01:09:36,007 --> 01:09:37,794
그리고 당신이 NaN을 가질 때
이것이 보통 무엇을 의미하는지

1305
01:09:37,794 --> 01:09:41,413
기본적으로 비용이 폭발했습니다.

1306
01:09:41,413 --> 01:09:45,195
그리고 그 이유는 일반적으로

1307
01:09:45,195 --> 01:09:47,862
학습률이 너무 높습니다.

1308
01:09:49,350 --> 01:09:51,236
따라서 학습 속도를 다시 낮출 수 있습니다.

1309
01:09:51,236 --> 01:09:53,683
여기서 나는 우리가 시도하고 있음을 볼 수있다.

1310
01:09:53,683 --> 01:09:55,225
세 E는 음수 세.

1311
01:09:55,225 --> 01:09:57,006
비용은 여전히 폭발적이다.

1312
01:09:57,006 --> 01:09:59,741
그래서 일반적으로 학습 속도의 대략적인 범위

1313
01:09:59,741 --> 01:10:00,981
우리가보고 싶어하는

1314
01:10:00,981 --> 01:10:04,901
하나의 E 음수 3과 1 E 음수 5 사이에 있습니다.

1315
01:10:04,901 --> 01:10:08,402
그리고 이것은 우리가 원하는 거친 범위입니다.

1316
01:10:08,402 --> 01:10:09,628
사이의 교차 유효성 검사.

1317
01:10:09,628 --> 01:10:11,840
따라서이 범위의 값을 시험해보고 싶습니다.

1318
01:10:11,840 --> 01:10:14,837
당신의 손실이 너무 느린 지 여부에 따라,

1319
01:10:14,837 --> 01:10:17,011
또는 너무 작 으면, 또는 너무 큰지 여부,

1320
01:10:17,011 --> 01:10:19,011
이것을 기초로 조정하십시오.

1321
01:10:21,228 --> 01:10:24,399
그렇다면이 하이퍼 매개 변수를 정확히
선택하는 방법은 무엇입니까?

1322
01:10:24,399 --> 01:10:26,659
하이퍼 매개 변수 최적화,

1323
01:10:26,659 --> 01:10:31,139
이러한 하이퍼 매개 변수의 모든 값을 선택하십시오.

1324
01:10:31,139 --> 01:10:33,067
그래서, 우리가 사용할 전략

1325
01:10:33,067 --> 01:10:35,979
예를 들어, 학습 속도와 같은 임의의
하이퍼 파라미터에 대한 것이고,

1326
01:10:35,979 --> 01:10:37,575
교차 유효성 검사를 수행하는 것입니다.

1327
01:10:37,575 --> 01:10:40,485
따라서 교차 검증은 교육 세트에 대한 교육입니다.

1328
01:10:40,485 --> 01:10:43,472
그런 다음 유효성 검사 집합에 대해 평가합니다.

1329
01:10:43,472 --> 01:10:45,469
이 하이퍼 매개 변수는 얼마나 잘합니까?

1330
01:10:45,469 --> 01:10:47,092
너희들이 벌써 한 짓이야.

1331
01:10:47,092 --> 01:10:48,960
너의 임무에서.

1332
01:10:48,960 --> 01:10:51,334
그래서 일반적으로 우리는 이것을 단계적으로하고 싶습니다.

1333
01:10:51,334 --> 01:10:54,600
그래서 우리는 물론 첫 단계를 할 수 있습니다.

1334
01:10:54,600 --> 01:10:57,310
우리는 값이 흩어져있는 것을 선택합니다.

1335
01:10:57,310 --> 01:11:00,190
그리고 우리는 단지 몇 개의 신 (新) 시대에 대해서 배웁니다.

1336
01:11:00,190 --> 01:11:01,788
단 몇 개의 신기원으로.

1337
01:11:01,788 --> 01:11:03,473
너는 이미 꽤 좋은 감각을 얻을 수있다.

1338
01:11:03,473 --> 01:11:05,552
그 중 하이퍼 파라미터,

1339
01:11:05,552 --> 01:11:07,993
어떤 가치가 좋든 싫든.

1340
01:11:07,993 --> 01:11:09,795
당신은 그것이 NaN이라는 것을 빨리 볼 수 있습니다,

1341
01:11:09,795 --> 01:11:11,580
또는 아무 일도 일어나지 않고 있음을 볼 수 있습니다.

1342
01:11:11,580 --> 01:11:13,712
그에 따라 조정할 수 있습니다.

1343
01:11:13,712 --> 01:11:15,726
그래서, 일반적으로 일단 그렇게하면,

1344
01:11:15,726 --> 01:11:18,506
그러면 당신은 꽤 좋은 범위의 종류인지 알 수 있습니다.

1345
01:11:18,506 --> 01:11:20,143
그리고 지금하고 싶은 범위

1346
01:11:20,143 --> 01:11:22,540
에서 값의 더 세밀한 샘플링.

1347
01:11:22,540 --> 01:11:24,155
그리고 이것은 두 번째 단계입니다.

1348
01:11:24,155 --> 01:11:27,236
이제는 더 오랜 시간 동안이
프로그램을 실행하고 싶을 것입니다.

1349
01:11:27,236 --> 01:11:30,779
그 지역에 대한 더 정밀한 검색을하십시오.

1350
01:11:30,779 --> 01:11:34,696
NaN과 같은 폭발을 탐지하기위한 팁 하나,

1351
01:11:35,632 --> 01:11:37,244
당신은 당신의 훈련 과정에서 가질 수 있습니다,

1352
01:11:37,244 --> 01:11:40,409
일부 하이퍼 파라미터를 올바르게 샘플링하고,

1353
01:11:40,409 --> 01:11:43,992
교육을 시작한 다음 비용을 살펴보십시오.

1354
01:11:44,947 --> 01:11:47,296
매 반복마다 또는 모든 시대마다.

1355
01:11:47,296 --> 01:11:49,556
그리고 당신이 비용을받는다면

1356
01:11:49,556 --> 01:11:52,264
원래 비용보다 훨씬 큽니다.

1357
01:11:52,264 --> 01:11:55,259
예를 들어, 원가의 3 배,

1358
01:11:55,259 --> 01:11:56,591
그러면 당신은 이것이

1359
01:11:56,591 --> 01:11:57,902
올바른 방향으로 향하고있다.

1360
01:11:57,902 --> 01:11:59,899
맞아, 매우 빠르게, 매우 빠르게,

1361
01:11:59,899 --> 01:12:01,339
그리고 당신은 당신의 루프에서 벗어날 수 있습니다,

1362
01:12:01,339 --> 01:12:03,582
이 하이퍼 매개 변수 선택을 중지하십시오.

1363
01:12:03,582 --> 01:12:06,335
다른 것을 선택하십시오.

1364
01:12:06,335 --> 01:12:08,193
좋습니다. 예를 들어,

1365
01:12:08,193 --> 01:12:10,943
우리가 지금 도망 치고 싶다고하자.

1366
01:12:12,313 --> 01:12:13,866
5 개의 신기원을위한 과정 수색.

1367
01:12:13,866 --> 01:12:16,532
이것은 비슷한 네트워크입니다.

1368
01:12:16,532 --> 01:12:18,332
우리가 더 일찍 이야기하고 있었던,

1369
01:12:18,332 --> 01:12:21,769
우리가 할 수있는 일은이 모든
것을 볼 수 있다는 것입니다.

1370
01:12:21,769 --> 01:12:24,611
우리가 얻고있는 검증 정확성.

1371
01:12:24,611 --> 01:12:27,154
그리고 빨간 색으로 강조 표시했습니다.

1372
01:12:27,154 --> 01:12:29,291
더 나은 가치를주는 것들.

1373
01:12:29,291 --> 01:12:30,644
그리고 이것들은 지역이 될 것입니다.

1374
01:12:30,644 --> 01:12:33,092
더 자세히 살펴 보겠습니다.

1375
01:12:33,092 --> 01:12:34,460
주목할 사실 중 하나는

1376
01:12:34,460 --> 01:12:37,067
일반적으로 로그 공간을 최적화하는 것이 좋습니다.

1377
01:12:37,067 --> 01:12:39,832
그리고 여기 샘플링 대신에, 나는 균일하게 말할 것입니다.

1378
01:12:39,832 --> 01:12:43,999
당신 사이에 하나의 E가 음의 0.01와 100을 알고,

1379
01:12:44,873 --> 01:12:49,040
당신은 실제로 어떤 범위의 힘을 10으로 할 것입니다.

1380
01:12:49,956 --> 01:12:52,625
맞아.

1381
01:12:52,625 --> 01:12:55,427
학습 속도는 그라디언트 업데이트를 곱하는 것입니다.

1382
01:12:55,427 --> 01:12:58,870
그리고이 곱셈 효과를 가지고 있습니다.

1383
01:12:58,870 --> 01:13:02,024
그래서 고려하는 것이 더 합리적입니다.

1384
01:13:02,024 --> 01:13:03,910
배가되는 학습 속도의 범위

1385
01:13:03,910 --> 01:13:07,524
또는 일정한 값으로 나누어 균등하게
표본 추출하지 않아도됩니다.

1386
01:13:07,524 --> 01:13:08,561
그래서, 당신은 거래하고 싶어합니다.

1387
01:13:08,561 --> 01:13:10,894
여기에 몇 가지 규모의 주문이 있습니다.

1388
01:13:10,894 --> 01:13:12,132
알았어. 그렇게되면,

1389
01:13:12,132 --> 01:13:14,379
범위를 조정할 수 있습니다.

1390
01:13:14,379 --> 01:13:17,878
이 경우에 우리는 당신이 알고있는 범위를 가지고 있습니다,

1391
01:13:17,878 --> 01:13:21,392
네거티브 네, 네, 네,

1392
01:13:21,392 --> 01:13:23,094
~ 10을 제로 전원으로 설정하십시오.

1393
01:13:23,094 --> 01:13:26,176
이것은 우리가 범위를 좁히고 자하는 좋은 범위입니다.

1394
01:13:26,176 --> 01:13:27,976
그리고 우리는 이것을 다시 할 수 있습니다.

1395
01:13:27,976 --> 01:13:29,315
여기서 우리는 우리가

1396
01:13:29,315 --> 01:13:33,087
상대적으로 좋은 53 %의 정확도.

1397
01:13:33,087 --> 01:13:37,962
그리고 이것은 우리가 올바른 방향으로
나아가고 있다는 것을 의미합니다.

1398
01:13:37,962 --> 01:13:40,190
제가 지적하고자하는 한가지는

1399
01:13:40,190 --> 01:13:42,377
여기에는 실제로 문제가 있습니다.

1400
01:13:42,377 --> 01:13:45,317
그래서 문제는

1401
01:13:45,317 --> 01:13:47,563
우리는 여기서 가장 정확한 정확성을 볼 수 있습니다.

1402
01:13:47,563 --> 01:13:50,396
대략 배우는 비율이있다,

1403
01:13:52,373 --> 01:13:55,281
알다시피, 우리의 모든 좋은 학습 속도

1404
01:13:55,281 --> 01:13:57,816
이 E에서 음의 4 범위입니다.

1405
01:13:57,816 --> 01:14:00,059
맞습니다. 그리고 우리가 지정한 학습 속도

1406
01:14:00,059 --> 01:14:03,646
10에서 0으로 네거티브 4에서 10으로 가고 있었고,

1407
01:14:03,646 --> 01:14:05,919
즉, 모든 좋은 학습 속도,

1408
01:14:05,919 --> 01:14:10,273
우리가 샘플링하는 범위의 가장자리에 있었다.

1409
01:14:10,273 --> 01:14:11,856
그래서 이것은 나쁘다.

1410
01:14:12,693 --> 01:14:15,903
이것은 우리가 탐험을하지 않았을 수도 있기 때문입니다.

1411
01:14:15,903 --> 01:14:17,113
우리의 공간은 충분히 옳다.

1412
01:14:17,113 --> 01:14:19,189
실제로 마이너스 5에 10을 가길 원할 수도 있습니다.

1413
01:14:19,189 --> 01:14:20,485
또는 10을 음수 6으로 설정하십시오.

1414
01:14:20,485 --> 01:14:21,908
여전히 더 나은 범위가있을 수 있습니다.

1415
01:14:21,908 --> 01:14:23,494
우리가 계속 아래로 이동하면.

1416
01:14:23,494 --> 01:14:25,352
그래서, 당신은 당신의 범위가

1417
01:14:25,352 --> 01:14:27,785
종류의 중간에 좋은 가치를 가지고,

1418
01:14:27,785 --> 01:14:30,089
또는 당신이 타격을 당했다는 느낌을받는 곳에서,

1419
01:14:30,089 --> 01:14:32,839
당신은 당신의 범위를 완전히 탐험했습니다.

1420
01:14:36,224 --> 01:14:39,853
좋아, 또 다른 점은

1421
01:14:39,853 --> 01:14:42,113
우리는 모든 다른 하이퍼 파라미터를
샘플링 할 수 있습니다.

1422
01:14:42,113 --> 01:14:43,741
그리드 검색의 종류를 사용하여, 오른쪽.

1423
01:14:43,741 --> 01:14:46,621
고정 된 조합의 조합을 샘플링 할 수 있습니다.

1424
01:14:46,621 --> 01:14:49,731
각 하이퍼 매개 변수에 대한 고정 값 집합입니다.

1425
01:14:49,731 --> 01:14:54,597
이 모든 값에 대해 그리드 방식으로 샘플링하고,

1426
01:14:54,597 --> 01:14:56,819
실제로 실제로는 샘플을 만드는 것이 더 좋습니다.

1427
01:14:56,819 --> 01:15:00,430
무작위 레이아웃에서 임의의 값을 샘플링

1428
01:15:00,430 --> 01:15:02,334
한 범위 내의 각 하이퍼 매개 변수의

1429
01:15:02,334 --> 01:15:03,817
그래서 대신 얻을 것입니다.

1430
01:15:03,817 --> 01:15:05,661
우리는이 두 가지 하이퍼 파라미터를
여기에 갖게 될 것입니다.

1431
01:15:05,661 --> 01:15:07,389
우리가 샘플을 원한다.

1432
01:15:07,389 --> 01:15:10,876
대신이 오른쪽면처럼 보이는 샘플을 얻을 수 있습니다.

1433
01:15:10,876 --> 01:15:14,401
그리고이 이유는 함수가 실제로

1434
01:15:14,401 --> 01:15:18,274
한 변수의 함수를 다른 것보다 더 많이 정렬하고,

1435
01:15:18,274 --> 01:15:19,816
이것은 사실입니다.

1436
01:15:19,816 --> 01:15:21,456
보통 조금 더 있습니다.

1437
01:15:21,456 --> 01:15:24,669
우리가 실제로 가지고있는 것보다 더 낮은 유효 차원.

1438
01:15:24,669 --> 01:15:26,969
그러면 더 많은 샘플을 얻게 될 것입니다.

1439
01:15:26,969 --> 01:15:30,342
당신이 가지고있는 중요한 변수의

1440
01:15:30,342 --> 01:15:32,560
이 모양을 볼 수 있습니다.

1441
01:15:32,560 --> 01:15:35,956
내가 그린 위에 그린이 녹색 함수에서,

1442
01:15:35,956 --> 01:15:38,326
좋은 가치가있는 곳을 보여주는

1443
01:15:38,326 --> 01:15:40,782
당신이 방금 그리드 레이아웃을 만든 경우와 비교하여

1444
01:15:40,782 --> 01:15:43,403
여기서 세 가지 값만 샘플링 할 수있었습니다.

1445
01:15:43,403 --> 01:15:46,459
좋은 지역이 어디 있는지 놓쳤습니다.

1446
01:15:46,459 --> 01:15:48,677
맞아요. 그래서 기본적으로 우리는
훨씬 더 많은 것을 얻을 것입니다.

1447
01:15:48,677 --> 01:15:51,741
우리가 더 많은 샘플을 가지고 있기 때문에 유용한 신호

1448
01:15:51,741 --> 01:15:55,685
중요한 변수의 다른 값들의

1449
01:15:55,685 --> 01:15:58,325
그래서, 함께 연주 할 하이퍼 파라미터들,

1450
01:15:58,325 --> 01:16:00,427
우리는 학습률에 대해 이야기했습니다.

1451
01:16:00,427 --> 01:16:03,163
다양한 종류의 붕괴 일정과 같은 것들,

1452
01:16:03,163 --> 01:16:05,160
업데이트 유형, 정규화,

1453
01:16:05,160 --> 01:16:07,697
또한 네트워크 아키텍처,

1454
01:16:07,697 --> 01:16:09,523
그래서 숨겨진 유닛의 수, 깊이,

1455
01:16:09,523 --> 01:16:12,405
이 모든 것은 당신이 최적화 할
수있는 하이퍼 파라미터입니다.

1456
01:16:12,405 --> 01:16:13,630
그리고 우리는 이것들에 대해 이야기했습니다,

1457
01:16:13,630 --> 01:16:15,531
하지만 우리는이 중 더 많은 것을 계속 이야기 할 것입니다.

1458
01:16:15,531 --> 01:16:16,928
다음 강연에서.

1459
01:16:16,928 --> 01:16:20,038
그래서 당신은 이것을 일종의 것으로 생각할 수 있습니다.

1460
01:16:20,038 --> 01:16:22,198
기본적으로 모든 노브를 올바르게 튜닝하는 경우,

1461
01:16:22,198 --> 01:16:24,781
네가있는 턴테이블의

1462
01:16:26,667 --> 01:16:28,390
당신은 신경망 의사입니다.

1463
01:16:28,390 --> 01:16:29,801
출력되는 음악을 생각해 볼 수 있습니다.

1464
01:16:29,801 --> 01:16:32,260
당신이 원하는 손실 함수입니다.

1465
01:16:32,260 --> 01:16:34,121
그리고 당신은 모든 것을 적절하게 조정하고 싶다.

1466
01:16:34,121 --> 01:16:36,313
당신이 원하는 출력을 얻을 수 있습니다.

1467
01:16:36,313 --> 01:16:40,480
좋아요, 그래서 그것은 당신이하는 일종의 예술입니다.

1468
01:16:42,194 --> 01:16:45,891
실제로, 당신은 할 것입니다.

1469
01:16:45,891 --> 01:16:48,593
많은 하이퍼 파라미터 최적화,

1470
01:16:48,593 --> 01:16:50,277
많은 교차 검증.

1471
01:16:50,277 --> 01:16:52,582
그래서 숫자를 얻으려면,

1472
01:16:52,582 --> 01:16:54,425
사람들이 교차 검증을 실행합니다.

1473
01:16:54,425 --> 01:16:58,371
수많은 하이퍼 파라미터를 모니터링하고,
모든 것을 모니터링하며,

1474
01:16:58,371 --> 01:16:59,535
어느 것이 더 잘하고 있는지 보아라.

1475
01:16:59,535 --> 01:17:00,368
어느 것이 더 나빠질 지.

1476
01:17:00,368 --> 01:17:02,091
여기에 우리는 이러한 모든 손실 곡선을 가지고 있습니다.

1477
01:17:02,091 --> 01:17:04,137
올바른 것을 고르고, 재조정하고,

1478
01:17:04,137 --> 01:17:07,895
이 과정을 계속 진행하십시오.

1479
01:17:07,895 --> 01:17:10,478
그리고 앞서 언급했듯이,

1480
01:17:11,409 --> 01:17:13,583
이 손실 곡선 각각을 모니터링 할 때,

1481
01:17:13,583 --> 01:17:15,311
학습률은 중요한 것으로,

1482
01:17:15,311 --> 01:17:18,739
그러나 당신은 어떻게 다른 학습
속도에 대한 감각을 얻을 것입니다,

1483
01:17:18,739 --> 01:17:20,654
어떤 학습 속도가 좋고 나쁘다.

1484
01:17:20,654 --> 01:17:23,850
그래서 당신은 매우 높은 폭발하는 것을 가지고 있다면,

1485
01:17:23,850 --> 01:17:25,779
맞아, 이건 너의 손실이다.

1486
01:17:25,779 --> 01:17:27,667
학습률이 너무 높습니다.

1487
01:17:27,667 --> 01:17:29,985
그것이 너무 일직선이고 너무 평평하다면,

1488
01:17:29,985 --> 01:17:34,060
너는 너무 낮다는 것을 알게 될
것이고, 충분히 바뀌지는 않을 것이다.

1489
01:17:34,060 --> 01:17:35,990
네가 뭔가를 얻으면

1490
01:17:35,990 --> 01:17:38,565
가파른 변화가있는 것처럼 보입니다.하지만 고원은,

1491
01:17:38,565 --> 01:17:41,660
이것은 또한 아마도 너무 높다는 지표입니다.

1492
01:17:41,660 --> 01:17:44,803
이 경우 당신은 너무 큰 도약을하고 있기 때문에,

1493
01:17:44,803 --> 01:17:48,460
그리고 당신은 당신의 지역 최적으로
잘 정착 할 수 없습니다.

1494
01:17:48,460 --> 01:17:50,015
그래서 좋은 학습 속도

1495
01:17:50,015 --> 01:17:51,959
보통 이런 식으로 끝내기 시작합니다.

1496
01:17:51,959 --> 01:17:53,572
비교적 가파른 곡선이있는 곳,

1497
01:17:53,572 --> 01:17:55,371
그러나 그것은 계속 내려 가고 있습니다.

1498
01:17:55,371 --> 01:17:56,668
조정을 계속할 수도 있습니다.

1499
01:17:56,668 --> 01:17:57,993
거기에서 당신의 학습 속도.

1500
01:17:57,993 --> 01:18:02,160
그래서 이것은 여러분이 연습을 통해 볼 수있는 것입니다.

1501
01:18:03,522 --> 01:18:05,825
좋아요, 그냥 우리가 끝까지 아주 가깝다고 생각합니다.

1502
01:18:05,825 --> 01:18:08,401
그래서 내가 지적하고 싶은 마지막 한가지

1503
01:18:08,401 --> 01:18:12,637
당신이 학습 속도 손실 곡선을 본 경우에 비해,

1504
01:18:12,637 --> 01:18:14,553
곳은...

1505
01:18:14,553 --> 01:18:16,454
그래서 잠시 동안 평평한 곳에 손실 곡선을 보게된다면,

1506
01:18:16,454 --> 01:18:20,069
갑자기 훈련을 시작합니다.

1507
01:18:20,069 --> 01:18:23,567
잠재적 인 이유는 나쁜 초기화 일 수 있습니다.

1508
01:18:23,567 --> 01:18:25,871
따라서이 경우에는 그라디언트가 실제로 흐르지 않습니다.

1509
01:18:25,871 --> 01:18:28,377
너무 초반부터 잘 배우기 때문에,

1510
01:18:28,377 --> 01:18:29,397
그리고 나서 어느 시점에서,

1511
01:18:29,397 --> 01:18:31,905
그것은 올바른 방법으로 조정되는 것입니다.

1512
01:18:31,905 --> 01:18:36,383
그런 일이 끝나고 일들이 바로 훈련을 시작하도록

1513
01:18:36,383 --> 01:18:40,602
그래서 이걸 보면서 많은 경험이 있습니다.

1514
01:18:40,602 --> 01:18:42,921
시간이 지남에 따라 얻을 수있는 잘못된 점을 확인하십시오.

1515
01:18:42,921 --> 01:18:45,568
따라서 일반적으로

1516
01:18:45,568 --> 01:18:47,901
귀하의 정확성을 시각화하십시오.

1517
01:18:48,826 --> 01:18:52,369
훈련 정확도 사이에 큰 차이가 있다면

1518
01:18:52,369 --> 01:18:54,860
유효성 확인의 정확성,

1519
01:18:54,860 --> 01:18:57,135
그것은 일반적으로 당신이
overfitting있을 수 있음을 의미합니다

1520
01:18:57,135 --> 01:18:58,187
그리고 너는 증가시키고 싶을지도 모른다.

1521
01:18:58,187 --> 01:18:59,652
당신의 정규화 강도.

1522
01:18:59,652 --> 01:19:00,678
간격이 없다면,

1523
01:19:00,678 --> 01:19:03,385
모델 용량을 늘려야 할 수도 있습니다.

1524
01:19:03,385 --> 01:19:05,070
너는 아직 과장되지 않았기 때문에.

1525
01:19:05,070 --> 01:19:08,137
잠재적으로 더 많이 늘릴 수 있습니다.

1526
01:19:08,137 --> 01:19:10,381
그리고 일반적으로 업데이트를 추적하고 싶습니다.

1527
01:19:10,381 --> 01:19:13,998
우리의 체중 변화에 대한 체중 업데이트의 비율.

1528
01:19:13,998 --> 01:19:16,028
우리는 단지 표준을 취할 수 있습니다.

1529
01:19:16,028 --> 01:19:19,281
우리가 가지고있는 매개 변수 중

1530
01:19:19,281 --> 01:19:21,428
그들이 얼마나 큰 지에 대한 감각을 얻으려면,

1531
01:19:21,428 --> 01:19:22,998
우리가 우리의 업데이트 크기를 가질 때,

1532
01:19:22,998 --> 01:19:24,723
우리는 또한 그것의 규범을 취할 수 있습니다.

1533
01:19:24,723 --> 01:19:26,353
얼마나 큰지에 대한 감각을 얻으십시오,

1534
01:19:26,353 --> 01:19:30,025
이 비율을 약 0.001 정도가 되길 원합니다.

1535
01:19:30,025 --> 01:19:33,611
이 범위에는 많은 차이가 있습니다.

1536
01:19:33,611 --> 01:19:35,598
그래서 당신은 이것에 정확하게있을 필요는 없습니다.

1537
01:19:35,598 --> 01:19:37,844
하지만 그것은 단지 당신이 원하지 않는 감각입니다.

1538
01:19:37,844 --> 01:19:40,455
귀하의 업데이트가 귀하의 가치에 비해 너무 크다.

1539
01:19:40,455 --> 01:19:41,477
또는 너무 작습니까, 맞습니까?

1540
01:19:41,477 --> 01:19:43,637
지배적이거나 효과가 없기를 원하지 않습니다.

1541
01:19:43,637 --> 01:19:45,811
그래서 이것은 디버깅을 도울 수있는 것입니다.

1542
01:19:45,811 --> 01:19:47,811
무엇이 문제 일 수 있습니다.

1543
01:19:49,843 --> 01:19:51,514
좋아요, 요약하자면,

1544
01:19:51,514 --> 01:19:54,073
오늘 우리는 활성화 함수를 살펴 보았습니다.

1545
01:19:54,073 --> 01:19:56,654
데이터 전처리, 가중치 초기화,

1546
01:19:56,654 --> 01:19:59,016
일괄 표준, 학습 과정의 보육,

1547
01:19:59,016 --> 01:20:01,694
및 하이퍼 매개 변수 최적화.

1548
01:20:01,694 --> 01:20:03,667
이것들은 각각을위한 테이크 어웨이의 종류입니다.

1549
01:20:03,667 --> 01:20:05,338
너희들은 명심해야한다.

1550
01:20:05,338 --> 01:20:08,491
ReLUs를 사용하고, 평균을 빼고, Xavier
Initialization을 사용하고,

1551
01:20:08,491 --> 01:20:12,499
일괄 표준을 사용하고 샘플 하이퍼
매개 변수를 무작위로 추출합니다.

1552
01:20:12,499 --> 01:20:14,222
다음에 우리는 계속 이야기 할 것입니다.

1553
01:20:14,222 --> 01:20:15,806
교육 신경 네트워크에 대해

1554
01:20:15,806 --> 01:20:18,355
이 모든 다른 주제들로

1555
01:20:18,355 --> -00:00:00,600
감사.

