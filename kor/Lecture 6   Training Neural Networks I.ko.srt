1
00:00:10,729 --> 00:00:12,896
시작하겠습니다.

2
00:00:16,381 --> 00:00:21,529
이번 시간에는 nerural network를 학습시키는 방법을
심도깊게 살펴보겠습니다.

3
00:00:23,166 --> 00:00:28,785
우선 공지사항먼저 전달드리자면
첫 번째 과제는 오늘까지입니다.

4
00:00:28,785 --> 00:00:36,521
금일 11:59 p.m 까지 Canvas에 제출해주시기 바라며,
더불어 과제 2가 새롭게 나갈 것입니다.

5
00:00:36,521 --> 00:00:40,082
그리고 여러분의 프로젝트 제안서는
25일까지 제출해 주시기 바랍니다.

6
00:00:40,082 --> 00:00:46,591
아직 준비가 안되신 분들이면 서둘러 해주시기 바랍니다.

7
00:00:46,591 --> 00:00:54,804
어느정도 구상한 분들이 계십니까?
몇 분정도 계시는군요

8
00:00:54,804 --> 00:01:03,937
어떻게 해야할지 모르겠다 싶으면
TA 사무실로 방문해 주시면 도와드리겠습니다.

9
00:01:05,657 --> 00:01:18,121
Piazza에 다른 연계 프로젝트도 있으니 확인해 주시기 바랍니다.

10
00:01:19,604 --> 00:01:28,004
그리고 Justin이 만든 "Linear Layer에서의 Backporp",
"vector/tensor 미분" 관련자료가 있으니 참고해주세요.

11
00:01:28,004 --> 00:01:33,964
이 자료를 보시면 vector/matrix에서 backporp이 어떻게
동작하는지 이해하실 수 있을 것입니다.

12
00:01:33,964 --> 00:01:40,484
Syllabus에 Lecture 4에 링크되어 있으니 보시면 됩니다.

13
00:01:45,110 --> 00:01:57,124
지난 시간에 함수를 computational graph로 표현하는
방법을 배웠습니다. 어떤 함수든 이렇게 표현할 수 있습니다.

14
00:01:57,124 --> 00:02:03,751
Neural network에 대해서 자세히 배웠습니다.
여기 보이는 것 처럼 Linear Layer들이 있었죠

15
00:02:03,751 --> 00:02:08,360
비 선형 연산자들을 끼워넣으면서 여러 층으로 쌓았습니다.

16
00:02:09,456 --> 00:02:13,360
지난 시간에 CNN도 배웠습니다.

17
00:02:13,360 --> 00:02:24,936
CNN은 "Spatial structure" 를 사용하기 위해
Conv Layer를 사용하는 NN의 특수한 형태이입니다.

18
00:02:24,936 --> 00:02:38,056
Conv 필터가 입력 이미지를 슬라이딩해서 계산한 값들이 모여
출력 Activation map을 만들게 됩니다.

19
00:02:38,056 --> 00:02:45,456
그리고 Conlayer에서는 보통 여러개의 필터를 사용했습니다.
각 필터는 서로 다른 Activation map을 생성합니다.

20
00:02:45,456 --> 00:02:50,655
여기 그림은 보면 어떤 입력이 있고 Depth가 있습니다.
이제 Activation map 을 만들어야 하는데

21
00:02:50,655 --> 00:02:58,771
Activation map는 필터의 갯수만큼 존재하며 각 map은
입력의 공간적인 정보를 보존하고 있습니다.

22
00:02:59,695 --> 00:03:05,895
자 이제 우리가 하고싶은 것은 가중치, 즉 파라미터를
업데이트 하고 싶은 것입니다.

23
00:03:05,895 --> 00:03:12,507
지난 시간에 배웠듯이 Optimization을 통해서
네트워크의 파라미터를 학습할 수 있었습니다.

24
00:03:12,507 --> 00:03:17,254
그리고 Loss 라는 산에서 Loss가 줄어드는 방향으로
이동하고 싶은 것입니다.

25
00:03:17,254 --> 00:03:23,053
그리고 이는 gradient의 반대 방향으로 이동하는 것과 같습니다.

26
00:03:23,053 --> 00:03:27,614
이를 Mini-batch Stochastic Gradient Descent
하고 했습니다.

27
00:03:27,614 --> 00:03:38,585
Mini-batch SGD는 우선 데이터의 일부만 가지고
Forword pass를 수행한 뒤에 Loss를 계산합니다.

28
00:03:38,585 --> 00:03:41,960
그리고 gradient를 계산하기 위해서
backprop를 수행합니다.

29
00:03:41,960 --> 00:03:47,986
그리고 그 gradient를 이용해서 파마미터를 업데이트합니다.

30
00:03:49,980 --> 00:03:58,321
향 후 몇번의 강의는 NN의 학습에 대해 다룰 것입니다.

31
00:03:58,321 --> 00:04:02,441
우선 NN 학습을 처음 시작할때 필요한 기본 설정에 대해 알아볼 것입니다.

32
00:04:02,441 --> 00:04:11,015
활성함수 선택, 데이터 전처리, 가중치 초기화
Regularization, gradient checking 등이 이에 속하죠

33
00:04:11,015 --> 00:04:16,118
또한 Training dynamics에 대해서도 다룰 것입니다.
학습이 잘 되고 있는지 확인하는 법에 대해 배울 것입니다.

34
00:04:16,118 --> 00:04:21,294
어떤 방식으로 파라미터를 업데이트할 것인지에 대해서도 배울 것이고

35
00:04:21,294 --> 00:04:26,241
가장 적절한 하이퍼파라미터를 찾기 위해
하이퍼파라미터를 Optimization 하는 방법도 배울 것입니다.

36
00:04:26,241 --> 00:04:28,281
그리고 평가에 대해서도 이야기 할 것입니다.

37
00:04:28,281 --> 00:04:29,948
그리고 Model ensemble에 대해서도 배웁니다.

38
00:04:33,000 --> 00:04:41,015
오늘은 Part 1 으로 활성함수, 데이터 전처리,
가중치 초기화, Batch Normalization

39
00:04:41,015 --> 00:04:45,412
학습 과정 다루기, 하이퍼 파라미터 Optimization 을
배워 보도록 하겠습니다.

40
00:04:47,348 --> 00:04:50,348
자 Activation Function 부터 시작해봅시다.

41
00:04:51,708 --> 00:04:55,095
자 먼저 지난시간에 봤던 Layer를 살펴보면

42
00:04:55,095 --> 00:05:01,481
데이터 입력이 들어오면 가중치와 곱합니다.
FC 나 CNN이 될 수 있겠죠

43
00:05:01,481 --> 00:05:06,388
그 다음 활성함수, 즉 비선형 연산을 거치게 됩니다.

44
00:05:06,388 --> 00:05:08,027
활성함수의 예시를 보겠습니다.

45
00:05:08,027 --> 00:05:13,295
지난 시간에 Sigmoid를 본적이 있죠
ReLU도 봤습니다.

46
00:05:13,295 --> 00:05:20,479
이번 시간에는 좀 더 다양한 종류의 활성함수와
그들간의 Trade-off에 대해 다뤄보도록 하겠습니다.

47
00:05:22,228 --> 00:05:27,241
자 우선 Sigmoid부터 보겠습니다.
우리한테 제일 만만한 놈이죠

48
00:05:27,241 --> 00:05:32,572
Sigmoid는 이렇게 생겼습니다.
1/(1+e^-x) 이죠

49
00:05:32,572 --> 00:05:45,201
얘가 하는 일은 각 입력을 받아서 그 입력을 [0, 1] 사이의
값이 되도록 해줍니다.

50
00:05:45,201 --> 00:05:50,427
입력의 값이 크면 Sigmoid의 출력은 1에 가까울 것이고

51
00:05:50,427 --> 00:05:55,321
값이 작으면 0에 가까울 것입니다.

52
00:05:55,321 --> 00:06:02,481
0 근처 구간(rigime)을 보면 선형스럽습니다 (/ 모양)
그 쪽은 선형함수 같아 보입니다.

53
00:06:02,481 --> 00:06:05,374
Sigmoid는 역사적으로 아주 유명했습니다.

54
00:06:05,374 --> 00:06:11,530
왜냐하면 Sigmoid가 일종의, 뉴런의 firing rate를
saturation 시키는 것으로 해석할 수 있기 때문입니다.

55
00:06:11,530 --> 00:06:15,455
어떤 값이 0에서 1 사이의 값을 가지면
이를 fireing rate라고 생각할 수 있을 것입니다.

56
00:06:15,455 --> 00:06:23,588
나중에 ReLU 같은 것을 배울 것인데, 실제로 ReLU가
생물학적 타당성이 더 크다는게 밝혀졌지만

57
00:06:23,588 --> 00:06:27,402
Sigmoid 또한 이런 식으로 해석할 수 있다는 점을
알고 넘어가면 될 것 같습니다.

58
00:06:30,015 --> 00:06:36,492
요 녀석을 한번 자세히 살펴보면
문제점이 몇 가지 있습니다.

59
00:06:36,492 --> 00:06:44,065
우선 Saturation되는게 gradient를 없앱니다.
이게 무슨 뜻일까요?

60
00:06:44,988 --> 00:06:48,801
이 Sigmoid gate를 Computraional graph에서
한번 살펴보겠습니다.

61
00:06:48,801 --> 00:06:54,566
여기 데이터 X가 있고 출력이 있습니다.

62
00:06:54,566 --> 00:06:59,236
Backprop에서 Gradient는 어떻게될까요?

63
00:06:59,236 --> 00:07:08,441
우선 dL/dSigma 가 있습니다. dL/dSigma가 밑으로
내려가죠 그리고 dX/dSigma를 곱하게 될 것입니다.

64
00:07:08,441 --> 00:07:11,081
이 값이 local sigmoid function의
gradient가 될 것입니다.

65
00:07:11,081 --> 00:07:16,495
그리고 이런 값들이 계속 연쇄적으로
Backporp될 것입니다.

66
00:07:16,495 --> 00:07:24,708
그렇다면 질문입니다. X가 -10이면 어떻게 될까요?
gradient는 어떻게 생기게 될까요?

67
00:07:24,708 --> 00:07:28,868
네 맞습니다. 0이 되겠죠.
gradient는 0이 됩니다.

68
00:07:28,868 --> 00:07:37,348
Sigmoid에서 음의 큰 값이면 sigmoid가
flat하게 되고 gradient가 0이 되겠죠

69
00:07:37,348 --> 00:07:40,001
그리고 이 값이 밑으로 내려갈 것입니다.

70
00:07:40,001 --> 00:07:46,501
거의 0에 가까운 값이 backprob 될 것입니다.

71
00:07:46,501 --> 00:07:55,381
이 부분에서 gradient가 죽어버리게 되고
밑으로 0이 계속 전달되게 됩니다.

72
00:07:58,869 --> 00:08:10,015
그렇다면 X가 0이면 어떻까요?
맞습니다. 이 구간에서는 잘 동작할 것입니다.

73
00:08:10,015 --> 00:08:15,135
아주 그럴싸한 gradient를 얻게 될 것이고
backprop 이 잘 될 것입니다.

74
00:08:15,135 --> 00:08:20,055
그러면 X가 10이면 어떻까요?

75
00:08:20,055 --> 00:08:31,108
X가 양의 큰 값일 경우에도 sigmoid가 flat하기 때문에
gradient들 다 죽이게 됩니다.

76
00:08:31,108 --> 00:08:35,275
그러면 gradient가 잘 흐르지 않겠죠

77
00:08:37,055 --> 00:08:42,454
두 번째 문제는 바로
sigmoid 의 출력이 zero centered 하지 않다는 것입니다.

78
00:08:42,454 --> 00:08:46,415
이게 왜 문제인지 한번 알아보도록 하겠습니다.

79
00:08:46,415 --> 00:08:51,892
자 그럼 뉴런의 입력이 항상 양수일 때
어떤 일이 벌어지는지 한번 살펴보겠습니다.

80
00:08:51,892 --> 00:08:54,948
이 경우에 모든 X가 양수일 때 입니다.

81
00:08:54,948 --> 00:09:04,348
이 x는 어떤 가중치랑 곱해지고 활성함수를 통과하겠죠

82
00:09:04,348 --> 00:09:08,015
그럼 W에 대한 gradient를 한번 생각해볼까요?

83
00:09:12,375 --> 00:09:18,135
이 Layer에서의 local gradient가 어떻게 될지
한번 생각해 보세요

84
00:09:18,135 --> 00:09:24,214
우선 dL/d(활성함수) 를 계산하겠죠 그렇게 Loss가 내려가고

85
00:09:24,214 --> 00:09:29,834
그리고 Local gradient가 있을텐데,
기본적으로 그냥 X입니다.

86
00:09:29,834 --> 00:09:34,001
그렇다면 모든 X가 양수라는 것은 무슨뜻일까요?

87
00:09:36,253 --> 00:09:44,401
누군가가 gradient가 전부 "양수" 가 된다고 하셨는데요.
적확히 말하면 "전부 양수" 또는 "전부 음수" 입니다.

88
00:09:44,401 --> 00:09:53,588
위에서 dL/df(활성함수) 가 넘어오겠죠
이 값이 음수 또는 양수가 될 것입니다.

89
00:09:53,588 --> 00:09:55,815
어떤 임의의 gradient가 내려온다고 생각해 봅시다.

90
00:09:55,815 --> 00:10:06,619
우선 local gradient는 이 값이랑 곱해질 것이고
dF(활성함수)/dW는, 그냥 X가 될 것입니다.

91
00:10:07,880 --> 00:10:20,800
그렇게 되면 gradient의 부호는 그저
위에서 내려온 gradient의 부호과 같아 질 것입니다.

92
00:10:20,800 --> 00:10:28,520
이것이 의미하는 바는 W가 모두 같은 방향으로만 움직일 것임을
의미하게 됩니다.

93
00:10:28,520 --> 00:10:42,467
파라미터를 업데이트 할 때 다 같이 증가하거나
다 같이 감소하거나 할 수 밖에 없습니다.

94
00:10:42,467 --> 00:10:48,867
여기서의 문제는 이런 gradient 업데이트는
아주 비효율적이라는 것입니다.

95
00:10:48,867 --> 00:10:59,507
여기 W가 이차원인 예제가 있습니다.
W에 대한 두개의 축으로 이루어져 있습니다.

96
00:10:59,507 --> 00:11:04,796
전부 양수 또는 음수로 업데이트된다는 것을 해석해보면

97
00:11:04,796 --> 00:11:12,400
이렇게 되면 gradient가 이동할 수 있는 방향은 4분면 중
이 두 영역뿐이 안 될 것입니다.

98
00:11:12,400 --> 00:11:17,213
이 두 방향으로 밖에 gradient가 업데이트되지 않습니다.

99
00:11:17,213 --> 00:11:25,399
그래서 이론적으로 가장 최적의 W 업데이트가
저 파란색 화살표라고 했을때

100
00:11:25,399 --> 00:11:30,773
우리가 어떤 시작점부터 내려간다고 했을 때,
여기에서 시작점은 빨간 화살표의 맨 처음이 되겠죠

101
00:11:30,773 --> 00:11:38,946
우리는 파란색 화살표 방향으로 gradient를 이동시킬 수 없습니다.
저 방향으로는 움직일 수 없기 때문이죠

102
00:11:38,946 --> 00:11:43,479
그렇기 때문에 우리는 여러번 gradient 업데이트를
수행해 줘야 합니다.

103
00:11:43,479 --> 00:11:51,953
가령 이렇게 빨간 화살표 방향와 같이, gradient가 이동 가능한
방향으로만 이동을 할 수 있게 됩니다.

104
00:11:53,039 --> 00:11:58,479
이것이 바로 우리가 일반적으로
zero-mean data을 원하는 이유입니다.

105
00:11:58,479 --> 00:12:11,893
입력 X가 양수/음수를 모두 가지고 있으면  전부 같은 방향으로 움직이는
일은 발생하지는 않을 것입니다.

106
00:12:11,893 --> 00:12:17,819
이해 하셨습니까? 질문 있으신가요?
좋습니다.

107
00:12:21,453 --> 00:12:24,930
자 우리는 지금까지 sigmoid가 가진 문제에 대해서 배웠습니다.

108
00:12:24,930 --> 00:12:30,586
Saturation이 gradient를 죽일 수 있었습니다.
양/음의 방햐으로 너무 큰 값을 가진다면 말이죠

109
00:12:30,586 --> 00:12:36,586
그리고 zero-centered 가 아니기 때문에
gradient 업데이트가 효율적이지 않았습니다.

110
00:12:36,586 --> 00:12:43,146
그리고 세 번째 문제는 exp() 가 있어서
계산이 비싸다는 것입니다.

111
00:12:43,146 --> 00:12:46,837
이 세 번째 문제는 그렇게 큰 문제는 아닙니다.

112
00:12:46,837 --> 00:12:51,186
큰 그림으로 봤을 때 다른 연산들, 가령
내적의 더 계산이 비쌉니다.

113
00:12:51,186 --> 00:12:55,103
하지만 굳이 문제로 뽑자면 이렇다는 것입니다.

114
00:12:58,986 --> 00:13:03,166
이제 두 번째 활성 함수를 한번 보겠습니다.
tanh입니다.

115
00:13:03,166 --> 00:13:10,999
sigmoid 랑 유사하죠. 하지만 범위가 [-1 , 1] 입니다.

116
00:13:10,999 --> 00:13:15,573
가장 큰 차이점이 있다면
이제 zero-centered 라는 것입니다.

117
00:13:15,573 --> 00:13:21,306
이를 통해 두 번째 문제가 해결됩니다.
하지만 satuation이 있어서 여전히 Gradient가 죽습니다.

118
00:13:21,306 --> 00:13:29,264
여기 보시면 여전히 gradient가 평평해 지는 구간이 있겠죠

119
00:13:29,264 --> 00:13:34,009
tanh는 sigmoid보다는 조금 낫지만
그래도 여전히 문제점은 있습니다.

120
00:13:36,586 --> 00:13:40,104
자 이제 ReLU 를 살펴보겠습니다.

121
00:13:40,104 --> 00:13:47,573
CNN 강의할 때 본 적 있으실 것입니다.

122
00:13:47,573 --> 00:13:53,279
Conv layer들 사이 사이에 ReLU가 있었죠

123
00:13:53,279 --> 00:13:58,253
ReLU의 함수는
f(x) = max(0,x) 입니다.

124
00:13:58,253 --> 00:14:06,573
이 함수는 element-wise 연산을 수행하며
입력이 음수면 값이 0이 됩니다.

125
00:14:06,573 --> 00:14:13,264
그리고 양수면 입력 값 그대로를 출력합니다.

126
00:14:13,264 --> 00:14:22,892
ReLU는 상당히 자주 쓰입니다. 그리고 기존 sigmoid와 tanh한테
있었던 문제점들을 한번 살펴보자면

127
00:14:22,892 --> 00:14:26,746
우선 ReLU는 양의 값에서는 saturetion되지 않습니다.

128
00:14:26,746 --> 00:14:34,465
적어도 입력스페이스의 절반은 saturation 되지 않을 것입니다.
ReLU의 가장 큰 장점이죠

129
00:14:34,465 --> 00:14:36,959
그리고 계산 효율이 아주 뛰어납니다.

130
00:14:36,959 --> 00:14:42,466
기존의 sigmoid는 함수 안에 지수텀이 있었죠

131
00:14:42,466 --> 00:14:48,968
반면에 ReLU는 그냥 max만 하면 되므로
엄청나게 빠릅니다.

132
00:14:48,968 --> 00:14:57,063
ReLU를 사용하면 실제로  sigmoid나 tanh보다 수렴속도가
훨씬 빠릅니다. 거의 6배정도 더 빠르죠

133
00:14:57,063 --> 00:15:01,090
그리고 사실 생물학적 타당성도 ReLU가 sigmoid보다 큽니다.

134
00:15:01,090 --> 00:15:11,450
실제 신경과학적인 실험을 통해서 뉴련을 관잘해보면

135
00:15:11,450 --> 00:15:18,303
실제 뉴런의 입/출력 값을 확인해보면 sigmoid보다
ReLU 스럽다는 것을 알 수 있습니다.

136
00:15:18,303 --> 00:15:33,798
ImageNet 2012에서 우승한 AlexNet이 처음 ReLU를
사용하기 시작했습니다.

137
00:15:36,775 --> 00:15:42,082
그런데 ReLU에 문제가 하나 있다면 ReLU는 더이상
zero-centered가 아니라는 것입니다.

138
00:15:42,082 --> 00:15:49,228
tanh가 이 문제는 해결했는데 ReLU는 다시 이 문제를
가지고 있게 됩니다.

139
00:15:49,228 --> 00:15:52,122
이는 ReLU가 가진 이슈중 하나입니다.

140
00:15:52,122 --> 00:15:55,357
그리고 ReLU가 가진 골칫거리가 하나 더 있습니다.

141
00:15:55,357 --> 00:16:04,222
ReLU에서 양의 수에서는 saturation 되지 않지만
음의 경우에서는 그렇지 않습니다.

142
00:16:04,222 --> 00:16:06,882
좀 더 자세히 알아보겠습니다.

143
00:16:06,882 --> 00:16:11,255
X가 -10이면 어떻게 될까요?

144
00:16:11,255 --> 00:16:12,855
gradient가 0이 되겠죠

145
00:16:12,855 --> 00:16:16,522
그렇다면 x가 10일때는 어떻게 될까요?

146
00:16:17,455 --> 00:16:20,175
맞습니다.
선형 영역(linear regime)에 속합니다.

147
00:16:20,175 --> 00:16:30,442
x가 0일 때는 어떨까요? 물론 여기에서는 정의되지 않지만
실제로는 0입니다.

148
00:16:30,442 --> 00:16:35,074
기본적으로 ReLU는 gradient의 절반을 죽여버리는 셈입니다.

149
00:16:37,948 --> 00:16:45,708
그래서 우리는 dead ReLU 라는 현상을 겪을 수 있습니다.

150
00:16:45,708 --> 00:16:51,212
이런 일은 몇 가지 상황에서 발생할 수 있는데

151
00:16:51,212 --> 00:16:57,192
자 여기 data cloud가 있습니다.
여러분이 가진 traing data라고 생각하시면 됩니다.

152
00:16:59,033 --> 00:17:09,092
ReLU에서는 평면의 절반만 activate 된다는 것을 알 수 있습니다.

153
00:17:11,948 --> 00:17:15,640
각 평면(초록 빨강 직선)이 각 ReLU를 의미한다고 보시면 됩니다.

154
00:17:15,640 --> 00:17:21,201
그리고 ReLU가 data cloud에서 떨어져 있는 경우에
"dead ReLU" 가 발생할 수 있습니다.

155
00:17:21,201 --> 00:17:26,588
dead ReLU에서는 activate 가 일어나지 않고
update되지 않습니다. 반면 active ReLU는

156
00:17:26,588 --> 00:17:31,732
일부는 active되고 일부는 active하지 않을 것입니다.

157
00:17:31,732 --> 00:17:33,480
몇 가지 이유로 이런 일이 발생할 수 있습니다.

158
00:17:33,480 --> 00:17:37,201
우선 첫 번째로는
초기화를 잘 못한 경우 입니다.

159
00:17:37,201 --> 00:17:45,015
여기 dead ReLU 처럼 생긴 경우인데, 가중치 평면이
data cloud에서 멀리 떨어져 있는 경우입니다.

160
00:17:45,015 --> 00:17:55,069
이런 경우 어떤 데이터 입력에서도 activate 되는 경우가
존재하지 않을 것이고 backporp이 일어나지 않을 것입니다.

161
00:17:56,108 --> 00:17:59,321
이런 경우 update되지도 activate되지도 않을 것입니다.

162
00:17:59,321 --> 00:18:03,880
그리고 더 흔한 경우는
Leraning rate가 지나치게 높은 경우입니다.

163
00:18:03,880 --> 00:18:11,561
처음에 "적절한 ReLU" 로 시작할 수 있다고 해도 만약
update를 지나치게 크게 해 버려 가중치가 날뛴다면

164
00:18:11,561 --> 00:18:18,028
ReLU 가 데이터의 manifold를 벗어나게 됩니다.
이런 일들은 학습과정에서 충분히 일어날 수 있습니다.

165
00:18:18,028 --> 00:18:22,975
그래서 처음에는 학습이 잘 되다가
갑자기 죽어버리는 경우가 생기는 것입니다.

166
00:18:22,975 --> 00:18:24,108
그리고 실제로,

167
00:18:24,108 --> 00:18:33,361
그리고 실제로 학습을 다 시켜놓은 네트워크를 살펴보면
10~20% 가량은 dead ReLU가 되어 있습니다.

168
00:18:33,361 --> 00:18:40,001
이게 문제이긴 하지만 ReLU를 사용하고 있다면 대부분의
네트워크가 이 문제를 겪을 수 있습니다.

169
00:18:40,001 --> 00:18:49,467
일부는 dead ReLU가 되버리는 것을 관찰해 볼 수 있습니다.
하지만 그 정도면 네트워크 학습에 크게 지장이 있진 않습니다.

170
00:18:49,467 --> 00:18:51,268
질문 있나요?

171
00:18:51,268 --> 00:18:54,851
[학생이 질문]

172
00:19:01,908 --> 00:19:05,335
네 맞습니다.
Data cloud는 그냥 training data입니다.

173
00:19:05,335 --> 00:19:08,918
[학생이 질문]

174
00:19:17,641 --> 00:19:25,708
질문은 바로 data cloud에서 ReLU가 죽어버리는지 아닌지를
어떻게 알 수 있냐느 것입니다.

175
00:19:25,708 --> 00:19:30,988
여기 예를 다시 한 번 보겠습니다.
간단한 2차원의 경우입니다.

176
00:19:30,988 --> 00:19:42,278
ReLU의 입력 값은 가령  W1*X1 + W2*X2 같은 값이 되겠죠

177
00:19:42,278 --> 00:19:46,080
그리고 이 가중치가 초평면(초록 빨강)을 이루게 되겠죠

178
00:19:46,080 --> 00:19:51,453
그리고 여기에서 positive인 절반만 가져갈 것입니다.
나머지 절반은 죽어버리겠죠

179
00:19:51,453 --> 00:20:03,789
그러니 W의 초평면의 위치와 Data의 위치를 고려했을 때
초평면 자체가 Data와 동떨어지는 경우가 발생할 수 있습니다.

180
00:20:05,560 --> 00:20:14,329
학습 과정에서 일부 ReLU는 data cloud와 동떨어져
있을 수가 있습니다.

181
00:20:16,480 --> 00:20:18,050
질문 있나요?

182
00:20:18,050 --> 00:20:21,633
[학생이 질문]

183
00:20:23,380 --> 00:20:33,478
sigmoid의 두가지 단점에 중에서 하나가 뉴런이
saturation 된다는 것이었습니다.

184
00:20:37,045 --> 00:20:40,500
sigmoid 슬라이드를 다시한번 보겠습니다.

185
00:20:40,500 --> 00:20:45,820
질문은 바로 input이 전부 positive면
saturation이 안되지 않냐는 것입니다.

186
00:20:45,820 --> 00:20:51,971
여기 입력이 모두 양수면
0의 오른쪽 지역으로 들어오는 것이겠죠

187
00:20:51,971 --> 00:20:54,464
그렇게 되도 여전히 saturation의 가능성이 있습니다.

188
00:20:54,464 --> 00:21:00,544
왜냐하면 positive region에서도 여전히
평평한 곳이 존재하기 때문입니다.

189
00:21:00,544 --> 00:21:08,846
양의 큰 값이 들어오면, 여기 평평한 지역 때문에 여전히
gradient가 0이 될 수 있을 것입니다.

190
00:21:10,715 --> 00:21:11,548
좋습니다.

191
00:21:16,355 --> 00:21:24,528
그래서 실제로 ReLU를 초기화할때
positive biases를 추가해 주는 경우가 있습니다.

192
00:21:24,528 --> 00:21:30,721
Update 시에 active ReLU가 될 가능성을 조금이라도 더
높혀주기 위해서죠

193
00:21:30,721 --> 00:21:40,430
하지만 이게 도움이 된다는 의견도 있고
그렇지 않다는 사람들도 있습니다.

194
00:21:40,430 --> 00:21:48,072
이 방법을 항상 사용하지는 않고, 대부분은 사람들은
zero-bias로 초기화합니다.

195
00:21:49,483 --> 00:21:54,777
ReLU 이후에 조금 수정된 버전이 나왔습니다.

196
00:21:54,777 --> 00:21:57,768
그 중 하나가 바로 leaky ReLU 라는 녀석입니다.

197
00:21:57,768 --> 00:22:04,429
ReLU와 유사하지만 negarive regime에서
더 이상 0이 아닙니다.

198
00:22:04,429 --> 00:22:11,955
negative에도 기울기를 살짝 주게 되면 앞서 설명했던
문제를 상당 부분 해결하게 됩니다.

199
00:22:11,955 --> 00:22:17,142
Leaky ReLU의 경우에는 negative space 에서도
saturation 되지 않습니다.

200
00:22:17,142 --> 00:22:23,968
여전히 계산이 효율적입니다. 그리고 sigmoid 나 tanh보다
수렴을 빨리 할 수 있습니다. 그리고 ReLU랑도 비슷하게 생겼죠

201
00:22:23,968 --> 00:22:27,218
그리고 dead ReLU 현상도 더 이상 없습니다.

202
00:22:28,923 --> 00:22:35,380
또 다른 예시로는
parametric rectifier,  PReLU 가 있습니다.

203
00:22:35,380 --> 00:22:42,195
PReLU는 negative space에 기울기가 있다는 점에서
Leaky ReLU와 유사하다는 것을 알 수 있습니다.

204
00:22:42,195 --> 00:22:47,088
다만 여기에서는 기울기가 alpha 라는
파라미터로 결정됩니다.

205
00:22:47,088 --> 00:22:52,982
alpha를 딱 정해놓는 것이 아니라 backpro으로
학습시키는 파라미터로 만든 것입니다.

206
00:22:52,982 --> 00:22:57,555
활성함수가 조금 더 유연해 질 수 있을 것입니다.

207
00:22:57,555 --> 00:23:02,342
또 다른 예시로
ELU라는 것도 있습니다.

208
00:23:02,342 --> 00:23:08,295
우리는 지금 LU 패밀리들을 보고 있습니다.

209
00:23:08,295 --> 00:23:10,341
ELU는 ReLu의 이점을 그대로 가져옵니다.

210
00:23:10,341 --> 00:23:14,508
하지만 ELU는 zero-mean 에 가까운 출력값을 보입니다.

211
00:23:16,181 --> 00:23:24,901
zero-mean에 가까운 출력은 앞서 leaky ReLU, PReLU
가 가진 이점이었죠

212
00:23:26,699 --> 00:23:36,538
하지만 Leaky ReLU와 비교해보면 ELU는 negative에서
"기울기" 를 가지는 것 대신에 또 다시 "satuation" 됩니다.

213
00:23:36,538 --> 00:23:43,029
ELU가 주장하는건 이런 saturation이 좀더
잡음(noise)에 강인할 수 있다는 것입니다.

214
00:23:43,029 --> 00:23:48,566
이런 deactivation이 좀더 강인함을 줄 수 있다고 주장합니다.

215
00:23:48,566 --> 00:23:55,885
ELU의 논문에서는 왜 ELU가 더 강인한지에
타당성을 다양하게 제시하고 있습니다.

216
00:23:55,885 --> 00:24:01,111
ELU는 ReLU와 Leaky ReLU의 중간 정도라고 보시면 됩니다.

217
00:24:01,111 --> 00:24:13,267
ELU는 Leaky ReLU처럼 zero-mean의 출력을 내지만
Saturation의 관점에서 ReLU의 특성도 가지고 있습니다.

218
00:24:13,267 --> 00:24:14,350
질문 있나요?

219
00:24:14,350 --> 00:24:17,933
[학생이 질문]

220
00:24:19,952 --> 00:24:24,365
질문은 바로 파라미터 alpha가 각 뉴런마다
제각각이냐는 것입니다.

221
00:24:24,365 --> 00:24:34,090
아마 각각일 것 같은데 정확히 기억나지 않습니다.
논문에서 어떻게 정의하였는지를 보시면 될 것 같습니다.

222
00:24:35,578 --> 00:24:45,050
하지만 확실한 것은 ELU가 좋은 장점을 지니도록
세심하게 디자인하였다는 것입니다.

223
00:24:45,050 --> 00:24:49,992
지금까지는 ReLU의 변종들이었습니다.

224
00:24:49,992 --> 00:24:58,192
다양한 활성함수들이 어떤 장단점이 있는지를
여러분 스스로 눈여겨 볼 수 있을 것입니다.

225
00:24:58,192 --> 00:25:04,950
실제로 사람들이 이것저것 실험해보고 경험적으로 어떤 것이
더 좋은지를 살펴보고 새로운 것들을 만들어낸 것입니다.

226
00:25:04,950 --> 00:25:08,612
이들 모두는 실험해볼만한 것들이라 할 수 있겠습니다.

227
00:25:10,135 --> 00:25:14,744
그럼 하나만 더 알아보겠습니다.
Maxout Neuron 이라는 것입니다.

228
00:25:14,744 --> 00:25:25,969
지금까지 본 활성함수들과는 좀 다르게 생겼습니다. 입력을
받아드리는 특정한 기본형식을 미리 정의하지 않습니다.

229
00:25:25,969 --> 00:25:34,670
대신에 w1에 x를 내적한 값 + b1과 w2에 x를 내적한 값 + b2
의 최댓값을 사용합니다.

230
00:25:38,230 --> 00:25:43,185
Maxout은 이 두 함수 중 최댓값을 취합니다.

231
00:25:44,870 --> 00:25:48,949
Maxout는 ReLU와 leaky ReLU의 좀 더 일반화된 형태입니다.

232
00:25:48,949 --> 00:25:54,112
왜냐하면 Maxout은 이 두 개의 선형함수를 취하기 때문이죠

233
00:25:55,023 --> 00:26:02,927
Maxout또한 선형이기때문에 saturation 되지 않으며
gradient가 죽지 않을 것입니다.

234
00:26:02,927 --> 00:26:15,984
여기서 문제점은 뉴런당 파라미터의 수가 두배가 된다는 것입니다.
이제는 W1과 W2를 지니고 있어야합니다.

235
00:26:17,765 --> 00:26:24,560
지금까지 다양한 활성함수를 살펴보았습니다. 실제로
가장 많이들 쓰는 것은 바로 ReLU 입니다.

236
00:26:24,560 --> 00:26:29,389
ReLU가 표준으로 많이 사용되며
대게는 잘 동작합니다

237
00:26:30,231 --> 00:26:36,497
다만 ReLU를 사용하려면 leraning rate를 아주 조심스럽게
결정해야 할 것입니다.

238
00:26:36,497 --> 00:26:40,091
learning rate에 관해서는 이번 강의의 후반부에 좀 더
다뤄보도록 하겠습니다.

239
00:26:40,091 --> 00:26:52,318
또한 여러분은 Leaky ReLU, Maxout, ELU와 같은 것들도
써볼 수 있겠습니다. 물론 이들은 아직 실험단계이긴 합니다.

240
00:26:53,828 --> 00:26:56,643
여러분들의 문제에 맞춰 어떤 활성함수가 잘 동작하는지
확인해 볼 수 있을 것입니다.

241
00:26:56,643 --> 00:27:04,035
tanh도 써볼 수 있겠죠. 하지만 대게는 ReLU와 ReLU의 변종들이
좀 더 잘 동작한다고 보시면 되겠습니다.

242
00:27:04,035 --> 00:27:15,243
Sigmoid는 사용하지 않는 편이 좋습니다. 가장 구식이고
요즘은 LU 패밀리 계열이 더 잘 동작합니다.

243
00:27:17,361 --> 00:27:21,517
자 그럼 이제 데이터 전처리에 대해 배워보겠습니다.

244
00:27:21,517 --> 00:27:24,602
지금까지는 활성 함수에 대해 배워보았습니다.

245
00:27:24,602 --> 00:27:30,361
이제는 실제로 네트워크를 훈련시켜 보려 합니다.
우선 우리에게는 입력 데이터가 있겠죠.

246
00:27:31,424 --> 00:27:39,495
일반적으로 입력 데이터는 전처리를 해줍니다. 
Machine Learning 수업에서 다뤄 보신 분들도 계실 것입니다.

247
00:27:39,495 --> 00:27:49,366
가장 대표적인 전처리 과정은 
"zero-mean"으로 만들고 "normalize" 하는 것입니다.

248
00:27:49,366 --> 00:27:57,367
normalization은 보통 표준편차로 합니다. 
그렇다면 이걸 왜 하는 것일까요?

249
00:27:57,367 --> 00:28:04,979
앞서 zero-centering에 대해 다룬 적이 있었죠.
입력이 전부 positive인 경우에 대해 말이죠

250
00:28:04,979 --> 00:28:12,772
그러면 모든 뉴런이 positive인 gradient를 얻게 됩니다.
이는 최적하지 못한(suboptimal) 최적화가 됩니다.

251
00:28:12,772 --> 00:28:21,710
데이터가 전부 positive 일 때 뿐만이 아니라 0 이거나
전부 negative일 경우에도 이런 일은 발생하곤 합니다.

252
00:28:23,770 --> 00:28:36,440
normalization을 해주는 이유는 모든 차원이 동일한 범위안 
있게 해줘서 전부 동등한 기여(contribute)를 하게 합니다.

253
00:28:36,440 --> 00:28:45,866
실제로는 이미지의 경우 전처리로 zero-centering 정도만 해주니다.

254
00:28:45,866 --> 00:28:56,616
normalization 하지는 않습니다. 왜냐 하면 이미지는
이미 각 차원 간에 스케일이 어느정도 맞춰져 있기 때문입니다.

255
00:28:56,616 --> 00:29:09,339
따라서 스케일이 다양한 여러 ML 문제와는 달리 이미지에서는
normalization을 엄청 잘 해줄 필요는 없습니다.

256
00:29:11,037 --> 00:29:19,983
그리고 Machine learning 에서는 PCA나 whitening 같은
더 복잡한 전처리 과정도 있긴 합니다만

257
00:29:19,983 --> 00:29:28,678
이미지에서는 단순히 zero-mean 정도만 사용하고 normalization
그 밖의 여러 복잡한 방법들은  잘 안씁니다.

258
00:29:29,519 --> 00:29:40,876
일반적으로는 이미지를 다룰 때는 굳이 입력을 
더 낮은 차원으로  projection 시키지 않습니다.

259
00:29:40,876 --> 00:29:48,184
CNN에서는 원본 이미지 자체의 spatial 정보를 이용해서
이미지의 spatial structure를 얻을 수 있도록 합니다.

260
00:29:48,184 --> 00:29:49,595
질문 있나요?

261
00:29:49,595 --> 00:29:53,178
[학생이 질문]

262
00:29:58,858 --> 00:30:06,968
질문은 Train Phase에서 전처리를 해줬으면 Test Phase
에서도 똑같은 전처리를 해 줘야 하는 것인 인데, 맞습니다.

263
00:30:06,968 --> 00:30:24,839
일반적으로 Train에서 계산한 평균을 Test 데이터에 똑 같은 값을 적용해
줍니다. Training data에서 구한 평균을 그대로 사용합니다.

264
00:30:24,839 --> 00:30:35,822
요약해보자면 이미지를 다룰때는
기본적으로 zero-mean 으로 전처리를 해준다는 것입니다.

265
00:30:38,151 --> 00:30:41,354
평균 값은 전체 Training data에서 계산합니다.

266
00:30:41,354 --> 00:30:54,777
보통 입력이미지의 사이즈를 서로 맞춰주는데 가령 32x32x3 이고
네트워크에 들어가기 전에 평균값을 빼주게 됩니다.

267
00:30:54,777 --> 00:31:00,532
그리고 Test time의 이미지에도 Training time에 계산한
평균 값으로 빼주게 됩니다.

268
00:31:00,532 --> 00:31:14,916
실제로 일부 네트워크는 채널 전체의 평균을 구하지 않고 채널마다 
평균을 따로 계산하는 경우도 있습니다.

269
00:31:14,916 --> 00:31:25,718
채널별로 평균이 비슷할 것인지, 혹은 따로 구해야 할 것인지를
여러분이 판단하기 나름입니다.

270
00:31:25,718 --> 00:31:36,936
AlexNet 이후 VGGNet 같은 네트워크가 이러한 형태인데 
추후에 다시 다루도록 하겠습니다.

271
00:31:36,936 --> 00:31:38,545
질문 있나요?

272
00:31:38,545 --> 00:31:42,128
[학생이 질문]

273
00:31:45,215 --> 00:31:52,049
질문은 두 가지 하셨습니다. 우선 첫 번째 질문은 
"채널 의 의미" 입니다.

274
00:31:52,049 --> 00:32:04,198
여기에서 채널은 RGB입니다. 이미지가 32x32x3 이면
Width(32) x Height(32) x RGB(3) 입니다.

275
00:32:04,198 --> 00:32:09,786
VGG식으로 하면 RGB별로 각각 평균을 구하는 것입니다.

276
00:32:09,786 --> 00:32:14,529
두 번째 질문이 무엇이었죠?

277
00:32:14,529 --> 00:32:18,112
[학생이 질문]

278
00:32:21,349 --> 00:32:26,827
질문은 "평균 을 어떻게 계산하는지" 입니다.

279
00:32:27,882 --> 00:32:39,114
평균은 여러분의 "학습 데이터 전부" 를 가지고 계산합니다.
이해가 되셨습니까?

280
00:32:39,114 --> 00:32:42,697
[학생이 질문]

281
00:32:48,432 --> 00:32:55,255
질문은 만약 미니배치 단위로 학습시킨다고 해도 평균 계산은
미니배치 단위로 각각이 아니라 전체로 계산하냐는 것입니다.

282
00:32:55,255 --> 00:32:57,904
맞습니다.

283
00:32:57,904 --> 00:33:03,984
우리가 구하는 평균 이라는 것은 트레이닝 데이터의 평균입니다.

284
00:33:03,984 --> 00:33:13,983
배치에서 뽑은 데이터도 사실 전체 데이터에서 나온 것들이죠
결국은 배치 평균이나 전체 평균이나 구해보면 같습니다.(이상적으로)

285
00:33:13,983 --> 00:33:19,126
그러니 처음에 따 한번만 전체 트레이닝 이미지로
평균을 구해 놓는 편이 낫겠죠

286
00:33:19,126 --> 00:33:28,296
반대로 엄청 큰 데이터의 전체 평균을 구할때도 굳이 모든 데이터를 
전부 쓰지 않고 적절하게 샘플링해서 구할수도 있겠죠

287
00:33:30,734 --> 00:33:35,560
좋습니다. 혹시 다른 질문도 있으십니까?

288
00:33:35,560 --> 00:33:38,654
[학생이 질문]

289
00:33:38,654 --> 00:33:42,187
질문은 바로 데이터 전처리가 Sigmoid Problem을
해결할 수 있는지 입니다.

290
00:33:42,187 --> 00:33:46,354
우리가 수행 할 전처리는 zero mean 입니다.

291
00:33:47,540 --> 00:33:50,535
앞서 sigmoid 에게는 zero-mean이 필요하다고 했습니다.

292
00:33:50,535 --> 00:33:56,262
데이터 전처리가 sigmoid의 zero-mean 문제를
단지 "첫 번째" 레이어에서만 해결할 수 있을 것입니다.

293
00:33:56,262 --> 00:34:00,263
처음에는 입력 이미지가 zero-mean 이므로 해결 할 순 있겠으나

294
00:34:00,263 --> 00:34:08,472
하지만 그 다음 레이어부터는 똑같은 문제가 반복될 것입니다.
Deep network라면 점점 더 심해지겠죠

295
00:34:08,472 --> 00:34:12,437
가면 갈수록 더 non-zero-mean 을 겪게 될 것입니다.

296
00:34:12,438 --> 00:34:19,350
따라서 이미지 전처리가 Sigmoid에서의 문제를 해결하기에는
충분하지 않을 것입니다.

297
00:34:21,784 --> 00:34:28,203
좋습니다. 이제 weight를 어떻게 초기화시켜야
하는지를 알아보겠습니다.

298
00:34:28,204 --> 00:34:34,471
"Two Layer Neural Network" 예시를 한번 봅시다.
우리가 할 일은 가중치 업데이트입니다.

299
00:34:34,472 --> 00:34:43,509
맨 처음에 어떤 초기 가중치들이 있겠죠 
그리고 gradient를 계산해서 가중치를 업데이트할 것입니다.

300
00:34:43,510 --> 00:34:56,157
첫 번째 질문입니다.  "모든 가중치 = 0" 이면 어떻게 될까요?
모든 파라미터를 0으로 세팅하면 어떻게될까요

301
00:34:56,157 --> 00:34:58,683
[학생이 대답]

302
00:34:58,683 --> 00:35:00,766
잘 못들었습니다?

303
00:35:02,039 --> 00:35:08,320
"모든 뉴런이 죽어버리고 업데이트가 되지 않을것이다"고 답했습니다. 
정확한 답은 아닙니다.

304
00:35:11,035 --> 00:35:16,995
그래도 잘 대답하신 부분은 "모든 뉴런이 <같은일> 을 한다는 것" 입니다.
그렇다고 다 죽어버리지는 않습니다.

305
00:35:16,995 --> 00:35:23,321
입력 데이터들이 뉴런의 어떤 영역에 속할 것이고 
죽지 않을 수도 있습니다.

306
00:35:23,321 --> 00:35:27,869
하지만 중요한 것은
전부 똑같은 일을 할 것이라는 것입니다.

307
00:35:27,869 --> 00:35:36,577
가중치가 0 이라서 모든 뉴런은 모두 다 같은 연산을 수행합니다.

308
00:35:36,577 --> 00:35:43,621
출력도 모두 같을 것이고. 결국 gradient도 서로 같습니다.

309
00:35:43,621 --> 00:35:47,571
결국 모든 가중치가 똑같은 값으로 업데이트 됩니다.

310
00:35:47,571 --> 00:35:51,983
결국 모든 뉴런이 모두 똑같이 생기게 됩니다. 
우리가 그다지 원하는 것은 아니죠

311
00:35:51,983 --> 00:35:54,075
우리는 뉴런들이 서로 다르게 생기길 원하죠

312
00:35:54,075 --> 00:35:58,514
이것이 바로 모든 가중치를 동일하게 초기화시키면
발생하는 일입니다.

313
00:35:58,514 --> 00:36:02,730
"Symmetry breaking"이 일어날 수 없습니다.

314
00:36:02,730 --> 00:36:05,961
질문 있나요?

315
00:36:05,961 --> 00:36:09,544
[학생이 질문]

316
00:36:19,699 --> 00:36:29,961
질문은 gradient는 가중치 뿐만 아니라 Loss의 영향도 받으므로 
결국 각각의 gradient는 다를 것이지 않냐는 것입니다.

317
00:36:29,961 --> 00:36:46,072
각 뉴런이 어떤 클래스에 연결되어 있는지에 따라  
뉴런들이 서로 다른 loss를 가질 수는 있습니다.

318
00:36:46,072 --> 00:36:54,352
하지만 네트워크 전체를 보면 많은 뉴런들이 동일한 가중치로 
연결되어 있을 것입니다.

319
00:36:54,352 --> 00:36:59,885
결국 모든 뉴런이 같은 방식으로 업데이트 될 것이고
이는 문제가 됩니다.

320
00:36:59,885 --> 00:37:10,885
우선 초기화 문제를 해결하는 첫번째 방법은 
임의의 작은 값으로 초기화하는 것입니다.

321
00:37:10,885 --> 00:37:16,002
이 경우 초기 W를  표준정규분포(standard gaussian)
에서  샘플링합니다.

322
00:37:16,002 --> 00:37:22,450
좀 더 작은 값을 위해 스케일링을 해줍니다. 
0.01을 나눠 표준편차를 1e-2 즉 0.01로 만들어 줍니다.

323
00:37:22,450 --> 00:37:25,640
이런 식으로 모든 가중치를 임의의 값으로 초기화합니다.

324
00:37:25,640 --> 00:37:30,729
여러분의 네트워크가 작은 네트워크라면 
symmetry breaking에 뭐 이정도면 충분합니다.

325
00:37:30,729 --> 00:37:34,896
하지만 이 방법은 더 깊은 네트워크에서 문제가 생길 수 있습니다.

326
00:37:35,970 --> 00:37:43,070
그래서 이것이 왜 그런지 살펴 보겠습니다.
이런 식으로 실험을 해 볼 수 있습니다.

327
00:37:43,070 --> 00:37:45,341
조금 더 깊은 네트워크를 가지고 실험해봅시다.

328
00:37:45,341 --> 00:37:53,622
여기 10개의 레이어로 이루어진 네트워크가 있습니다. 
레이어당 500개의 뉴런이 있습니다.

329
00:37:53,622 --> 00:37:56,437
nonlinearities로는 tanh를 사용해 봅시다

330
00:37:56,437 --> 00:38:06,116
그리고 가중치는 "임의의 작은 값" 으로 초기화시킵니다.

331
00:38:06,116 --> 00:38:12,356
데이터를 랜덤으로 만들어주고
이 데이터를 forward pass시켜보겠습니다.

332
00:38:12,356 --> 00:38:18,725
그리고 각 레이어별 activations 수치를 통계화 시켜보겠습니다.

333
00:38:22,476 --> 00:38:25,485
자 그럼 한번 분석을 해 보겠습니다. 가장 상단의 수치들은 글씨가
너무 작아서 잘 안보일 수도 있습니다.

334
00:38:25,485 --> 00:38:31,156
어쨋든 이 결과는 각 레이어 출력의 평균과
평균과 표준편차를 계산한 것입니다.

335
00:38:31,156 --> 00:38:39,410
우선 첫번째 레이어를 한번 봅시다. 
평균은 항상 0 근처에 있습니다.(tanh 특성상)

336
00:38:40,267 --> 00:38:48,219
[마이크 잡음 관련]

337
00:38:49,613 --> 00:38:58,153
자 다시 여기 출력 분포들을 보면 
당연하게도 평균은 항상 0 근처에 있습니다.

338
00:38:58,153 --> 00:39:01,175
여기 코드를 한번 봅시다.

339
00:39:01,175 --> 00:39:11,420
X와 W를 내적한 값에 tanh를 취합니다. 
그리고 그 값을 저장합니다.

340
00:39:12,315 --> 00:39:16,780
tanh가 zero-centered 이니까 평균이 
0에 가까운건 당연하겠죠

341
00:39:16,780 --> 00:39:22,450
하지만 표준편차를 보게되면 줄어듭니다. 
아주 가파르게 줄어들어서 0에 수렴합니다.

342
00:39:22,450 --> 00:39:32,019
여기 가운데 그래프를 보시면 레이어당 평균과 표준편차를 나타낸 것입니다.

343
00:39:32,019 --> 00:39:38,592
그리고 맨 밑에 똑같은 결과를 분포로 표현했습니다.

344
00:39:38,592 --> 00:39:45,206
첫 번째 레이어에서는 가우시안스럽게 생긴
좋은 분포를 형성하고 있는것을 볼 수 있습니다.

345
00:39:45,206 --> 00:39:58,591
하지만 문제는 우리가 W를 곱하면 곱할수록 
W가 너무 작은 값들이라서 출력 값이 급격히 줄어드는 것입니다.

346
00:39:58,591 --> 00:40:02,191
그리고 결국 0이 되겠죠

347
00:40:02,191 --> 00:40:04,262
우리가 원하는 것이 아닙니다.

348
00:40:04,262 --> 00:40:07,457
그래서 모든 활성함수 결과가 0이됩니다.

349
00:40:07,457 --> 00:40:10,420
자 그럼  backwards pass로 다시한번 생각해 봅시다.

350
00:40:10,420 --> 00:40:16,144
방금 전까지 한 것을 forward pass했다고 가정해보고
이제  gradient를 구해봅시다.

351
00:40:16,144 --> 00:40:20,024
우선 각 가중치들에 해당하는 gradient가 어떨 것이라고
다들 생각하십니까?

352
00:40:24,155 --> 00:40:26,238
정답 아는분 있으신가요?

353
00:40:28,571 --> 00:40:36,531
이렇게 생각해봅시다. 각 레이어의 "입력" 이 엄청 작은 것입니다.

354
00:40:36,531 --> 00:40:43,273
입력 값이 점점 0에 수렴합니다. Backporp을 생각해보면 
"upstream gradient"가 점점 전파됩니다.

355
00:40:43,273 --> 00:40:53,483
자 그럼 현재 가중치를 업데이트하려면 "upstream gradient" 
에 local gradient를 곱해주면 됩니다. 우선 WX를-

356
00:40:53,483 --> 00:40:56,985
W에 대해 미분해보면 local gradient는 입력 X가 됩니다.

357
00:40:56,985 --> 00:41:00,571
자 이렇게 생각해보면 앞서 다뤘던 문제와 
유사한 문제가 생긴다는 것을 알 수 있습니다.

358
00:41:00,571 --> 00:41:07,058
X가 엄청 작은 값이기 때문에 gradient도 작을 것이고
결국 업데이트가 잘 일어나지 않습니다.

359
00:41:07,058 --> 00:41:13,488
이런 실험은 gradient의 흐름이 네트워크에 어떤 영향을 
미치는지 생각해 볼 수 있는 좋은 예시입니다.

360
00:41:13,488 --> 00:41:20,329
이제는 forward pass의 형태를 살펴보고는 이런 경우 gradient가
어떤 식으로 전파되는지 짐작 하실 수 있을 것입니다.

361
00:41:20,329 --> 00:41:28,562
그리고 다양한 입력 타입에 따라 weight와 gradient가 어떤
영향을 미치는지도 생각해 볼 수 있겠습니다.

362
00:41:28,562 --> 00:41:38,025
그리고 또한 gradients가 연결되면서(chaining) 어떤 식으로
전파(flowing back)되는지를 한번 생각해본다면

363
00:41:40,004 --> 00:41:50,291
gradient를 backprop하는 과정은 그 반대입니다. 
upstream gradient에 W의 gradient인 X를 곱합니다.

364
00:41:50,291 --> 00:41:53,085
그리고 입력 X은 어떤 내적(dot product)의 결과입니다.

365
00:41:53,085 --> 00:42:06,208
그리고 backward pass의 과정에서 upstream gradient를
구하는 것은 현재 upstream에 가중치를 곱하는 것 입니다.

366
00:42:07,283 --> 00:42:18,198
우리는 W를 계속해서 곱하기 때문에 Backwork pass에서도
Forward 처럼 점점 gradient값이 작아지게 됩니다.

367
00:42:18,198 --> 00:42:23,541
따라서 upstream gradients는 0으로 수렴하게됩니다.

368
00:42:23,541 --> 00:42:24,869
질문 있나요?

369
00:42:24,869 --> 00:42:28,452
[학생이 질문]

370
00:42:30,731 --> 00:42:37,945
예 물론 upstream과 downstream은 전혀 다른 것입니다.

371
00:42:37,945 --> 00:42:43,907
이 경우에는  backpropb를 하는 것이니 backward pass를
한다고 보시면 됩니다.

372
00:42:43,907 --> 00:42:51,409
upstream 은 gradient이 흘러간다고 보시면 됩니다. 
loss에서부터 시작해서 최초 입력까지 흘러가죠

373
00:42:51,409 --> 00:42:58,684
upstream은 "우리가 이미 계산한 곳" 으로 부터 옵니다.
그리고 현재 노드를 계산하고 또 밑으로 내려가죠

374
00:43:00,270 --> 00:43:07,521
계속 아래로 흐른다고 보시면 됩니다. 그리고 backprop
으로 노트로 흘러 들어온 것이 upstream 입니다.

375
00:43:13,888 --> 00:43:21,102
 자 지금까지는 가중치가 엄청 작을 때 발생하는 문제를 살펴보았습니다.

376
00:43:21,102 --> 00:43:26,133
그렇다면 가중치를 좀 더 큰 값으로 초기화하면 어떨지
한번 생각해 볼 수 있을 것입니다.

377
00:43:26,133 --> 00:43:38,273
자 그럼 가중치의 편차를 0.01 이 아니라 1로 하면
어떻게 될까요?

378
00:43:44,558 --> 00:43:54,750
자 이제 가중치가 큽니다. 이렇게 큰 가중치를 통과한 출력
WX를 구하고 이를 tanh를 거치게 된다면 어떨까요?

379
00:43:54,750 --> 00:44:01,883
앞서 tanh의 입력에 따라 어떤 일이 발생하는지 배웠습니다. 
그렇다면 문제가 뭘까요?

380
00:44:01,883 --> 00:44:06,289
맞습니다. 값들이 saturation 될 것입니다.

381
00:44:06,289 --> 00:44:15,966
가중치가 큰 값을 가지므로 tanh의 출력은
항상 saturation될 것입니다.

382
00:44:15,966 --> 00:44:29,695
그렇게 되면 다음과 같은 결과를 확인 해 볼 수 있는데. 
출력이 항상 -1 이거나 +1 일 것입니다.

383
00:44:30,855 --> 00:44:40,447
앞서 말했듯 값이 saturation 될 것이고 gradient는
0이 될 것입니다. 가중치 업데이트가 일어나지 않겠죠.

384
00:44:41,397 --> 00:44:46,363
결국 적절한 가중치를 얻기는 너무 어렵습니다.

385
00:44:46,363 --> 00:44:50,296
너무 작으면 사라져버리고 
너무 크면 saturation되어 버립니다

386
00:44:50,296 --> 00:44:55,553
그래서 사람들이 어떻게 하면 가중치 초기화를 잘 할 수 있을까
하고 열심히 고민하였습니다.

387
00:44:55,553 --> 00:45:02,507
널리 알려진 좋은 방법 중 하나는 바로 
Xavier initialization 입니다.

388
00:45:02,507 --> 00:45:07,388
Glorot가 2010에 발표한 논문입니다.

389
00:45:07,388 --> 00:45:15,962
자 그럼 맨 위에 W의 공식을 한번 보겠습니다.

390
00:45:17,403 --> 00:45:22,653
Standard gaussian으로 뽑은 값을 
"입력의 수" 로 스케일링해줍니다.

391
00:45:22,653 --> 00:45:28,599
웹에 에된 Lecture note를 보시면 상세한 수식을
확인 해 보실 수 있습니다.

392
00:45:28,599 --> 00:45:35,789
기본적으로 Xavier initialization가 하는 일은 입/출력의
분산을 맞춰주는 것입니다.

393
00:45:35,789 --> 00:45:42,789
그리고 이 수식을 통해서 직관적으로 이해할 수 있는 것은

394
00:45:42,789 --> 00:45:52,654
입력의 수가 작으면 더 작은 값으로 나누고 좀 더 큰 값을 얻습니다.  
그리고 우리는 더 큰 가중치가 필요합니다. 왜냐하면-

395
00:45:52,654 --> 00:45:58,993
작은 입력의 수가 가중치와 곱해지기 때문에, 가중치가 더
커야만 출력의 분산 만큼 큰 값을 얻을 수 있기 때문입니다.

396
00:45:58,993 --> 00:46:08,505
반대로 입력의 수가 많은 경우에는 더 작은 가중치가 필요합니다.

397
00:46:08,505 --> 00:46:10,795
더 자세한 내용은 Lecture Note를 참고하시기 바랍니다.

398
00:46:10,795 --> 00:46:23,150
각 레이어의 입력이 Unit gaussian이길 원한다면 이런 류의
초기화 기법을 사용해 보실 수 있습니다.

399
00:46:23,150 --> 00:46:27,669
각 레이어의 입력을 Unit gaussian스럽게(approximately) 
만들어 줄 수 있습니다.

400
00:46:29,057 --> 00:46:35,032
여기서 가정하는 것은 현재 
Linear activation이 있다고 가정하는 것입니다.

401
00:46:35,032 --> 00:46:40,837
가령 tanh의 경우 우리가 지금 tanh의 active region안에 
있다고 가정하는 것입니다.

402
00:46:40,837 --> 00:46:46,051
다시 한번 말씀드리지만 lecture note를 보시면 
좀 더 자세한 이해를 하실 수 있을 것입니다.

403
00:46:46,051 --> 00:46:51,255
하지만 문제가 하나 있습니다. 
ReLU를 쓰면 잘 동작하지 않는다는 것입니다.

404
00:46:51,255 --> 00:46:54,849
ReLU를 쓰면 무슨 일이 일어나는지를 보면

405
00:46:54,849 --> 00:47:04,685
ReLU는 출력의 절반을 죽입니다. 그 절반은 매번 0이 됩니다.
그래서 결국 출력의 분산을 반토막 내버립니다.

406
00:47:04,685 --> 00:47:16,193
그러므로 이전과 같은 가정을 해버리면 ReLU에서는 잘 작동하지 않습니다.
값이 너무 작아지는 것입니다.

407
00:47:16,193 --> 00:47:23,323
여기 보이는 것이 그런 현상인데 
분포가 또 줄어들기 시작했습니다.

408
00:47:23,323 --> 00:47:28,019
점점 더 많은 값들이 0이 되고
결국은 비활성(deactivated) 되버립니다.

409
00:47:29,541 --> 00:47:41,580
이 문제를 해결하기 위한 일부 논문이 있는데
여기에서는 추가적으로 2를 더 나눠줍니다.

410
00:47:41,580 --> 00:47:47,023
뉴런 들 중 절반이 없어진다는 사실을 고려하기 위해서 입니다.

411
00:47:48,636 --> 00:47:58,122
일제 입력은 반밖에 안들어가므로 반으로 나눠주는 텀을
추가적으로 더해 주는 것이고 실제로 잘 동작합니다.

412
00:47:59,332 --> 00:48:05,348
결과를 보시면 전체적으로 좋은 분포를 형성하고 
있는것을 볼 수 있습니다.

413
00:48:06,959 --> 00:48:16,161
실제로 이 작은 변화는 트레이닝에 있어서 엄청난 차이를 보입니다.

414
00:48:16,161 --> 00:48:28,309
가령 일부 논문을 보면 그런 차이가 트레이닝이 정말 잘 되거나 
하나도 안되거나를 결정하는 결과를 보이기도 합니다.

415
00:48:32,548 --> 00:48:36,321
적절한 초기화는 여전히 활발이 연구되는 분야입니다.

416
00:48:36,321 --> 00:48:40,281
관심 있는 분들은 이 논문과 리소스를 활용하시면 될 것입니다.

417
00:48:40,281 --> 00:48:51,701
초기화는 우선 Xavier Initialization을 추천드리며 
그 밖에 다른 방법을 시도해 보는 것도 좋을 것입니다.

418
00:48:53,871 --> 00:49:01,405
gaussian의 범위로 activation을 유지시키는 것에 관련한
또 다른 아이디어를 하나 소개해 드리겠습니다.

419
00:49:03,330 --> 00:49:09,672
우리는 레이어의 출력이 unit gaussian 이길 바랍니다.
Batch normalization이라 불리는 이 아이디어는 -

420
00:49:09,672 --> 00:49:14,240
그렇다면 그렇게 만들어보자 하는 것입니다. 
그냥 강제로 그렇게 만들어보자

421
00:49:14,240 --> 00:49:15,834
그럼 이것이 어떻게 동작할까요?

422
00:49:15,834 --> 00:49:25,640
어떤 레이어로부터 나온 Batch 단위 만큼의 activations 이 있다고
했을때, 우린 이 값들이 Unit gaussian 이기를 원합니다.

423
00:49:25,640 --> 00:49:29,368
누구나 이 데이터를 Unit Gaussian으로
(empirically) 만들 수 있습니다.

424
00:49:29,368 --> 00:49:39,392
현재 Batch에서 계산한 mean과 variance를 이용해서
Normalization 해 볼 수 있을 것입니다.

425
00:49:39,392 --> 00:49:50,867
가중치를 잘 초기화 시키는 것 대신에 학습 할 때 마다 각 레이어에
이런 일을 해줘서 모든 레이어가 Unit gaussian이 되도록 해줍니다.

426
00:49:50,867 --> 00:49:53,096
그래서 결국 학습하는 동안 모든 레이어의 입력이
Unit gaussian이 됐으면 좋겠는 것입니다.

427
00:49:53,096 --> 00:49:58,336
그래서 이제부터 할 일은 네트워크의 forward pass 동안에
그렇게 되도록 "명시적으로"  만들어 주는 것입니다.

428
00:49:58,336 --> 00:50:06,787
각 뉴런을 평균과 분산으로 Normalization 해주므로서 
이런 일을 함수로(functionally) 구현하는 것입니다.

429
00:50:08,139 --> 00:50:15,754
Batch단위로 한 레이어에 입력으로 들어오는 모든 값들을 이용해서
평균과 분산을 구해서 Normalization 해주는 것입니다.

430
00:50:15,754 --> 00:50:19,928
그리고 이런게 만큼 함수를 보면 
미분 가능한 함수입니다.

431
00:50:19,928 --> 00:50:31,098
평균과 분산을 "상수" 로 가지고만 있으면 언제든지 미분이 
가능하며 따라서 Backprop이 가능하게 됩니다.

432
00:50:33,115 --> 00:50:47,065
여기 보시면 Batch당 N개의 학습 데이터가 있고 
각 데이터가 D차원이라고 해봅시다.

433
00:50:47,065 --> 00:50:56,063
그리고 각 차원별로(feature element별로) 평균을 각각 구해줍니다.

434
00:50:56,063 --> 00:51:02,406
한 Batch 내에 이걸 전부 계산해서 Normalize 합니다.

435
00:51:05,786 --> 00:51:09,988
그리고 이 연산은 FC나 Cov Layer 직후에 넣어줍니다.

436
00:51:09,988 --> 00:51:18,932
깊은 네트워크에서 각 레이어의 W가 지속적으로 곱해져서 
Bad scaling effect가 발생했습니다만

437
00:51:18,932 --> 00:51:22,731
Normalization은 그 bad effect를 상쇄시켜 버립니다.

438
00:51:22,731 --> 00:51:37,132
BN은 입력의 스케일만 살짝 조정해 주는 역할이기 때문에
FC와  Conv 어디에든 적용할 수 있습니다.

439
00:51:37,132 --> 00:51:45,895
Conv Layer에서 차이점이 있다면 Normalization을 차원마다 
독립적으로 수행하는 것이 아니라

440
00:51:45,895 --> 00:51:58,895
같은 Activation Map의 같은 채널에 있는 요소들은 같이
Normalize 해 줍니다.

441
00:51:58,895 --> 00:52:05,903
왜냐하면 Conv 특성상 (convolutional property) 
같은 방식으로 normalize 시켜야 하기 떄문입니다.

442
00:52:05,903 --> 00:52:13,489
Conv Layer의 경우 Activation map(채널, Depth)
마다 평균과 분산을 하나만 구합니다.

443
00:52:13,489 --> 00:52:18,094
그리고 현재 Batch 에 있는 모든 데이터로 Normalize 해줍니다.

444
00:52:18,094 --> 00:52:23,098
Batch Normalization는 숙제로 나갈 것입니다.

445
00:52:23,098 --> 00:52:29,367
Batch Normalization 2015년도 논문에
더 자세하게 설명이 되어 있습니다.

446
00:52:29,367 --> 00:52:35,621
Batch Normalization 은 아주 유용한 기법이고
실제로 정말 많이 사용하는 방법입니다.

447
00:52:35,621 --> 00:52:46,129
BN을 사용하려면 논문을 읽어보셔야 합니다. 
BN의 흐름도 한번 살펴보시고

448
00:52:46,129 --> 00:52:53,718
BN에서 Gradient를 어떻게 구하는 지도 
한번 살펴보시길 바랍니다.

449
00:52:56,626 --> 00:52:59,993
그런데 한가지 문제가 있습니다.

450
00:52:59,993 --> 00:53:05,930
FC layer를 거칠때마다 매번 normalization를 해주는
것에 대한 의문이 있습니다.

451
00:53:05,930 --> 00:53:12,031
우리가 정말 tanh의 입력이 unit gaussian 
이기를 바라는 것일까요?

452
00:53:12,031 --> 00:53:17,107
normalization이 하는 일은 입력이 tanh의 linear한 영역에만
존재하도록 강제하는 것입니다.

453
00:53:17,107 --> 00:53:21,974
그렇게 되면 saturation이 전혀 일어나지 않게 됩니다.

454
00:53:21,974 --> 00:53:30,821
saturation 이 전혀 일어나지 않는것 보다는 "얼마나"
saturation이 일어날지를 우리가 조절하면 더 좋겠죠

455
00:53:31,845 --> 00:53:39,512
Batch normalization에서는 normalization연산이 있었죠
(왼쪽 상단 Narmalize: 수식)

456
00:53:39,512 --> 00:53:44,453
그 이후에 BN에서는 scaling 연산을 추가합니다.

457
00:53:44,453 --> 00:53:52,515
이를 통해 Unit gaussian으로 normalize 된 값들을 
감마로는 스케일링의 효과를, 베타는 이동의 효과를 줍니다.

458
00:53:53,349 --> 00:54:02,071
이렇게 하면 normalized 된 값들을 다시 원상복구 
할 수 있도록 해줍니다. 그렇게 하고 싶다면 말이죠

459
00:54:02,071 --> 00:54:10,613
네트워크가 값 들을 원상복구 하고 싶다면 
"감마 = 분산", "베타 = 평균" 로 하면 됩니다.

460
00:54:10,613 --> 00:54:16,659
 BN을 하기 전 값으로 돌아가는 것입니다.

461
00:54:16,659 --> 00:54:32,225
네트워크가 데이터를 tanh에 얼마나 saturation 시킬지를 학습
하기 때문에 우리는 유연성을 얻을 수 있습니다.

462
00:54:38,166 --> 00:54:42,285
 Batch normalization를 다시한번 요약해 보자면

463
00:54:42,285 --> 00:54:52,906
입력이 있고 우리는 mini-batch에서의 평균을 계산합니다. 모든
mini-batch 마다 각각 계산해 줍니다. 분산도 계산합니다.

464
00:54:52,906 --> 00:54:58,342
그리고 평균과 분산으로 Normalize 한 이후에 
다시 추가적인 scaling, shifting factor를 사용합니다.

465
00:54:58,342 --> 00:55:05,484
BN은 gradient의 흐름을 보다 원활하게 해주며 
결국 더 학습이 더 잘되게(robust) 해줍니다.

466
00:55:05,484 --> 00:55:10,562
BN을 쓰면 learning rates를 더 키울 수도 있고 다양한
초기화 기법들도 사용해 볼 수 있습니다.

467
00:55:10,562 --> 00:55:16,955
그래서 사람들이 BN을 쓰면 학습이 더 쉬워진다고 합니다.
BN를 써야하는 이유죠.

468
00:55:16,955 --> 00:55:27,162
그리고 또 하나 짚고 넘어가야 할 것은 BN이 
regularization의 역할도 한다는 것입니다.

469
00:55:27,162 --> 00:55:31,329
이제는 각 레이어의 출력물에 있기 때문에,

470
00:55:32,189 --> 00:55:35,706
이러한 각각의 활성화들, 이들 각각의 출력들,

471
00:55:35,706 --> 00:55:38,282
귀하의 입력 X,

472
00:55:38,282 --> 00:55:41,165
배치의 다른 예제들

473
00:55:41,165 --> 00:55:42,733
그것은, 오른쪽으로,

474
00:55:42,733 --> 00:55:48,266
왜냐하면 각 입력은 batch의 평균으로 normalize하기 때문입니다.

475
00:55:48,266 --> 00:55:54,021
이 때문에 각각의 학습 데이터에서 발생하는 
deterministic한 값을이 없어지고

476
00:55:54,021 --> 00:55:57,543
하나의 batch 묶음으로 다 같이 묶이는 것입니다.

477
00:55:57,543 --> 00:56:07,215
더이상 deterministic하지 않기 때문에
regularization의 효과를 주는 것입니다.

478
00:56:08,941 --> 00:56:10,490
질문있나요?

479
00:56:10,490 --> 00:56:13,401
[학생이 질문]

480
00:56:13,401 --> 00:56:17,354
질문은 감마와 베타가 학습가능한 파라미터 냐는 것인데
예 맞습니다.

481
00:56:17,354 --> 00:56:20,937
[학생이 질문]

482
00:56:27,754 --> 00:56:34,618
질문은, 왜

483
00:56:34,618 --> 00:56:36,484
그 이유는

484
00:56:36,484 --> 00:56:38,481
당신은 그것에게 유연성을주고 싶다.

485
00:56:38,481 --> 00:56:41,518
맞아, 일괄 정규화가하고있는 일,

486
00:56:41,518 --> 00:56:46,080
우리 데이터가이 단위 가우시안이되도록 강요하고 있습니까?

487
00:56:46,080 --> 00:56:48,381
우리의 입력은 단위 가우스,

488
00:56:48,381 --> 00:56:50,890
그러나 일반적으로 이것은 좋은 생각이지만,

489
00:56:50,890 --> 00:56:54,232
이것이 항상 최선의 방법은 아닙니다.

490
00:56:54,232 --> 00:56:57,240
그리고 우리는 특히 tanh와 같은 것을 보았습니다.

491
00:56:57,240 --> 00:56:58,379
너는 통제하고 싶을지도 모른다.

492
00:56:58,379 --> 00:57:00,279
당신이 가진 채도의 정도.

493
00:57:00,279 --> 00:57:02,720
그리고 이것이하는 일은 유연성을 제공합니다.

494
00:57:02,720 --> 00:57:07,018
단위 가우스 정규화와 똑같이하는 것,

495
00:57:07,018 --> 00:57:09,979
원한다면 어쩌면 그걸 배우 겠지.

496
00:57:09,979 --> 00:57:12,101
네트워크의이 특정 부분에서,

497
00:57:12,101 --> 00:57:14,195
어쩌면 그렇게하는 것이 최선의 방법이 아닙니다.

498
00:57:14,195 --> 00:57:16,926
어쩌면 우리는이 일반적인 생각에
여전히 어떤 것을 원할 것입니다.

499
00:57:16,926 --> 00:57:19,838
약간 다른 권리, 약간의 규모 또는 이동.

500
00:57:19,838 --> 00:57:23,125
따라서 이러한 매개 변수는 유연성을 제공합니다.

501
00:57:23,125 --> 00:57:25,968
원한다면 배우기.

502
00:57:25,968 --> 00:57:28,184
그리고 나서 네가 할 수있는 최선의 일이라면

503
00:57:28,184 --> 00:57:31,712
그냥 일괄 정규화 다음 배울 것입니다

504
00:57:31,712 --> 00:57:34,457
그것에 대한 올바른 매개 변수.

505
00:57:34,457 --> 00:57:35,665
네?

506
00:57:35,665 --> 00:57:39,710
[마이크 끄기 학생]

507
00:57:39,710 --> 00:57:42,877
그래, 기본적으로 각 신경 출력.

508
00:57:44,164 --> 00:57:46,424
그래서, 우리는 완전히 연결된
레이어의 출력을 가지고 있습니다.

509
00:57:46,424 --> 00:57:48,366
우리는 W 곱하기 X를가집니다.

510
00:57:48,366 --> 00:57:51,347
그래서 우리는이 각각의 산출 값을 가지고 있습니다,

511
00:57:51,347 --> 00:57:52,650
그런 다음 우리는 신청할 것입니다.

512
00:57:52,650 --> 00:57:57,365
각 뉴런에 개별적으로 일괄 정규화.

513
00:57:57,365 --> 00:57:58,835
문제?

514
00:57:58,835 --> 00:58:02,418
[마이크 끄기 학생]

515
00:58:10,031 --> 00:58:11,445
그래, 문제는

516
00:58:11,445 --> 00:58:13,023
강화 학습,

517
00:58:13,023 --> 00:58:14,825
당신은 정말로 작은 배치 크기를 가질 것입니다.

518
00:58:14,825 --> 00:58:17,517
이걸 어떻게 처리하니?

519
00:58:17,517 --> 00:58:21,149
그래서 실제로, 배치 정규화가 사용 된 것 같습니다.

520
00:58:21,149 --> 00:58:24,324
표준 길쌈 신경 네트워크와 같은 많은 것들,

521
00:58:24,324 --> 00:58:27,595
우리가 어떻게하고 싶은지에 대한 논문이 실제로 있습니다.

522
00:58:27,595 --> 00:58:30,900
여러 종류의 반복 네트워크에 대한 정규화,

523
00:58:30,900 --> 00:58:32,203
또는이 네트워크 중 일부를 알고 있습니다.

524
00:58:32,203 --> 00:58:34,520
그것은 또한 보강 학습에있을 수도 있습니다.

525
00:58:34,520 --> 00:58:36,240
그래서 다른 고려 사항이 있습니다.

526
00:58:36,240 --> 00:58:37,543
네가 거기서 생각하고 싶어 할지도 몰라.

527
00:58:37,543 --> 00:58:40,532
그리고 이것은 여전히 활발한 연구 분야입니다.

528
00:58:40,532 --> 00:58:42,681
이것에 대한 논문이 있습니다.

529
00:58:42,681 --> 00:58:44,442
나중에 이것에 대해 좀 더 이야기 해보십시오.

530
00:58:44,442 --> 00:58:47,783
그러나 전형적인 길쌈 신경 네트워크를 위해

531
00:58:47,783 --> 00:58:49,490
이 일반적으로 잘 작동합니다.

532
00:58:49,490 --> 00:58:52,042
그리고 작은 배치 크기가 있다면,

533
00:58:52,042 --> 00:58:55,657
어쩌면 이것이 조금 덜 정확해질 수 있습니다.

534
00:58:55,657 --> 00:58:57,741
그러나 당신은 여전히 같은 효과를 얻습니다.

535
00:58:57,741 --> 00:58:59,599
그리고 가능하다는 것도 알고 있습니다.

536
00:58:59,599 --> 00:59:02,840
당신은 당신의 평균과 분산을 설계 할 수 있습니다.

537
00:59:02,840 --> 00:59:06,088
아마도 더 많은 예제를 통해 계산 될 수 있습니다.

538
00:59:06,088 --> 00:59:08,514
실제로 나는 보통 괜찮다고 생각합니다.

539
00:59:08,514 --> 00:59:09,942
그래서 너는 이것을 너무 많이 보지 않는다.

540
00:59:09,942 --> 00:59:12,994
하지만 이것은 아마도 도움이 될만한 것입니다.

541
00:59:12,994 --> 00:59:14,755
그게 문제 였다면.

542
00:59:14,755 --> 00:59:16,128
그래, 질문 있니?

543
00:59:16,128 --> 00:59:19,711
[마이크 끄기 학생]

544
00:59:24,947 --> 00:59:27,180
그래서 질문,

545
00:59:27,180 --> 00:59:30,896
그래서 질문은 우리가 입력을 가우스로 강제한다면,

546
00:59:30,896 --> 00:59:32,979
우리가 그 구조를 잃어 버렸습니까?

547
00:59:35,211 --> 00:59:39,056
그래서, 어떤 의미에서 당신이
생각할 수있는 것은 아닙니다.

548
00:59:39,056 --> 00:59:41,608
모든 기능을 배포했다면

549
00:59:41,608 --> 00:59:42,898
예를 들어 가우스 (gaussian)

550
00:59:42,898 --> 00:59:45,221
데이터 사전 처리 만하는 경우에도,

551
00:59:45,221 --> 00:59:47,925
이 가우스는 당신에게 어떤 구조도 잃지 않습니다.

552
00:59:47,925 --> 00:59:50,879
모든 것, 그것은 단지 변화하고있다.

553
00:59:50,879 --> 00:59:52,778
데이터를 정권으로 확장하고,

554
00:59:52,778 --> 00:59:55,689
운영에 잘 작동하는

555
00:59:55,689 --> 00:59:57,913
당신이 그것을 수행 할 것입니다.

556
00:59:57,913 --> 01:00:00,604
길쌈 계층에서, 당신은 어떤 구조를 가지고 있습니다,

557
01:00:00,604 --> 01:00:03,169
당신이 공간적으로 보존하기를 원하는 것입니다.

558
01:00:03,169 --> 01:00:05,967
활성화 맵을 보는 것처럼,

559
01:00:05,967 --> 01:00:09,156
당신은 그들 모두가 상대적으로
모든 것을 이해하기를 원합니다.

560
01:00:09,156 --> 01:00:11,581
그래서이 경우에는

561
01:00:11,581 --> 01:00:12,899
그것을 고려하십시오.

562
01:00:12,899 --> 01:00:15,077
이제는 정상화 할 것입니다.

563
01:00:15,077 --> 01:00:17,823
전체 정품 인증 맵에 대해 하나의 의미를 찾고,

564
01:00:17,823 --> 01:00:20,749
그래서 우리는 경험적 평균과 분산만을 발견 할 수있다.

565
01:00:20,749 --> 01:00:22,815
over 훈련 예.

566
01:00:22,815 --> 01:00:25,022
그리고 그것은 뭔가입니다.

567
01:00:25,022 --> 01:00:27,340
네가 네 숙제에서 할 수있는 일,

568
01:00:27,340 --> 01:00:30,405
또한이 논문에서도 설명했다.

569
01:00:30,405 --> 01:00:32,455
그래서, 당신은 그것을 참조해야합니다.

570
01:00:32,455 --> 01:00:33,288
예.

571
01:00:34,287 --> 01:00:37,870
[마이크 끄기 학생]

572
01:00:43,065 --> 01:00:45,922
그래서 문제는 우리가 체중을 표준화하는 것입니까?

573
01:00:45,922 --> 01:00:47,849
그래서 가우스가된다.

574
01:00:47,849 --> 01:00:49,665
따라서, 귀하의 질문을 정확하게 이해한다면,

575
01:00:49,665 --> 01:00:51,634
그 때 대답은,

576
01:00:51,634 --> 01:00:54,560
우리는 각 레이어의 입력을 정규화하고 있습니다.

577
01:00:54,560 --> 01:00:58,727
그래서 우리는이 과정에서 가중치를 변경하지 않습니다.

578
01:01:00,895 --> 01:01:04,562
[마이크 끄기 학생]

579
01:01:15,208 --> 01:01:18,023
그래, 문제는 일단 평균을 빼면

580
01:01:18,023 --> 01:01:20,033
표준 편차로 나누면,

581
01:01:20,033 --> 01:01:24,512
이것이 가우스 적 (gaussian)이되고
대답은 '예'입니다.

582
01:01:24,512 --> 01:01:28,605
따라서 현재 진행중인 작업에 대해 생각해 보면,

583
01:01:28,605 --> 01:01:30,960
기본적으로 당신은 평균에 의해 바뀌고 있습니다.

584
01:01:30,960 --> 01:01:33,843
그리고 이러한 변화는 제로 중심으로 진행됩니다.

585
01:01:33,843 --> 01:01:36,410
그런 다음 표준 편차로 비율을 조정합니다.

586
01:01:36,410 --> 01:01:40,243
이제 이것을 단위 가우스로 변환합니다.

587
01:01:41,249 --> 01:01:45,229
그리고 만약 당신이 그것에 대해 더 많이보고 싶다면,

588
01:01:45,229 --> 01:01:46,324
너가 볼 수 있다고 생각해.

589
01:01:46,324 --> 01:01:48,630
기계 학습에 관한 많은 설명이 있습니다.

590
01:01:48,630 --> 01:01:51,139
이게 정확히 무슨 일인지,

591
01:01:51,139 --> 01:01:52,942
이 작업으로 시각화하면,

592
01:01:52,942 --> 01:01:54,980
하지만 이건 기본적으로 데이터를 필요로합니다.

593
01:01:54,980 --> 01:01:58,563
이것을 가우시안 분포로 바꾼다.

594
01:02:00,458 --> 01:02:02,375
그래, 그래 그래?

595
01:02:03,436 --> 01:02:07,019
[마이크 끄기 학생]

596
01:02:08,262 --> 01:02:09,095
어 허.

597
01:02:26,194 --> 01:02:28,008
그래서 질문은,

598
01:02:28,008 --> 01:02:29,880
우리가 변화와 규모를 수행하려고한다면,

599
01:02:29,880 --> 01:02:32,944
그리고 이것들을 배우는 것은 일괄 정규화가 중복되어,

600
01:02:32,944 --> 01:02:35,634
ID 매핑을 복구 할 수 있기 때문에?

601
01:02:35,634 --> 01:02:38,324
따라서 네트워크가 학습하는 경우

602
01:02:38,324 --> 01:02:40,571
그 신원 매핑은 항상 최고입니다,

603
01:02:40,571 --> 01:02:41,985
그리고이 매개 변수들을 배우고,

604
01:02:41,985 --> 01:02:44,523
예, 일괄 정규화에 대한 아무런 의미가 없을 것입니다.

605
01:02:44,523 --> 01:02:46,215
그러나 실제로 이것은 일어나지 않습니다.

606
01:02:46,215 --> 01:02:49,818
그래서 실제로, 우리는이 감마와 베타를 배울 것입니다.

607
01:02:49,818 --> 01:02:52,579
이는 ID 매핑과 동일하지 않습니다.

608
01:02:52,579 --> 01:02:55,242
그래서, 그것은 약간의 금액만큼 이동하고 확장 할 것이고,

609
01:02:55,242 --> 01:02:56,157
금액은 아니지만

610
01:02:56,157 --> 01:02:58,858
그것은 당신에게 신원 매핑을 줄 것입니다.

611
01:02:58,858 --> 01:02:59,873
그래서 당신이 얻는 것은

612
01:02:59,873 --> 01:03:03,201
당신은 여전히이 일괄 정규화 효과를 얻습니다.

613
01:03:03,201 --> 01:03:05,753
맞아요, 그래서이 정체성 매핑을 거기에 가지고있어서,

614
01:03:05,753 --> 01:03:09,938
나는 극단적으로 말하기 위해서 이것을 여기에두고있다.

615
01:03:09,938 --> 01:03:12,033
그것은 신원 매핑을 배울 수 있었고,

616
01:03:12,033 --> 01:03:14,266
그러나 실제로는 그렇지 않습니다.

617
01:03:14,266 --> 01:03:15,970
그래, 질문.

618
01:03:15,970 --> 01:03:19,553
[마이크 끄기 학생]

619
01:03:21,368 --> 01:03:22,561
네.

620
01:03:22,561 --> 01:03:26,144
[마이크 끄기 학생]

621
01:03:30,825 --> 01:03:32,242
아 맞다.

622
01:03:33,709 --> 01:03:36,177
네, 죄송합니다, 이것에 대해서는 명확하지 않았습니다.

623
01:03:36,177 --> 01:03:37,405
하지만 그래,이게 관련이 있다고 생각해.

624
01:03:37,405 --> 01:03:38,972
이전에 다른 질문에,

625
01:03:38,972 --> 01:03:40,886
우리가이 일을 할 때 예.

626
01:03:40,886 --> 01:03:43,285
우리는 실제로 제로 평균과 단위 가우스를 얻고 있습니다.

627
01:03:43,285 --> 01:03:45,240
이것을 멋진 모양으로 만들었습니다.

628
01:03:45,240 --> 01:03:49,814
실제로 가우스가 될 필요는 없습니다.

629
01:03:49,814 --> 01:03:51,438
그래서 그래, 나는 이상적으로,

630
01:03:51,438 --> 01:03:56,001
우리가 입력하는 것과 같은 입력을보고 있다면, 아시다시피,

631
01:03:56,001 --> 01:03:57,830
일종의 가우스 (gaussian)

632
01:03:57,830 --> 01:03:59,925
우리는 이런 종류의 효과를 원합니다.

633
01:03:59,925 --> 01:04:03,592
하지만 그래, 실제로 그것은 필요하지 않습니다.

634
01:04:06,658 --> 01:04:07,741
좋아, 그럼...

635
01:04:09,438 --> 01:04:11,547
좋아요, 그래서 이것에 대해 제가 언급하고 싶은 마지막 것

636
01:04:11,547 --> 01:04:15,714
그렇기 때문에 테스트 시간에 일괄 정규화 레이어,

637
01:04:17,064 --> 01:04:19,564
우리는 이제 경험적 평균을 취한다.

638
01:04:21,464 --> 01:04:24,839
및 훈련 데이터로부터의 분산.

639
01:04:24,839 --> 01:04:26,932
따라서 우리는 테스트시 이것을 다시 계산하지 않습니다.

640
01:04:26,932 --> 01:04:29,010
우리는 훈련 시간에 이것을 추정합니다.

641
01:04:29,010 --> 01:04:31,189
예를 들어, 러닝 평균 (running average)

642
01:04:31,189 --> 01:04:35,128
그리고 나서 우리는 테스트 시간에 이것을 사용할 것입니다.

643
01:04:35,128 --> 01:04:38,295
그래서, 우리는 그것으로 확장 할 것입니다.

644
01:04:40,078 --> 01:04:41,684
좋아, 이제 나는 계속 나아갈거야.

645
01:04:41,684 --> 01:04:43,725
학습 과정을 보육하는 것.

646
01:04:43,725 --> 01:04:46,110
그렇습니다. 이제 우리는 네트워크 아키텍처를 정의했습니다.

647
01:04:46,110 --> 01:04:49,784
우리는 어떻게 우리가 훈련을 모니터 할 지,

648
01:04:49,784 --> 01:04:54,264
우리가가는 동안 하이퍼 파라미터를 조정하는 방법,

649
01:04:54,264 --> 01:04:56,681
좋은 학습 결과를 얻으려면?

650
01:04:58,091 --> 01:05:00,184
항상 그렇듯이 우리가하고 싶은 첫 번째 단계는

651
01:05:00,184 --> 01:05:02,251
우리는 데이터를 전처리하고 싶습니다.

652
01:05:02,251 --> 01:05:04,192
맞아, 우리는 0을 의미한다.

653
01:05:04,192 --> 01:05:05,773
우리가 이전에 말했던 것처럼.

654
01:05:05,773 --> 01:05:07,381
그런 다음 우리는 아키텍처를 선택하고자합니다.

655
01:05:07,381 --> 01:05:11,458
여기서 우리는 하나의 숨겨진 레이어로 시작합니다.

656
01:05:11,458 --> 01:05:13,455
예를 들어, 50 개의 뉴런 중에서,

657
01:05:13,455 --> 01:05:16,700
하지만 우리는 기본적으로 모든
아키텍처를 선택할 수 있습니다.

658
01:05:16,700 --> 01:05:18,950
우리가 시작하고 싶습니다.

659
01:05:20,223 --> 01:05:21,863
그리고 나서 우리가하고 싶은 첫 번째 일

660
01:05:21,863 --> 01:05:23,934
우리는 네트워크를 초기화하고 있습니까?

661
01:05:23,934 --> 01:05:25,858
우리는 그것을 통해 전달 전달,

662
01:05:25,858 --> 01:05:28,600
우리는 우리의 손실이 합리적임을 확인하기를 원합니다.

663
01:05:28,600 --> 01:05:30,773
그래서 우리는 몇 가지 강의를 전에 이야기했습니다.

664
01:05:30,773 --> 01:05:33,106
우리는 기본적으로,

665
01:05:34,902 --> 01:05:37,493
여기 Softmax 분류기가 있다고 가정 해 봅시다.

666
01:05:37,493 --> 01:05:39,480
우리는 우리의 손실이 무엇인지 알고 있습니다.

667
01:05:39,480 --> 01:05:41,013
우리의 무게가 작을 때,

668
01:05:41,013 --> 01:05:44,012
일반적으로 확산 분포가 있습니다.

669
01:05:44,012 --> 01:05:48,133
그런 다음 Softmax 분류기가 손실되기를 원합니다.

670
01:05:48,133 --> 01:05:50,293
귀하의 부정적인 대립 가능성이 될 것입니다.

671
01:05:50,293 --> 01:05:51,920
우리가 10 개의 과목을 가지고 있다면,

672
01:05:51,920 --> 01:05:54,826
그것은 10 이상의 하나의 부정적인
로그와 같은 것이 될 것입니다.

673
01:05:54,826 --> 01:05:58,986
여기 2.3 정도입니다. 그래서 우리는

674
01:05:58,986 --> 01:06:03,213
우리의 상실은 우리가 기대하는 바입니다.

675
01:06:03,213 --> 01:06:07,629
그래서, 이것은 우리가 원했던 좋은 정신 체크입니다.

676
01:06:07,629 --> 01:06:09,453
항상, 항상 그래야합니다.

677
01:06:09,453 --> 01:06:12,253
자, 이제 우리가 원래의 손실이 좋았다는 것을 알게되면,

678
01:06:12,253 --> 01:06:13,503
이제 우리가 원한다.

679
01:06:14,853 --> 01:06:17,946
그래서 우리는 먼저 정규화가 0 인 이것을하고 싶습니다.

680
01:06:17,946 --> 01:06:20,039
그래서 우리가 정규화를 불가능하게 할 때,

681
01:06:20,039 --> 01:06:22,613
이제 우리의 유일한 손실 기간은이 데이터 손실입니다.

682
01:06:22,613 --> 01:06:25,463
여기에 2.3을 줄 예정입니다.

683
01:06:25,463 --> 01:06:28,893
그리고 여기에서 우리는 이제 정규화를 시작하려고합니다.

684
01:06:28,893 --> 01:06:33,039
우리가 그렇게 할 때, 우리는 우리의
손실이 증가한다는 것을보고 싶습니다.

685
01:06:33,039 --> 01:06:36,226
이 추가 정규화 용어를 추가했기 때문입니다.

686
01:06:36,226 --> 01:06:37,693
그래서, 이것은 좋은 다음 단계입니다.

687
01:06:37,693 --> 01:06:40,879
당신의 정신 건강 체크를 위해 할 수있는 일.

688
01:06:40,879 --> 01:06:43,559
그리고 이제 우리는 훈련을 시작할 수 있습니다.

689
01:06:43,559 --> 01:06:46,309
이제 우리는 훈련을 시작합니다.

690
01:06:47,331 --> 01:06:49,719
우리가하는 일은 좋은 방법입니다.

691
01:06:49,719 --> 01:06:53,026
매우 적은 양의 데이터로 시작하는 것입니다.

692
01:06:53,026 --> 01:06:55,491
아주 작은 훈련 세트를 가지고 있다면,

693
01:06:55,491 --> 01:06:57,678
너는 이것에 아주 잘 맞을 수 있어야한다.

694
01:06:57,678 --> 01:07:00,944
여기에 아주 좋은 훈련 손실을.

695
01:07:00,944 --> 01:07:03,527
이 경우에는

696
01:07:04,554 --> 01:07:06,530
우리의 정규화를 다시 끄십시오,

697
01:07:06,530 --> 01:07:10,697
손실을 0으로 만들 수 있는지 확인하십시오.

698
01:07:12,199 --> 01:07:14,293
그래서 우리는 우리의 손실이 어떻게
변하는지를 볼 수 있습니다.

699
01:07:14,293 --> 01:07:16,346
우리는이 모든 신 (新) 시대를 가지고 있기 때문입니다.

700
01:07:16,346 --> 01:07:19,138
우리는 각 시대마다 우리의 손실을 계산합니다.

701
01:07:19,138 --> 01:07:21,961
그리고 우리는 이것이 0으로 내려가는 것을보고 싶습니다.

702
01:07:21,961 --> 01:07:23,151
맞아. 여기 우리가 볼 수있어.

703
01:07:23,151 --> 01:07:24,406
우리의 훈련 정확도

704
01:07:24,406 --> 01:07:27,124
모든 것이 하나가 될 것이고, 이것은 당연한 것입니다.

705
01:07:27,124 --> 01:07:28,980
데이터의 수가 매우 적은 경우,

706
01:07:28,980 --> 01:07:32,813
당신은 이것에 완벽하게 맞을 수 있어야합니다.

707
01:07:34,726 --> 01:07:36,033
자, 이제 일단 그렇게하면,

708
01:07:36,033 --> 01:07:38,700
이들은 모두 온 전성 검사입니다.

709
01:07:38,700 --> 01:07:40,366
이제 정말로 훈련을 시작할 수 있습니다.

710
01:07:40,366 --> 01:07:43,540
이제 전체 교육 데이터를 가져올 수 있습니다.

711
01:07:43,540 --> 01:07:46,460
이제는 작은 양의 정규화로 시작합니다.

712
01:07:46,460 --> 01:07:49,480
우선 좋은 학습 속도가 무엇인지 알아 봅시다.

713
01:07:49,480 --> 01:07:50,813
따라서 학습률은 가장

714
01:07:50,813 --> 01:07:52,184
중요한 하이퍼 매개 변수,

715
01:07:52,184 --> 01:07:54,942
그리고 그것은 당신이 먼저 조정하기를 원하는 것입니다.

716
01:07:54,942 --> 01:07:58,196
그래서, 당신은 학습 속도의 가치를 시험하고 싶습니다.

717
01:07:58,196 --> 01:08:00,954
그리고 여기 나는 하나의 E negative 6을 시도했다.

718
01:08:00,954 --> 01:08:04,096
손실이 거의 변하지 않았 음을 알 수 있습니다.

719
01:08:04,096 --> 01:08:06,076
맞아요. 그리고 이것이 간신히 변하는 이유입니다.

720
01:08:06,076 --> 01:08:10,244
학습 속도가 너무 낮기 때문에 보통입니다.

721
01:08:10,244 --> 01:08:11,380
그래서 너무 작 으면,

722
01:08:11,380 --> 01:08:12,862
그래디언트 업데이트로는 충분하지 않습니다.

723
01:08:12,862 --> 01:08:16,362
비용은 기본적으로 거의 같습니다.

724
01:08:17,423 --> 01:08:18,256
그래,

725
01:08:21,151 --> 01:08:24,020
제가 여기서 지적하고자하는 한가지,

726
01:08:24,020 --> 01:08:25,699
비록 우리가

727
01:08:25,699 --> 01:08:27,239
간신히 변화하면서 우리의 손실,

728
01:08:27,239 --> 01:08:29,694
훈련 및 검증 정확성

729
01:08:29,694 --> 01:08:32,701
매우 빠르게 20 %까지 뛰었습니다.

730
01:08:32,701 --> 01:08:35,568
그리고 누구도 아이디어를 가지고 있지 않습니다.

731
01:08:35,569 --> 01:08:38,152
왜 이것이 사실일까요?

732
01:08:40,089 --> 01:08:42,460
왜, Softmax 기능이 있다는 것을 기억하십시오.

733
01:08:42,460 --> 01:08:43,736
우리의 손실은 실제로 변하지 않았습니다.

734
01:08:43,737 --> 01:08:46,404
그러나 우리의 정확성은 많이 향상되었습니다.

735
01:08:50,263 --> 01:08:53,820
좋아, 그 이유는

736
01:08:53,820 --> 01:08:56,857
여기서 확률은 여전히 꽤 분산되어 있습니다.

737
01:08:56,857 --> 01:08:59,727
그래서 우리의 손실 기간은 여전히 꽤 유사합니다.

738
01:08:59,727 --> 01:09:03,368
그러나이 모든 확률을 바꿀 때

739
01:09:03,368 --> 01:09:05,076
약간 오른쪽 방향으로,

740
01:09:05,076 --> 01:09:06,183
우리가 올바르게 배우기 때문에?

741
01:09:06,183 --> 01:09:08,210
우리의 무게는 올바른 방향으로 변화하고 있습니다.

742
01:09:08,210 --> 01:09:11,954
이제 갑작스런 정확성이 뛰어 날 수 있습니다.

743
01:09:11,954 --> 01:09:14,866
우리는 최대 정확한 값을 취하고 있기 때문에,

744
01:09:14,866 --> 01:09:17,902
그래서 우리는 정확도가 크게 향상 될 것입니다.

745
01:09:17,902 --> 01:09:21,985
비록 우리의 손실은 여전히 상대적으로 확산되어 있습니다.

746
01:09:23,588 --> 01:09:25,995
자, 이제 우리가 다른 학습 속도를 시도한다면,

747
01:09:25,995 --> 01:09:28,122
지금 나는 여기에서 다른 극단에 뛰어 들고있다.

748
01:09:28,123 --> 01:09:31,326
매우 큰 학습 속도, 한 E 부정 6.

749
01:09:31,326 --> 01:09:36,007
현재 비용은 우리에게 NaN을 제공하고 있습니다.

750
01:09:36,007 --> 01:09:37,794
그리고 당신이 NaN을 가질 때
이것이 보통 무엇을 의미하는지

751
01:09:37,794 --> 01:09:41,413
기본적으로 비용이 폭발했습니다.

752
01:09:41,413 --> 01:09:45,195
그리고 그 이유는 일반적으로

753
01:09:45,195 --> 01:09:47,862
학습률이 너무 높습니다.

754
01:09:49,350 --> 01:09:51,236
따라서 학습 속도를 다시 낮출 수 있습니다.

755
01:09:51,236 --> 01:09:53,683
여기서 나는 우리가 시도하고 있음을 볼 수있다.

756
01:09:53,683 --> 01:09:55,225
세 E는 음수 세.

757
01:09:55,225 --> 01:09:57,006
비용은 여전히 폭발적이다.

758
01:09:57,006 --> 01:09:59,741
그래서 일반적으로 학습 속도의 대략적인 범위

759
01:09:59,741 --> 01:10:00,981
우리가보고 싶어하는

760
01:10:00,981 --> 01:10:04,901
하나의 E 음수 3과 1 E 음수 5 사이에 있습니다.

761
01:10:04,901 --> 01:10:08,402
그리고 이것은 우리가 원하는 거친 범위입니다.

762
01:10:08,402 --> 01:10:09,628
사이의 교차 유효성 검사.

763
01:10:09,628 --> 01:10:11,840
따라서이 범위의 값을 시험해보고 싶습니다.

764
01:10:11,840 --> 01:10:14,837
당신의 손실이 너무 느린 지 여부에 따라,

765
01:10:14,837 --> 01:10:17,011
또는 너무 작 으면, 또는 너무 큰지 여부,

766
01:10:17,011 --> 01:10:19,011
이것을 기초로 조정하십시오.

767
01:10:21,228 --> 01:10:24,399
그렇다면이 하이퍼 매개 변수를 정확히
선택하는 방법은 무엇입니까?

768
01:10:24,399 --> 01:10:26,659
하이퍼 매개 변수 최적화,

769
01:10:26,659 --> 01:10:31,139
이러한 하이퍼 매개 변수의 모든 값을 선택하십시오.

770
01:10:31,139 --> 01:10:33,067
그래서, 우리가 사용할 전략

771
01:10:33,067 --> 01:10:35,979
예를 들어, 학습 속도와 같은 임의의
하이퍼 파라미터에 대한 것이고,

772
01:10:35,979 --> 01:10:37,575
교차 유효성 검사를 수행하는 것입니다.

773
01:10:37,575 --> 01:10:40,485
따라서 교차 검증은 교육 세트에 대한 교육입니다.

774
01:10:40,485 --> 01:10:43,472
그런 다음 유효성 검사 집합에 대해 평가합니다.

775
01:10:43,472 --> 01:10:45,469
이 하이퍼 매개 변수는 얼마나 잘합니까?

776
01:10:45,469 --> 01:10:47,092
너희들이 벌써 한 짓이야.

777
01:10:47,092 --> 01:10:48,960
너의 임무에서.

778
01:10:48,960 --> 01:10:51,334
그래서 일반적으로 우리는 이것을 단계적으로하고 싶습니다.

779
01:10:51,334 --> 01:10:54,600
그래서 우리는 물론 첫 단계를 할 수 있습니다.

780
01:10:54,600 --> 01:10:57,310
우리는 값이 흩어져있는 것을 선택합니다.

781
01:10:57,310 --> 01:11:00,190
그리고 우리는 단지 몇 개의 신 (新) 시대에 대해서 배웁니다.

782
01:11:00,190 --> 01:11:01,788
단 몇 개의 신기원으로.

783
01:11:01,788 --> 01:11:03,473
너는 이미 꽤 좋은 감각을 얻을 수있다.

784
01:11:03,473 --> 01:11:05,552
그 중 하이퍼 파라미터,

785
01:11:05,552 --> 01:11:07,993
어떤 가치가 좋든 싫든.

786
01:11:07,993 --> 01:11:09,795
당신은 그것이 NaN이라는 것을 빨리 볼 수 있습니다,

787
01:11:09,795 --> 01:11:11,580
또는 아무 일도 일어나지 않고 있음을 볼 수 있습니다.

788
01:11:11,580 --> 01:11:13,712
그에 따라 조정할 수 있습니다.

789
01:11:13,712 --> 01:11:15,726
그래서, 일반적으로 일단 그렇게하면,

790
01:11:15,726 --> 01:11:18,506
그러면 당신은 꽤 좋은 범위의 종류인지 알 수 있습니다.

791
01:11:18,506 --> 01:11:20,143
그리고 지금하고 싶은 범위

792
01:11:20,143 --> 01:11:22,540
에서 값의 더 세밀한 샘플링.

793
01:11:22,540 --> 01:11:24,155
그리고 이것은 두 번째 단계입니다.

794
01:11:24,155 --> 01:11:27,236
이제는 더 오랜 시간 동안이
프로그램을 실행하고 싶을 것입니다.

795
01:11:27,236 --> 01:11:30,779
그 지역에 대한 더 정밀한 검색을하십시오.

796
01:11:30,779 --> 01:11:34,696
NaN과 같은 폭발을 탐지하기위한 팁 하나,

797
01:11:35,632 --> 01:11:37,244
당신은 당신의 훈련 과정에서 가질 수 있습니다,

798
01:11:37,244 --> 01:11:40,409
일부 하이퍼 파라미터를 올바르게 샘플링하고,

799
01:11:40,409 --> 01:11:43,992
교육을 시작한 다음 비용을 살펴보십시오.

800
01:11:44,947 --> 01:11:47,296
매 반복마다 또는 모든 시대마다.

801
01:11:47,296 --> 01:11:49,556
그리고 당신이 비용을받는다면

802
01:11:49,556 --> 01:11:52,264
원래 비용보다 훨씬 큽니다.

803
01:11:52,264 --> 01:11:55,259
예를 들어, 원가의 3 배,

804
01:11:55,259 --> 01:11:56,591
그러면 당신은 이것이

805
01:11:56,591 --> 01:11:57,902
올바른 방향으로 향하고있다.

806
01:11:57,902 --> 01:11:59,899
맞아, 매우 빠르게, 매우 빠르게,

807
01:11:59,899 --> 01:12:01,339
그리고 당신은 당신의 루프에서 벗어날 수 있습니다,

808
01:12:01,339 --> 01:12:03,582
이 하이퍼 매개 변수 선택을 중지하십시오.

809
01:12:03,582 --> 01:12:06,335
다른 것을 선택하십시오.

810
01:12:06,335 --> 01:12:08,193
좋습니다. 예를 들어,

811
01:12:08,193 --> 01:12:10,943
우리가 지금 도망 치고 싶다고하자.

812
01:12:12,313 --> 01:12:13,866
5 개의 신기원을위한 과정 수색.

813
01:12:13,866 --> 01:12:16,532
이것은 비슷한 네트워크입니다.

814
01:12:16,532 --> 01:12:18,332
우리가 더 일찍 이야기하고 있었던,

815
01:12:18,332 --> 01:12:21,769
우리가 할 수있는 일은이 모든
것을 볼 수 있다는 것입니다.

816
01:12:21,769 --> 01:12:24,611
우리가 얻고있는 검증 정확성.

817
01:12:24,611 --> 01:12:27,154
그리고 빨간 색으로 강조 표시했습니다.

818
01:12:27,154 --> 01:12:29,291
더 나은 가치를주는 것들.

819
01:12:29,291 --> 01:12:30,644
그리고 이것들은 지역이 될 것입니다.

820
01:12:30,644 --> 01:12:33,092
더 자세히 살펴 보겠습니다.

821
01:12:33,092 --> 01:12:34,460
주목할 사실 중 하나는

822
01:12:34,460 --> 01:12:37,067
일반적으로 로그 공간을 최적화하는 것이 좋습니다.

823
01:12:37,067 --> 01:12:39,832
그리고 여기 샘플링 대신에, 나는 균일하게 말할 것입니다.

824
01:12:39,832 --> 01:12:43,999
당신 사이에 하나의 E가 음의 0.01와 100을 알고,

825
01:12:44,873 --> 01:12:49,040
당신은 실제로 어떤 범위의 힘을 10으로 할 것입니다.

826
01:12:49,956 --> 01:12:52,625
맞아.

827
01:12:52,625 --> 01:12:55,427
학습 속도는 그라디언트 업데이트를 곱하는 것입니다.

828
01:12:55,427 --> 01:12:58,870
그리고이 곱셈 효과를 가지고 있습니다.

829
01:12:58,870 --> 01:13:02,024
그래서 고려하는 것이 더 합리적입니다.

830
01:13:02,024 --> 01:13:03,910
배가되는 학습 속도의 범위

831
01:13:03,910 --> 01:13:07,524
또는 일정한 값으로 나누어 균등하게
표본 추출하지 않아도됩니다.

832
01:13:07,524 --> 01:13:08,561
그래서, 당신은 거래하고 싶어합니다.

833
01:13:08,561 --> 01:13:10,894
여기에 몇 가지 규모의 주문이 있습니다.

834
01:13:10,894 --> 01:13:12,132
알았어. 그렇게되면,

835
01:13:12,132 --> 01:13:14,379
범위를 조정할 수 있습니다.

836
01:13:14,379 --> 01:13:17,878
이 경우에 우리는 당신이 알고있는 범위를 가지고 있습니다,

837
01:13:17,878 --> 01:13:21,392
네거티브 네, 네, 네,

838
01:13:21,392 --> 01:13:23,094
~ 10을 제로 전원으로 설정하십시오.

839
01:13:23,094 --> 01:13:26,176
이것은 우리가 범위를 좁히고 자하는 좋은 범위입니다.

840
01:13:26,176 --> 01:13:27,976
그리고 우리는 이것을 다시 할 수 있습니다.

841
01:13:27,976 --> 01:13:29,315
여기서 우리는 우리가

842
01:13:29,315 --> 01:13:33,087
상대적으로 좋은 53 %의 정확도.

843
01:13:33,087 --> 01:13:37,962
그리고 이것은 우리가 올바른 방향으로
나아가고 있다는 것을 의미합니다.

844
01:13:37,962 --> 01:13:40,190
제가 지적하고자하는 한가지는

845
01:13:40,190 --> 01:13:42,377
여기에는 실제로 문제가 있습니다.

846
01:13:42,377 --> 01:13:45,317
그래서 문제는

847
01:13:45,317 --> 01:13:47,563
우리는 여기서 가장 정확한 정확성을 볼 수 있습니다.

848
01:13:47,563 --> 01:13:50,396
대략 배우는 비율이있다,

849
01:13:52,373 --> 01:13:55,281
알다시피, 우리의 모든 좋은 학습 속도

850
01:13:55,281 --> 01:13:57,816
이 E에서 음의 4 범위입니다.

851
01:13:57,816 --> 01:14:00,059
맞습니다. 그리고 우리가 지정한 학습 속도

852
01:14:00,059 --> 01:14:03,646
10에서 0으로 네거티브 4에서 10으로 가고 있었고,

853
01:14:03,646 --> 01:14:05,919
즉, 모든 좋은 학습 속도,

854
01:14:05,919 --> 01:14:10,273
우리가 샘플링하는 범위의 가장자리에 있었다.

855
01:14:10,273 --> 01:14:11,856
그래서 이것은 나쁘다.

856
01:14:12,693 --> 01:14:15,903
이것은 우리가 탐험을하지 않았을 수도 있기 때문입니다.

857
01:14:15,903 --> 01:14:17,113
우리의 공간은 충분히 옳다.

858
01:14:17,113 --> 01:14:19,189
실제로 마이너스 5에 10을 가길 원할 수도 있습니다.

859
01:14:19,189 --> 01:14:20,485
또는 10을 음수 6으로 설정하십시오.

860
01:14:20,485 --> 01:14:21,908
여전히 더 나은 범위가있을 수 있습니다.

861
01:14:21,908 --> 01:14:23,494
우리가 계속 아래로 이동하면.

862
01:14:23,494 --> 01:14:25,352
그래서, 당신은 당신의 범위가

863
01:14:25,352 --> 01:14:27,785
종류의 중간에 좋은 가치를 가지고,

864
01:14:27,785 --> 01:14:30,089
또는 당신이 타격을 당했다는 느낌을받는 곳에서,

865
01:14:30,089 --> 01:14:32,839
당신은 당신의 범위를 완전히 탐험했습니다.

866
01:14:36,224 --> 01:14:39,853
좋아, 또 다른 점은

867
01:14:39,853 --> 01:14:42,113
우리는 모든 다른 하이퍼 파라미터를
샘플링 할 수 있습니다.

868
01:14:42,113 --> 01:14:43,741
그리드 검색의 종류를 사용하여, 오른쪽.

869
01:14:43,741 --> 01:14:46,621
고정 된 조합의 조합을 샘플링 할 수 있습니다.

870
01:14:46,621 --> 01:14:49,731
각 하이퍼 매개 변수에 대한 고정 값 집합입니다.

871
01:14:49,731 --> 01:14:54,597
이 모든 값에 대해 그리드 방식으로 샘플링하고,

872
01:14:54,597 --> 01:14:56,819
실제로 실제로는 샘플을 만드는 것이 더 좋습니다.

873
01:14:56,819 --> 01:15:00,430
무작위 레이아웃에서 임의의 값을 샘플링

874
01:15:00,430 --> 01:15:02,334
한 범위 내의 각 하이퍼 매개 변수의

875
01:15:02,334 --> 01:15:03,817
그래서 대신 얻을 것입니다.

876
01:15:03,817 --> 01:15:05,661
우리는이 두 가지 하이퍼 파라미터를
여기에 갖게 될 것입니다.

877
01:15:05,661 --> 01:15:07,389
우리가 샘플을 원한다.

878
01:15:07,389 --> 01:15:10,876
대신이 오른쪽면처럼 보이는 샘플을 얻을 수 있습니다.

879
01:15:10,876 --> 01:15:14,401
그리고이 이유는 함수가 실제로

880
01:15:14,401 --> 01:15:18,274
한 변수의 함수를 다른 것보다 더 많이 정렬하고,

881
01:15:18,274 --> 01:15:19,816
이것은 사실입니다.

882
01:15:19,816 --> 01:15:21,456
보통 조금 더 있습니다.

883
01:15:21,456 --> 01:15:24,669
우리가 실제로 가지고있는 것보다 더 낮은 유효 차원.

884
01:15:24,669 --> 01:15:26,969
그러면 더 많은 샘플을 얻게 될 것입니다.

885
01:15:26,969 --> 01:15:30,342
당신이 가지고있는 중요한 변수의

886
01:15:30,342 --> 01:15:32,560
이 모양을 볼 수 있습니다.

887
01:15:32,560 --> 01:15:35,956
내가 그린 위에 그린이 녹색 함수에서,

888
01:15:35,956 --> 01:15:38,326
좋은 가치가있는 곳을 보여주는

889
01:15:38,326 --> 01:15:40,782
당신이 방금 그리드 레이아웃을 만든 경우와 비교하여

890
01:15:40,782 --> 01:15:43,403
여기서 세 가지 값만 샘플링 할 수있었습니다.

891
01:15:43,403 --> 01:15:46,459
좋은 지역이 어디 있는지 놓쳤습니다.

892
01:15:46,459 --> 01:15:48,677
맞아요. 그래서 기본적으로 우리는
훨씬 더 많은 것을 얻을 것입니다.

893
01:15:48,677 --> 01:15:51,741
우리가 더 많은 샘플을 가지고 있기 때문에 유용한 신호

894
01:15:51,741 --> 01:15:55,685
중요한 변수의 다른 값들의

895
01:15:55,685 --> 01:15:58,325
그래서, 함께 연주 할 하이퍼 파라미터들,

896
01:15:58,325 --> 01:16:00,427
우리는 학습률에 대해 이야기했습니다.

897
01:16:00,427 --> 01:16:03,163
다양한 종류의 붕괴 일정과 같은 것들,

898
01:16:03,163 --> 01:16:05,160
업데이트 유형, 정규화,

899
01:16:05,160 --> 01:16:07,697
또한 네트워크 아키텍처,

900
01:16:07,697 --> 01:16:09,523
그래서 숨겨진 유닛의 수, 깊이,

901
01:16:09,523 --> 01:16:12,405
이 모든 것은 당신이 최적화 할
수있는 하이퍼 파라미터입니다.

902
01:16:12,405 --> 01:16:13,630
그리고 우리는 이것들에 대해 이야기했습니다,

903
01:16:13,630 --> 01:16:15,531
하지만 우리는이 중 더 많은 것을 계속 이야기 할 것입니다.

904
01:16:15,531 --> 01:16:16,928
다음 강연에서.

905
01:16:16,928 --> 01:16:20,038
그래서 당신은 이것을 일종의 것으로 생각할 수 있습니다.

906
01:16:20,038 --> 01:16:22,198
기본적으로 모든 노브를 올바르게 튜닝하는 경우,

907
01:16:22,198 --> 01:16:24,781
네가있는 턴테이블의

908
01:16:26,667 --> 01:16:28,390
당신은 신경망 의사입니다.

909
01:16:28,390 --> 01:16:29,801
출력되는 음악을 생각해 볼 수 있습니다.

910
01:16:29,801 --> 01:16:32,260
당신이 원하는 손실 함수입니다.

911
01:16:32,260 --> 01:16:34,121
그리고 당신은 모든 것을 적절하게 조정하고 싶다.

912
01:16:34,121 --> 01:16:36,313
당신이 원하는 출력을 얻을 수 있습니다.

913
01:16:36,313 --> 01:16:40,480
좋아요, 그래서 그것은 당신이하는 일종의 예술입니다.

914
01:16:42,194 --> 01:16:45,891
실제로, 당신은 할 것입니다.

915
01:16:45,891 --> 01:16:48,593
많은 하이퍼 파라미터 최적화,

916
01:16:48,593 --> 01:16:50,277
많은 교차 검증.

917
01:16:50,277 --> 01:16:52,582
그래서 숫자를 얻으려면,

918
01:16:52,582 --> 01:16:54,425
사람들이 교차 검증을 실행합니다.

919
01:16:54,425 --> 01:16:58,371
수많은 하이퍼 파라미터를 모니터링하고,
모든 것을 모니터링하며,

920
01:16:58,371 --> 01:16:59,535
어느 것이 더 잘하고 있는지 보아라.

921
01:16:59,535 --> 01:17:00,368
어느 것이 더 나빠질 지.

922
01:17:00,368 --> 01:17:02,091
여기에 우리는 이러한 모든 손실 곡선을 가지고 있습니다.

923
01:17:02,091 --> 01:17:04,137
올바른 것을 고르고, 재조정하고,

924
01:17:04,137 --> 01:17:07,895
이 과정을 계속 진행하십시오.

925
01:17:07,895 --> 01:17:10,478
그리고 앞서 언급했듯이,

926
01:17:11,409 --> 01:17:13,583
이 손실 곡선 각각을 모니터링 할 때,

927
01:17:13,583 --> 01:17:15,311
학습률은 중요한 것으로,

928
01:17:15,311 --> 01:17:18,739
그러나 당신은 어떻게 다른 학습
속도에 대한 감각을 얻을 것입니다,

929
01:17:18,739 --> 01:17:20,654
어떤 학습 속도가 좋고 나쁘다.

930
01:17:20,654 --> 01:17:23,850
그래서 당신은 매우 높은 폭발하는 것을 가지고 있다면,

931
01:17:23,850 --> 01:17:25,779
맞아, 이건 너의 손실이다.

932
01:17:25,779 --> 01:17:27,667
학습률이 너무 높습니다.

933
01:17:27,667 --> 01:17:29,985
그것이 너무 일직선이고 너무 평평하다면,

934
01:17:29,985 --> 01:17:34,060
너는 너무 낮다는 것을 알게 될
것이고, 충분히 바뀌지는 않을 것이다.

935
01:17:34,060 --> 01:17:35,990
네가 뭔가를 얻으면

936
01:17:35,990 --> 01:17:38,565
가파른 변화가있는 것처럼 보입니다.하지만 고원은,

937
01:17:38,565 --> 01:17:41,660
이것은 또한 아마도 너무 높다는 지표입니다.

938
01:17:41,660 --> 01:17:44,803
이 경우 당신은 너무 큰 도약을하고 있기 때문에,

939
01:17:44,803 --> 01:17:48,460
그리고 당신은 당신의 지역 최적으로
잘 정착 할 수 없습니다.

940
01:17:48,460 --> 01:17:50,015
그래서 좋은 학습 속도

941
01:17:50,015 --> 01:17:51,959
보통 이런 식으로 끝내기 시작합니다.

942
01:17:51,959 --> 01:17:53,572
비교적 가파른 곡선이있는 곳,

943
01:17:53,572 --> 01:17:55,371
그러나 그것은 계속 내려 가고 있습니다.

944
01:17:55,371 --> 01:17:56,668
조정을 계속할 수도 있습니다.

945
01:17:56,668 --> 01:17:57,993
거기에서 당신의 학습 속도.

946
01:17:57,993 --> 01:18:02,160
그래서 이것은 여러분이 연습을 통해 볼 수있는 것입니다.

947
01:18:03,522 --> 01:18:05,825
좋아요, 그냥 우리가 끝까지 아주 가깝다고 생각합니다.

948
01:18:05,825 --> 01:18:08,401
그래서 내가 지적하고 싶은 마지막 한가지

949
01:18:08,401 --> 01:18:12,637
당신이 학습 속도 손실 곡선을 본 경우에 비해,

950
01:18:12,637 --> 01:18:14,553
곳은...

951
01:18:14,553 --> 01:18:16,454
그래서 잠시 동안 평평한 곳에 손실 곡선을 보게된다면,

952
01:18:16,454 --> 01:18:20,069
갑자기 훈련을 시작합니다.

953
01:18:20,069 --> 01:18:23,567
잠재적 인 이유는 나쁜 초기화 일 수 있습니다.

954
01:18:23,567 --> 01:18:25,871
따라서이 경우에는 그라디언트가 실제로 흐르지 않습니다.

955
01:18:25,871 --> 01:18:28,377
너무 초반부터 잘 배우기 때문에,

956
01:18:28,377 --> 01:18:29,397
그리고 나서 어느 시점에서,

957
01:18:29,397 --> 01:18:31,905
그것은 올바른 방법으로 조정되는 것입니다.

958
01:18:31,905 --> 01:18:36,383
그런 일이 끝나고 일들이 바로 훈련을 시작하도록

959
01:18:36,383 --> 01:18:40,602
그래서 이걸 보면서 많은 경험이 있습니다.

960
01:18:40,602 --> 01:18:42,921
시간이 지남에 따라 얻을 수있는 잘못된 점을 확인하십시오.

961
01:18:42,921 --> 01:18:45,568
따라서 일반적으로

962
01:18:45,568 --> 01:18:47,901
귀하의 정확성을 시각화하십시오.

963
01:18:48,826 --> 01:18:52,369
훈련 정확도 사이에 큰 차이가 있다면

964
01:18:52,369 --> 01:18:54,860
유효성 확인의 정확성,

965
01:18:54,860 --> 01:18:57,135
그것은 일반적으로 당신이
overfitting있을 수 있음을 의미합니다

966
01:18:57,135 --> 01:18:58,187
그리고 너는 증가시키고 싶을지도 모른다.

967
01:18:58,187 --> 01:18:59,652
당신의 정규화 강도.

968
01:18:59,652 --> 01:19:00,678
간격이 없다면,

969
01:19:00,678 --> 01:19:03,385
모델 용량을 늘려야 할 수도 있습니다.

970
01:19:03,385 --> 01:19:05,070
너는 아직 과장되지 않았기 때문에.

971
01:19:05,070 --> 01:19:08,137
잠재적으로 더 많이 늘릴 수 있습니다.

972
01:19:08,137 --> 01:19:10,381
그리고 일반적으로 업데이트를 추적하고 싶습니다.

973
01:19:10,381 --> 01:19:13,998
우리의 체중 변화에 대한 체중 업데이트의 비율.

974
01:19:13,998 --> 01:19:16,028
우리는 단지 표준을 취할 수 있습니다.

975
01:19:16,028 --> 01:19:19,281
우리가 가지고있는 매개 변수 중

976
01:19:19,281 --> 01:19:21,428
그들이 얼마나 큰 지에 대한 감각을 얻으려면,

977
01:19:21,428 --> 01:19:22,998
우리가 우리의 업데이트 크기를 가질 때,

978
01:19:22,998 --> 01:19:24,723
우리는 또한 그것의 규범을 취할 수 있습니다.

979
01:19:24,723 --> 01:19:26,353
얼마나 큰지에 대한 감각을 얻으십시오,

980
01:19:26,353 --> 01:19:30,025
이 비율을 약 0.001 정도가 되길 원합니다.

981
01:19:30,025 --> 01:19:33,611
이 범위에는 많은 차이가 있습니다.

982
01:19:33,611 --> 01:19:35,598
그래서 당신은 이것에 정확하게있을 필요는 없습니다.

983
01:19:35,598 --> 01:19:37,844
하지만 그것은 단지 당신이 원하지 않는 감각입니다.

984
01:19:37,844 --> 01:19:40,455
귀하의 업데이트가 귀하의 가치에 비해 너무 크다.

985
01:19:40,455 --> 01:19:41,477
또는 너무 작습니까, 맞습니까?

986
01:19:41,477 --> 01:19:43,637
지배적이거나 효과가 없기를 원하지 않습니다.

987
01:19:43,637 --> 01:19:45,811
그래서 이것은 디버깅을 도울 수있는 것입니다.

988
01:19:45,811 --> 01:19:47,811
무엇이 문제 일 수 있습니다.

989
01:19:49,843 --> 01:19:51,514
좋아요, 요약하자면,

990
01:19:51,514 --> 01:19:54,073
오늘 우리는 활성화 함수를 살펴 보았습니다.

991
01:19:54,073 --> 01:19:56,654
데이터 전처리, 가중치 초기화,

992
01:19:56,654 --> 01:19:59,016
일괄 표준, 학습 과정의 보육,

993
01:19:59,016 --> 01:20:01,694
및 하이퍼 매개 변수 최적화.

994
01:20:01,694 --> 01:20:03,667
이것들은 각각을위한 테이크 어웨이의 종류입니다.

995
01:20:03,667 --> 01:20:05,338
너희들은 명심해야한다.

996
01:20:05,338 --> 01:20:08,491
ReLUs를 사용하고, 평균을 빼고, Xavier
Initialization을 사용하고,

997
01:20:08,491 --> 01:20:12,499
일괄 표준을 사용하고 샘플 하이퍼
매개 변수를 무작위로 추출합니다.

998
01:20:12,499 --> 01:20:14,222
다음에 우리는 계속 이야기 할 것입니다.

999
01:20:14,222 --> 01:20:15,806
교육 신경 네트워크에 대해

1000
01:20:15,806 --> 01:20:18,355
이 모든 다른 주제들로

1001
01:20:18,355 --> -00:00:00,600
감사.

