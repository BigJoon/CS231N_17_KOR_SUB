1
00:00:07,755 --> 00:00:11,682
CS 231N 3 번째 강의에 오신 것을 환영합니다.

2
00:00:11,682 --> 00:00:14,594
오늘 우리는 손실 함수와 최적화에
대해 이야기 할 것입니다.

3
00:00:14,594 --> 00:00:16,720
하지만 평소처럼, 우리가 주요 내용에 도달하기 전에

4
00:00:16,720 --> 00:00:18,962
강의 중 몇 가지 행정적인 일이있어.

5
00:00:18,962 --> 00:00:20,129
얘기하고.

6
00:00:21,094 --> 00:00:24,547
그래서 첫 번째 과제는 할당 된 것입니다.

7
00:00:24,547 --> 00:00:26,889
웹 사이트에서 링크를 찾을 수 있습니다.

8
00:00:26,889 --> 00:00:28,376
그리고 우리가 조금 늦었 기 때문에

9
00:00:28,376 --> 00:00:30,086
이 임무를 너희들에게 넘기기 위해서,

10
00:00:30,086 --> 00:00:33,181
우리는 목요일까지 기한을 변경하기로 결정했습니다.

11
00:00:33,181 --> 00:00:35,264
4 월 20 일 오후 11시 59 분,

12
00:00:36,374 --> 00:00:39,281
배정 된 지 2 주일 만에 줄거야.

13
00:00:39,281 --> 00:00:42,702
출시일을 정하고 실제로 마무리하고 작업합니다.

14
00:00:42,702 --> 00:00:46,499
그래서 우리는이 새로운 만기일에 대한
강의 요강을 업데이트 할 것입니다.

15
00:00:46,499 --> 00:00:49,087
오늘은 나중에.

16
00:00:49,087 --> 00:00:51,025
그리고 상기 과제를 완료 할 때,

17
00:00:51,025 --> 00:00:54,617
당신은 Canvas의 최종 zip 파일을 돌려야합니다.

18
00:00:54,617 --> 00:00:56,779
그래서 우리는 그것을 채점하고
성적을 빨리 되돌릴 수 있습니다.

19
00:00:56,779 --> 00:00:57,779
가능한 한.

20
00:00:58,799 --> 00:01:03,433
그래서 다음은 항상 피아자를
재미있게 체크 아웃하는 것입니다.

21
00:01:03,433 --> 00:01:04,879
관리 물건.

22
00:01:04,879 --> 00:01:07,788
그래서 이번 주에 나는 우리가

23
00:01:07,788 --> 00:01:11,432
예를 들어 피아자에 고정 된 게시물로 프로젝트 아이디어.

24
00:01:11,432 --> 00:01:14,930
그래서 우리는 나가서 프로젝트
아이디어의 예를 간청했습니다.

25
00:01:14,930 --> 00:01:17,220
스탠포드 지역 사회의 다양한 사람들로부터

26
00:01:17,220 --> 00:01:20,151
스탠포드 (Stanford)에게. 그리고 그들은 재미있는

27
00:01:20,151 --> 00:01:22,913
학생들이 원하는 프로젝트 제안

28
00:01:22,913 --> 00:01:24,583
수업 시간에.

29
00:01:24,583 --> 00:01:26,986
피아자에있는이 고정 된 게시물을 확인하고 원하는 경우

30
00:01:26,986 --> 00:01:30,584
이 프로젝트들 중 하나를
작업하려면, 언제든지 연락하십시오.

31
00:01:30,584 --> 00:01:34,231
프로젝트는 이러한 것들에 대해 직접 멘토링합니다.

32
00:01:34,231 --> 00:01:37,090
또한 우리는 코스 웹 사이트에 근무 시간을 게시했으며,

33
00:01:37,090 --> 00:01:40,756
이것은 Google 캘린더이기 때문에 사람들이

34
00:01:40,756 --> 00:01:45,077
묻고 있었고 지금은 저기에 있습니다.

35
00:01:45,077 --> 00:01:48,307
최종 관리 메모는 Google Cloud,

36
00:01:48,307 --> 00:01:51,745
Google Cloud에서 지원하기 때문에 알림으로

37
00:01:51,745 --> 00:01:54,331
이 수업에서는 각자에게 추가 수업을 제공 할 수 있습니다.

38
00:01:54,331 --> 00:01:57,033
과제에 대한 Google Cloud의 기여도 100 달러

39
00:01:57,033 --> 00:02:00,687
프로젝트 및 사용 방법에 대한 정확한 정보

40
00:02:00,687 --> 00:02:05,246
그 신용은 오늘 후반에 나올 것이며,
아마도 Piazza에있을 것입니다.

41
00:02:05,246 --> 00:02:07,810
그래서 만약에 질문이 없다면

42
00:02:07,810 --> 00:02:11,977
행정적 내용으로 넘어갈 것입니다.

43
00:02:13,440 --> 00:02:14,273
좋아.

44
00:02:15,559 --> 00:02:17,997
강의 2에서 마지막으로 기억합니다.

45
00:02:17,997 --> 00:02:20,412
우리는 실제로 인식의 도전에 대해 이야기하고있었습니다.

46
00:02:20,412 --> 00:02:22,202
이 아이디어에 착수하려고 노력하고있다.

47
00:02:22,202 --> 00:02:24,476
데이터 중심 접근 방식

48
00:02:24,476 --> 00:02:26,921
이미지 분류에 대한이 아이디어에 대해 이야기했습니다.

49
00:02:26,921 --> 00:02:29,160
왜 그것이 어려운지에 대해
이야기했다.이 의미 적 차이가있다.

50
00:02:29,160 --> 00:02:33,202
컴퓨터가 보는 거대한 숫자의 숫자 사이

51
00:02:33,202 --> 00:02:35,812
그리고 당신이 보는 실제 이미지.

52
00:02:35,812 --> 00:02:37,645
우리는이 문제와 관련된 다양한 문제에 관해 이야기했습니다.

53
00:02:37,645 --> 00:02:39,957
조명, 변형 등의 주변,

54
00:02:39,957 --> 00:02:42,124
왜 이것이 실제로 실제로 정말로 어려운 문제인지

55
00:02:42,124 --> 00:02:44,186
사람들이 할 수있는 일은 아주 쉽지만

56
00:02:44,186 --> 00:02:47,912
그들의 인간의 눈과 인간의 시각 시스템.

57
00:02:47,912 --> 00:02:50,421
마지막으로 우리가 가장 가까운 k-

58
00:02:50,421 --> 00:02:53,489
간단한 소개의 종류 인 이웃 분류 자

59
00:02:53,489 --> 00:02:55,309
이 모든 데이터 중심 사고 방식에

60
00:02:55,309 --> 00:02:57,992
우리는 CIFAR-10 데이터 세트에 대해 이야기했습니다.

61
00:02:57,992 --> 00:03:00,824
여기 왼쪽 상단에있는이 이미지의 예는

62
00:03:00,824 --> 00:03:03,688
CIFAR-10은이 10 가지 카테고리를 제공합니다.

63
00:03:03,688 --> 00:03:05,787
비행기, 자동차, 썸네일,

64
00:03:05,787 --> 00:03:08,627
우리는 k- 최근 접 이웃 분류 자

65
00:03:08,627 --> 00:03:11,202
결정 경계를 학습하는 데 사용할 수 있습니다.

66
00:03:11,202 --> 00:03:13,604
이들 데이터 포인트를 클래스로 분리한다.

67
00:03:13,604 --> 00:03:15,746
교육 데이터를 기반으로합니다.

68
00:03:15,746 --> 00:03:18,599
이것은 또한 우리에게 십자가의
개념에 대한 토론을하게했습니다.

69
00:03:18,599 --> 00:03:20,955
검증 및 하이퍼 매개 변수 설정

70
00:03:20,955 --> 00:03:25,190
데이터를 기차, 검증 및 테스트
세트에 통합 할 수 있습니다.

71
00:03:25,190 --> 00:03:27,208
마지막으로 우리가 선형에 대해서 이야기했던 마지막 시간

72
00:03:27,208 --> 00:03:30,057
빌딩 블록의 첫 번째 종류로 분류

73
00:03:30,057 --> 00:03:32,410
우리가 신경 네트워크쪽으로 이동할 때.

74
00:03:32,410 --> 00:03:34,726
선형 분류기가 예제라는 것을 상기하라.

75
00:03:34,726 --> 00:03:38,538
우리가 알고있는 모든 매개 변수 분류 자의

76
00:03:38,538 --> 00:03:40,528
교육 데이터에 대한 요약

77
00:03:40,528 --> 00:03:43,346
이 파라미터 행렬 W에 설정된다

78
00:03:43,346 --> 00:03:45,444
훈련 과정에서

79
00:03:45,444 --> 00:03:48,448
이 선형 분류기 리콜은 매우 간단합니다.

80
00:03:48,448 --> 00:03:50,315
우리가 이미지를 가져 와서 그것을 펼칠 곳.

81
00:03:50,315 --> 00:03:51,810
긴 벡터로.

82
00:03:51,810 --> 00:03:54,974
여기 이미지는 x이고 그 다음에
우리는 그 이미지를 취합니다.

83
00:03:54,974 --> 00:03:58,295
32 x 32 픽셀, 3 픽셀, 스트레치 아웃

84
00:03:58,295 --> 00:04:01,251
32 배의 긴 열 벡터로 32

85
00:04:01,251 --> 00:04:02,918
시간 3 항목,

86
00:04:04,344 --> 00:04:06,403
여기서 32와 32는 높이와 너비이며,

87
00:04:06,403 --> 00:04:08,223
3은 3 가지 색상 채널을 제공합니다.

88
00:04:08,223 --> 00:04:09,722
빨강, 녹색, 파랑.

89
00:04:09,722 --> 00:04:13,561
그 다음에 몇 가지 매개 변수 행렬 W

90
00:04:13,561 --> 00:04:15,681
이 긴 열 벡터를 취할 것입니다

91
00:04:15,681 --> 00:04:18,517
이미지 픽셀을 나타내는

92
00:04:18,517 --> 00:04:20,842
점수를주는 10 개의 숫자를 알려주세요.

93
00:04:20,842 --> 00:04:24,387
CIFAR-10의 경우 10 개의 클래스 각각에 대해

94
00:04:24,387 --> 00:04:26,116
우리가 이런 해석을했을 때

95
00:04:26,116 --> 00:04:29,617
그 점수의 큰 값,

96
00:04:29,617 --> 00:04:32,347
cat 클래스에 대한 더 큰 값은 분류자를 의미합니다.

97
00:04:32,347 --> 00:04:34,881
고양이가 그 이미지에 더 가깝다고 생각합니다.

98
00:04:34,881 --> 00:04:37,550
어쩌면 개 또는 차 종류를위한 더 낮은 가치

99
00:04:37,550 --> 00:04:40,553
존재하는 클래스의 낮은 확률을 나타냅니다.

100
00:04:40,553 --> 00:04:42,443
이미지에서.

101
00:04:42,443 --> 00:04:45,764
또한, 나는이 점이 약간 불분명하다고 생각한다.

102
00:04:45,764 --> 00:04:49,409
선형 분류가이 해석을 가지고있는 마지막 시간

103
00:04:49,409 --> 00:04:51,625
클래스별로 학습 템플릿으로,

104
00:04:51,625 --> 00:04:54,328
왼쪽 하단의 다이어그램을 보면,

105
00:04:54,328 --> 00:04:57,499
당신은 이미지의 모든 픽셀에 대해 이렇게 생각합니다.

106
00:04:57,499 --> 00:04:59,611
우리의 10 개 수업 모두에 대해,

107
00:04:59,611 --> 00:05:02,444
이 행렬 W에 몇 가지 항목이 있습니다.

108
00:05:02,444 --> 00:05:06,554
그 픽셀이 그 클래스에 얼마나 영향을 미치는지 말해줍니다.

109
00:05:06,554 --> 00:05:09,616
즉, 행렬 W의 각 행

110
00:05:09,616 --> 00:05:12,412
클래스의 템플릿에 해당합니다.

111
00:05:12,412 --> 00:05:14,679
그리고 만약 우리가 그 행을 풀고 풀었다면,

112
00:05:14,679 --> 00:05:16,924
각 행은 다시

113
00:05:16,924 --> 00:05:19,740
의 값들 사이의 가중치에,

114
00:05:19,740 --> 00:05:22,551
이미지와 그 클래스의 픽셀 값들 사이에서,

115
00:05:22,551 --> 00:05:25,446
그래서 우리가 그 행을 가지고 그것을 다시 이미지로 풀면,

116
00:05:25,446 --> 00:05:27,987
학습 된 템플릿을 각각 시각화 할 수 있습니다.

117
00:05:27,987 --> 00:05:29,900
이러한 클래스 중.

118
00:05:29,900 --> 00:05:32,524
우리는 또한 선형 분류에 대한 이러한 해석을 가지고있다.

119
00:05:32,524 --> 00:05:35,399
픽셀 간의 선형 결정 경계를 학습하는 것으로서

120
00:05:35,399 --> 00:05:37,788
일부 고 차원 공간에서 치수

121
00:05:37,788 --> 00:05:40,811
공간의 픽셀 값은 픽셀의 값에 대응한다

122
00:05:40,811 --> 00:05:43,774
이미지의 강도 값.

123
00:05:43,774 --> 00:05:47,571
그래서 이것은 우리가 마지막으로 중단 한 곳입니다.

124
00:05:47,571 --> 00:05:50,815
그리고 우리가 멈추는 곳, 우리가 마지막에 끝났던 곳

125
00:05:50,815 --> 00:05:54,141
선형 분류 자에 대한 아이디어가 있습니다.

126
00:05:54,141 --> 00:05:57,554
실제로 W를 선택하는 방법에 대해서는 언급하지 않았습니다.

127
00:05:57,554 --> 00:05:59,389
교육 데이터를 실제로 사용하는 방법

128
00:05:59,389 --> 00:06:02,628
W의 어떤 가치가 가장 좋은지를 결정한다.

129
00:06:02,628 --> 00:06:04,456
우리가 멈춰 섰던 종류의 종류

130
00:06:04,456 --> 00:06:08,292
W의 일부 설정에 대해서는 W를 사용할 수 있습니다.

131
00:06:08,292 --> 00:06:12,068
모든 이미지에 대해 10 점 만점으로 수업을합니다.

132
00:06:12,068 --> 00:06:15,597
따라서이 등급 점수 중 일부는
더 좋거나 나쁠 수 있습니다.

133
00:06:15,597 --> 00:06:17,164
여기이 간단한 예에서,

134
00:06:17,164 --> 00:06:20,833
우리는 3 개의 이미지로 구성된 훈련
데이터 세트를 보여 주었을 것입니다.

135
00:06:20,833 --> 00:06:24,584
어떤 가치의 W에 대해 예측 된
10 가지 등급 점수와 함께

136
00:06:24,584 --> 00:06:26,046
그 이미지들.

137
00:06:26,046 --> 00:06:27,847
이 점수 중 일부가 더 좋음을 알 수 있습니다.

138
00:06:27,847 --> 00:06:29,506
또는 다른 것보다 나쁘다.

139
00:06:29,506 --> 00:06:32,344
그래서 예를 들어 왼쪽 이미지에서, 당신이 위로 볼 경우,

140
00:06:32,344 --> 00:06:34,242
당신이 인간이기 때문에 그것은 실제로 고양이입니다.

141
00:06:34,242 --> 00:06:35,924
당신은 이런 것들을 말할 수 있습니다.

142
00:06:35,924 --> 00:06:38,952
그러나 우리가 할당 된 확률을 보면, 고양이,

143
00:06:38,952 --> 00:06:41,068
잘 확률은 아니지만 점수,

144
00:06:41,068 --> 00:06:43,436
W의이 설정에 대한 분류 자일 수도 있습니다.

145
00:06:43,436 --> 00:06:48,082
이 이미지에 대해 고양이 수업에 2.9 점을주었습니다.

146
00:06:48,082 --> 00:06:51,018
개구리 계급은 3.78을 나타냈다.

147
00:06:51,018 --> 00:06:53,109
아마 분류 자 (classifier)가별로
좋지 않을 것입니다.

148
00:06:53,109 --> 00:06:55,436
이 이미지에서, 그것은 나쁘다,
우리는 진정한 계급을 원했다.

149
00:06:55,436 --> 00:06:57,920
실제로 최고 수준의 점수가되도록

150
00:06:57,920 --> 00:07:00,109
반면에 이러한 다른 예들 중 일부는 자동차처럼

151
00:07:00,109 --> 00:07:02,729
예를 들어 자동차 클래스가

152
00:07:02,729 --> 00:07:04,393
훨씬 더 높은 6 점을 받는다.

153
00:07:04,393 --> 00:07:06,819
다른 사람들보다 더 좋았습니다.

154
00:07:06,819 --> 00:07:10,633
그리고 개구리, 예상 점수는 아마 부정적인 4,

155
00:07:10,633 --> 00:07:12,837
그것은 다른 모든 것보다 훨씬 낮습니다.

156
00:07:12,837 --> 00:07:14,357
그래서 그것은 실제로 나쁘다.

157
00:07:14,357 --> 00:07:16,531
이것은 일종의 물결 모양의 접근입니다.

158
00:07:16,531 --> 00:07:18,340
점수와 눈을 보며하는 것만으로

159
00:07:18,340 --> 00:07:20,654
어느 것이 좋고 어떤 것이 나쁜지.

160
00:07:20,654 --> 00:07:22,810
그러나 실제로 이러한 것에 대한 알고리즘을 작성하려면

161
00:07:22,810 --> 00:07:25,264
실제로 어떤 W

162
00:07:25,264 --> 00:07:28,860
가장 좋을 것입니다. 우리는 불량을
수량화 할 방법이 필요합니다.

163
00:07:28,860 --> 00:07:31,032
특정 W.

164
00:07:31,032 --> 00:07:35,026
그리고 그것은 W를 취하는이 함수입니다.

165
00:07:35,026 --> 00:07:38,483
점수를보고 정량적으로 얼마나 나쁜지 알려줍니다.

166
00:07:38,483 --> 00:07:41,987
W라는 것이 우리가 손실 함수라고 부르는 것입니다.

167
00:07:41,987 --> 00:07:44,667
이 강의에서는 몇 가지 예를 살펴 보겠습니다.

168
00:07:44,667 --> 00:07:47,293
이 이미지에 사용할 수있는 다양한 손실 함수

169
00:07:47,293 --> 00:07:49,782
분류 문제.

170
00:07:49,782 --> 00:07:52,683
그래서 일단 우리가 손실 함수에 대한 생각을 갖게되면,

171
00:07:52,683 --> 00:07:56,732
이것은 우리가 W의 임의의 주어진
값에 대해 정량화 할 수있게하며,

172
00:07:56,732 --> 00:07:58,498
얼마나 좋든 나쁘지?

173
00:07:58,498 --> 00:08:00,034
하지만 실제로 우리는

174
00:08:00,034 --> 00:08:01,930
효율적인 절차를 제안하십시오.

175
00:08:01,930 --> 00:08:04,770
가능한 모든 W의 공간을 검색하기 위해

176
00:08:04,770 --> 00:08:08,134
실제로 올바른 값이 무엇인지 생각해냅니다.

177
00:08:08,134 --> 00:08:10,688
가장 나쁜 W의

178
00:08:10,688 --> 00:08:12,860
이 과정은 최적화 절차가 될 것입니다.

179
00:08:12,860 --> 00:08:16,276
우리는이 강의에서 더 자세히 이야기 할 것입니다.

180
00:08:16,276 --> 00:08:18,291
그래서이 예제를 조금 축소 할 것입니다.

181
00:08:18,291 --> 00:08:21,003
왜냐하면 10 개의 클래스는 다루기 힘들 기 때문입니다.

182
00:08:21,003 --> 00:08:23,931
그래서 우리는이 작은 장난감 데이터
세트로 일을 할 것입니다.

183
00:08:23,931 --> 00:08:26,751
세 가지 예와 앞으로 세 가지 클래스

184
00:08:26,751 --> 00:08:28,886
이 강연에서.

185
00:08:28,886 --> 00:08:32,839
다시이 예에서 고양이는 정확하지 않을 수 있습니다.

186
00:08:32,839 --> 00:08:37,607
분류, 차를 정확하게 분류하고, 개구리,

187
00:08:37,607 --> 00:08:40,520
W의이 설정은이 개구리 이미지를 완전히 잘못 잡았습니다.

188
00:08:40,520 --> 00:08:44,425
개구리 점수가 다른 것보다 훨씬 낮기 때문입니다.

189
00:08:44,425 --> 00:08:46,964
그래서 이것을 공식화하기 위해, 보통 우리가 말할 때

190
00:08:46,964 --> 00:08:48,817
손실 함수에 관해서, 우리는 상상한다.

191
00:08:48,817 --> 00:08:52,870
우리는 xs와 ys의 훈련 데이터 세트를 가지고 있고,

192
00:08:52,870 --> 00:08:56,196
일반적으로 xs가 입력 값 인 이들의 N 예제

193
00:08:56,196 --> 00:08:59,204
이미지 분류의 경우에는 알고리즘에,

194
00:08:59,204 --> 00:09:03,062
xs는 실제로 이미지의 픽셀 값이되고,

195
00:09:03,062 --> 00:09:05,407
ys는 알고리즘을 원한다.

196
00:09:05,407 --> 00:09:08,930
예측하기 위해, 우리는 일반적으로
이것들을 레이블이나 표적이라고 부릅니다.

197
00:09:08,930 --> 00:09:10,982
따라서 이미지 분류의 경우,

198
00:09:10,982 --> 00:09:13,740
우리가 각 이미지를 분류하려고한다는 것을 기억하십시오.

199
00:09:13,740 --> 00:09:16,797
CIFAR-10은 10 개 카테고리 중 하나에 속합니다.

200
00:09:16,797 --> 00:09:19,001
여기서 y라는 레이블은 정수가 될 것입니다.

201
00:09:19,001 --> 00:09:22,148
1에서 10 사이 또는 어쩌면 0에서 9 사이

202
00:09:22,148 --> 00:09:24,414
사용중인 프로그래밍 언어에 따라,

203
00:09:24,414 --> 00:09:26,245
그러나 그것은 당신에게 말하는 정수가 될 것입니다.

204
00:09:26,245 --> 00:09:30,270
그 이미지 x 각각에 대해 올바른 카테고리는 무엇입니까?

205
00:09:30,270 --> 00:09:34,484
그리고 이제 우리의 손실 함수는 L_i를 나타내며,

206
00:09:34,484 --> 00:09:36,893
그래서 우리는이 예측 함수 x를가집니다.

207
00:09:36,893 --> 00:09:40,969
예제 x와 우리의 가중치 행렬 W를 취합니다.

208
00:09:40,969 --> 00:09:42,838
y에 대한 예측을합니다.

209
00:09:42,838 --> 00:09:44,435
이미지 분류의 경우

210
00:09:44,435 --> 00:09:46,446
이것들은 우리의 10 가지 숫자가 될 것입니다.

211
00:09:46,446 --> 00:09:49,938
그런 다음 손실 함수 L_i를 정의합니다.

212
00:09:49,938 --> 00:09:52,600
예상 점수를 받아 들일 것입니다.

213
00:09:52,600 --> 00:09:54,183
함수 f에서 나온다.

214
00:09:54,183 --> 00:09:56,804
진정한 표적 또는 라벨과 함께

215
00:09:56,804 --> 00:09:59,312
얼마나 나쁜지에 대한 양적 가치

216
00:09:59,312 --> 00:10:02,510
그 예측은 그 훈련 예를위한 것입니다.

217
00:10:02,510 --> 00:10:06,127
그리고 이제 최종 손실 L은 이러한
손실의 평균이 될 것입니다

218
00:10:06,127 --> 00:10:08,976
N 개의 예제 각각에 대해 전체 데이터 세트에 대해 합산

219
00:10:08,976 --> 00:10:10,478
우리의 데이터 세트에서.

220
00:10:10,478 --> 00:10:13,432
그래서 이것은 실제로 매우 일반적인 공식입니다.

221
00:10:13,432 --> 00:10:16,421
이미지 분류를 넘어 실제로 확장됩니다.

222
00:10:16,421 --> 00:10:19,018
우리가 앞으로 나아가고 다른 일들을 볼 때,

223
00:10:19,018 --> 00:10:21,323
과제와 깊은 학습의 다른 예,

224
00:10:21,323 --> 00:10:24,426
일반적인 설정의 종류는 모든 작업에 대한 것입니다.

225
00:10:24,426 --> 00:10:26,839
너는 몇 xs와 ys가 있고 너는 적어두고 싶다.

226
00:10:26,839 --> 00:10:30,049
정확하게 얼마나 행복한지를 정량화하는 손실 함수

227
00:10:30,049 --> 00:10:33,535
당신은 당신의 특정 매개 변수 설정 W

228
00:10:33,535 --> 00:10:35,815
결국 W의 공간을 검색하게됩니다.

229
00:10:35,815 --> 00:10:39,982
교육 자료의 손실을 최소화하는 W를 찾으십시오.

230
00:10:41,081 --> 00:10:45,530
그래서 구체적인 손실 함수의 첫 번째 예제

231
00:10:45,530 --> 00:10:49,322
그것은 이미지 분류에서 작동하는 좋은 점입니다.

232
00:10:49,322 --> 00:10:52,755
우리는 다중 클래스 SVM 손실에 대해 이야기 할 것입니다.

233
00:10:52,755 --> 00:10:56,269
바이너리 SVM, 우리의 지원 벡터

234
00:10:56,269 --> 00:10:59,602
CS 229의 기계 및 다중 선 SVM

235
00:11:00,662 --> 00:11:05,263
여러 클래스를 처리하는 일반화입니다.

236
00:11:05,263 --> 00:11:09,447
229에서 볼 수 있듯이 바이너리 SVM의 경우,

237
00:11:09,447 --> 00:11:11,797
당신은 오직 두 개의 클래스 만 가지고 있습니다, 각 예제 x

238
00:11:11,797 --> 00:11:13,750
긍정적 인 것으로 분류 될 것인가?

239
00:11:13,750 --> 00:11:15,159
또는 부정적인 예를 들면,

240
00:11:15,159 --> 00:11:17,615
그러나 이제 우리에게는 10 개의
범주가 있으므로 일반화해야합니다.

241
00:11:17,615 --> 00:11:20,762
이 개념은 여러 클래스를 처리합니다.

242
00:11:20,762 --> 00:11:24,854
따라서이 손실 함수는 재미있는
함수 형태를 가지고 있습니다.

243
00:11:24,854 --> 00:11:26,206
그래서 우리는 좀 더 자세히 살펴볼 것입니다.

244
00:11:26,206 --> 00:11:29,801
슬라이드의 다음 몇 가지에 대해 꽤 자세히 설명합니다.

245
00:11:29,801 --> 00:11:33,094
그러나 이것이 말하는 것은 손실 L_i

246
00:11:33,094 --> 00:11:35,584
모든 개별적인 예를 들어, 우리가 계산할 방법

247
00:11:35,584 --> 00:11:40,298
우리는 모든 범주에 걸쳐 합계를 수행 할 것인가? Y,

248
00:11:40,298 --> 00:11:43,373
실제 범주 Y_i를 제외하고,

249
00:11:43,373 --> 00:11:46,204
그래서 우리는 모든 잘못된 카테고리를 합산하려고합니다.

250
00:11:46,204 --> 00:11:48,442
우리는 점수를 비교하려고합니다.

251
00:11:48,442 --> 00:11:50,246
올바른 카테고리의 점수와 점수

252
00:11:50,246 --> 00:11:52,509
잘못된 카테고리의

253
00:11:52,509 --> 00:11:55,079
이제 올바른 카테고리의 점수가

254
00:11:55,079 --> 00:11:59,589
잘못된 카테고리의 점수보다 큽니다.

255
00:11:59,589 --> 00:12:03,814
일부 안전 여유로 잘못된 점수보다 큼

256
00:12:03,814 --> 00:12:07,149
우리가 1로 설정 한 경우에 해당하는 경우

257
00:12:07,149 --> 00:12:11,340
진실한 점수는 다량, 또는 진실한 종류를위한 점수

258
00:12:11,340 --> 00:12:14,661
그것이 거짓 범주들 중 어느 것보다 훨씬 큰 경우,

259
00:12:14,661 --> 00:12:17,663
그러면 우리는 0의 손실을 얻습니다.

260
00:12:17,663 --> 00:12:21,703
그리고 우리는 이것을 모든 잘못된
범주들에 대해 요약 할 것입니다.

261
00:12:21,703 --> 00:12:24,454
우리의 이미지를 위해 이것은 우리에게
우리의 마지막 손실을 줄 것이다.

262
00:12:24,454 --> 00:12:27,155
데이터 세트에서이 한 예를 들어 보겠습니다.

263
00:12:27,155 --> 00:12:29,711
그리고 다시 우리는이 손실의 평균을 취할 것입니다.

264
00:12:29,711 --> 00:12:31,994
전체 교육 데이터 세트에 대해

265
00:12:31,994 --> 00:12:36,937
그래서 이런 종류의 진술은 진실

266
00:12:36,937 --> 00:12:41,263
학급 점수는 다른 사람들보다 훨씬 큽니다.

267
00:12:41,263 --> 00:12:45,160
이런 종류의 제재가 우리가 종종 압축한다면

268
00:12:45,160 --> 00:12:49,327
이 단일 최대 S_j 마이너스
S_Yi 플러스 한 가지 더하기,

269
00:12:50,496 --> 00:12:53,049
그러나 나는 항상 그 표기법을 조금 혼란스럽게 생각합니다.

270
00:12:53,049 --> 00:12:54,294
항상 도움이됩니다.

271
00:12:54,294 --> 00:12:56,678
이런 종류의 사례에 기반한 표기법으로 써라.

272
00:12:56,678 --> 00:12:58,636
두 경우가 정확히 무엇인지 알아 내야합니다.

273
00:12:58,636 --> 00:13:00,920
그리고 무슨 일이 일어나고 있는지.

274
00:13:00,920 --> 00:13:03,789
그리고 그런데,이 스타일의 손실 기능

275
00:13:03,789 --> 00:13:06,805
여기서 우리는 0의 최대 값과 다른 어떤 양을 취한다.

276
00:13:06,805 --> 00:13:10,127
종종 힌지 손실의 일종으로 불리우며,

277
00:13:10,127 --> 00:13:13,202
이 이름은 그래프의 모양에서 유래합니다.

278
00:13:13,202 --> 00:13:14,627
당신이 가서 그것을 계획 할 때,

279
00:13:14,627 --> 00:13:18,127
여기에서 x 축은 S_Yi에 해당하며,

280
00:13:19,103 --> 00:13:22,275
그것은 일부 훈련에 대한 진정한 학급의 점수입니다

281
00:13:22,275 --> 00:13:25,353
예를 들어, 이제 y 축은 손실이고,

282
00:13:25,353 --> 00:13:29,520
실제 카테고리의 점수로 볼 수 있습니다.

283
00:13:30,523 --> 00:13:32,338
이 예제가 증가하면 손실

284
00:13:32,338 --> 00:13:34,174
선형 적으로 내려갈 것이다.

285
00:13:34,174 --> 00:13:37,805
우리가이 안전 마진을 넘을 때까지,

286
00:13:37,805 --> 00:13:40,235
그 후에 손실은 0이 될 것입니다.

287
00:13:40,235 --> 00:13:44,402
우리는 이미이 예를 정확하게 분류했기 때문입니다.

288
00:13:45,550 --> 00:13:47,836
그래서, 오, 질문 하죠?

289
00:13:47,836 --> 00:13:50,464
- [Student] 죄송합니다, 표기법에 관해서

290
00:13:50,464 --> 00:13:52,410
S는 무엇입니까?

291
00:13:52,410 --> 00:13:55,170
그게 너의 옳은 점수 야?

292
00:13:55,170 --> 00:13:56,321
- 네, 그럼 질문은

293
00:13:56,321 --> 00:14:00,097
표기법에서 S는 무엇이고 SYI는 무엇입니까?

294
00:14:00,097 --> 00:14:03,906
특히, Ss는 예측 된 점수입니다.

295
00:14:03,906 --> 00:14:07,246
분류 자에서 나오는 클래스의 경우.

296
00:14:07,246 --> 00:14:10,934
그래서 하나가 고양이 클래스이고
두 개가 개 클래스이면 S1

297
00:14:10,934 --> 00:14:13,942
S2는 각각 개 고양이 점수가됩니다.

298
00:14:13,942 --> 00:14:17,434
우리는 이순신이 땅의 범주라고 말했던 것을 기억하십시오.

299
00:14:17,434 --> 00:14:21,056
예제의 진리 레이블은 정수입니다.

300
00:14:21,056 --> 00:14:25,223
그래서 S Sub Y sub i,
이중 첨자를 유감스럽게 생각합니다.

301
00:14:26,057 --> 00:14:28,966
진정한 등급의 점수에 해당하는

302
00:14:28,966 --> 00:14:32,683
훈련 세트의 i 번째 예제

303
00:14:32,683 --> 00:14:33,870
문제?

304
00:14:33,870 --> 00:14:35,921
- [Student] 그렇다면이
컴퓨팅은 정확히 무엇입니까?

305
00:14:35,921 --> 00:14:38,677
- 그래,이 컴퓨팅이 정확히 여기서 뭐지?

306
00:14:38,677 --> 00:14:41,625
그것은 조금 재미있다, 나는 그것이
더 명백하게 될 것이라고 생각한다.

307
00:14:41,625 --> 00:14:44,590
우리가 명백한 모범을 보았을 때 어떤 의미에서

308
00:14:44,590 --> 00:14:48,675
이 손실이 말하는 것은 우리가
행복하면 행복하다는 것입니다.

309
00:14:48,675 --> 00:14:51,984
점수는 다른 모든 점수보다 훨씬 높습니다.

310
00:14:51,984 --> 00:14:54,593
다른 모든 점수보다 높아야합니다.

311
00:14:54,593 --> 00:14:58,364
일부 안전 여유로, 그리고 진정한 점수

312
00:14:58,364 --> 00:15:01,892
다른 점수보다 더 높지는 않습니다.

313
00:15:01,892 --> 00:15:06,534
그러면 우리는 약간의 손실을 입을
것이고 그것은 나쁠 것입니다.

314
00:15:06,534 --> 00:15:08,657
그래서 이것은 조금 더 이해할 수 있습니다.

315
00:15:08,657 --> 00:15:10,600
명백한 예제를 살펴보면

316
00:15:10,600 --> 00:15:13,223
이 작은 세 가지 예제 데이터 세트.

317
00:15:13,223 --> 00:15:16,096
여기서 내가 사례 공간을 제거했다는 것을 기억하십시오.

318
00:15:16,096 --> 00:15:19,513
표기법을 바꾸고 0 표기법으로 다시 전환하면,

319
00:15:19,513 --> 00:15:21,110
그리고 지금 우리가 보면,

320
00:15:21,110 --> 00:15:24,544
이 다중 클래스 SVM 손실 계산에 대해 생각해 보면

321
00:15:24,544 --> 00:15:26,186
이 첫 번째 훈련 예를 들어 보자.

322
00:15:26,186 --> 00:15:28,758
왼쪽에있는 다음 반복 할 것임을 기억하십시오.

323
00:15:28,758 --> 00:15:31,913
모든 잘못된 클래스들, 그래서이 예제에서,

324
00:15:31,913 --> 00:15:34,922
고양이는 올바른 학급입니다. 그래서
우리는 차를 돌아 다닐 예정입니다.

325
00:15:34,922 --> 00:15:39,089
그리고 개구리 수업, 그리고 지금은 차를 위해, 우리는,

326
00:15:41,088 --> 00:15:44,615
우리는 차 점수 5.1에서 고양이
점수를 뺀 것을 볼 것입니다.

327
00:15:44,615 --> 00:15:48,782
3.2 플러스 1, 우리가 고양이와
차를 비교할 때 우리는 기대한다.

328
00:15:49,969 --> 00:15:53,127
차 점수가 더 높기 때문에 약간의 손실을 여기에서 겪는다.

329
00:15:53,127 --> 00:15:55,134
나쁜 고양이 점수보다.

330
00:15:55,134 --> 00:15:59,153
그래서이 한 가지 예를 들어,

331
00:15:59,153 --> 00:16:01,333
우리는 2.9의 손실을 입을 것이고,

332
00:16:01,333 --> 00:16:03,387
가서 고양이 점수를 비교하면

333
00:16:03,387 --> 00:16:06,512
그리고 우리가 본 개구리 점수는 3.2입니다.

334
00:16:06,512 --> 00:16:08,480
개구리는 마이너스 1.7,

335
00:16:08,480 --> 00:16:11,561
그래서 고양이는 개구리보다 더 큰 고양이입니다.

336
00:16:11,561 --> 00:16:14,728
이는이 두 클래스 사이에서

337
00:16:14,728 --> 00:16:16,409
우리는 손실이 전혀 없습니다.

338
00:16:16,409 --> 00:16:20,748
그럼이 훈련 예를위한 다중 클래스 SVM 손실

339
00:16:20,748 --> 00:16:23,094
각각의 쌍에 걸친 손실의 합계가 될 것입니다

340
00:16:23,094 --> 00:16:27,112
클래스는 2.9와 2.9가 될 것이며, 이는 2.9입니다.

341
00:16:27,112 --> 00:16:30,480
2.9는 양적 측정이라고 말하는 것과 같습니다.

342
00:16:30,480 --> 00:16:32,150
우리 분류가 얼마나 엉망이 됐는지의

343
00:16:32,150 --> 00:16:34,567
이 한 가지 훈련 예를 들어 보겠습니다.

344
00:16:35,795 --> 00:16:37,097
그런 다음이 절차를 반복하면

345
00:16:37,097 --> 00:16:41,574
다음 차 이미지에서 다시 진짜 클래스는 차다.

346
00:16:41,574 --> 00:16:44,051
그래서 우리는 다른 모든 범주들에 대해 반복 할 것입니다.

347
00:16:44,051 --> 00:16:47,205
우리가 차와 고양이 점수를 비교할 때,

348
00:16:47,205 --> 00:16:50,094
우리는 차가 고양이보다 하나 이상
크다는 것을 알 수 있습니다.

349
00:16:50,094 --> 00:16:52,072
그래서 우리는 여기서 손실을 입지 않습니다.

350
00:16:52,072 --> 00:16:54,219
자동차와 개구리를 비교하면 다시 볼 수 있습니다.

351
00:16:54,219 --> 00:16:57,557
차 점수가 개구리보다 하나 이상 크고,

352
00:16:57,557 --> 00:16:59,984
그래서 우리는 여기서 다시 손실을
입지 않으며, 우리의 총 손실

353
00:16:59,984 --> 00:17:02,993
이 훈련 예는 0입니다.

354
00:17:02,993 --> 00:17:05,765
그리고 지금 나는 당신이 희망적으로
사진을 찍을 것이라고 생각하지만,

355
00:17:05,765 --> 00:17:09,051
개구리, 지금 개구리를 보시면
개구리를 다시 비교해 보겠습니다.

356
00:17:09,051 --> 00:17:11,766
고양이는 개구리 점수 때문에 많은 손실을 입습니다.

357
00:17:11,766 --> 00:17:14,895
개구리와 자동차를 비교할 때 매우
낮으며 손실이 많이 발생합니다.

358
00:17:14,895 --> 00:17:18,363
점수가 매우 낮기 때문에 우리는

359
00:17:18,364 --> 00:17:19,697
예는 12.9입니다.

360
00:17:21,150 --> 00:17:23,857
전체 데이터 세트에 대한 최종 손실

361
00:17:23,857 --> 00:17:25,163
이 손실의 평균입니다.

362
00:17:25,164 --> 00:17:26,578
다른 예를 통해,

363
00:17:26,578 --> 00:17:29,277
그래서 당신이 그것들을 요약 할
때 그것은 대략 5.3에 온다.

364
00:17:29,277 --> 00:17:31,530
그러면 그것은 일종의 것입니다.
이것은 우리의 양적 측정입니다.

365
00:17:31,530 --> 00:17:34,929
우리 분류기가이 데이터 세트에서
5.3이라는 것을 알았습니다.

366
00:17:34,929 --> 00:17:36,732
질문 있니?

367
00:17:36,732 --> 00:17:39,152
- [학생] 당신은 플러스를 어떻게 선택합니까?

368
00:17:39,152 --> 00:17:41,920
- 그래, 문제는 플러스를 어떻게 선택 하느냐는거야?

369
00:17:41,920 --> 00:17:43,574
그것은 실제로 정말로 좋은 질문입니다.

370
00:17:43,574 --> 00:17:46,662
그것은 임의적 인 선택의 종류처럼 여기에 보인다.

371
00:17:46,662 --> 00:17:48,778
그것은 손실 함수에 나타나는 유일한 상수이다.

372
00:17:48,778 --> 00:17:50,950
그게 당신의 미적 감각을 불쾌하게하는 것 같아요.

373
00:17:50,950 --> 00:17:52,532
어쩌면 조금.

374
00:17:52,532 --> 00:17:53,850
그러나 이것이 다소

375
00:17:53,850 --> 00:17:57,869
우리가 실제로 신경 쓰지 않기 때문에

376
00:17:57,869 --> 00:18:00,305
점수의 절대 값에 대해

377
00:18:00,305 --> 00:18:02,795
이 손실 함수에서 우리는

378
00:18:02,795 --> 00:18:05,389
점수 사이의 상대적인 차이에 대해

379
00:18:05,389 --> 00:18:06,718
우리는 오직 정확한 점수

380
00:18:06,718 --> 00:18:08,944
잘못된 점수보다 훨씬 큽니다.

381
00:18:08,944 --> 00:18:11,540
그래서 사실 당신이 당신의 전체 W

382
00:18:11,540 --> 00:18:14,742
위 또는 아래, 모든 종류의 점수를 재조정합니다.

383
00:18:14,742 --> 00:18:17,539
그에 상응하여 세부 사항을 통해 일하는 경우

384
00:18:17,539 --> 00:18:20,808
코스 노트에 자세한 내용이 나와 있습니다.

385
00:18:20,808 --> 00:18:24,362
온라인으로, 당신은 하나의 선택이 실제로
중요하지 않다는 것을 알게됩니다.

386
00:18:24,362 --> 00:18:27,657
한 가지 종류의이 무료 매개 변수가

387
00:18:27,657 --> 00:18:29,312
이 규모로 취소됩니다.

388
00:18:29,312 --> 00:18:32,625
W의 규모의 전체 설정과 같습니다.

389
00:18:32,625 --> 00:18:34,656
그리고 코스 노트에서 조금 더 자세하게 확인하십시오.

390
00:18:34,656 --> 00:18:35,489
그걸로.

391
00:18:37,953 --> 00:18:40,374
그래서 나는 생각하는 것이 유용하다고 생각한다.

392
00:18:40,374 --> 00:18:42,519
이해하려고하는 몇 가지 질문에 대해

393
00:18:42,519 --> 00:18:45,374
직감적으로이 손실이 무엇을하는지.

394
00:18:45,374 --> 00:18:48,715
그래서 첫 번째 질문은 손실에 어떤
일이 일어날 것인가하는 것입니다.

395
00:18:48,715 --> 00:18:53,349
우리가 차 이미지의 점수를 조금만 바꾸면?

396
00:18:53,349 --> 00:18:54,182
어떤 아이디어?

397
00:18:56,479 --> 00:18:59,856
모두들 질문하기가 너무 무서워요?

398
00:18:59,856 --> 00:19:00,689
대답?

399
00:19:00,689 --> 00:19:04,272
[희미하게 말하는 학생]

400
00:19:06,983 --> 00:19:10,013
- 그래, 대답은 우리가 점수를 흔들면

401
00:19:10,013 --> 00:19:13,333
이 차 이미지에 대해 조금은 손실이 변하지 않을 것입니다.

402
00:19:13,333 --> 00:19:15,626
따라서 SVM 손실은 기억하고있는 유일한 것입니다.

403
00:19:15,626 --> 00:19:19,073
약 1보다 큰 점수를 얻고 있습니다.

404
00:19:19,073 --> 00:19:21,918
잘못된 점수보다 더 많이,하지만이 경우,

405
00:19:21,918 --> 00:19:25,506
차 점수는 이미 다른 사람보다는 확실히 더 크다,

406
00:19:25,506 --> 00:19:28,048
그래서이 클래스의 점수가이 예제에서 바뀌면

407
00:19:28,048 --> 00:19:30,665
조금 바뀌었다.

408
00:19:30,665 --> 00:19:33,270
여전히 유지되며 손실은 변하지 않을 것입니다.

409
00:19:33,270 --> 00:19:35,437
우리는 여전히 손실이 0이 될 것입니다.

410
00:19:36,870 --> 00:19:39,317
다음 질문은 최소 및 최대 손실 가능성

411
00:19:39,317 --> 00:19:40,150
SVM에 대한?

412
00:19:43,265 --> 00:19:44,445
[희미하게 말하는 학생]

413
00:19:44,445 --> 00:19:45,764
오, 나는 중얼 거리는 소리가 들린다.

414
00:19:45,764 --> 00:19:49,374
따라서 최소 손실은 0입니다.

415
00:19:49,374 --> 00:19:52,018
정확한 점수가 많으면 모든 수업에서

416
00:19:52,018 --> 00:19:55,533
더 크면 모든 수업에서 손실이 0 점이됩니다.

417
00:19:55,533 --> 00:19:57,357
그리고 그것은 0이 될 것이고,

418
00:19:57,357 --> 00:20:00,652
그리고 우리가 가진이 경첩 손실
음모로 돌아가서 생각해 보면,

419
00:20:00,652 --> 00:20:03,495
그러면 정확한 점수가

420
00:20:03,495 --> 00:20:06,192
매우 부정적인면, 우리는

421
00:20:06,192 --> 00:20:07,981
잠재적으로 무한한 손실.

422
00:20:07,981 --> 00:20:11,538
그래서 min은 0이고 max는 무한대입니다.

423
00:20:10,738 --> 00:20:15,320
처음부터 교육을 시작하십시오.

424
00:20:15,320 --> 00:20:20,442
결과적으로 당신의 점수가 작은 무작위 값으로

425
00:20:20,442 --> 00:20:24,735
훈련 시작시.

426
00:20:24,735 --> 00:20:31,449
모든 점수가 대략 0이고 대략 동일하면,

427
00:20:31,449 --> 00:20:34,940
그러면 다중 클래스 SVM을 사용할 때 어떤 종류의 손실이 발생합니까?
멀티 클래스 SVM을 사용할 때?

428
00:20:35,741 --> 00:20:37,502
- [학생] 수업 수에서 1을 뺍니다.

429
00:20:42,448 --> 00:20:53,738
왜냐면 Loss를 계산할때 정답이 아닌 클래스를 순회합니다. 그러면
C - 1 클래스를 순회하겠죠. 비교하는 두 스코어가 거의 비슷하니

430
00:20:53,738 --> 00:20:57,536
Margin때문에 우리는 1 스코어를 얻게 될 것입니다. 
그리고 전에 Loss는 C - 1을 얻게 되는 것이죠

431
00:20:57,536 --> 00:21:01,964
이는 실제로 유용합니다. 특히 "디버깅 전략"으로 말이죠

432
00:21:01,964 --> 00:21:04,734
여러분들이 이런 전략을 가지고 트레이닝을 시작한다면

433
00:21:04,734 --> 00:21:08,082
여러분들은 Loss가 어떻게 될지를 짐작할 수 있게 됩니다.

434
00:21:08,082 --> 00:21:14,999
다시말해 트레이닝을 처음 시작할때 Loss가 C-1이 아니라면

435
00:21:14,999 --> 00:21:16,353
아마 버그가 있는 것이고 고쳐야 할 것입니다.

436
00:21:16,353 --> 00:21:20,905
실제로 아무 유용합니다.

437
00:21:21,886 --> 00:21:31,487
또 다른 질문입니다. SVM Loss는 정답인 클래스는 빼고 다 더했습니다.
그렇다면 정답인 놈도 같이 더하면 어떻게될까요?

438
00:21:31,487 --> 00:21:34,323
만약 다 더하면?

439
00:21:34,323 --> 00:21:36,034
- [학생이 대답]

440
00:21:36,034 --> 00:21:39,326
답은 Loss에 1이 더 증가한다는 것입니다.

441
00:21:39,326 --> 00:21:45,052
일제로 우리가 정답 클래스만 빼고 계산하는 이유는,
일반적으로 Loss가 0이 되야지만

442
00:21:45,052 --> 00:21:52,755
우리가 "아무것도 잃는 것이 없다"고 쉽게 해석할 수 있으며.

443
00:21:52,755 --> 00:21:56,484
Loss에 모든 클래스를 다 더한다고 해서 
다른 분류기가 학습되는 것은 아닙니다.

444
00:21:56,484 --> 00:22:02,729
하지만 관례상 정답 클래스는 빼고 계산을 하며, 
그렇게 되면 최소 Loss는 0이 됩니다.

445
00:22:04,931 --> 00:22:08,008
또다른 질문입니다. Loss에서 전체 합을 쓰는게 아니라 
평균을 쓰면 어떻게 될까요?

446
00:22:09,943 --> 00:22:11,233
- [학생]

447
00:22:11,233 --> 00:22:13,076
네 답은 영향을 미치지 않는다는 것입니다.

448
00:22:13,076 --> 00:22:15,788
클래스의 수는 어짜피 정해져 있으니

449
00:22:15,788 --> 00:22:20,500
평균을 취한다는건 그저 손실 함수를 리스케일 할 뿐입니다.

450
00:22:20,500 --> 00:22:23,991
그러니 상관이 없을 것입니다. 
단지 스케일만 변할 뿐입니다.

451
00:22:23,991 --> 00:22:29,664
왜냐하면 우리는 스코어 값이 몇인지는 신경쓰지 않기 때문입니다.

452
00:22:30,541 --> 00:22:37,934
자 이제 또 다른 질문입니다. 만약 우리가 손실함수를
아래와 같이 제곱 항으로 바꾸면 어떻게 될까요?

453
00:22:37,934 --> 00:22:42,968
그럼 결과가 달라질까요 같이질까요?

454
00:22:42,968 --> 00:22:44,178
- [학생]

455
00:22:44,178 --> 00:22:45,213
결과는 달라질 것입니다.

456
00:22:45,213 --> 00:22:50,903
좋은것 과 나쁜것 사이의 트레이드 오프를 
비 선형적인 방식으로 바꿔주는 것인데요

457
00:22:50,903 --> 00:22:54,264
그렇게 되면 손실함수의 계산 자체가 바뀌게 됩니다.

458
00:22:54,264 --> 00:22:59,915
실제로도 squared hinge loss를 종종 사용합니다.

459
00:22:59,915 --> 00:23:05,066
이는 여러분들이 손실함수를 설계할때 
쓸 수 있는 한가지 방법이 될 수 있습니다.

460
00:23:05,066 --> 00:23:07,831
질문 있나요?

461
00:23:07,831 --> 00:23:11,596
- [학생]

462
00:23:11,596 --> 00:23:16,760
질문은, 왜 굳이 제곱 항을 고려해야 하냐는 것입니다.

463
00:23:16,760 --> 00:23:22,281
손실 함수의 요지는 "얼마나 구린지"를 정량화 하는 것입니다.

464
00:23:22,281 --> 00:23:25,143
그리고 분류자가 다양한 종류의 실수를 저지르고 있다면,

465
00:23:25,143 --> 00:23:30,141
어떻게 해야 이 분류기가 만드는 다양한 Loss들 마다
상대적으로 패널티를 부여할 수 있을까요?

466
00:23:30,141 --> 00:23:37,834
만약 Loss에 제곱을 한다면 이제 "엄청 엄청
안좋은 것들" 은 정말로 "곱절로 안좋은 것" 이 됩니다.

467
00:23:37,834 --> 00:23:39,192
그러니 정말로 나빠지게 되는겁니다.

468
00:23:39,192 --> 00:23:43,623
마치 우리가 심하게 잘못 분류되는 것들을 
정말 원하지 않는 것이 됩니다.

469
00:23:43,623 --> 00:23:47,321
반면에 hinge loss를 사용하게 되면

470
00:23:47,321 --> 00:23:52,553
실제로 "조금 잘못된 것" 과 "많이 잘못된 것" 을 크게 신경쓰지
않게 되는 것입니다.

471
00:23:52,553 --> 00:23:57,051
만약 "많이 잘못된 것" 이 있다면 우리는 Loss를 증가시키고
학습을 통해 Loss를 줄일 것인데

472
00:23:57,051 --> 00:24:04,375
그 줄어드는 Loss의 량이 "조금 잘못된 것"이던  "많이 잘못된 것"
이던 큰 차이가 없을 것입니다.

473
00:24:04,375 --> 00:24:06,268
제가 좀 개략적으로 말씀드리긴 했지만

474
00:24:06,268 --> 00:24:13,597
둘 중 어떤 loss를 선택하느냐는 우리가 에러에 대해 
얼마나 신경쓰고 있고, 그것을 어떻게 정량화 할 것인지에 달려있습니다.

475
00:24:13,597 --> 00:24:17,938
그리고 이 문제는 실제 여러분들이 손실함수를 만들때
고려해야만 하는 것입니다.

476
00:24:17,938 --> 00:24:22,770
왜냐하면 손실 함수라는 것이 여러분이 여러분들의 알고리즘에게 
"어떤 에러를 내가 신경쓰고 있는지"

477
00:24:22,770 --> 00:24:26,267
그리고 "어떤 에러가 트레이드오프 되는 것인지" 를 알려주는 것입니다.

478
00:24:26,267 --> 00:24:30,407
그러니 실제로는 여러분들의 문제에 따라서 손실함수를
잘 설계하는 것은 엄청 중요하다고 할 수 있습니다.

479
00:24:32,315 --> 00:24:40,962
여기 Numpy 코드가 있습니다. 아마 여러분들도 첫 과제에서
이처럼 구현해야 할 것입니다.

480
00:24:40,962 --> 00:24:48,768
여기서 말씀드리고 싶었던건 Numpy를 사용하면 코드 몇줄이면
이 손실 함수를 코딩할 수 있다는 것입니다.

481
00:24:48,768 --> 00:24:50,926
여기 재미있는 기법을 하나 볼 수 있습니다.

482
00:24:50,926 --> 00:24:57,856
max로 나온 결과에서 정답 클래스만 0으로 만들어 주는 것이죠
(margin[y] = 0)

483
00:24:57,856 --> 00:25:06,069
이것은 굳이 전체를 순회할
필요가 없게 해주는 일종의 vectorized 기법입니다.

484
00:25:06,069 --> 00:25:09,854
전체 합을 구할때 제외하고 싶은 부분만 0으로 만들어줍니다.

485
00:25:09,854 --> 00:25:13,315
아마 과제를 할 때 아주 유용할 것입니다.

486
00:25:13,315 --> 00:25:17,106
자 이제 손실 함수에 대한 또 다른 질문이 있습니다.

487
00:25:17,106 --> 00:25:24,139
운이 좋게도 W가 0인 정답을 찾았다고 해 봅시다. 잃은 것이
전혀 없는 것이고, 여러분이 이긴 것입니다!

488
00:25:24,139 --> 00:25:32,390
그렇다면 여기서 질문입니다. 이렇게 Loss가 0이 되게 하는 
W가 유일하게 하나만 존재하는 것일까요?

489
00:25:33,251 --> 00:25:34,271
- [학생]

490
00:25:34,271 --> 00:25:37,444
맞습니다. 다른 W도 존재합니다.

491
00:25:37,444 --> 00:25:43,976
특히나 좀 전에도 언급했듯이 W의 스케일은 변합니다.

492
00:25:43,976 --> 00:25:52,978
그렇다면 W에 두배를 한다고 해도 변하지 않을 것입니다. 
(double double U 니까 quad U인가요?, 모르겠네요)

493
00:25:52,978 --> 00:25:54,110
[웃음]

494
00:25:54,110 --> 00:25:56,601
어쨋든 그 W도 역시 Loss가 0 이겠죠

495
00:25:56,601 --> 00:25:58,568
그래서 이것의 구체적인 예로서,

496
00:25:58,568 --> 00:26:00,066
좋아하는 예제로 돌아갈 수 있습니다.

497
00:26:00,066 --> 00:26:01,184
어쩌면 숫자를 통해 일할 수도 있습니다.

498
00:26:01,184 --> 00:26:02,351
나중에 조금,

499
00:26:02,351 --> 00:26:10,583
만약 여러분에게 W와 2W가 있다면, 정답 스코어 와 정답이 아닌
스코어의 차이의 마진(margins)또한 두배가 될 것입니다.

500
00:26:10,583 --> 00:26:14,521
그러니 모든 마진(margins)이 이미 1보다 더 크다면, 
우리가 두배를 한다고 해도

501
00:26:14,521 --> 00:26:18,857
여전히 1보다 클 것이고 Loss가 0 일 것입니다.

502
00:26:20,180 --> 00:26:24,572
여기서 아주 흥미로운 점이 있습니다.

503
00:26:24,572 --> 00:26:29,045
손실 함수 라는 것이. 분류기에게 우리는 어떤 W를 찾고 있고 
어떤 W에 신경쓰고 있는지를 말해주는 것이라면

504
00:26:29,045 --> 00:26:31,795
이것은 조금 이상합니다.
"불일치" 하는 점이 있습니다.

505
00:26:31,795 --> 00:26:39,112
다양한 W중 Loss가 0인 것을 선택하는 것은 모순입니다.

506
00:26:40,099 --> 00:26:45,303
왜냐하면 여기에서는 오직 데이터의 loss에 대해서만 
신경쓰고 있고

507
00:26:45,303 --> 00:26:52,239
그리고 분류기에게 트레이닝 데이터에 꼭 맞는 W를
찾으라고 말하는 것 과 같습니다.

508
00:26:52,239 --> 00:26:56,465
하지만 실제로 우리는 트레이닝 데이터에 얼마나 꼭 
맞는지는 전혀 신경쓰지는 않습니다.

509
00:26:56,465 --> 00:27:01,553
기계학습의 핵심은, 트레이닝 데이터를 이용해서
어떤 분류기를 찾는 것인데

510
00:27:01,553 --> 00:27:04,222
그 분류기는 테스트 데이터에 적용할 것이기 때문입니다.

511
00:27:04,222 --> 00:27:06,858
그러니 우리는 트레이닝 데이터의 성능에는 
관심이 있는 것이 아니라

512
00:27:06,858 --> 00:27:10,928
우리는 테스트 데이터에서의 성능에 관심이 있는 것입니다.

513
00:27:10,928 --> 00:27:16,176
그러니 분류기에게 트레이닝 데이터의 Loss에만 신경쓰라고 한다면

514
00:27:16,176 --> 00:27:20,307
아마 좋지 않은 상황이 벌어질 것입니다.

515
00:27:20,307 --> 00:27:24,079
아마 분류기가 이해할 수 없는 행동을 할 수도 있습니다.

516
00:27:24,079 --> 00:27:27,786
좀 더 구체적인 예를 들어 보겠습니다.

517
00:27:27,786 --> 00:27:32,918
하지만 지금의 예는, 지금까지 배웠던 선현 분류기에 대한 것은 아니고 
기계학습에서 다루는 좀 더 일반적인 개념이라 보시면 됩니다.

518
00:27:32,918 --> 00:27:35,815
여기 파란 점의 데이터 셋이 있습니다.

519
00:27:35,815 --> 00:27:39,827
이제 우리가 할 일은 어떤 곡선을 가지고 
저 파란색 점들에 피팅시키는 것입니다.

520
00:27:39,827 --> 00:27:44,532
우리가 분류기에게 유일하게 말할 수 있는 것은, 그저 
트레이닝 데이터에 핏 하게 하라고 말하는 것입니다.

521
00:27:44,532 --> 00:27:49,647
그러면 분류기는 모든 트레이닝 데이터를 완벽하게 분류해
내기 위해서 구불구불한 곡선을 만들 것입니다.

522
00:27:49,647 --> 00:27:53,519
하지만 이러면 안좋습니다. 왜냐하면 이 상황에서는
"성능" 에 대해서 전혀 고려하지 않았기 때문입니다.

523
00:27:53,519 --> 00:27:56,329
우린 항상 테스트 데이터의 성능을 고려해야 합니다.

524
00:27:56,329 --> 00:28:00,710
만약 새로운 데이터가 들어오게 되면

525
00:28:00,710 --> 00:28:03,872
앞서 만든 구불구불한 곡선은 완전히 틀리게 됩니다.

526
00:28:03,872 --> 00:28:09,334
사실 우리가 의도했던건 초록색 선인 것입니다.

527
00:28:09,334 --> 00:28:15,171
완벽하게 트레이닝 데이터에 핏한 
복잡하고 구불 구불한 곡선을 원한 것이 아닌 것입니다.

528
00:28:15,171 --> 00:28:19,232
이는 기계학습에서 가장 중요한 문제 입니다.

529
00:28:19,232 --> 00:28:22,816
그리고 보통 이를 해결하는 방법을 통틀어
Regularization이라 합니다.

530
00:28:22,816 --> 00:28:26,443
손실 함수에 항을 하나 추가합니다.

531
00:28:26,443 --> 00:28:30,255
"Data Loss Term"에서는 분류기가 트레이닝 데이터에 핏하게 하고

532
00:28:30,255 --> 00:28:36,057
또한, 보통 손실 함수에 "Regularization term"을 추가하는데

533
00:28:36,057 --> 00:28:40,691
이는 모델이 좀 더 단순한 W를 선택하도록 도와줍니다.

534
00:28:40,691 --> 00:28:45,992
"단순하다" 라는 개념은 우리가 해결해야 할 문제나 
모델에 따라 조금씩 달라집니다.

535
00:28:47,925 --> 00:28:52,868
과학 계에서 널리 쓰이는 "오컴의 면도날" 이라는 말이 있습니다.

536
00:28:52,868 --> 00:28:59,158
만약 여러분들에게 다양한 가설들을 가지고 있고, 
그 가설들 모두가 어떤 현상에 대해 설명 가능하다면

537
00:28:59,158 --> 00:29:01,039
그럼 여러분은 일반적으로 "더 단순한 것" 을 선호해야 한다는 것입니다.

538
00:29:01,039 --> 00:29:06,801
왜냐하면 좀 "더 일반적인(더 단순한)" 것이
미래1에 일어날 현상을 잘  설명할 가능성이 더 높기 때문입니다.

539
00:29:06,801 --> 00:29:12,538
 그리고 기계학습에서는 이런 류의 직관을 써먹기 위해서 
"Regularization penalty"라는 것을 만들어 냈습니다.

540
00:29:12,538 --> 00:29:15,121
Regularization은 보통 R로 표기합니다.

541
00:29:16,312 --> 00:29:20,409
그렇게 되면, 일반적인 손실 함수의 형태는 
두가지 항을 가지게 됩니다.

542
00:29:20,409 --> 00:29:22,417
Data loss와 Regularization loss입니다.

543
00:29:22,417 --> 00:29:27,200
그리고 하이퍼파라미터인 람다(lambda)도 생겼습니다. 
두 항 간의 트레이드오프입니다.

544
00:29:27,200 --> 00:29:31,054
지난 강의에서 하이퍼파라미터와 크로스벨리데이션을 배웠습니다.

545
00:29:31,054 --> 00:29:39,363
여기 Regularization의 하이퍼파라미터 람다는 실제로 여러분들이
모델을 훈련시킬 때 고려해야 할 중요한 요소 중 하나입니다.

546
00:29:40,229 --> 00:29:41,062
질문 있나요?

547
00:29:42,097 --> 00:29:48,846
- [학생 질문]

548
00:29:50,685 --> 00:29:51,941
질문은 바로

549
00:29:51,941 --> 00:29:58,072
Wx + lambda R 이란 것이 도대체 뭐길래 구불구불한 곡선을
직선으로 바꿔 준다는 것입니까? 라는 것입니다.

550
00:29:59,912 --> 00:30:03,550
사실 그 부분을 깊에 다루려면 너무 오래걸려서
여기에서는 언급하려 하지 않았습니다.

551
00:30:03,550 --> 00:30:09,830
하지만 짐작해 볼 수 있습니다. 여러분들에게 회귀문제가 있습니다.
일종의 다항식 기저 함수로 나타낼 수 있겠죠

552
00:30:09,830 --> 00:30:15,775
일반적으로 모델은 고차 다항식을 이용해서 문제를 풀려 할 것입니다.

553
00:30:15,775 --> 00:30:25,569
여기에 어떤 regression term을 추가하면 모델이 데이터를 
핏 할 떄 저차 다항식을 더 선호하도록 하게 하는 것입니다.

554
00:30:25,569 --> 00:30:29,021
그러니 여러분은 Regularization의 두 가지 
역할에 대해 생각해 볼 수 있습니다.

555
00:30:29,021 --> 00:30:32,897
하나는 여러분의 모델이 더 복잡해 지지 못하도록 하는 것이고

556
00:30:32,897 --> 00:30:40,846
또는 모델에 soft penalty를 추가하는 것으로 보는 것입니다.
모델은 여전히 더 복잡한 모델이 될 가능성이 있는 것입니다.

557
00:30:40,846 --> 00:30:43,112
이 다항식의 예라면, 더 복잡한 차수에도 접근할 수 있는 것이지요

558
00:30:43,112 --> 00:30:51,316
하지만 이 soft한 제약을 하나 추가하면, "만약 너가 복잡한 모델을 계속
쓰고싶으면, 이 penalty를 감수해야 할 거야!" 라는 것과 같습니다.

559
00:30:51,316 --> 00:30:55,282
이것으로 인해 완전한 선형 분류문제가 되진 않겠지만

560
00:30:55,282 --> 00:31:01,691
많은 사람들이 "Regularization"에 대해 이런 식으로 떠올립니다.

561
00:31:02,731 --> 00:31:06,917
실제로 Regularization에는 여러 종류가 있습니다.

562
00:31:06,917 --> 00:31:11,022
가장 보편적인 것은 L2 Regularization입니다. 
Weight decay라고도 하죠.

563
00:31:11,022 --> 00:31:13,905
이 외에도 다양한 것이 많습니다.

564
00:31:13,905 --> 00:31:19,470
L2 Regularization은 가중치 행렬W에 대한
Euclidean Norm입니다.

565
00:31:19,470 --> 00:31:21,892
간혹 squared norm이라고도 하죠

566
00:31:21,892 --> 00:31:26,738
또는 1/2 * squared norm 을 사용하기도 합니다. 
미분이 더 깔끔해집니다.

567
00:31:26,738 --> 00:31:32,650
하지만 L2 Regularization의 주요 아이디어는 
가중치 행렬 W의 euclidean norm에 패널티를 주는 것입니다.

568
00:31:32,650 --> 00:31:38,957
그리고 L1 regularization도 보게 될 것인데, 
이는 L1 norm으로 W에 패널티를 부과하는 것입니다.

569
00:31:38,957 --> 00:31:46,401
L1 Regularization을 하면 행렬 W가 희소행렬이 되도록 합니다.

570
00:31:46,401 --> 00:31:52,744
그리고 Elastic net regularization도 있습니다. 
L1과 L2를 짬뽕해놓은 것이죠

571
00:31:52,744 --> 00:32:00,004
간혹 Max norm regularization도 보게 될 것입니다. 
L1, L2 대신에 max norm을 쓰는 것이죠

572
00:32:01,119 --> 00:32:07,374
하지만 이런 regularization은 딥러닝에서만 쓰는 것이 아니라 
많은 기계학습 분야를 통틀어서 자주 사용하는 것입니다.

573
00:32:07,374 --> 00:32:10,997
심지어는 더 광범위하게 최적화 할 수도 있습니다.

574
00:32:10,997 --> 00:32:17,138
Regularization 기법 중 특히나 딥러닝에 관련된 기법들을
앞으로 배우게 될 것입니다.

575
00:32:17,138 --> 00:32:25,157
예를 들어, 앞으로 배울 Dropout이나 Batch Normalization
, Stochastic depth와 같은 최근 아주 쩌는 것들이죠

576
00:32:25,157 --> 00:32:37,061
Regularization은 모델이 트레이닝 데이터셋에 완벽히 
핏 하지 못하도록 모델의 복잡도에 penalty를 부여하는 방법입니다.

577
00:32:37,061 --> 00:32:38,306
질문 있나요?

578
00:32:38,306 --> 00:32:41,889
[말하는 학생]

579
00:32:44,858 --> 00:32:50,186
질문은, "L2 Regularization이 모델이 복잡한지 아닌지를
어떻게 알(측정) 수 있냐" 는 것입니다.

580
00:32:50,186 --> 00:32:54,202
고맙게도 다음에 할 내용이 바로 그것입니다.

581
00:32:55,437 --> 00:33:00,058
여기 한 예시가 있습니다. 트레이닝 데이터 x가 있고 
서로 다른 두개의 W가 있습니다.

582
00:33:00,058 --> 00:33:07,157
x는 4줄짜리 벡터이고, 여기에 두개의 서로 다른 W에 대해
생각해 볼 수 있겠습니다. x = [1 1 1 1]

583
00:33:07,157 --> 00:33:10,367
하W중 하나는 처음에만 1이 있고 나머지 세 원소는 0입니다.
w1 = [1 0 0 0]

584
00:33:10,367 --> 00:33:14,191
다른 하나는 원소가 모두 0.25입니다.
 w2 = [0.25 0.25 0.25 0.25]

585
00:33:14,191 --> 00:33:19,702
이걸 가지고 Linear classification을 할때 
우리는 x와 w의 내적을 구합니다.(dot product)

586
00:33:19,702 --> 00:33:24,747
Linear classification의 관점에서 w1와 w2는 같습니다.

587
00:33:24,747 --> 00:33:28,302
왜냐하면 x와의 내적이 서로 같기 때문이죠.

588
00:33:28,302 --> 00:33:34,383
여기서 문제는, w1, w2중 L2 regression이 
더 선호하는것은 어떤 것일까요?

589
00:33:36,052 --> 00:33:41,030
L2 regression은 w2를 더 선호할 것입니다.
왜냐면 L2 regression에서는 w2가 더 norm이 작기 때문이죠

590
00:33:41,030 --> 00:33:49,440
그러니 L2 Regression은 분류기의 복잡도를 w1와 w2중 어떤 것이 
더 상대적으로 coarse한지를 측정합니다. (값이 매끄러워야함)

591
00:33:49,440 --> 00:34:01,920
Linear classification에서 W가 의미하는 것은, "얼마나 x가
Output Class와 닮았는지" 입니다.

592
00:34:01,920 --> 00:34:09,080
그러니 L2 Regularization이 말하고자 하는것은 
x의 모든 요소가 영향을 줬으면 하는 것입니다.

593
00:34:09,080 --> 00:34:14,585
그러니 변동이 심한 어떤 입력 x가 있고 
그 x의 특정 요소에만 의존하기 보다

594
00:34:14,585 --> 00:34:22,059
모든 x의 요소가 골고루 영향을 미치길 원한다면, 
L2 Regularization을 통해 더 강인해 질 수 있을 것입니다.

595
00:34:22,060 --> 00:34:26,839
반면 L1 Regularization의 경우에는 정 반대입니다.

596
00:34:26,839 --> 00:34:32,946
L1 Regularization을 쓰게 되면 W2보다는 
W1을 더 선호하게 됩니다.

597
00:34:32,946 --> 00:34:35,595
L1 Regularization은 "복잡하다"는 걸 다르게 생각합니다.

598
00:34:35,595 --> 00:34:46,080
L1 Regularization은 가중치 W에 0의 갯수가 많으면
"복잡하다" 라고 생각합니다.

599
00:34:46,080 --> 00:34:51,917
그러니 "복잡함" 을 어떻게 정의하느냐, 그리고 
L2 Regularization은 "복잡함"을 어떻게 측정하느냐는

600
00:34:51,917 --> 00:34:54,196
우리가 어떤 문제를 가지고 있으냐에 따라 다릅니다.

601
00:34:54,196 --> 00:34:58,921
그러니 여러분이 어떤 문제가 있고, 어떤 모델이 있고 데이터가 있을때

602
00:34:58,921 --> 00:35:02,837
여러분은 이 문제에서 "복잡하다"는 것을
어떻게 정의할 지를 반드시 고민해야 할 것입니다.

603
00:35:03,921 --> 00:35:04,788
질문 있나요?

604
00:35:04,788 --> 00:35:09,129
- [학생]

605
00:35:09,129 --> 00:35:10,385
네 맞습니다.

606
00:35:10,385 --> 00:35:13,830
w1과 w2의 경우에는 L1의 값이 같습니다.

607
00:35:15,193 --> 00:35:21,546
하지만 여러분은 L1 Regularization이 선호하는 w1를
손쉽게 만들어 볼 수 있습니다.

608
00:35:21,546 --> 00:35:26,908
L1에 대한 일반적인 직관은
"일반적으로 L1은 sparse한 solutions을 선호한다"는 것입니다.

609
00:35:26,908 --> 00:35:35,016
그리고 이것은 W의 요소 중 대부분이 0이 되게 할 것입니다.

610
00:35:35,016 --> 00:35:40,191
그렇게 때문에 L1이 "복잡하다"고 느끼고 측정하는 것은
0이 아닌 요소들의 갯수가 될 수 있습니다.

611
00:35:40,191 --> 00:35:45,682
반면 L2의 경우에는 W의 요소가 전체적으로 퍼져있을 때
"덜 복잡하다" 라고 생각하게 됩니다.

612
00:35:45,682 --> 00:35:48,920
그러니 선택은 여러분들의 데이터와 문제에 달려 있습니다.

613
00:35:48,920 --> 00:35:54,584
좀 더 나아가서, 여러분이 Bayesian에 대해 잘 안하면 
L2 Regularization을 사용하다는 것을

614
00:35:54,584 --> 00:35:58,897
parameter vector로 Gaussian prior를 쓰는 
MAP inference로 볼 수도 있다는 것입니다.

615
00:35:58,897 --> 00:36:05,343
해당 하는 내용이 CS229의 과제에 있었던 것으로 기억하지만
이 강의에서는 더이상 다루지는 않을 것입니다.

616
00:36:05,343 --> 00:36:11,400
지금 우리는 multi-class SVM Loss에 대해
깊게 알아보고 있는 중입니다.

617
00:36:11,400 --> 00:36:12,783
질문 있나요?

618
00:36:12,783 --> 00:36:32,648
[학생 질문]

619
00:36:33,982 --> 00:36:39,798
질문은, Regularization된다고 
Hypothesis class가 변하지 않을 것이라는 것입니다.

620
00:36:39,798 --> 00:36:44,036
그리고 Linear classifier 라는 것이 변하지 않을 것이라는
것입니다.

621
00:36:44,036 --> 00:36:50,038
이 Polynomial regression의 예는 확실이
Linear regression은 아닙니다.

622
00:36:50,038 --> 00:36:56,826
입력의 차수가 증가하는 상황에서의 Linear regression으로
볼 수는 있습니다.

623
00:36:56,826 --> 00:37:05,802
이 경우에는 우리가 많은 polynomial coefficients를 
사용하고 싶지는 않다는 것을 의미하는 것입니다.

624
00:37:05,802 --> 00:37:07,385
이런 식으로 생각해 보시면 될 것 같습니다.

625
00:37:07,385 --> 00:37:09,290
여러분이 polynomial regression을 한다고 했을때

626
00:37:09,290 --> 00:37:17,963
f(x)=A0 * 1 + A1 * x + A2 * x^2 + A3 * x^3
와 같은 식으로 나타낼 수 있겠죠

627
00:37:17,963 --> 00:37:23,093
이 경우 우리가 가진 parameters는 A집합이 되겠습니다.

628
00:37:24,211 --> 00:37:28,190
A에 penalty를 준다는 것은 polynomial의 차원이
더 적어지도록 하는 것입니다.

629
00:37:28,190 --> 00:37:31,491
Polynomial regression의 경우가 아니라면 
실제로 A의 형태로 파라미터가 생기진 않을 것입니다.

630
00:37:31,491 --> 00:37:34,725
여러분이 사용하고 싶은 다른 파마미터를 설정하게 될 것입니다.

631
00:37:34,725 --> 00:37:36,364
그러나 그것은 일반적인 생각입니다,

632
00:37:36,364 --> 00:37:44,285
hypothesis class 중에 더 간단한 것을 선택하기 위해서 
우리는 model에 Penalty를 주는 것입니다.

633
00:37:45,229 --> 00:37:50,340
아직 헷갈리면 개인적으로 다시 질문해 주시기 바랍니다.

634
00:37:50,340 --> 00:37:56,349
지금까지 Multi-class SVM Loss에 대해 알아보았습니다.
그리고 부차적으로 또 하나가 있습니다.

635
00:37:56,349 --> 00:38:03,097
지금까지 본 것은 SVM Loss를 Multi class로 확장시킨 
아주 일반적인 손실함수일 뿐입니다.

636
00:38:03,097 --> 00:38:06,596
여러 문헌을 찾아보게 되면 몇가지 다른 공식들도 존재합니다.

637
00:38:06,596 --> 00:38:13,813
하지만, 적어도 딥러닝에서는 그런 여러가지 손실함수들이
사실은 거의 비슷하게 동착합니다.

638
00:38:13,813 --> 00:38:19,949
그렇기 때문에, 이 수업에서는 지금까지 배웠던 
multi-class SVM loss만 가지고 이야기할 것입니다.

639
00:38:21,061 --> 00:38:25,158
하지만 당연히 여러분이 생각해볼만한 여러 다양한
손실 함수들이 존재합니다.

640
00:38:25,158 --> 00:38:30,603
multi-class SVM loss 외에도 인기있는 것이 하나 있습니다.

641
00:38:31,761 --> 00:38:37,769
딥러닝에서 자주쓰는 Multinomial logistic 
regression, 즉 softmax입니다.

642
00:38:39,405 --> 00:38:43,222
사실 딥러닝에서는 이걸 더 많이 씁니다.

643
00:38:43,222 --> 00:38:48,127
하지만 몇가지 이유가 있어 두번째로 소개해 드립니다.

644
00:38:48,127 --> 00:38:55,096
multi-class SVM loss에서 우리는 스코어 자체에 대한
해석은 고려하지 않았습니다.

645
00:38:55,096 --> 00:39:02,670
어떤 분류문제가 있고 어떤 모델 F가 각 클래스에
해당하는 10개의 숫자를 출력할때면

646
00:39:02,670 --> 00:39:07,787
mulfi-class SVM의 경우에 우리는 그 스코어 자체는
크게 신경쓰지 않았습니다.

647
00:39:07,787 --> 00:39:13,497
우린 단지 정답 클래스가 정답이 아닌 클래스들 보다 더 높은
스코어를 내기만을 원했죠

648
00:39:13,497 --> 00:39:17,712
그 이상, 스코어 자체가 실제로 의미하는 것에는
관심이 없었습니다.

649
00:39:17,712 --> 00:39:27,668
하지만 Multinomial Logistic regression의 손실함수는 
스코어 자체에 추가적인 의미를 부여합니다.

650
00:39:27,668 --> 00:39:33,907
그리고 특히, 저 수식을 이용해서 스코어를 가지고 클래스 별
확률 분포를 계산하게 될 것입니다.

651
00:39:33,907 --> 00:39:37,324
여기에서는 softmax라고 불리는 함수를 쓸 것입니다.

652
00:39:37,324 --> 00:39:43,192
스코어를 전부 이용하는데, 스코어들에 지수를 취해서
양수가 되게 만듭니다.

653
00:39:43,192 --> 00:39:46,540
그리고 그 지수들의 합으로 다시 정규화 시킵니다.

654
00:39:46,540 --> 00:39:53,053
그래서 softmax 함수를 거치게 되면
우리는 결국 확률 분포를 얻을 수 있고

655
00:39:53,053 --> 00:39:55,792
그것은 바로 해당 클래스일 확률이 되는 것입니다.

656
00:39:55,792 --> 00:39:58,193
 확률이기 때문에 0에서 1 사이의 값이고

657
00:39:58,193 --> 00:40:02,287
모든 확률들의 합은 1이 됩니다.

658
00:40:03,954 --> 00:40:07,169
그리고 이제 해석은 우리가 원하는 것입니다.

659
00:40:07,169 --> 00:40:10,113
이 계산 된 확률 분포가 있습니다.

660
00:40:10,113 --> 00:40:12,020
그것은 우리의 점수에 의해 암시 된,

661
00:40:12,020 --> 00:40:14,650
우리는 이것을 목표와 비교하기를 원합니다.

662
00:40:14,650 --> 00:40:17,138
또는 진정한 확률 분포.

663
00:40:17,138 --> 00:40:19,166
우리가 그 고양이가 고양이라는 것을 알면,

664
00:40:19,166 --> 00:40:22,083
목표 확률 분포

665
00:40:22,083 --> 00:40:24,735
고양이에 모든 확률 질량을 넣을 것이고,

666
00:40:24,735 --> 00:40:26,904
그래서 우리는 고양이가 1과 같을 확률을 가질 것입니다.

667
00:40:26,904 --> 00:40:29,754
다른 모든 클래스에 대해서는 확률이 0입니다.

668
00:40:29,754 --> 00:40:31,612
이제 우리가하고 싶은 일은 격려입니다.

669
00:40:31,612 --> 00:40:33,528
우리의 계산 된 확률 분포

670
00:40:33,528 --> 00:40:35,574
이 softmax 함수에서 나온 것입니다.

671
00:40:35,574 --> 00:40:38,376
이 목표 확률 분포와 일치시킨다.

672
00:40:38,376 --> 00:40:40,671
올바른 클래스의 모든 질량을가집니다.

673
00:40:40,671 --> 00:40:42,175
그리고 우리가 이렇게하는 방법,

674
00:40:42,175 --> 00:40:45,181
내 말은, 당신은 여러 방면에서이
방정식을 할 수 있다는 것입니다.

675
00:40:45,181 --> 00:40:46,795
당신은 KL 발산으로 이것을 할 수 있습니다.

676
00:40:46,795 --> 00:40:48,283
표적 사이

677
00:40:48,283 --> 00:40:51,102
상기 계산 된 확률 분포,

678
00:40:51,102 --> 00:40:53,221
최대 우도 추정치로이를 수행 할 수 있습니다.

679
00:40:53,221 --> 00:40:54,466
그러나 하루가 끝날 때,

680
00:40:54,466 --> 00:40:56,474
우리가 정말로 원하는 것은 확률

681
00:40:56,474 --> 00:41:00,839
진정한 계급은 높고 하나에 가깝습니다.

682
00:41:00,839 --> 00:41:04,015
그래서 우리의 손실은 이제 음의 로그가 될 것입니다.

683
00:41:04,015 --> 00:41:06,389
진실한 계급의 확율의.

684
00:41:06,389 --> 00:41:08,151
이것은 우리가 이것을 넣기 때문에 혼란 스럽습니다.

685
00:41:08,151 --> 00:41:09,707
여러 다른 것들을 통해,

686
00:41:09,707 --> 00:41:11,745
그러나 우리는 확률을 원했다는 것을 기억하십시오.

687
00:41:11,745 --> 00:41:13,524
하나에 가까워지면,

688
00:41:13,524 --> 00:41:17,071
그래서 이제 로그는 단조로운 함수입니다.

689
00:41:17,071 --> 00:41:18,414
수학적으로 밝혀졌습니다.

690
00:41:18,414 --> 00:41:20,840
로그를 최대화하는 것이 더 쉽다.

691
00:41:20,840 --> 00:41:23,277
원시 확률을 최대화하는 것보다,

692
00:41:23,277 --> 00:41:25,604
그래서 우리는 일지를 고수합니다.

693
00:41:25,604 --> 00:41:26,984
그리고 이제 로그는 단조롭지 만,

694
00:41:26,984 --> 00:41:30,244
그래서 우리가 정확한 클래스의 로그 P를 최대화한다면,

695
00:41:30,244 --> 00:41:32,599
그건 우리가 그걸 원한다는 뜻이고,

696
00:41:32,599 --> 00:41:36,024
그러나 손실 함수는 좋지 않음을 측정한다.

697
00:41:36,024 --> 00:41:37,454
그래서 우리는 마이너스 1을 넣을 필요가있다.

698
00:41:37,454 --> 00:41:40,051
그것이 올바른 방향으로 나아갈 수 있도록.

699
00:41:40,051 --> 00:41:42,314
SVM에 대한 우리의 손실 함수

700
00:41:42,314 --> 00:41:44,648
확률의 마이너스 로그가 될 것입니다.

701
00:41:44,648 --> 00:41:46,148
진정한 계급의

702
00:41:48,909 --> 00:41:51,322
네, 여기 그것이 요약입니다.

703
00:41:51,322 --> 00:41:53,782
우리가 점수를 받고, 우리가 softmax를 돌며,

704
00:41:53,782 --> 00:41:56,075
이제 우리의 손실은 확률의 로그를 뺀 것이다.

705
00:41:56,075 --> 00:41:57,575
진정한 계급의

706
00:42:01,697 --> 00:42:03,743
좋아, 그럼이게 어떻게 생겼는지 봐.

707
00:42:03,743 --> 00:42:05,043
구체적인 예를 들면,

708
00:42:05,043 --> 00:42:07,749
그 다음 우리는 우리 아주 좋아하는 아름다운 고양이

709
00:42:07,749 --> 00:42:10,634
세 가지 예를 통해 우리는이 세 가지 점수를 얻었습니다.

710
00:42:10,634 --> 00:42:14,486
우리 선형 분류기에서 나오는

711
00:42:14,486 --> 00:42:16,296
이 점수는 정확하게 그들이했던 방식입니다.

712
00:42:16,296 --> 00:42:18,712
SVM 손실의 맥락에서

713
00:42:18,712 --> 00:42:20,826
하지만 이제는이 점수를받는 것보다

714
00:42:20,826 --> 00:42:22,817
그들을 우리의 손실 함수에 직접적으로 넣는 것,

715
00:42:22,817 --> 00:42:25,422
우리는 그것들을 모두 가져 와서 그들을 압도 할 것입니다.

716
00:42:25,422 --> 00:42:26,990
그래서 그들은 모두 긍정적입니다,

717
00:42:26,990 --> 00:42:29,095
그리고 우리는 그들을 정상화시켜 확실하게 할 것입니다.

718
00:42:29,095 --> 00:42:31,025
그들 모두가 하나가된다.

719
00:42:31,025 --> 00:42:33,788
그리고 이제 우리의 손실은 마이너스 로그가 될 것입니다.

720
00:42:33,788 --> 00:42:35,788
진실한 학급 점수의.

721
00:42:36,643 --> 00:42:38,893
그래서 그것은 softmax 손실입니다,

722
00:42:40,156 --> 00:42:43,823
다항 로지스틱 회귀 (multinomial
logistic regression)라고도합니다.

723
00:42:45,496 --> 00:42:47,253
이제 몇 가지 질문을했습니다.

724
00:42:47,253 --> 00:42:50,750
다중 클래스 SVM 손실에 대한
직감을 얻으려고 시도 할 때,

725
00:42:50,750 --> 00:42:53,778
동일한 질문에 대해 생각하는 것이 유용합니다.

726
00:42:53,778 --> 00:42:57,360
softmax 손실과 대비됩니다.

727
00:42:57,360 --> 00:42:58,614
그럼 질문은,

728
00:42:58,614 --> 00:43:02,697
Softmax 손실의 최소 및 최대 값은 얼마입니까?

729
00:43:04,984 --> 00:43:06,785
좋아, 아마 그렇게 확신 할 수는 없어.

730
00:43:06,785 --> 00:43:08,303
너무 많은 로그와 합계가 있습니다.

731
00:43:08,303 --> 00:43:09,720
여기 들어가.

732
00:43:11,298 --> 00:43:13,759
그래서 대답은 분 손실이 0이라는 것입니다.

733
00:43:13,759 --> 00:43:15,430
최대 손실은 무한대입니다.

734
00:43:15,430 --> 00:43:18,263
그리고 당신이 이것을 볼 수있는 방법,

735
00:43:19,422 --> 00:43:21,165
우리가 원하는 확률 분포

736
00:43:21,165 --> 00:43:24,467
올바른 클래스에 하나, 잘못된 클래스에 0,

737
00:43:24,467 --> 00:43:25,842
우리가하는 방식은,

738
00:43:25,842 --> 00:43:27,199
그래서 그 경우라면,

739
00:43:27,199 --> 00:43:31,366
로그 안에있는이 것은 결국 하나가 될 것이고,

740
00:43:33,662 --> 00:43:36,750
실제 클래스의 로그 확률이기 때문에,

741
00:43:36,750 --> 00:43:40,917
하나의 로그가 0 일 때, 하나의
로그를 뺀 것이 여전히 0입니다.

742
00:43:41,893 --> 00:43:44,001
그래서 우리가 그 일을 완전히 올바르게한다면,

743
00:43:44,001 --> 00:43:46,515
우리의 손실은 0이 될 것입니다.

744
00:43:46,515 --> 00:43:50,249
그러나 그런데, 그 일을 완전히 옳게하기 위해서,

745
00:43:50,249 --> 00:43:53,582
우리 점수는 어떻게 생겼을까요?

746
00:43:55,963 --> 00:43:57,252
불평, 불평.

747
00:43:57,252 --> 00:44:00,135
그래서 점수는 실제로 상당히 극단적으로되어야 할 것입니다.

748
00:44:00,135 --> 00:44:01,572
무한으로 향한 것처럼.

749
00:44:01,572 --> 00:44:04,384
그래서 우리는 실제로이 지수를 가지고 있기 때문에,

750
00:44:04,384 --> 00:44:06,098
이 정규화, 유일한 방법

751
00:44:06,098 --> 00:44:09,029
우리는 실제로 하나의 확률 분포를 얻을 수있다.

752
00:44:09,029 --> 00:44:11,970
그리고 제로는 실제로 무한 점수를 넣고 있습니다.

753
00:44:11,970 --> 00:44:16,006
올바른 클래스의 경우 및 무한대 점수를 뺀 경우

754
00:44:16,006 --> 00:44:17,509
잘못된 모든 클래스에 대해.

755
00:44:17,509 --> 00:44:20,652
그리고 컴퓨터는 무한 성으로 잘하지 못합니다.

756
00:44:20,652 --> 00:44:22,239
그래서 실제로, 당신은 결코 손실이 없어 질 것입니다.

757
00:44:22,239 --> 00:44:24,108
유한 정밀도를 가진이 물건에.

758
00:44:24,108 --> 00:44:25,755
그러나 당신은 여전히이 해석을 가지고 있습니다.

759
00:44:25,755 --> 00:44:29,223
여기서 0은 이론상의 최소 손실입니다.

760
00:44:29,223 --> 00:44:31,607
그리고 최대 손실은 제한되지 않습니다.

761
00:44:31,607 --> 00:44:35,180
우리가 확률 질량이 0 일 때

762
00:44:35,180 --> 00:44:39,483
올바른 클래스에서 마이너스 로그를 얻습니다.

763
00:44:39,483 --> 00:44:42,643
0의 로그는 마이너스 무한대이며,

764
00:44:42,643 --> 00:44:46,283
그래서 0의 마이너스 로그는 플러스 무한대가됩니다.

765
00:44:46,283 --> 00:44:47,334
그래서 그것은 정말로 나쁘다.

766
00:44:47,334 --> 00:44:49,071
그러나 다시, 당신은 결코 여기에 결코 도착하지 않을 것입니다.

767
00:44:49,071 --> 00:44:53,748
이 확률을 실제로 얻을 수있는 유일한 방법이기 때문에

768
00:44:53,748 --> 00:44:58,563
0이 되려면 올바른 클래스 점수에 대한 e가 0이고,

769
00:44:58,563 --> 00:45:00,093
그 올바른 클래스 점수가

770
00:45:00,093 --> 00:45:01,630
음의 무한대입니다.

771
00:45:01,630 --> 00:45:04,034
다시 한번 말하지만, 당신은 실제로 이러한
최소한의 것을 얻지 못할 것입니다.

772
00:45:04,034 --> 00:45:07,117
유한 정밀도의 최대 값.

773
00:45:08,863 --> 00:45:11,032
그럼 우리가이 디버깅을했는지 기억해.

774
00:45:11,032 --> 00:45:14,340
다중 클래스 SVM의 컨텍스트에서 온 전성
체크 (sanity check) 질문,

775
00:45:14,340 --> 00:45:16,401
우리는 softmax에 대해서도 같은 질문을 할 수 있습니다.

776
00:45:16,401 --> 00:45:19,138
모든 S가 작고 약 0이면,

777
00:45:19,138 --> 00:45:21,287
그러면 여기서 손실이 무엇입니까?

778
00:45:21,287 --> 00:45:22,292
그래, 대답?

779
00:45:22,292 --> 00:45:24,363
- [학생] C에서 1을 제외한 로그.

780
00:45:24,363 --> 00:45:27,045
- C에서 1의 로그를 뺀거야?

781
00:45:27,045 --> 00:45:28,795
내 생각 엔 그래,

782
00:45:30,026 --> 00:45:33,352
그래서 C를 넘어서는 하나의 로그를 뺀 것입니다.

783
00:45:33,352 --> 00:45:34,693
로그가 그 일을 뒤집을 수 있기 때문에

784
00:45:34,693 --> 00:45:36,526
그러면 C의 로그 일뿐입니다.

785
00:45:36,526 --> 00:45:38,079
네, 그래서 C의 로그 일뿐입니다.

786
00:45:38,079 --> 00:45:39,909
그리고 다시, 이것은 훌륭한 디버깅 일입니다.

787
00:45:39,909 --> 00:45:41,911
이 softmax 손실로 모델을 훈련하는 경우,

788
00:45:41,911 --> 00:45:43,977
첫 번째 반복을 확인해야합니다.

789
00:45:43,977 --> 00:45:47,894
로그 C가 아니면 뭔가 잘못되었습니다.

790
00:45:50,051 --> 00:45:53,257
그래서 우리는이 두 손실 함수를
비교하고 대조 할 수 있습니다.

791
00:45:53,257 --> 00:45:54,600
약간.

792
00:45:54,600 --> 00:45:56,111
선형 분류의 관점에서,

793
00:45:56,111 --> 00:45:57,532
이 설정은 동일하게 보입니다.

794
00:45:57,532 --> 00:45:59,246
곱해진 W 행렬이 있습니다.

795
00:45:59,246 --> 00:46:02,072
이 유령의 유령을 산출하기위한 우리의 의견에 반하여,

796
00:46:02,072 --> 00:46:04,046
이제 두 손실 함수의 차이점

797
00:46:04,046 --> 00:46:06,434
우리가 점수를 해석하는 방법은

798
00:46:06,434 --> 00:46:09,327
나중에 불량을 정량적으로 측정 할 수 있습니다.

799
00:46:09,327 --> 00:46:11,562
그래서 SVM을 위해 우리는
들어가서 마진을 살펴볼 것입니다.

800
00:46:11,562 --> 00:46:14,987
올바른 수업의 점수 사이에

801
00:46:14,987 --> 00:46:17,138
잘못된 수업의 점수,

802
00:46:17,138 --> 00:46:20,256
이 소프트 맥스 또는 크로스 엔트로피 손실에 대해서는,

803
00:46:20,256 --> 00:46:22,703
우리는 가야하고 확률 분포를 계산할 것입니다.

804
00:46:22,703 --> 00:46:24,980
그런 다음 마이너스 로그 확률을 살펴보십시오.

805
00:46:24,980 --> 00:46:26,663
올바른 클래스의.

806
00:46:26,663 --> 00:46:28,996
그래서 때때로 당신이 보면,

807
00:46:30,198 --> 00:46:33,216
관점에서, 나는 그 점을 건너 뛸 것이다.

808
00:46:33,216 --> 00:46:34,917
[웃음]

809
00:46:34,917 --> 00:46:36,358
흥미로운 또 다른 질문입니다.

810
00:46:36,358 --> 00:46:40,854
이 두 손실 함수를 대조하면 생각할 때,

811
00:46:40,854 --> 00:46:44,241
이 예제 포인트가 있다고 가정 해 보겠습니다.

812
00:46:44,241 --> 00:46:46,058
당신이 그것의 점수를 바꾸면,

813
00:46:46,058 --> 00:46:49,975
우리는이 점에 대해 세 가지 점수가 있다고 가정합니다.

814
00:46:52,859 --> 00:46:54,042
바닥에있는 부분을 무시하십시오.

815
00:46:54,042 --> 00:46:56,558
그러나이 예제로 돌아 가면

816
00:46:56,558 --> 00:46:59,814
여기서 다중 - 클래스 SVM 손실에서,

817
00:46:59,814 --> 00:47:04,144
우리가 차를 가지고 있었을 때,
차 점수가 훨씬 좋았습니다.

818
00:47:04,144 --> 00:47:06,293
모든 잘못된 수업보다

819
00:47:06,293 --> 00:47:08,546
그 차 이미지의 점수를 흔들어 쓴다.

820
00:47:08,546 --> 00:47:11,246
다중 클래스 SVM 손실을 전혀 변경하지 않았습니다.

821
00:47:11,246 --> 00:47:13,141
유일한 이유는 SVM 손실

822
00:47:13,141 --> 00:47:15,282
그 정확한 점수를 얻는 것에 관심이있었습니다.

823
00:47:15,282 --> 00:47:18,359
잘못된 점수보다 큰 여백보다 커야합니다.

824
00:47:18,359 --> 00:47:20,392
하지만 이제 softmax 손실은 실제로 상당히 다릅니다.

825
00:47:20,392 --> 00:47:21,726
이 점에서.

826
00:47:21,726 --> 00:47:24,174
softmax 손실은 실제로 항상 운전하기를 원합니다.

827
00:47:24,174 --> 00:47:26,438
그 확률 덩어리가 하나가 될 때까지.

828
00:47:26,438 --> 00:47:29,771
따라서 점수가 매우 높더라도

829
00:47:31,143 --> 00:47:32,606
올바른 수업에, 그리고 매우 낮은 점수

830
00:47:32,606 --> 00:47:34,298
모든 잘못된 수업에

831
00:47:34,298 --> 00:47:36,852
softmax 당신이 더 많은
확률 질량을 쌓기를 원할 것입니다

832
00:47:36,852 --> 00:47:40,044
올바른 수업을 듣고 점수를 계속 누르십시오.

833
00:47:40,044 --> 00:47:42,350
무한대쪽으로 올라가는 올바른 클래스의

834
00:47:42,350 --> 00:47:44,152
잘못된 수업의 점수

835
00:47:44,152 --> 00:47:46,138
음의 무한대쪽으로 내려 갔다.

836
00:47:46,138 --> 00:47:47,435
그래서 그것은 흥미로운 차이입니다.

837
00:47:47,435 --> 00:47:49,968
실제로이 두 손실 함수 사이.

838
00:47:49,968 --> 00:47:53,530
그 SVM, 막대 위에이 데이터 포인트를 가져올거야.

839
00:47:53,530 --> 00:47:55,920
정확하게 분류하고 그냥 포기하면

840
00:47:55,920 --> 00:47:57,739
그 데이터 포인트에 대해서는 더 이상 신경 쓰지 않습니다.

841
00:47:57,739 --> 00:48:00,296
softmax는 항상 지속적으로 개선하려고 노력하지만

842
00:48:00,296 --> 00:48:01,968
모든 단일 데이터 요소가 더 좋아지고 나아질 것입니다.

843
00:48:01,968 --> 00:48:03,838
그리고 더 좋고 더 낫다.

844
00:48:03,838 --> 00:48:05,405
그래서 그것은 흥미로운 차이입니다.

845
00:48:05,405 --> 00:48:07,378
이 두 기능 사이.

846
00:48:07,378 --> 00:48:09,974
실제로, 나는 그것이 큰 차이를 만들어
내지 않는 경향이 있다고 생각한다.

847
00:48:09,974 --> 00:48:12,240
당신이 선택하는 것은 그들이 수행하는 경향이 있습니다.

848
00:48:12,240 --> 00:48:14,040
꽤 유사하게,

849
00:48:14,040 --> 00:48:15,966
적어도 많은 깊은 학습 응용 프로그램.

850
00:48:15,966 --> 00:48:19,137
그러나 이러한 차이를 유지하는 것이 매우 유용합니다.

851
00:48:19,137 --> 00:48:19,970
마음에.

852
00:48:23,054 --> 00:48:26,176
네, 여기에서 우리가 어디로 왔는지 다시 정리하려면,

853
00:48:26,176 --> 00:48:29,585
xs와 ys의 데이터 세트를 가지고 있다는 것입니다.

854
00:48:29,585 --> 00:48:33,018
우리는 우리의 선형 분류기를 사용하여
몇 가지 점수 함수를 얻습니다.

855
00:48:33,018 --> 00:48:36,595
우리의 점수 S를 계산하기 위해서, 우리의 입력에서 x,

856
00:48:36,595 --> 00:48:38,311
그리고 나서 우리는 손실 함수를 사용할 것입니다,

857
00:48:38,311 --> 00:48:41,134
어쩌면 softmax 또는 SVM
또는 일부 다른 손실 함수

858
00:48:41,134 --> 00:48:45,997
양적으로 얼마나 나쁜지를 예측하는 것이었다.

859
00:48:45,997 --> 00:48:48,954
이 땅에 비하면 진정한 목표, y.

860
00:48:48,954 --> 00:48:52,410
그리고 나서 우리는 종종이 손실 함수를 증가시킬 것입니다.

861
00:48:52,410 --> 00:48:53,849
정규화 용어로,

862
00:48:53,849 --> 00:48:56,174
훈련 자료를 맞추는 사이에

863
00:48:56,174 --> 00:48:59,059
더 단순한 모델을 선호합니다.

864
00:48:59,059 --> 00:49:01,490
그래서 이것은 꽤 일반적인 개요입니다.

865
00:49:01,490 --> 00:49:03,966
감독 학습이라고 불리는 많은 것을

866
00:49:03,966 --> 00:49:07,065
우리가 앞으로 나아갈 때 깊은 학습에서 볼 수있는 것은,

867
00:49:07,065 --> 00:49:10,888
일반적으로 당신은 어떤 함수
f를 지정하기를 원할 것입니다.

868
00:49:10,888 --> 00:49:12,664
구조가 매우 복잡 할 수 있습니다.

869
00:49:12,664 --> 00:49:14,489
결정하는 몇 가지 손실 함수를 지정하십시오.

870
00:49:14,489 --> 00:49:18,028
당신의 알고리즘이 얼마나 잘하고 있는지,

871
00:49:18,028 --> 00:49:19,645
파라미터의 임의의 값이 주어지면,

872
00:49:19,645 --> 00:49:21,053
일부 정규화 용어

873
00:49:21,053 --> 00:49:24,260
모델 복잡성을 처벌하는 방법

874
00:49:24,260 --> 00:49:26,141
그런 다음이 것들을 하나로 결합합니다.

875
00:49:26,141 --> 00:49:27,624
W를 찾으려고 노력해.

876
00:49:27,624 --> 00:49:30,866
이 최종 손실 함수를 최소화합니다.

877
00:49:30,866 --> 00:49:32,120
그러나 그때 질문은,

878
00:49:32,120 --> 00:49:33,636
우리가 실제로 그 일을 어떻게 수행할까요?

879
00:49:33,636 --> 00:49:37,132
손실을 최소화하는이 W를 실제로 어떻게 찾을 수 있습니까?

880
00:49:37,132 --> 00:49:40,461
그리고 그것은 우리를 최적화 주제로 이끌고 있습니다.

881
00:49:40,461 --> 00:49:43,033
그래서 우리가 최적화를 할 때,

882
00:49:43,033 --> 00:49:45,495
나는 보통 걷는 관점에서 생각한다.

883
00:49:45,495 --> 00:49:47,482
큰 계곡 주위에.

884
00:49:47,482 --> 00:49:51,951
그래서이 큰 계곡을 걷고 있다는 생각이 들었습니다.

885
00:49:51,951 --> 00:49:54,183
다른 산과 계곡과 시내와 함께

886
00:49:54,183 --> 00:49:56,903
물건들, 그리고이 풍경의 모든 지점

887
00:49:56,903 --> 00:50:00,729
파라미터 W의 일부 설정에 대응한다.

888
00:50:00,729 --> 00:50:03,054
그리고이 계곡을 걸어 다니는이 작은 녀석입니다.

889
00:50:03,054 --> 00:50:04,517
그리고 당신은 찾으려고 노력하고 있습니다.

890
00:50:04,517 --> 00:50:06,216
이들 각 점의 높이,

891
00:50:06,216 --> 00:50:10,728
미안하지만, W의 설정에 의해 초래 된 손실과 같습니다.

892
00:50:10,728 --> 00:50:12,879
그리고이 작은 남자로서의 당신 직업

893
00:50:12,879 --> 00:50:14,181
이 풍경을 돌아 다니며,

894
00:50:14,181 --> 00:50:18,000
당신은 어떻게 든이 계곡의 바닥을 찾을 필요가 있습니다.

895
00:50:18,000 --> 00:50:20,517
그리고 이것은 일반적으로 어려운 문제입니다.

896
00:50:20,517 --> 00:50:22,851
너는 아마도 내가 정말로 영리하다고 생각할지도 모른다.

897
00:50:22,851 --> 00:50:25,223
분석 속성에 대해 정말 열심히 생각할 수 있습니다.

898
00:50:25,223 --> 00:50:27,379
내 손실 기능, 내 모든 정규화,

899
00:50:27,379 --> 00:50:30,246
어쩌면 나는 미니 마이저를 적어 두거나,

900
00:50:30,246 --> 00:50:33,089
그리고 그것은 마술처럼 순간 이동에 해당합니다

901
00:50:33,089 --> 00:50:35,509
이 골짜기의 바닥까지.

902
00:50:35,509 --> 00:50:38,740
그러나 실제로, 일단 당신의 예측 함수, f,

903
00:50:38,740 --> 00:50:40,725
당신의 손실 함수와 당신의 정규식,

904
00:50:40,725 --> 00:50:42,442
일단 이러한 것들이 크고 복잡 해지면

905
00:50:42,442 --> 00:50:44,609
신경 네트워크를 사용하여,

906
00:50:46,190 --> 00:50:48,055
적어 두려는 데별로 희망이 없습니다.

907
00:50:48,055 --> 00:50:49,698
노골적인 분석 솔루션

908
00:50:49,698 --> 00:50:52,017
그것은 당신을 미니 마에 직접 데려갑니다.

909
00:50:52,017 --> 00:50:53,271
그래서 실제로

910
00:50:53,271 --> 00:50:55,485
우리는 다양한 유형의 반복적 인
방법을 사용하는 경향이 있습니다

911
00:50:55,485 --> 00:50:57,390
우리는 몇 가지 해결책으로 시작한다.

912
00:50:57,390 --> 00:51:00,524
시간이 지남에 따라 서서히 개선하십시오.

913
00:51:00,524 --> 00:51:03,357
그래서 아주 첫 번째, 어리석은 일

914
00:51:04,527 --> 00:51:06,985
당신이 상상할 수있는 것은 무작위 검색입니다.

915
00:51:06,985 --> 00:51:09,024
그것은 단지 W의 무리를 취할 것입니다,

916
00:51:09,024 --> 00:51:12,180
무작위로 샘플링하여 손실 함수에 던져 넣습니다.

917
00:51:12,180 --> 00:51:14,963
그들이 얼마나 잘하는지보십시오.

918
00:51:14,963 --> 00:51:17,416
그래서 스포일러 경고, 이것은 정말 나쁜 알고리즘입니다,

919
00:51:17,416 --> 00:51:18,828
당신은 아마 이것을 사용하면 안됩니다.

920
00:51:18,828 --> 00:51:23,323
그러나 적어도 그것은 당신이 시도하는
것을 상상할 수도있는 한 가지입니다.

921
00:51:23,323 --> 00:51:25,180
그리고 우리는 실제로 이것을 할 수 있습니다.

922
00:51:25,180 --> 00:51:27,856
선형 분류기를 실제로 훈련 할 수 있습니다.

923
00:51:27,856 --> 00:51:30,813
무작위 검색을 통해 CIFAR-10

924
00:51:30,813 --> 00:51:34,152
그리고 이것을 위해 10 개의 클래스가 있습니다.

925
00:51:34,152 --> 00:51:35,997
그래서 무작위 확률은 10 %입니다.

926
00:51:35,997 --> 00:51:39,768
우리가 몇 가지 무작위 시도를했다면,

927
00:51:39,768 --> 00:51:42,212
우리는 단지 투명한 바보 같은 운을 통하여 단지 발견했다.

928
00:51:42,212 --> 00:51:45,645
어쩌면 15 %의 정확도를 가진 W의 일부 설정.

929
00:51:45,645 --> 00:51:48,019
그래서 무작위보다는 낫다.

930
00:51:48,019 --> 00:51:50,238
그러나 미술 수준은 아마 95 %

931
00:51:50,238 --> 00:51:53,831
그래서 여기에 약간의 격차가 있습니다.

932
00:51:53,831 --> 00:51:56,748
다시 한번 말하지만 실제로 이것을 사용하지 마십시오.

933
00:51:56,748 --> 00:51:58,138
하지만 당신은 이것이 뭔가 있다고 상상할 수 있습니다.

934
00:51:58,138 --> 00:52:00,677
당신은 잠재적으로 할 수 있습니다.

935
00:52:00,677 --> 00:52:02,467
따라서 실제로는 더 나은 전략 일 것입니다.

936
00:52:02,467 --> 00:52:04,664
실제로 일부 로컬 지오메트리를 사용하고 있습니다.

937
00:52:04,664 --> 00:52:06,168
이 풍경.

938
00:52:06,168 --> 00:52:07,614
그래서 당신이 걷고있는이 작은 녀석이라면

939
00:52:07,614 --> 00:52:09,910
이 풍경 주변,

940
00:52:09,910 --> 00:52:12,178
어쩌면 당신은 직접 경로를 볼 수 없습니다.

941
00:52:12,178 --> 00:52:13,802
골짜기의 바닥에 이르기까지,

942
00:52:13,802 --> 00:52:16,031
그러나 당신이 할 수있는 것은 당신의 발로 느끼는 것입니다.

943
00:52:16,031 --> 00:52:19,531
로컬 지오메트리가 무엇인지 파악하고,

944
00:52:20,697 --> 00:52:21,927
내가 여기 서 있으면,

945
00:52:21,927 --> 00:52:24,145
어떤 방법으로 내리막 길을 조금 걸릴까요?

946
00:52:24,145 --> 00:52:25,595
그래서 당신은 당신의 발로 느낄 수 있습니다.

947
00:52:25,595 --> 00:52:28,136
지상의 기울기가 어디인지 느껴보십시오.

948
00:52:28,136 --> 00:52:30,861
이 방향으로 조금 나를 쓰러 뜨 렸어?

949
00:52:30,861 --> 00:52:32,649
그리고 그 방향으로 나아갈 수 있습니다.

950
00:52:32,649 --> 00:52:34,037
그리고 너는 조금 내려갈 것이다.

951
00:52:34,037 --> 00:52:36,260
당신의 발로 다시 어떤 느낌이 내려 졌는지 알아 내려고,

952
00:52:36,260 --> 00:52:37,704
그런 다음 반복해서 반복하십시오.

953
00:52:37,704 --> 00:52:39,529
당신이 바닥에 끝나기를 희망합니다.

954
00:52:39,529 --> 00:52:41,526
결국 계곡의.

955
00:52:41,526 --> 00:52:45,296
그래서 이것은 또한 상대적으로
간단한 알고리즘처럼 보입니다.

956
00:52:45,296 --> 00:52:47,236
하지만 실제로 이것은 정말 잘 작동하는 경향이 있습니다.

957
00:52:47,236 --> 00:52:50,195
실제로 모든 세부 사항을 올바르게 얻는다면.

958
00:52:50,195 --> 00:52:52,209
이것이 일반적으로 우리가 따라야 할 전략입니다.

959
00:52:52,209 --> 00:52:53,963
이 거대한 신경 네트워크를 훈련 할 때

960
00:52:53,963 --> 00:52:57,028
선형 분류 자 및 다른 것들.

961
00:52:57,028 --> 00:52:58,769
그래서, 그것은 물결 모양의 작은 손이었습니다.

962
00:52:58,769 --> 00:52:59,952
그래서 사면은 무엇입니까?

963
00:52:59,952 --> 00:53:02,337
미적분 클래스를 기억한다면,

964
00:53:02,337 --> 00:53:03,842
적어도 하나의 차원에서,

965
00:53:03,842 --> 00:53:07,673
기울기는이 함수의 미분 값입니다.

966
00:53:07,673 --> 00:53:09,971
그래서 우리가 1 차원 함수 f를 가지면,

967
00:53:09,971 --> 00:53:12,969
스칼라 x를 취한 다음 높이를 출력합니다.

968
00:53:12,969 --> 00:53:16,460
곡선의 일부를 구하면 기울기를 계산할 수 있습니다.

969
00:53:16,460 --> 00:53:19,717
또는 파생 상품을 언제든지 상상할 수 있습니다.

970
00:53:19,717 --> 00:53:23,467
우리가 작은 걸음을 내딛으면 어떤 방향 으로든,

971
00:53:26,298 --> 00:53:28,057
작은 걸음 걸음, 그리고 그 차이를 비교해 보라.

972
00:53:28,057 --> 00:53:29,798
해당 단계의 함수 값에서

973
00:53:29,798 --> 00:53:31,679
단계 크기를 0으로 드래그하고,

974
00:53:31,679 --> 00:53:33,237
그 함수의 기울기를 우리에게 줄 것이다.

975
00:53:33,237 --> 00:53:34,895
그 시점에서.

976
00:53:34,895 --> 00:53:36,094
그리고 이것은 아주 자연스럽게 일반화됩니다.

977
00:53:36,094 --> 00:53:38,333
다중 변수 기능도 제공합니다.

978
00:53:38,333 --> 00:53:41,377
그래서 실제로 x는 스칼라가 아닙니다.

979
00:53:41,377 --> 00:53:42,612
그러나 전체 벡터,

980
00:53:42,612 --> 00:53:46,445
기억하기 때문에, x는 전체 벡터 일 수 있습니다.

981
00:53:47,532 --> 00:53:49,063
그래서 우리는이 개념을 일반화 할 필요가있다.

982
00:53:49,063 --> 00:53:51,941
다중 변수에 이르기까지.

983
00:53:51,941 --> 00:53:54,658
그리고 우리가 파생 상품을 사용하는 일반화

984
00:53:54,658 --> 00:53:57,896
다중 변수 설정에서 그라데이션,

985
00:53:57,896 --> 00:54:01,168
그래디언트는 부분 미분의 벡터입니다.

986
00:54:01,168 --> 00:54:04,409
그래디언트는 x와 동일한 모양을 갖습니다.

987
00:54:04,409 --> 00:54:07,473
그라디언트의 각 요소는 우리에게 알려줍니다.

988
00:54:07,473 --> 00:54:09,595
함수 f의 기울기는 얼마인가?

989
00:54:09,595 --> 00:54:12,391
우리가 그 좌표 방향으로 움직인다면.

990
00:54:12,391 --> 00:54:13,899
그리고 그라디언트가 밝혀졌습니다.

991
00:54:13,899 --> 00:54:16,816
이 아주 좋은 재산을 가지려면,

992
00:54:18,373 --> 00:54:21,036
그래디언트는 이제 부분 미분의 벡터입니다.

993
00:54:21,036 --> 00:54:23,228
그러나 그것은 가장 큰 증가의 방향을 가리킨다.

994
00:54:23,228 --> 00:54:25,657
따라서,

995
00:54:25,657 --> 00:54:27,545
음의 그래디언트 방향을 보면,

996
00:54:27,545 --> 00:54:29,770
그것은 당신에게 가장 큰 감소의 방향을 제시합니다.

997
00:54:29,770 --> 00:54:31,612
함수의.

998
00:54:31,612 --> 00:54:34,210
그리고 더 일반적으로, 당신이 알고 싶다면,

999
00:54:34,210 --> 00:54:37,418
어떤 방향 으로든 내 풍경의 경사는 무엇입니까?

1000
00:54:37,418 --> 00:54:39,357
그러면 그라데이션의 내적과 같습니다.

1001
00:54:39,357 --> 00:54:42,693
그 방향을 설명하는 단위 벡터와 함께.

1002
00:54:42,693 --> 00:54:44,549
그래서이 그라데이션은 매우 중요합니다.

1003
00:54:44,549 --> 00:54:47,719
이 선형 일차 근사를 제공하기 때문에

1004
00:54:47,719 --> 00:54:50,198
귀하의 현재 지점에서 귀하의 기능에.

1005
00:54:50,198 --> 00:54:51,382
그래서 실제로, 많은 깊은 학습

1006
00:54:51,382 --> 00:54:53,661
함수의 그라데이션 계산하기

1007
00:54:53,661 --> 00:54:56,290
그런 그라디언트를 사용하여 반복적으로 업데이트

1008
00:54:56,290 --> 00:54:58,123
매개 변수 벡터.

1009
00:54:59,204 --> 00:55:02,195
당신이 상상할 수있는 순진한 방법 하나

1010
00:55:02,195 --> 00:55:04,812
실제로 컴퓨터에서이 그라디언트를 평가하면

1011
00:55:04,812 --> 00:55:06,955
유한 차이의 방법을 사용하고 있습니다.

1012
00:55:06,955 --> 00:55:09,488
그래디언트의 한계 정의로 돌아갑니다.

1013
00:55:09,488 --> 00:55:12,621
그래서 왼쪽에서 우리는 우리의 현재 W

1014
00:55:12,621 --> 00:55:14,012
이 매개 변수 벡터입니다.

1015
00:55:14,012 --> 00:55:17,432
어쩌면 우리에게 어쩌면 1.25의
현재 손실을 줄 수 있습니다.

1016
00:55:17,432 --> 00:55:21,127
우리의 목표는 기울기를 계산하는 것입니다, dW,

1017
00:55:21,127 --> 00:55:23,922
이것은 W와 같은 모양의 벡터가 될 것입니다.

1018
00:55:23,922 --> 00:55:26,021
그 그라디언트의 각 슬롯은 우리에게 알려줄 것입니다.

1019
00:55:26,021 --> 00:55:29,050
얼마나 많은 손실이 발생할 것인가?

1020
00:55:29,050 --> 00:55:31,734
그 좌표 방향으로 미미한 양.

1021
00:55:31,734 --> 00:55:33,241
그래서 당신이 상상할 수있는 한가지

1022
00:55:33,241 --> 00:55:35,741
이 유한 차분을 계산하는 것입니다.

1023
00:55:35,741 --> 00:55:38,336
우리가 W를 가지고 있다면, 우리는

1024
00:55:38,336 --> 00:55:41,902
W의 첫 번째 원소, 작은 값, h,

1025
00:55:41,902 --> 00:55:44,210
손실 함수를 사용하여 손실을 다시 계산하십시오.

1026
00:55:44,210 --> 00:55:45,842
우리의 분류 자와 모든 것.

1027
00:55:45,842 --> 00:55:48,112
그리고 아마도이 상황에서 우리가 조금 움직이면

1028
00:55:48,112 --> 00:55:50,792
첫 번째 차원에서 손실이 줄어들 것입니다.

1029
00:55:50,792 --> 00:55:53,792
1.2534에서 1.25322로 조금 증가했습니다.

1030
00:55:55,945 --> 00:55:57,574
그런 다음이 한계 정의를 사용할 수 있습니다.

1031
00:55:57,574 --> 00:56:01,363
이 유한 차분 근사값을 생각해 내야한다.

1032
00:56:01,363 --> 00:56:04,378
이 첫 번째 차원의 그래디언트로

1033
00:56:04,378 --> 00:56:06,194
이제이 절차를 반복하는 것을 상상할 수 있습니다.

1034
00:56:06,194 --> 00:56:07,795
두 번째 차원에서,

1035
00:56:07,795 --> 00:56:09,371
이제 우리는 첫 번째 차원을 취하고,

1036
00:56:09,371 --> 00:56:11,029
원래 값으로 다시 설정하십시오.

1037
00:56:11,029 --> 00:56:13,728
작은 단계로 두 번째 방향을 증가시킵니다.

1038
00:56:13,728 --> 00:56:15,294
그리고 다시, 우리는 손실을 계산합니다.

1039
00:56:15,294 --> 00:56:17,593
이 유한 차분 근사법을 사용하십시오.

1040
00:56:17,593 --> 00:56:19,444
그라데이션에 대한 근사치를 계산하는

1041
00:56:19,444 --> 00:56:21,165
두 번째 슬롯에서.

1042
00:56:21,165 --> 00:56:22,843
그리고 이제 세 번째로 이것을 반복하십시오.

1043
00:56:22,843 --> 00:56:25,135
그리고 계속해서.

1044
00:56:25,135 --> 00:56:27,683
그래서 이것은 실제로 끔찍한 생각입니다.

1045
00:56:27,683 --> 00:56:29,150
왜냐하면 그것은 매우 느리기 때문입니다.

1046
00:56:29,150 --> 00:56:31,980
그래서 여러분은이 함수 f를 계산하면,

1047
00:56:31,980 --> 00:56:34,230
그것이 크다면 실제로는 매우 느릴지도 모른다.

1048
00:56:34,230 --> 00:56:35,920
길쌈 신경 네트워크.

1049
00:56:35,920 --> 00:56:38,369
그리고이 매개 변수 벡터 W는,

1050
00:56:38,369 --> 00:56:40,693
아마 여기에있는 것처럼 10 개의 항목이 없을 것입니다.

1051
00:56:40,693 --> 00:56:42,266
수천만 명이 될 수도있다.

1052
00:56:42,266 --> 00:56:44,344
또는이 수백만 달러 중 일부는 수백만 달러,

1053
00:56:44,344 --> 00:56:46,446
복잡한 심층 학습 모델.

1054
00:56:46,446 --> 00:56:48,482
따라서 실제적으로, 당신은 결코
계산을 원하지 않을 것입니다.

1055
00:56:48,482 --> 00:56:50,381
당신의 유한 차이에 대한 당신의 그라디언트,

1056
00:56:50,381 --> 00:56:52,864
당신이 수억 명을 기다려야하기 때문에

1057
00:56:52,864 --> 00:56:54,749
잠재적으로 기능 평가

1058
00:56:54,749 --> 00:56:56,908
하나의 그래디언트를 얻는다면 그것은 매우 느려질 것입니다.

1059
00:56:56,908 --> 00:56:58,075
슈퍼 나쁜.

1060
00:56:59,351 --> 00:57:02,524
그러나 고맙게도 우리는 그렇게 할 필요가 없습니다.

1061
00:57:02,524 --> 00:57:03,676
바라건대 당신은 미적분 과정을 택했을 것입니다.

1062
00:57:03,676 --> 00:57:05,315
네 인생의 어느 시점에서,

1063
00:57:05,315 --> 00:57:08,146
그래서 당신은이 사람들 덕분에,

1064
00:57:08,146 --> 00:57:11,206
우리는 손실에 대한 표현을 적어 둘 수 있습니다.

1065
00:57:11,206 --> 00:57:13,658
미적분학의 마법 망치를 사용하십시오.

1066
00:57:13,658 --> 00:57:15,584
표현을 적어 두는 것

1067
00:57:15,584 --> 00:57:17,372
이 기울기가 무엇을 위해 있어야합니다.

1068
00:57:17,372 --> 00:57:18,708
그리고 이것은 훨씬 더 효율적 일 것입니다.

1069
00:57:18,708 --> 00:57:20,245
분석적으로 계산하는 것보다

1070
00:57:20,245 --> 00:57:21,658
유한 한 차이를 통해.

1071
00:57:21,658 --> 00:57:22,929
하나, 정확 할거야.

1072
00:57:22,929 --> 00:57:25,433
둘째, 계산이 필요하기 때문에 훨씬 빠릅니다.

1073
00:57:25,433 --> 00:57:27,350
이 단일 표현.

1074
00:57:28,945 --> 00:57:31,405
그래서 이것이 어떻게 생겼는지는 지금입니다.

1075
00:57:31,405 --> 00:57:33,513
우리가 현재의 W의 그림으로 돌아 가면,

1076
00:57:33,513 --> 00:57:36,848
W의 모든 차원을 반복하는 것이 아니라,

1077
00:57:36,848 --> 00:57:38,311
우리가 미리 알아낼거야.

1078
00:57:38,311 --> 00:57:40,653
그래디언트의 분석 식은 무엇입니까?

1079
00:57:40,653 --> 00:57:44,279
그런 다음 그것을 적어서 W에서 직접 이동하십시오.

1080
00:57:44,279 --> 00:57:47,337
하나의 단계에서 dW 또는
그래디언트를 계산할 수 있습니다.

1081
00:57:47,337 --> 00:57:50,875
그리고 그것은 실제로 더 나아질 것입니다.

1082
00:57:50,875 --> 00:57:53,846
요약하자면이 수치 그라디언트

1083
00:57:53,846 --> 00:57:56,738
간단하고 의미있는 것입니다.

1084
00:57:56,738 --> 00:57:58,745
그러나 실제로는 실제로 사용하지 않을 것입니다.

1085
00:57:58,745 --> 00:58:01,794
실제로는 분석 그라디언트를 항상 사용합니다.

1086
00:58:01,794 --> 00:58:03,039
그것을 사용하십시오.

1087
00:58:03,039 --> 00:58:05,301
실제로 이러한 그래디언트 계산을 수행 할 때

1088
00:58:05,301 --> 00:58:06,951
그러나 흥미로운 점은

1089
00:58:06,951 --> 00:58:09,360
이 숫자 그라디언트는 실제로 매우 유용합니다.

1090
00:58:09,360 --> 00:58:10,610
디버깅 도구.

1091
00:58:12,572 --> 00:58:13,841
몇 가지 코드를 작성했다고 가정 해 보겠습니다.

1092
00:58:13,841 --> 00:58:16,169
당신은 손실을 계산하는 코드를 작성했습니다.

1093
00:58:16,169 --> 00:58:17,770
손실의 기울기,

1094
00:58:17,770 --> 00:58:19,562
그렇다면 어떻게 디버깅합니까?

1095
00:58:19,562 --> 00:58:21,909
이 분석식이

1096
00:58:21,909 --> 00:58:24,085
코드에서 파생하고 적어 둔

1097
00:58:24,085 --> 00:58:25,684
실제로 맞습니까?

1098
00:58:25,684 --> 00:58:28,443
따라서 이러한 것들을위한 일반적인 디버깅 전략

1099
00:58:28,443 --> 00:58:31,159
방법으로 숫자 그라디언트를 사용하는 것입니다.

1100
00:58:31,159 --> 00:58:32,823
확실한 단위 테스트의 일종으로

1101
00:58:32,823 --> 00:58:35,141
분석 기울기가 맞는지 확인하십시오.

1102
00:58:35,141 --> 00:58:38,320
다시 말하지만, 이것은 매우 느리고 정확하지 않기 때문에,

1103
00:58:38,320 --> 00:58:41,435
이 숫자 그라디언트 검사를 수행 할 때,

1104
00:58:41,435 --> 00:58:43,739
호출 될 때 매개 변수의 크기를 줄이는 경향이 있습니다.

1105
00:58:43,739 --> 00:58:45,184
문제가 실제로 실행되도록

1106
00:58:45,184 --> 00:58:46,755
합리적인 시간에

1107
00:58:46,755 --> 00:58:49,376
하지만 이것은 유용한 유용한 디버깅 전략이됩니다.

1108
00:58:49,376 --> 00:58:51,721
자신의 그라디언트 계산을 작성할 때.

1109
00:58:51,721 --> 00:58:54,112
그래서 실제로 이것은 실제로 실제로 많이 사용됩니다.

1110
00:58:54,112 --> 00:58:58,610
그리고 당신은 당신의 과제에도 이것을 할 것입니다.

1111
00:58:58,610 --> 00:59:01,834
그래서 일단 그라디언트를 계산하는 방법을 알게되면,

1112
00:59:01,834 --> 00:59:04,547
그러면 우리를이 초간단 단순 알고리즘으로 이끈다.

1113
00:59:04,547 --> 00:59:06,990
그것은 세 줄과 같지만 마음 속에있는 것으로 밝혀졌습니다.

1114
00:59:06,990 --> 00:59:09,480
우리가 이처럼 매우 큰 것을 훈련하는 방법에 대해서,

1115
00:59:09,480 --> 00:59:11,607
가장 복잡한 심층 학습 알고리즘,

1116
00:59:11,607 --> 00:59:13,152
그것은 구배 강하입니다.

1117
00:59:13,152 --> 00:59:16,991
그래디언트 강하가 먼저 W를 초기화합니다.

1118
00:59:16,991 --> 00:59:19,544
어떤 무작위로, 그렇다면 사실,

1119
00:59:19,544 --> 00:59:21,555
우리는 우리의 손실과 그라디언트를 계산할 것입니다.

1120
00:59:21,555 --> 00:59:24,521
우리는 우리의 가중치를 업데이트 할 것입니다.

1121
00:59:24,521 --> 00:59:27,547
그레디언트 방향의 반대 방향에서,

1122
00:59:27,547 --> 00:59:28,722
그라디언트를 기억하십시오.

1123
00:59:28,722 --> 00:59:30,710
가장 큰 증가의 방향을 가리키고 있었다.

1124
00:59:30,710 --> 00:59:32,305
함수의 - 그래디언트 빼기

1125
00:59:32,305 --> 00:59:34,047
가장 큰 감소 방향의 포인트,

1126
00:59:34,047 --> 00:59:36,727
그래서 우리는 방향으로 조금 나아갈 것입니다.

1127
00:59:36,727 --> 00:59:39,262
빼기 그라디언트를 사용하고 영원히 이것을 반복하십시오.

1128
00:59:39,262 --> 00:59:40,774
결국 네트워크가 수렴 할 것입니다.

1129
00:59:40,774 --> 00:59:43,255
희망적으로 당신은 매우 행복 할 것입니다.

1130
00:59:43,255 --> 00:59:45,596
그러나이 단계 크기는 실제로 하이퍼 매개 변수입니다.

1131
00:59:45,596 --> 00:59:48,219
이것은 그라디언트를 계산할 때마다,

1132
00:59:48,219 --> 00:59:50,842
우리는 그 방향으로 얼마나 멀리 나아갈 것입니다.

1133
00:59:50,842 --> 00:59:53,602
그리고이 단계 크기는 학습 속도라고도하며,

1134
00:59:53,602 --> 00:59:55,445
아마 가장 중요한 것 중 하나 일 것입니다.

1135
00:59:55,445 --> 00:59:57,378
설정해야하는 하이퍼 매개 변수

1136
00:59:57,378 --> 01:00:00,033
실제로 이러한 것들을 실제로 훈련 할 때.

1137
01:00:00,033 --> 01:00:02,117
사실 나는이 일을 훈련 할 때 나를 위해,

1138
01:00:02,117 --> 01:00:04,200
이 단계 크기를 알아 내려고

1139
01:00:04,200 --> 01:00:06,340
또는이 학습 률은 첫 번째 하이퍼 파라미터입니다.

1140
01:00:06,340 --> 01:00:07,502
나는 항상 확인한다.

1141
01:00:07,502 --> 01:00:10,949
모형 크기 또는 정규화 강도와 같은 것

1142
01:00:10,949 --> 01:00:12,434
나중에 조금 떠날 때까지,

1143
01:00:12,434 --> 01:00:15,408
학습 속도 또는 단계 크기를 올바르게 얻는 방법

1144
01:00:15,408 --> 01:00:19,361
처음에 설정하려고하는 첫 번째 것입니다.

1145
01:00:19,361 --> 01:00:23,015
그래서 그림처럼 보이는 것입니다.

1146
01:00:23,015 --> 01:00:25,212
여기에 2 차원의 간단한 예제가 있습니다.

1147
01:00:25,212 --> 01:00:28,004
그래서 여기에 우리는 아마도이 사발을 가지고있을 것입니다.

1148
01:00:28,004 --> 01:00:30,051
우리의 손실 함수를 보여주고있다.

1149
01:00:30,051 --> 01:00:33,635
중앙의이 빨간 지역

1150
01:00:33,635 --> 01:00:36,598
우리가 가고 싶은 낮은 손실의이 지역

1151
01:00:36,598 --> 01:00:38,852
에지쪽으로 청색 및 녹색 영역

1152
01:00:38,852 --> 01:00:41,187
우리가 피하고자하는 손실이 더 큽니다.

1153
01:00:41,187 --> 01:00:43,204
이제 우리는 우리의 W를 시작할 것입니다.

1154
01:00:43,204 --> 01:00:44,750
우주의 어떤 임의의 지점에서,

1155
01:00:44,750 --> 01:00:47,536
음의 그래디언트 방향을 계산하고,

1156
01:00:47,536 --> 01:00:49,680
바라건대 우리를 방향으로 안내 할 것입니다.

1157
01:00:49,680 --> 01:00:51,387
결국 미니 마의.

1158
01:00:51,387 --> 01:00:53,171
그리고 우리가 이것을 반복해서 반복한다면,

1159
01:00:53,171 --> 01:00:56,407
우리는 최후에 정확한 미니 마를 얻을 수 있기를 바랍니다.

1160
01:00:56,407 --> 01:01:00,283
실제로 이것이 어떻게 생겼는지는,

1161
01:01:00,283 --> 01:01:03,140
오,이 쥐 문제가 다시 발생했습니다.

1162
01:01:03,140 --> 01:01:04,358
그래서 이것이 실제로 어떻게 생겼는지

1163
01:01:04,358 --> 01:01:09,250
우리가이 일을 반복해서 반복한다면,

1164
01:01:09,250 --> 01:01:12,061
그러면 우리는 어느 시점부터 시작합니다.

1165
01:01:12,061 --> 01:01:15,266
결국에는 매번 작은 그래디언트 단계를 거치면서,

1166
01:01:15,266 --> 01:01:19,221
매개 변수가 중심을 향해 원호 모양으로 표시되고,

1167
01:01:19,221 --> 01:01:20,626
이 미니 마의 영역,

1168
01:01:20,626 --> 01:01:22,236
그리고 그것은 당신이 정말로 원하는 것입니다.

1169
01:01:22,236 --> 01:01:24,241
낮은 손실을 원하기 때문입니다.

1170
01:01:24,241 --> 01:01:26,812
그리고 그런데, 티저의 비트로서,

1171
01:01:26,812 --> 01:01:28,139
우리는 이전 슬라이드에서 보았습니다.

1172
01:01:28,139 --> 01:01:30,905
아주 간단한 그래디언트 디센트의이 예제,

1173
01:01:30,905 --> 01:01:32,705
모든 단계에서 우리는 방향으로 나아가고 있습니다.

1174
01:01:32,705 --> 01:01:33,998
그라디언트의

1175
01:01:33,998 --> 01:01:35,997
그러나 실제로, 다음 강의에서,

1176
01:01:35,997 --> 01:01:38,679
우리는 약간 더 매끈한 단계가 있음을 볼 것입니다,

1177
01:01:38,679 --> 01:01:40,885
그들이이 업데이트 규칙이라고 부르는 것,

1178
01:01:40,885 --> 01:01:43,598
약간 애호가를 취할 수있는 곳

1179
01:01:43,598 --> 01:01:46,037
여러 시간 단계에 걸쳐 그라디언트 통합

1180
01:01:46,037 --> 01:01:48,206
그리고 그런 것들은 조금 더 잘 작동하는 경향이 있습니다.

1181
01:01:48,206 --> 01:01:50,836
실제로는 훨씬 더 일반적으로 사용됩니다.

1182
01:01:50,836 --> 01:01:52,513
이 바닐라 그라데이션 하강보다

1183
01:01:52,513 --> 01:01:54,610
실제로 이러한 것들을 훈련 할 때.

1184
01:01:54,610 --> 01:01:55,877
그리고 약간의 미리보기로서,

1185
01:01:55,877 --> 01:01:59,101
우리는이 약간의 애호가 방법 중 일부를 볼 수 있습니다.

1186
01:01:59,101 --> 01:02:01,054
동일한 문제를 최적화합니다.

1187
01:02:01,054 --> 01:02:04,701
다시 검정은이 같은 그라디언트 계산이 될 것입니다.

1188
01:02:04,701 --> 01:02:07,530
그리고 이것들은 그들이 어떤 색인지 잊어 버렸습니다.

1189
01:02:07,530 --> 01:02:08,871
그러나이 두 개의 다른 곡선

1190
01:02:08,871 --> 01:02:11,580
약간 더 까다로운 업데이트 규칙을 사용하고 있습니다.

1191
01:02:11,580 --> 01:02:13,960
그래디언트 정보를 사용하는 방법을 정확하게 결정하는

1192
01:02:13,960 --> 01:02:15,929
우리의 다음 단계를 만들기 위해.

1193
01:02:15,929 --> 01:02:20,451
따라서 이들 중 하나는 기세가있는 그라데이션 강하입니다.

1194
01:02:20,451 --> 01:02:22,835
다른 하나는이 Adam Optimizer입니다.

1195
01:02:22,835 --> 01:02:24,273
자세한 내용은

1196
01:02:24,273 --> 01:02:25,389
나중에 코스에서.

1197
01:02:25,389 --> 01:02:28,300
그러나 아이디어는 우리가이 아주 기본적인
알고리즘을 가지고 있다는 것입니다.

1198
01:02:28,300 --> 01:02:29,795
그라디언트 디센트라는,

1199
01:02:29,795 --> 01:02:31,544
매 단계마다 그라디언트를 사용합니다.

1200
01:02:31,544 --> 01:02:33,424
다음으로 나아갈 위치를 결정하기 위해,

1201
01:02:33,424 --> 01:02:35,874
우리에게 알려주는 다른 업데이트 규칙이 있습니다.

1202
01:02:35,874 --> 01:02:38,559
그 기울기 정보를 얼마나 정확하게 사용하는지.

1203
01:02:38,559 --> 01:02:40,391
그러나 그것은 모두 동일한 기본 알고리즘입니다.

1204
01:02:40,391 --> 01:02:44,058
매 단계마다 내리막 길을 가려고했다.

1205
01:02:50,022 --> 01:02:51,852
하지만 실제로는 조금 더 주름살이 있습니다.

1206
01:02:51,852 --> 01:02:53,212
우리가 얘기해야 할 것.

1207
01:02:53,212 --> 01:02:56,818
그래서 우리가 손실 함수를 정의했음을 기억하십시오.

1208
01:02:56,818 --> 01:02:59,356
우리는 얼마나 나쁜지를 계산하는 손실을 정의했다.

1209
01:02:59,356 --> 01:03:02,237
어떤 단일 훈련 예에서 우리
분류 자 (classifier)

1210
01:03:02,237 --> 01:03:04,536
데이터 세트에 대한 우리의 모든 손실이

1211
01:03:04,536 --> 01:03:06,077
평균 손실이 될거야.

1212
01:03:06,077 --> 01:03:08,314
전체 교육 세트에서

1213
01:03:08,314 --> 01:03:12,572
그러나 실제로이 N은 매우 커질 수 있습니다.

1214
01:03:12,572 --> 01:03:15,274
예를 들어 이미지 넷 데이터 세트를 사용하는 경우,

1215
01:03:15,274 --> 01:03:17,010
우리가 첫 번째 강의에서 이야기 한 내용,

1216
01:03:17,010 --> 01:03:19,199
N은 130 만 명,

1217
01:03:19,199 --> 01:03:21,403
그래서 실제로이 손실을 계산합니다.

1218
01:03:21,403 --> 01:03:23,208
실제로는 매우 비쌀 수있다.

1219
01:03:23,208 --> 01:03:26,081
아마도 수백만 가지 평가 계산이 필요합니다.

1220
01:03:26,081 --> 01:03:28,206
이 함수의

1221
01:03:28,206 --> 01:03:29,821
그래서 그것은 정말로 느릴 수 있습니다.

1222
01:03:29,821 --> 01:03:32,094
실제로 그라디언트는 선형 연산자이므로,

1223
01:03:32,094 --> 01:03:34,109
그라디언트를 실제로 계산하려고 할 때

1224
01:03:34,109 --> 01:03:37,034
이 표현에서 우리는 손실의 기울기

1225
01:03:37,034 --> 01:03:39,504
이제는 손실 그라디언트의 합계입니다.

1226
01:03:39,504 --> 01:03:41,461
각 개별 용어에 대해

1227
01:03:41,461 --> 01:03:43,734
이제 그라디언트를 다시 계산하려면,

1228
01:03:43,734 --> 01:03:45,248
그것은 우리에게 반복을 요구합니다.

1229
01:03:45,248 --> 01:03:46,930
전체 교육 데이터 세트에 대해

1230
01:03:46,930 --> 01:03:48,469
이 모든 N 개의 예제.

1231
01:03:48,469 --> 01:03:50,390
그래서 만약 우리 N이 백만 달러라면,

1232
01:03:50,390 --> 01:03:51,960
이것은 슈퍼 슈퍼 천천히,

1233
01:03:51,960 --> 01:03:54,078
우리는 아주 오랜 시간을 기다려야 할 것입니다.

1234
01:03:54,078 --> 01:03:56,978
우리가 W를 개별적으로 업데이트하기 전에.

1235
01:03:56,978 --> 01:03:58,577
그래서 실제로, 우리는

1236
01:03:58,577 --> 01:04:00,728
확률 적 구배 강하라고 불리는 것,

1237
01:04:00,728 --> 01:04:04,061
손실 및 기울기 계산보다는

1238
01:04:04,061 --> 01:04:05,697
전체 훈련 세트에서,

1239
01:04:05,697 --> 01:04:08,938
대신 매 반복마다 샘플 세트

1240
01:04:08,938 --> 01:04:12,540
minibatch라고 불리는 훈련 예를들 수 있습니다.

1241
01:04:12,540 --> 01:04:14,213
보통 이것은 관습에 따라 2의 힘입니다.

1242
01:04:14,213 --> 01:04:17,705
32, 64, 128은 일반적인 숫자입니다.

1243
01:04:17,705 --> 01:04:19,887
그런 다음이 작은 미니 바를 사용합니다.

1244
01:04:19,887 --> 01:04:22,483
전체 합계의 추정치를 계산하기 위해,

1245
01:04:22,483 --> 01:04:25,047
그리고 진정한 그라데이션의 추정치.

1246
01:04:25,047 --> 01:04:27,703
그리고 이것은 당신이 이것을 볼
수 있기 때문에 확률 적입니다.

1247
01:04:27,703 --> 01:04:31,845
몬테 카를로의 기대치에 대한 추정치 일 수도 있습니다.

1248
01:04:31,845 --> 01:04:33,345
진정한 가치의

1249
01:04:34,716 --> 01:04:37,318
그래서 이것은 우리의 알고리즘을
약간 더 매끈하게 만듭니다.

1250
01:04:37,318 --> 01:04:38,945
하지만 여전히 4 줄 밖에 없습니다.

1251
01:04:38,945 --> 01:04:43,112
이제 데이터가 무작위로 추출됩니다.

1252
01:04:44,291 --> 01:04:46,682
minibatch에서 손실 및 그라디언트를 평가하고,

1253
01:04:46,682 --> 01:04:48,690
이제 매개 변수를 업데이트하십시오.

1254
01:04:48,690 --> 01:04:51,113
이 손실 추정에 기초하여,

1255
01:04:51,113 --> 01:04:53,661
이 기울기의 추정치.

1256
01:04:53,661 --> 01:04:56,769
그리고 다시 약간 더 매끄러운 업데이트 규칙을 보겠습니다.

1257
01:04:56,769 --> 01:04:59,811
여러 가지 그라디언트를 통합하는 방법을 정확히 설명합니다.

1258
01:04:59,811 --> 01:05:02,780
시간이지 나면서,하지만 이것은 기본적인 훈련 알고리즘입니다

1259
01:05:02,780 --> 01:05:04,948
우리가 거의 모든 심층 신경 네트워크에 사용하는

1260
01:05:04,948 --> 01:05:05,948
실제로.

1261
01:05:06,875 --> 01:05:10,235
그래서 우리는 또 다른 대화 형 웹 데모를 가지고 있습니다.

1262
01:05:10,235 --> 01:05:12,625
선형 분류기로 실제로 놀고,

1263
01:05:12,625 --> 01:05:14,970
확률 적 구배 강하를 통해 이러한 것들을 훈련하고,

1264
01:05:14,970 --> 01:05:17,782
그러나 웹 데모가 얼마나 비참한지를 생각해 보면,

1265
01:05:17,782 --> 01:05:20,122
나는 실제로 링크를 열지 않을 것이다.

1266
01:05:20,122 --> 01:05:23,269
대신, 나는이 비디오를 재생할 것입니다.

1267
01:05:23,269 --> 01:05:25,339
[웃음]

1268
01:05:25,339 --> 01:05:26,594
하지만 이걸 확인해 보시길 바랍니다.

1269
01:05:26,594 --> 01:05:27,749
온라인으로 게임을 즐기고,

1270
01:05:27,749 --> 01:05:29,256
실제로 어떤 직감을 구축하는 데 도움이되기 때문에

1271
01:05:29,256 --> 01:05:31,036
선형 분류기에 대해 배우고 훈련하기

1272
01:05:31,036 --> 01:05:32,735
그라데이션 강하를 통해.

1273
01:05:32,735 --> 01:05:34,919
여기 왼쪽에서 볼 수 있습니다.

1274
01:05:34,919 --> 01:05:37,485
우리가 분류하고있는이 문제가 있습니다.

1275
01:05:37,485 --> 01:05:40,146
3 개의 다른 종류,

1276
01:05:40,146 --> 01:05:42,551
우리는이 녹색, 파란색 및 빨간색 점을 가지고 있습니다.

1277
01:05:42,551 --> 01:05:45,753
그것은이 세 가지 수업에서 얻은 우리의 훈련 견본입니다.

1278
01:05:45,753 --> 01:05:48,351
그리고 이제 우리는 결정 경계를 이끌어 냈습니다.

1279
01:05:48,351 --> 01:05:52,068
컬러 백그라운드 영역 인 이러한 클래스의 경우,

1280
01:05:52,068 --> 01:05:54,270
이러한 방향뿐만 아니라,

1281
01:05:54,270 --> 01:05:57,487
학급 점수 인상 방향을 알려줍니다.

1282
01:05:57,487 --> 01:05:59,007
이 세 가지 클래스 각각에 대해.

1283
01:05:59,007 --> 01:06:03,108
그리고 지금 보시다시피, 실제로 가서 놀면

1284
01:06:03,108 --> 01:06:04,814
이 물건을 온라인으로

1285
01:06:04,814 --> 01:06:07,579
당신은 우리가 가서 W를 조정할 수
있다는 것을 볼 수 있습니다.

1286
01:06:07,579 --> 01:06:09,442
Ws 값 변경

1287
01:06:09,442 --> 01:06:12,176
이러한 결정 경계가 회전하게됩니다.

1288
01:06:12,176 --> 01:06:14,189
편향을 변경하면 결정 경계

1289
01:06:14,189 --> 01:06:17,476
회전하지 않고 대신 좌우로 움직입니다.

1290
01:06:17,476 --> 01:06:18,662
또는 위 아래로.

1291
01:06:18,662 --> 01:06:19,989
그러면 실제로 단계를 밟을 수 있습니다.

1292
01:06:19,989 --> 01:06:21,855
이 손실을 업데이트하려고 시도하는

1293
01:06:21,855 --> 01:06:23,984
또는이 슬라이더로 스텝 크기를 변경할 수 있습니다.

1294
01:06:23,984 --> 01:06:26,009
이 버튼을 눌러 실제 작동시킬 수 있습니다.

1295
01:06:26,009 --> 01:06:27,296
이제 큰 단계 크기로,

1296
01:06:27,296 --> 01:06:29,076
우리는 현재 그라디언트 디센트를 실행 중입니다.

1297
01:06:29,076 --> 01:06:30,624
이러한 결정의 경계는

1298
01:06:30,624 --> 01:06:32,874
데이터를 맞추려고합니다.

1299
01:06:34,553 --> 01:06:36,432
이제는 괜찮아요.

1300
01:06:36,432 --> 01:06:39,890
실제 손실 함수를 실시간으로 변경할 수 있습니다.

1301
01:06:39,890 --> 01:06:41,845
이 서로 다른 SVM 공식들 사이

1302
01:06:41,845 --> 01:06:43,567
및 다른 softmax.

1303
01:06:43,567 --> 01:06:44,754
그리고 당신이 볼 때 볼 수 있듯이

1304
01:06:44,754 --> 01:06:47,695
손실 함수의 이들 상이한 공식들 사이에서,

1305
01:06:47,695 --> 01:06:50,219
그것은 일반적으로 똑같은 일을합니다.

1306
01:06:50,219 --> 01:06:52,340
우리의 의사 결정 영역은 대부분 같은 장소에 있습니다.

1307
01:06:52,340 --> 01:06:54,945
그러나 그들이 어떻게 서로에 관해서는 결국

1308
01:06:54,945 --> 01:06:56,263
정확하게 상충되는 점은 무엇입니까?

1309
01:06:56,263 --> 01:06:59,139
이 다른 것들을 범주화하는 것

1310
01:06:59,139 --> 01:07:00,743
조금 바뀐다.

1311
01:07:00,743 --> 01:07:02,137
그래서 나는 정말로 당신이 온라인에 가기를 권장합니다.

1312
01:07:02,137 --> 01:07:03,877
이 일로 약간의 직감을 얻으려고 노력한다.

1313
01:07:03,877 --> 01:07:05,699
실제로 어떻게 생겼는지

1314
01:07:05,699 --> 01:07:07,428
이러한 선형 분류자를 훈련 시키려고

1315
01:07:07,428 --> 01:07:09,178
그라데이션 강하를 통해.

1316
01:07:12,343 --> 01:07:16,245
이제는 제쳐두고, 저는 다른 생각에
대해 이야기하고 싶습니다.

1317
01:07:16,245 --> 01:07:18,102
그것은 이미지 기능의 것입니다.

1318
01:07:18,102 --> 01:07:20,668
지금까지 선형 분류기에 대해 이야기했습니다.

1319
01:07:20,668 --> 01:07:23,032
그냥 우리의 원시 이미지 픽셀을 복용하는 것입니다

1320
01:07:23,032 --> 01:07:25,184
원시 픽셀 자체를 공급하는 단계

1321
01:07:25,184 --> 01:07:27,434
우리의 선형 분류 자로.

1322
01:07:28,464 --> 01:07:31,075
그러나 마지막 강연에서 우리가 이야기 한 것처럼,

1323
01:07:31,075 --> 01:07:33,343
이것은 아마도 그렇게 대단한 것이 아니며,

1324
01:07:33,343 --> 01:07:36,233
멀티 - 모달과 같은 것들 때문에.

1325
01:07:36,233 --> 01:07:39,368
따라서 실제로는 원본 픽셀 값을 실제로 공급합니다.

1326
01:07:39,368 --> 01:07:42,789
선형 분류기로 변환하는 것이 잘
작동하지 않는 경향이 있습니다.

1327
01:07:42,789 --> 01:07:45,742
그래서 그것은 지배 이전에 실제로 일반적이었습니다.

1328
01:07:45,742 --> 01:07:47,145
깊은 신경 네트워크,

1329
01:07:47,145 --> 01:07:49,592
대신에이 2 단계 접근법을 사용하는 것이 었습니다.

1330
01:07:49,592 --> 01:07:51,105
먼저, 당신은 당신의 이미지를 찍을 것입니다.

1331
01:07:51,105 --> 01:07:53,916
다양한 특징 표현을 계산할 수 있습니다.

1332
01:07:53,916 --> 01:07:55,886
어쩌면 계산중인 이미지의

1333
01:07:55,886 --> 01:07:59,219
외관과 관련된 여러 종류의 양

1334
01:07:59,219 --> 01:08:00,179
이미지의

1335
01:08:00,179 --> 01:08:02,117
이들 서로 다른 특징 벡터들을 연결

1336
01:08:02,117 --> 01:08:05,137
이미지의 일부 기능 표현을 제공하려면,

1337
01:08:05,137 --> 01:08:07,019
이제 이미지의이 특징 표현

1338
01:08:07,019 --> 01:08:08,836
선형 분류기에 공급 될 것이고,

1339
01:08:08,836 --> 01:08:10,683
원시 픽셀 자체를 먹이기보다는

1340
01:08:10,683 --> 01:08:12,902
분류기에 넣습니다.

1341
01:08:12,902 --> 01:08:15,671
그리고 여기서의 동기는,

1342
01:08:15,671 --> 01:08:17,700
그래서 왼쪽에 훈련 데이터 세트가 있다고 상상해보십시오.

1343
01:08:17,701 --> 01:08:20,193
이 빨간 점들과 중간에 빨간 점들

1344
01:08:20,193 --> 01:08:22,244
그리고 그 주변의 푸른 점.

1345
01:08:22,244 --> 01:08:23,693
그리고 이러한 종류의 데이터 세트의 경우,

1346
01:08:23,693 --> 01:08:26,440
선형 결정 경계를 그릴 수있는 방법이 없습니다.

1347
01:08:26,441 --> 01:08:29,157
빨간색 점과 파란색 점을 구분합니다.

1348
01:08:29,157 --> 01:08:32,155
그리고 우리는 마지막 강의에서 이것에
대한 더 많은 예를 보았습니다.

1349
01:08:32,156 --> 01:08:34,459
그러나 영리한 피처 변환을 사용한다면,

1350
01:08:34,459 --> 01:08:36,660
이 경우에는 극좌표로 변환,

1351
01:08:36,660 --> 01:08:39,079
이제 우리가 피쳐 변환을 한 후에,

1352
01:08:39,079 --> 01:08:42,361
이 복잡한 데이터 세트는 실제로

1353
01:08:42,361 --> 01:08:43,677
선형으로 분리 가능한,

1354
01:08:43,677 --> 01:08:45,160
실제로 정확하게 분류 될 수있다.

1355
01:08:45,160 --> 01:08:46,858
선형 분류기에 의해

1356
01:08:46,858 --> 01:08:48,435
그리고 여기있는 모든 트릭은 이제 알아내는 것입니다.

1357
01:08:48,436 --> 01:08:51,034
올바른 피쳐 변환은 무엇입니까?

1358
01:08:51,034 --> 01:08:53,197
그것은 올바른 양의 계산입니다.

1359
01:08:53,197 --> 01:08:55,129
당신이 걱정하는 문제에 대해서.

1360
01:08:55,129 --> 01:08:58,017
따라서 이미지의 경우 픽셀을 변환 할 수 있습니다.

1361
01:08:58,017 --> 01:08:59,751
극좌표로, 이해가 안 돼,

1362
01:08:59,751 --> 01:09:01,505
그러나 실제로 당신은 적어 내려고 노력할 수 있습니다.

1363
01:09:01,505 --> 01:09:03,084
이미지의 특징 표현

1364
01:09:03,085 --> 01:09:04,749
그게 말이 되겠지,

1365
01:09:04,749 --> 01:09:06,458
실제로 너를 도울지도 모른다.

1366
01:09:06,458 --> 01:09:08,385
원시 픽셀을 넣는 것보다 효과적 일 수 있습니다.

1367
01:09:08,385 --> 01:09:10,391
분류기에 넣습니다.

1368
01:09:10,392 --> 01:09:13,157
그래서 이런 종류의 피쳐 표현의 한 예

1369
01:09:13,157 --> 01:09:16,343
그것은 매우 간단합니다. 색상 히스토그램의 아이디어입니다.

1370
01:09:16,343 --> 01:09:18,526
그래서 당신은 아마 각 픽셀을 가져갈 것입니다,

1371
01:09:18,526 --> 01:09:21,188
당신은이 색조 스펙트럼을 취할 것입니다.

1372
01:09:21,188 --> 01:09:23,985
버킷으로 나눈 다음 모든 픽셀에 대해

1373
01:09:23,986 --> 01:09:26,425
당신은 그 색 버킷 중 하나에 그것을 매핑 할 것입니다.

1374
01:09:26,425 --> 01:09:28,535
얼마나 많은 픽셀을 카운트하는지

1375
01:09:28,536 --> 01:09:31,162
이 각각의 버킷으로 떨어지십시오.

1376
01:09:31,162 --> 01:09:34,638
따라서 이미지에 어떤 색상이 있는지
전 세계적으로 알 수 있습니다.

1377
01:09:34,639 --> 01:09:36,278
어쩌면이 개구리의 예가

1378
01:09:36,278 --> 01:09:37,500
이 특징 벡터는 우리에게

1379
01:09:37,500 --> 01:09:39,076
녹색 물건이 많이 있어요.

1380
01:09:39,076 --> 01:09:40,938
어쩌면 자주색이나 붉은 색이 아닌 것도있을 것입니다.

1381
01:09:40,938 --> 01:09:43,043
그리고 이것은 여러분이 볼 수있는 단순한 특징 벡터입니다

1382
01:09:43,043 --> 01:09:44,043
실제로.

1383
01:09:45,108 --> 01:09:47,720
우리가 본 또 다른 공통 특징 벡터

1384
01:09:47,720 --> 01:09:49,430
신경 네트워크의 부상 전에,

1385
01:09:49,431 --> 01:09:50,983
또는 신경 네트워크의 지배 이전에

1386
01:09:50,983 --> 01:09:53,219
지향 그라디언트의 히스토그램이었습니다.

1387
01:09:53,220 --> 01:09:54,952
첫 번째 강의에서 기억하십시오.

1388
01:09:54,952 --> 01:09:57,829
Hubel과 Wiesel은 이러한
지향성 가장자리를 발견했습니다.

1389
01:09:57,829 --> 01:10:00,046
인간 시각 시스템에서 정말로 중요합니다.

1390
01:10:00,046 --> 01:10:02,209
지향성 그라디언트의 히스토그램

1391
01:10:02,209 --> 01:10:04,690
피쳐 표현은 캡쳐를 시도한다.

1392
01:10:04,690 --> 01:10:07,680
똑같은 직감과 현지 오리엔테이션 측정

1393
01:10:07,680 --> 01:10:09,974
이미지 가장자리.

1394
01:10:09,974 --> 01:10:11,280
그래서이 일이 무엇을 할 것인지,

1395
01:10:11,280 --> 01:10:13,286
우리의 이미지를 받아 그것을 나눕니다.

1396
01:10:13,286 --> 01:10:16,354
이 작은 8x8 픽셀 영역으로

1397
01:10:16,354 --> 01:10:19,142
그리고 나서, 각각의 8 × 8 픽셀 영역 내에서,

1398
01:10:19,142 --> 01:10:22,268
지배적 인 에지 방향을 계산합니다.

1399
01:10:22,268 --> 01:10:24,921
각 픽셀의 에지 방향

1400
01:10:24,921 --> 01:10:27,776
여러 버킷에 넣은 다음 각 영역 내에서

1401
01:10:27,776 --> 01:10:31,857
이들 서로 다른 에지 방향에
대해 히스토그램을 계산하십시오.

1402
01:10:31,857 --> 01:10:33,417
이제 전체 기능 벡터

1403
01:10:33,417 --> 01:10:35,797
이러한 서로 다른 버킷 히스토그램이됩니다.

1404
01:10:35,797 --> 01:10:37,382
가장자리 방위의

1405
01:10:37,382 --> 01:10:39,121
8 개 지역마다 8 개 지역에 걸쳐

1406
01:10:39,121 --> 01:10:40,204
이미지에서.

1407
01:10:41,660 --> 01:10:43,450
그래서 이것은 어떤면에서는 이중적인 것입니다.

1408
01:10:43,450 --> 01:10:47,029
이전에 본 색상 히스토그램 분류기로

1409
01:10:47,029 --> 01:10:49,704
그래서 색 막대 그래프는 전 세계적으로 어떤 색

1410
01:10:49,704 --> 01:10:51,082
이미지에 존재한다.

1411
01:10:51,082 --> 01:10:53,751
이것은 전반적으로 어떤 유형의 에지 정보

1412
01:10:53,751 --> 01:10:55,305
이미지에 존재합니다.

1413
01:10:55,305 --> 01:10:57,991
그리고 심지어 이미지의 다른 부분에 국한되어,

1414
01:10:57,991 --> 01:11:01,191
다른 영역에 어떤 유형의 모서리가 있는지.

1415
01:11:01,191 --> 01:11:03,546
어쩌면 왼쪽에있는이 개구리의 경우,

1416
01:11:03,546 --> 01:11:04,938
당신은 그가 잎에 앉아있는 것을 볼 수 있습니다,

1417
01:11:04,938 --> 01:11:07,561
이 나뭇잎들은이 지배적 인 대각선 모서리를 가지며,

1418
01:11:07,561 --> 01:11:10,480
방향 그라디언트의 히스토그램을 시각화하면

1419
01:11:10,480 --> 01:11:12,833
기능을 사용하면이 지역에서

1420
01:11:12,833 --> 01:11:14,667
우리는 대각선 가장자리가 많습니다.

1421
01:11:14,667 --> 01:11:16,227
방향성 그라디언트의 히스토그램

1422
01:11:16,227 --> 01:11:19,340
특징 표현의 캡쳐.

1423
01:11:19,340 --> 01:11:21,509
그래서 이것은 매우 일반적인 특징 표현이었습니다.

1424
01:11:21,509 --> 01:11:23,535
물체 인식에 많이 사용되었습니다.

1425
01:11:23,535 --> 01:11:25,702
사실 너무 오래 전에.

1426
01:11:26,573 --> 01:11:29,789
거기에서 볼 수있는 또 다른 특징 표현

1427
01:11:29,789 --> 01:11:32,810
단어의 가방이 아이디어입니다.

1428
01:11:32,810 --> 01:11:34,202
그래서 이것은 영감을 얻고 있습니다.

1429
01:11:34,202 --> 01:11:36,355
자연 언어 처리에서.

1430
01:11:36,355 --> 01:11:38,220
그래서 단락이 있다면,

1431
01:11:38,220 --> 01:11:40,799
그런 다음 단락을 나타낼 수있는 방법

1432
01:11:40,799 --> 01:11:43,398
특징 벡터에 의해 발생 횟수를 세고있다.

1433
01:11:43,398 --> 01:11:45,732
그 단락의 다른 단어들.

1434
01:11:45,732 --> 01:11:47,666
그래서 우리는 그 직감을 받아 적용하고 싶습니다.

1435
01:11:47,666 --> 01:11:49,664
어떤 식 으로든 이미지에.

1436
01:11:49,664 --> 01:11:51,708
그러나 문제는 실제로는 단순한 것이 아니라,

1437
01:11:51,708 --> 01:11:54,288
이미지에 대한 단어의 직접적인 유추,

1438
01:11:54,288 --> 01:11:56,632
그래서 우리는 우리 자신의 어휘를 정의 할 필요가있다.

1439
01:11:56,632 --> 01:11:57,965
시각적 단어의

1440
01:11:58,880 --> 01:12:01,106
그래서 우리는이 2 단계 접근법을 취합니다.

1441
01:12:01,106 --> 01:12:04,318
먼저 우리는 많은 이미지를 얻을 것입니다.

1442
01:12:04,318 --> 01:12:06,455
작은 무작위 농작물을 한꺼번에 채취하다.

1443
01:12:06,455 --> 01:12:07,995
그 이미지들로부터

1444
01:12:07,995 --> 01:12:09,723
K와 같은 것을 사용하는 것을 의미합니다.

1445
01:12:09,723 --> 01:12:12,820
이 다른 클러스터 센터를 생각해 내야한다.

1446
01:12:12,820 --> 01:12:15,189
어쩌면 다른 유형을 나타내는 것입니다.

1447
01:12:15,189 --> 01:12:17,139
이미지의 시각적 단어.

1448
01:12:17,139 --> 01:12:19,341
이 예제를 여기 오른쪽에서 보면,

1449
01:12:19,341 --> 01:12:21,160
이것은 클러스터링의 실제 예입니다.

1450
01:12:21,160 --> 01:12:23,335
이미지에서 실제로 다른 이미지 패치,

1451
01:12:23,335 --> 01:12:25,627
이 클러스터링 단계 후에,

1452
01:12:25,627 --> 01:12:28,450
우리의 시각적 인 단어는이 다른 색을 포착합니다.

1453
01:12:28,450 --> 01:12:30,552
빨간색과 파란색과 노란색처럼,

1454
01:12:30,552 --> 01:12:32,553
이러한 다양한 유형의 지향 에지

1455
01:12:32,553 --> 01:12:34,556
다른 방향으로,

1456
01:12:34,556 --> 01:12:36,482
우리가보기 시작한 것이 흥미 롭습니다.

1457
01:12:36,482 --> 01:12:38,909
이러한 지향 에지는 데이터에서 나옵니다.

1458
01:12:38,909 --> 01:12:40,480
데이터 중심 방식으로

1459
01:12:40,480 --> 01:12:43,097
그리고 이제 우리가 이러한 일련의
시각적 단어들을 얻게되면,

1460
01:12:43,097 --> 01:12:44,291
또한 코드북이라고 불리는,

1461
01:12:44,291 --> 01:12:47,249
그러면 우리는 우리의 이미지를 인 코드 할 수 있습니다.

1462
01:12:47,249 --> 01:12:48,862
이러한 시각적 단어들 각각에 대해,

1463
01:12:48,862 --> 01:12:52,468
이 시각적 단어는 이미지에서 얼마나 발생합니까?

1464
01:12:52,468 --> 01:12:54,082
그리고 이제 이것은 우리에게 다시금,

1465
01:12:54,082 --> 01:12:55,463
약간 다른 정보

1466
01:12:55,463 --> 01:12:59,427
이 이미지의 시각적 모양은 무엇입니까?

1467
01:12:59,427 --> 01:13:02,124
실제로 이것은 일종의 특징 표현입니다.

1468
01:13:02,124 --> 01:13:04,638
황비홍 (Fei-Fei)은 대학원생이었을 때 일했고,

1469
01:13:04,638 --> 01:13:07,555
그래서 이것은 당신이 실제로 보았던 무언가입니다.

1470
01:13:07,555 --> 01:13:08,972
그리 오래 전 아니에요.

1471
01:13:10,783 --> 01:13:13,033
그래서 약간의 티저로,

1472
01:13:14,951 --> 01:13:16,837
이 모든 것을 다시 묶어서,

1473
01:13:16,837 --> 01:13:19,743
이 이미지 분류 파이프 라인

1474
01:13:19,743 --> 01:13:20,886
같이 보일지도 모른다.

1475
01:13:20,886 --> 01:13:22,725
어쩌면 5 년에서 10 년 전쯤에

1476
01:13:22,725 --> 01:13:24,421
당신이 당신의 이미지를 찍을 것이고,

1477
01:13:24,421 --> 01:13:26,677
그런 다음 이러한 다양한 특징 표현을 계산할 수 있습니다.

1478
01:13:26,677 --> 01:13:28,809
너의 심상의, 낱말의 부대 같이 것,

1479
01:13:28,809 --> 01:13:31,173
또는 방향 그라디언트의 히스토그램,

1480
01:13:31,173 --> 01:13:33,381
모든 기능을 함께 연결하고,

1481
01:13:33,381 --> 01:13:35,519
이러한 피쳐 추출기로 피드

1482
01:13:35,519 --> 01:13:38,590
일부 선형 분류기로

1483
01:13:38,590 --> 01:13:39,461
나는 조금 단순화하고있다.

1484
01:13:39,461 --> 01:13:42,018
파이프 라인은 그보다 조금 더 복잡했습니다.

1485
01:13:42,018 --> 01:13:44,576
그러나 이것은 일반적인 직감입니다.

1486
01:13:44,576 --> 01:13:48,158
그리고 그 아이디어는 당신이 추출한 후였습니다.

1487
01:13:48,158 --> 01:13:50,196
이러한 기능들,이 피쳐 추출기

1488
01:13:50,196 --> 01:13:52,326
업데이트되지 않는 고정 된 블록이 될 것입니다.

1489
01:13:52,326 --> 01:13:53,563
훈련 도중.

1490
01:13:53,563 --> 01:13:54,396
그리고 훈련 도중,

1491
01:13:54,396 --> 01:13:55,933
선형 분류 자만 업데이트하면됩니다.

1492
01:13:55,933 --> 01:13:57,907
기능 상단에서 작업하는 경우

1493
01:13:57,907 --> 01:13:59,972
그리고 실제로, 저는 일단 우리가 움직이면

1494
01:13:59,972 --> 01:14:01,774
길쌈 신경망 (convolutional neural networks)

1495
01:14:01,774 --> 01:14:03,140
이 깊은 신경 네트워크,

1496
01:14:03,140 --> 01:14:06,486
실제로 다른 것을 보지 못합니다.

1497
01:14:06,486 --> 01:14:08,651
유일한 차이점은 쓰기보다는

1498
01:14:08,651 --> 01:14:10,322
미리 기능,

1499
01:14:10,322 --> 01:14:12,687
우리는 데이터에서 직접 기능을 배우려고합니다.

1500
01:14:12,687 --> 01:14:15,916
그래서 우리는 원시 픽셀을 가져다가 먹일 것입니다.

1501
01:14:15,916 --> 01:14:17,530
이것을 컨볼 루션 네트워크에 연결함으로써,

1502
01:14:17,530 --> 01:14:19,687
이것은 여러 다른 레이어를 통해 컴퓨팅을 끝낼 것입니다.

1503
01:14:19,687 --> 01:14:21,488
어떤 유형의 피쳐 표현

1504
01:14:21,488 --> 01:14:23,459
데이터에 의해 주도되고 실제로 훈련하게됩니다.

1505
01:14:23,459 --> 01:14:26,120
이 전체 네트워크에 대한이 전체 가중치,

1506
01:14:26,120 --> 01:14:27,954
선형 분류기의 가중치가 아닌

1507
01:14:27,954 --> 01:14:28,787
위에.

1508
01:14:30,329 --> 01:14:32,970
그럼, 다음에 우리는이 아이디어에 뛰어들 것입니다.

1509
01:14:32,970 --> 01:14:36,131
좀 더 자세히 살펴보면, 우리는 몇
가지 신경망을 소개 할 것이며,

